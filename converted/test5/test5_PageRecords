{"id": "ade34edc-1764-5ca3-93f8-7edeea2830e9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1, "real_page_number": null, "text": "Table of Contents\n1\nProgramming Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2\nPointers and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2.1\nProgram Execution and the Machine Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2.2\nReferences and Reference Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2.3\nPointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3\nAddress Space and Dynamic Memory\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.4\nCompound Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.4.1\nPrimitive Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.4.2\nStructs and Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.4.3\nConstructors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.4.4\nMember Variable Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.5\nOperator Overloading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.6\nFunction Objects and Comparators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.7\nScope and Namespaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.7.1\nScope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.7.2\nNamespaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n1.8\nCommon C++ Keywords . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.8.1\nThe Const Keyword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.8.2\nThe Constexpr Keyword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n1.8.3\nThe Static Keyword\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n1.8.4\nThe Mutable Keyword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n1.8.5\nThe Explicit Keyword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21", "word_count": 1398, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "65d9be95-40ad-5ca3-95d4-d869dcd85549", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 2, "real_page_number": null, "text": "ii\n1.9\nTemplates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n1.9.1\nTemplates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n1.9.2\nVariadic Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n1.10\nType Aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n1.11\nAutomatic Type Deduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n1.11.1\nThe Auto Keyword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n1.11.2\nDeducing Function Return Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n1.12\nPreﬁx and Postﬁx Incrementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1.13\nLoops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1.13.1\nFor and While Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1.13.2\nRange-Based For Loop\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.13.3\nDo While Loop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.13.4\nThe Break Keyword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.13.5\nThe Continue Keyword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n1.14\nTernary Operator\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n1.15\nInheritance and Polymorphism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.15.1\nInheritance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.15.2\nPolymorphism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n1.16\nCasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n1.16.1\nStatic Cast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n1.16.2\nDynamic Cast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n1.16.3\nReinterpret Cast\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n1.16.4\nConst Cast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n1.16.5\nC-Style Casts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n1.17\nException Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2\nFile and Stream I/O\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n2.1\nStandard I/O Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n2.2\nInput and Output Redirection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n2.3\nReading Input in a Loop\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n2.4\nStream Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.4.1\nThe Extraction Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.4.2\nGetline\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.4.3\nIgnore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n2.4.4\nSummary of Stream Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n2.5\nStream Buﬀers and Flushing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n2.6\nC and C++ Stream Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n2.7\nStringstreams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n2.8\nFile Streams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n2.9\nReading Input Using Polymorphism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n3\nCommand Line Parsing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n3.1\nandargc argv\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n3.2\nSwitch Statements and Enumerated Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n3.2.1\nSwitch Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n3.2.2\nEnums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n3.3\nGetopt Long . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n3.3.1\nCommand Line Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n3.3.2\nUsing Getopt Long . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n3.4\nBoost Program Options\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n3.4.1\nUsing Boost Program Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n3.4.2\nSwitch Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n3.4.3\nNotiﬁers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.4.4\nRequired Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n3.4.5\nDefault and Implicit Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n3.4.6\nMultitoken and Composing Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n3.4.7\nPositional Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79", "word_count": 3431, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e067160e-ed79-51f9-9f67-2ed3d51d573f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 3, "real_page_number": null, "text": "iii\n4\nComplexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n4.1\nAsymptotic Runtime\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n4.2\nComplexity Classes and Big-O Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n4.3\nMeasuring Complexity by Counting Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n4.4\nLogarithmic Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n4.4.1\nAnalyzing Logarithmic Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n4.4.2\nDoes the Base of a Logarithm Matter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.4.3\nLogarithmic and Power Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n4.5\nLoop Dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n4.6\nBig-O, Big-Θ, and Big-Ω . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n4.7\nComplexity Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n4.8\nAdditional Rules for Determining Complexity\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n4.9\nSpace Complexity and Auxiliary Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n5\nRecursion and Recurrence Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n5.1\nRecursion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n5.2\nThe Program Stack\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n5.3\nTail Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n5.4\nStack Frames and Space Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n5.5\nIdentifying Recurrence Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n5.6\nThe Iterative Substitution Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n5.7\nThe Master Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n5.7.1\nApplying the Master Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n5.7.2\nDeriving the Master Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n5.7.3\nThe Extended Master Theorem for Polylogarithmic Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n5.8\nComplexities of Common Recurrence Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n5.9\nAnalysis of Recursive Algorithms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n6\nFixed-Size Arrays and Array-Based Containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n6.1\nIntroduction to Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n6.2\nArray Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n6.2.1\nC-Style Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n6.2.2\nThe STL Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n6.2.3\nCommon Oﬀ-By-One Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n6.3\nStoring Multidimensional Data in Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n6.4\nHeap-Allocated Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n6.5\nCopying with Pointers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n6.6\nSequential and Random Access\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n6.7\nData Storage and Retrieval\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n6.8\nImplementing an Array-Based Container\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n6.8.1\nA Blueprint of the Custom Array-Based Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n6.8.2\nThe Big Three . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n6.8.3\nImplementing the Destructor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n6.8.4\nImplementing the Copy Constructor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n6.8.5\nImplementing the Overloaded Assignment Operator and the Copy-Swap Method . . . . . . . . . . . . . 161\n6.8.6\nImplementing the Subscript Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n6.8.7\nImplementing Insert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n6.9\nLvalues, Rvalues, and Move Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n6.9.1\nLvalues and Rvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n6.9.2\nImplementing the Move Constructor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\n6.9.3\nImplementing the Overloaded Move Assignment Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n6.9.4\nReturn Value Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n6.9.5\nPerfect Forwarding and Forwarding References\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172", "word_count": 2855, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d6fb5e94-152f-51d0-8b0d-ed5c7cc73af3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 4, "real_page_number": null, "text": "iv\n7\nVectors: Dynamically Resizable Arrays in C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n7.1\nThe STL Vector Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n7.2\nInserting and Removing Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n7.2.1\nPush Back\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n7.2.2\nPop Back . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n7.2.3\nEmplace Back . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n7.2.4\nInserting and Erasing From Any Position . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n7.3\nResize and Reserve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n7.3.1\nThe Costs of Reallocation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n7.3.2\nReserving a Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n7.3.3\nResizing a Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n7.3.4\nAccessing a Vector’s Underlying Data Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\n7.4\nStoring Multidimensional Data in Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\n7.5\nVector Performance and Memory Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n7.6\nSummary of Vector Complexities\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n8\nLinked Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n8.1\nSingly- and Doubly-Linked Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n8.2\nNode Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n8.3\nNode Deletion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n8.4\nReversing a Linked List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n8.4.1\nA Naïve Solution For Reversing a Linked List\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n8.4.2\nAn Optimized Iterative Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n8.4.3\nAn Optimized Recursive Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n8.5\nTechniques for Solving Linked List Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\n8.6\nThe STL List Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n8.6.1\nstd::list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n8.6.2\nstd::forward_list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\n8.7\nSummary of List Complexities\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\n9\nStacks and Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n9.1\nIntroduction to Stacks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n9.1.1\nImplementing a Stack Using an Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240\n9.1.2\nImplementing a Stack Using a Linked List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n9.2\nThe STL Stack Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n9.3\nIntroduction to Queues\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n9.3.1\nImplementing a Queue Using an Array (Circular Buﬀer) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n9.3.2\nImplementing a Queue Using a Linked List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\n9.4\nThe STL Queue Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n9.5\nSolving Problems Using Stacks and Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n9.5.1\nImplementing a Queue Using Two Stacks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n9.5.2\nSorting a Stack\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n9.5.3\nSorting a Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n9.5.4\nEvaluating Reverse Polish Notation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n9.6\nThe STL Deque Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\n9.7\nContainer Adaptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260\n10\nPriority Queues and Heaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\n10.1\nIntroduction to Priority Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\n10.2\nList and Sequence Container Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n10.2.1\nArray of Linked Lists Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n10.2.2\nUnordered Sequence Container Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n10.2.3\nSorted Sequence Container Implementation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n10.3\nBinary Heaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n10.3.1\nHeap Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n10.3.2\nFix Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n10.3.3\nFix Down . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n10.3.4\nInserting and Removing Elements\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283", "word_count": 3167, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "85012e54-9bd1-5b3d-ac99-c9de0a389b9f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 5, "real_page_number": null, "text": "v\n10.4\nHeapify . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n10.4.1\nTop-Down and Bottom-Up Heapify . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n10.4.2\nSTL Heapify . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\n10.4.3\nSummary of Binary Heaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291\n10.5\nPairing Heaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291\n10.5.1\nPairing Heap Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291\n10.5.2\nMeld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n10.5.3\nInserting and Removing Elements\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\n10.5.4\nImplementing the Pairing Heap Copy Constructor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\n10.5.5\nImplementing the Pairing Heap Destructor\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n10.5.6\nUpdating Priorities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\n10.5.7\nIncreasing the Priority of an Existing Element . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\n10.6\nThe STL Priority Queue Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\n10.7\nSolving Problems Using Priority Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\n10.7.1\nk Largest Elements in an Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\n10.7.2\nStreaming Median Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\n10.7.3\nMerging Sorted Arrays\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\n11\nIterators and the Standard Template Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\n11.1\nThe Standard Template Library (STL) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\n11.2\nPairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\n11.3\nTuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n11.4\nIterators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328\n11.4.1\nIterator Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328\n11.4.2\nIterator Categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330\n11.5\nIterator Adaptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\n11.5.1\nReverse Iterators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\n11.5.2\nStream Iterators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\n11.5.3\nInsert Iterators\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\n11.6\nAuxiliary Iterator Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\n11.6.1\nstd::advance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\n11.6.2\nstd::distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\n11.6.3\nstd::next and std::prev . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\n11.7\nPredicates in the STL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n11.8\nStandard Math Functions and Numeric Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n11.8.1\nThe Math Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n11.8.2\nSetting Precision for Floating Point Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n11.8.3\nNumeric Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n11.8.4\nstd::iota . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n11.8.5\nstd::accumulate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n11.9\nThe Algorithm Library\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\n11.9.1\nIntroduction to the Algorithm Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\n11.9.2\nMinimum and Maximum Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n11.9.3\nSorting Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\n11.9.4\nFind Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n11.9.5\nRemoval Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\n11.9.6\nReplacement Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\n11.9.7\nTransformation Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\n11.9.8\nCopy Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\n11.9.9\nReversal Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346\n11.9.10\nnth Element\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346\n11.9.11\nLower and Upper Bound Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346\n11.9.12\nOther Algorithms in the STL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n11.10\nLambdas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n11.10.1\nAnatomy of a Lambda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n11.10.2\nLambdas and the STL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\n11.10.3\nGeneric Lambdas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\n11.11\nThe Functional Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\n11.11.1\nstd::function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\n11.11.2\nBinding Function Arguments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\n11.11.3\nCallback Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355", "word_count": 3498, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7e9de93b-0647-5510-92a9-c15c09dc99e6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 6, "real_page_number": null, "text": "vi\n11.12\nRandom Number Generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\n11.12.1\nrand . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\n11.12.2\nThe Random Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\n11.13\nAdditional Features in C++17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360\n11.13.1\nStructured Bindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360\n11.13.2\nstd::optional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\n11.13.3\nstd::variant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n11.13.4\nstd::visit and the Visitor Pattern\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\n11.13.5\nstd::any . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365\n12\nAmortization and Amortized Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\n12.1\nAmortized Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\n12.2\nAggregate Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376\n12.3\nThe Accounting Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378\n12.4\nThe Potential Method\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\n12.5\nAmortized Analysis: Implementing a Queue with Two Stacks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\n12.5.1\nAggregate Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n12.5.2\nThe Accounting Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n12.5.3\nThe Potential Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n12.6\nAmortized Analysis: Binary Counter\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\n12.6.1\nAggregate Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\n12.6.2\nThe Accounting Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\n12.6.3\nThe Potential Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n12.7\nAmortized Analysis: Pairing Heap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\n12.8\nAmortized vs. Average-Case Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\n13\nSets and Union-Find . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389\n13.1\nSets and Set Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389\n13.1.1\nSet Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389\n13.1.2\nSet Implementation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389\n13.1.3\nSet Operations in the STL\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n13.2\nUnion-Find and Path Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394\n13.2.1\nDisjoint Sets and the Union-Find Data Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394\n13.2.2\nPath Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396\n13.3\nImplementing a Union-Find Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397\n13.4\nUnion-by-Size and Union-by-Rank\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\n13.4.1\nUnion-by-Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\n13.4.2\nUnion-by-Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403\n13.4.3\nUnion-Find Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404\n14\nSorting Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n14.1\nTypes of Sorting Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n14.2\nBubble Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412\n14.3\nSelection Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413\n14.4\nInsertion Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414\n14.5\nHeapsort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n14.6\nQuicksort\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418\n14.7\nMergesort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\n14.8\nAnalysis of Comparison Sorts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n14.8.1\nPerformance of Elementary and Advanced Sorts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n14.8.2\nCan Comparison Sorts Do Better Than Θ(n log n) Time? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n14.8.3\nIntrosort and STL Sorting Implementations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430\n14.9\nCounting Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431\n14.10\nRadix Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432\n14.11\nIndex Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433", "word_count": 3105, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "67406d90-a6f3-5c62-820d-7614fa9c0cc8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 7, "real_page_number": null, "text": "vii\n15\nBinary Search and Additional Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n15.1\nOrdered and Sorted Containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n15.2\nBinary Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\n15.3\nMoore’s Voting Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\n15.4\nDutch National Flag Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454\n15.5\nTwo Pointer Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n15.6\nSliding Window Technique\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\n15.7\nMonotonic Stacks and Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463\n15.8\nMedian Finding Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473\n15.8.1\nQuickselect\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473\n15.8.2\nMedian of Medians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\n16\nStrings and Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485\n16.1\nC Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485\n16.2\nC++ Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488\n16.2.1\nString Implementations and Short String Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488\n16.2.2\nString Library Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489\n16.2.3\nstd::string_view . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\n16.2.4\nAdditional String Formatting Methods\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495\n16.3\nLexicographical String Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495\n16.4\nSTL Sequence Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496\n16.4.1\nstd::equal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496\n16.4.2\nstd::unique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496\n16.5\nBrute Force String Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497\n16.6\nRabin-Karp String Searching\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498\n16.6.1\nRabin Fingerprinting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498\n16.6.2\nImplementing Rabin-Karp String Search\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502\n16.6.3\nRabin-Karp Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504\n16.7\nKnuth-Morris-Pratt (KMP) String Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505\n16.7.1\nPreﬁx Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505\n16.7.2\nImplementing Knuth-Morris-Pratt String Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507\n16.7.3\nKnuth-Morris-Pratt Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514\n17\nHash Tables and Collision Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521\n17.1\nThe Dictionary ADT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521\n17.2\nIntroduction to Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\n17.2.1\nHash Tables\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\n17.2.2\nHash and Compression Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523\n17.2.3\nProperties of Hash and Compression Functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525\n17.2.4\nAddressing Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525\n17.3\nCollision Resolution Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526\n17.3.1\nSeparate Chaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526\n17.3.2\nLinear Probing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527\n17.3.3\nQuadratic Probing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533\n17.3.4\nDouble Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537\n17.4\nLoad Factor and Dynamic Hashing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539\n17.4.1\nLoad Factor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539\n17.4.2\nDynamic Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540\n17.4.3\nSummary of Collision Resolution Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541\n17.5\nThe STL Unordered Map Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542\n17.6\nThe STL Unordered Set Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547\n17.7\nThe STL Unordered Multimap and Unordered Multiset Containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549\n17.8\nSTL Hashing and Composite Hash Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550\n17.8.1\nstd::hash\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550\n17.8.2\nComposite Hash Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550\n17.8.3\nHashing a Custom Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551\n17.9\nSolving Problems Using Hash Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552", "word_count": 3157, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "97339446-c232-56b2-a18c-00a88380c876", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 8, "real_page_number": null, "text": "viii\n18\nTrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587\n18.1\nIntroduction to Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587\n18.2\nBinary Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589\n18.2.1\nArray-Based Tree Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589\n18.2.2\nPointer-Based Tree Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591\n18.2.3\nConverting a Generic Tree into a Binary Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592\n18.3\nTree Traversals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594\n18.3.1\nPreorder Traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594\n18.3.2\nInorder Traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598\n18.3.3\nPostorder Traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n18.3.4\nLevel-Order Traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606\n18.4\nSolving Problems Using Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 610\n18.5\nConstructing Binary Trees from Preorder, Inorder, and Postorder Traversals . . . . . . . . . . . . . . . . . . . . . . . . . . 615\n18.5.1\nConstructing a Binary Tree from Inorder and Preorder Traversals . . . . . . . . . . . . . . . . . . . . . . . . 615\n18.5.2\nConstructing a Binary Tree from Inorder and Postorder Traversals . . . . . . . . . . . . . . . . . . . . . . . . 616\n18.5.3\nConstructing a Binary Tree from Preorder and Postorder Traversals . . . . . . . . . . . . . . . . . . . . . . . 617\n18.6\nBinary Search Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618\n18.6.1\nBinary Search Tree Structure\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618\n18.6.2\nSearching in a Binary Search Tree\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619\n18.6.3\nInserting into a Binary Search Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620\n18.6.4\nRemoving from a Binary Search Tree\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n18.7\nAVL Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627\n18.7.1\nProperties of AVL Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627\n18.7.2\nBalance Factor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629\n18.7.3\nAVL Tree Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630\n18.7.4\nInserting and Removing Elements\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637\n18.8\nThe STL Map Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643\n18.9\nThe STL Set Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648\n18.10\nThe STL Multimap and Multiset Containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653\n18.11\nTries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655\n18.11.1\nTrie Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655\n18.11.2\nInserting into a Trie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656\n18.11.3\nSearching in a Trie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659\n18.11.4\nRemoving from a Trie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659\n18.11.5\nImplementing the Trie Destructor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661\n19\nGraphs and Elementary Graph Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 689\n19.1\nIntroduction to Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 689\n19.2\nGraph Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n19.2.1\nAdjacency Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n19.2.2\nAdjacency List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 695\n19.2.3\nSummary of Graph Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 696\n19.3\nDepth-First Search\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 699\n19.3.1\nIterative Depth-First Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 700\n19.3.2\nRecursive Depth-First Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 703\n19.3.3\nDepth-First Search Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 707\n19.3.4\nSolving Problems Using Depth-First Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 707\n19.4\nBreadth-First Search\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 712\n19.4.1\nBreadth-First Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 712\n19.4.2\nBreadth-First Search Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 716\n19.4.3\nSolving Problems Using Breadth-First Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 716\n19.5\nCycle Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 724\n19.6\nTopological Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727\n19.6.1\nTopological Orderings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727\n19.6.2\nDepth-First Search Topological Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 728\n19.6.3\nKahn’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 732\n19.7\nAlgorithms for Strongly Connected Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 736\n19.7.1\nStrongly Connected Components\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 736\n19.7.2\nKosaraju’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 737", "word_count": 3257, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fa73e380-836b-58a4-83e9-9087fb7d0ced", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 9, "real_page_number": null, "text": "ix\n19.8\nAlgorithms for Articulation Points and Bridges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747\n19.8.1\nArticulation Points and Bridges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747\n19.8.2\nTarjan’s Bridge-Finding Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\n20\nMinimum Spanning Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 765\n20.1\nIntroduction to Minimum Spanning Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 765\n20.2\nPrim’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766\n20.3\nKruskal’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 772\n20.4\nComparing Prim’s and Kruskal’s Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776\n21\nGreedy Algorithms and Divide-and-Conquer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785\n21.1\nIntroduction to Algorithm Families\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785\n21.2\nBrute Force . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 786\n21.3\nGreedy Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 786\n21.3.1\nThe Greedy Approach and Optimization Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 786\n21.3.2\nProving the Correctness of a Greedy Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787\n21.3.3\nActivity Selection Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 790\n21.3.4\nBreakpoint Selection Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 793\n21.3.5\nHuﬀman Coding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795\n21.3.6\nSolving Problems Using a Greedy Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 798\n21.4\nDivide-and-Conquer\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 801\n21.4.1\nDivide-and-Conquer and Independent Subproblems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 801\n21.4.2\nPower Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 803\n21.4.3\nInteger Multiplication and the Karatsuba Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 804\n21.4.4\nThe Maximum Subarray Problem Using Divide-and-Conquer . . . . . . . . . . . . . . . . . . . . . . . . . . . 806\n21.4.5\nThe Maximum Subarray Problem Using Kadane’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 809\n22\nBacktracking and Branch and Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817\n22.1\nBacktracking\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817\n22.1.1\nIntroduction to Backtracking\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817\n22.1.2\nBacktracking Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 821\n22.1.3\nSolving Problems Using Backtracking: Combinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 822\n22.1.4\nSolving Problems Using Backtracking: N-Queens\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 825\n22.1.5\nSolving Problems Using Backtracking: Sudoku Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 830\n22.1.6\nBacktracking Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 831\n22.2\nBranch and Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 832\n22.2.1\nBranch and Bound: Minimization Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 832\n22.2.2\nBranch and Bound: Maximization Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 835\n22.3\nThe Traveling Salesperson Problem (TSP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836\n22.3.1\nEstimating Bounds for TSP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836\n22.3.2\nGenerating Permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840\n22.4\nTSP Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 842\n22.4.1\nNearest Neighbor and 2-Opt\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 842\n22.4.2\nNearest Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 851\n22.4.3\nFarthest Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 853\n22.4.4\nArbitrary Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855\n23\nDynamic Programming\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 863\n23.1\nFoundations of Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 863\n23.1.1\nIntroduction to Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 863\n23.1.2\nFibonacci Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 864\n23.1.3\nTop-Down and Bottom-Up Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 868\n23.1.4\nBinomial Coeﬃcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 869\n23.2\nDynamic Programming Implementation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 871\n23.3\nCommon Dynamic Programming Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 872\n23.3.1\nCounting Distinct Ways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 872\n23.3.2\nPath and Decision Optimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 881\n23.3.3\nDecision Making: Take It or Leave It . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 889\n23.3.4\nInterval Merging\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 891\n23.3.5\nDynamic Programming on Strings and Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 902", "word_count": 2960, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "207600fe-47ba-55b7-ac1a-6944efe815da", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 10, "real_page_number": null, "text": "x\n23.4\nMemo Space Optimization for Bottom-Up Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 909\n23.5\nSummary of Dynamic Programming Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912\n24\nThe Knapsack Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 927\n24.1\nTypes of Knapsack Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 927\n24.2\n0-1 Knapsack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 928\n24.2.1\nSolving 0-1 Knapsack Using Brute Force . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 928\n24.2.2\nWhy Greedy Fails for 0-1 Knapsack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 928\n24.2.3\nSolving 0-1 Knapsack Using Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 929\n24.2.4\nSolving 0-1 Knapsack Using Branch and Bound\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 934\n24.3\nFractional Knapsack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 935\n24.4\nBounded Knapsack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 937\n24.5\nUnbounded Knapsack\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 939\n25\nShortest Path Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 951\n25.1\nThe Shortest Path Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 951\n25.2\nDijkstra’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 952\n25.2.1\nImplementing Dijkstra’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 952\n25.2.2\nProving Dijkstra’s Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 957\n25.2.3\nWhy Dijkstra’s Algorithm Fails on Negative Edges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 957\n25.2.4\nChanging Edge Weights in the Shortest Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 958\n25.2.5\nSolving Problems Using Dijkstra’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 959\n25.3\nBellman-Ford Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 960\n25.4\nFloyd-Warshall Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 966\n25.5\nJohnson’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 969\n25.6\nA* Search\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 975\n25.7\nSummary of Shortest Path Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 976\n26\nComputational Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 987\n26.1\nPoints, Segments, and Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 987\n26.2\nSweep Line Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 991\n26.2.1\nApplications of the Sweep Line Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 991\n26.2.2\nSolving Problems Using the Sweep Line Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 992\n26.3\nThe Closest Pair of Points Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 996\n26.3.1\nThe Closest Pair of Points Problem in One Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 996\n26.3.2\nThe Closest Pair of Points Problem in Two Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 997\n26.4\nThe Clockwise Test and Triangle Area\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 999\n26.5\nArea of a Polygon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1001\n26.5.1\nThe Shoelace Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1001\n26.5.2\nVisualizing the Shoelace Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1005\n26.5.3\nShoelace Formula Edge Cases: Axis Intersection and Concavity . . . . . . . . . . . . . . . . . . . . . . . .\n1005\n26.6\nPoint Inside Polygon\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1008\n26.7\nConvex Hull Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1009\n26.7.1\nJarvis’s March . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1010\n26.7.2\nGraham’s Scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1010\n26.7.3\nComparing Convex Hull Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1013\n26.8\nBounding Box Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1014\n26.9\nk-d Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1016\n26.9.1\nPartitioning Data in a k-d Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1016\n26.9.2\nInserting into a k-d Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1017\n26.9.3\nDeleting from a k-d Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1021\n26.10\nSegment Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1024\n26.10.1\nSegment Trees in One Dimension\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1024\n26.10.2\nLazy Propagation Segment Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1030\n26.10.3\nSegment Trees in Multiple Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1031\n26.11\nImages and Graphics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1036\n26.11.1\nRaster and Vector Graphics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1036\n26.11.2\nColor Representation in Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1037\n26.11.3\nImage Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1037", "word_count": 3127, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "629b7b44-b028-5346-bab2-b48599153355", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 11, "real_page_number": null, "text": "xi\n27\nSmart Pointers and Memory Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1043\n27.1\nMemory Allocation and Storage Duration Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1043\n27.1.1\nThe Memory Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1043\n27.1.2\nThe Stack and Automatic Storage\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1044\n27.1.3\nGlobal Variables and Static Storage\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1044\n27.1.4\nLinking Global Variables Across Multiple Translation Units . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1045\n27.1.5\nExecutable Instructions and Text Storage\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1046\n27.1.6\nThe Heap and Dynamic Storage\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1047\n27.1.7\nCommon Issues Involving Dynamic Memory: Memory Leaks . . . . . . . . . . . . . . . . . . . . . . . . . .\n1047\n27.1.8\nCommon Issues Involving Dynamic Memory: Double Deletions . . . . . . . . . . . . . . . . . . . . . . . .\n1048\n27.1.9\nCommon Issues Involving Dynamic Memory: Dangling Pointers . . . . . . . . . . . . . . . . . . . . . . . .\n1048\n27.1.10\nGarbage Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1049\n27.2\nResource Acquisition is Initialization (RAII)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1049\n27.2.1\nMotivations for RAII . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1049\n27.2.2\nManaging Dynamic Memory Using RAII\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1049\n27.3\nIntroduction to Smart Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1051\n27.3.1\nAnatomy of a Smart Pointer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1051\n27.3.2\nCategories of Ownership . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1052\n27.3.3\nReference Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1053\n27.4\nUnique Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1054\n27.4.1\nstd::unique_ptr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1054\n27.4.2\nTransferring Ownership of a Unique Pointer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1056\n27.4.3\nUsing Unique Pointers in Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1057\n27.4.4\nStoring Unique Pointers in Containers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1058\n27.4.5\nConst Smart Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1059\n27.4.6\nSmart Pointers and Polymorphism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1059\n27.5\nShared Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1060\n27.5.1\nstd::shared_ptr\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1060\n27.5.2\nConverting Between Unique and Shared Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1061\n27.5.3\nShared Pointers and Reference Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1061\n27.5.4\nstd::enable_shared_from_this . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1062\n27.6\nWeak Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1063\n27.6.1\nThe Cycle Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1063\n27.6.2\nImplementing a Weak Pointer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1066\n27.6.3\nstd::weak_ptr\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1068\n27.7\nWhen Should Smart Pointers Be Used?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1070", "word_count": 1848, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5c335b05-b7ae-543b-82cb-46dda45f2406", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 12, "real_page_number": null, "text": "", "word_count": 0, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f252561c-47be-59b1-bb85-4dacc89ffa39", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 13, "real_page_number": null, "text": "Chapter 1\nProgramming Foundations\n1.1\nIntroduction\nIf you are reading this, there’s a good chance you are preparing to take EECS 281 here at the University of Michigan. Some of you may have\njust completed EECS 280 and are looking forward to see what this next class offers. Others of you may have taken EECS 280 earlier and have\nnot done any programming for over a semester. Regardless of which camp you are in, a lot of material covered in the class is brand new, so\nsuccess in this class is fully attainable if you put in the time and effort.\nThat being said, EECS 281 is undoubtedly a challenging course. As an IA who taught this class for five semesters, I can certainly attest\nto the vastness and thought-provoking nature of the class material. However, this also makes EECS 281 quite fulfilling, as there is always\nsomething new to learn, regardless of where you are in your programming career. In fact, this was a motivating factor behind these notes in the\nfirst place; I wanted to consolidate everything I learned from three years of 281 experience into a single resource, so that students will be able to\nbetter understand and appreciate the full scope of the class material.\nBefore we begin, this first chapter will review some concepts from previous courses (mostly from EECS 280) that may be applicable to\nEECS 281. Thus, if you are already comfortable with any of this information, feel free to skip it. Throughout these notes, you may also see\nsections labeled with this asterisk symbol (✽). This indicates that the section contains bonus material that is not covered in the course, but\nrather something I personally think is useful or just good to know. This material usually includes C++ features that may be helpful for projects,\nalgorithms and data structures that may show up during job interviews, as well as additional knowledge that may supplement material that is\nactually covered in the class. If you see an ‘✽’ anywhere, feel free to skip its corresponding section (unless the material was actually explicitly\ncovered in a lecture or lab, since it is possible for the course material to change over time). Similarly, if you see anything that you do not\nrecognize in a section without an ‘✽’ (and is not mentioned in the class at all), feel free to skip that material as well. The material covered in\nlectures and labs will determine what you are responsible for knowing in the class, and not the content in these notes.\nIt should also be noted that this resource should NOT be used as a substitute for going to lab or lecture! Most of your learning will be done\nin the classroom, where you will have an opportunity to engage in an interactive study environment with your fellow classmates and instructors.\nHowever, feel free to use this text as a resource to supplement your studies. Good luck, and I hope you enjoy your experience in EECS 281!", "word_count": 501, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2105c0b2-5e02-5454-84b4-554452407234", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 14, "real_page_number": null, "text": "2\nChapter 1. Programming Foundations\n1.2\nPointers and References\n¸ 1.2.1\nProgram Execution and the Machine Model\nTo start off, we will begin by talking about the C++ discussed in EECS 280, which allows us to visualize how code behavesmachine model\nduring runtime. Consider the following code, which initializes an integer, copies that integer, and then assigns it to a new value:\n1\nint main() {\n2\nint x = 280;\n3\nint y = x;\n4\nx = 281;\n5\ny = x;\n6\n} // main()\nmain() x 280.The program’s execution begins with on line 1. On line 2, we declare an integer that is initialized with a value of When this\nline is executed during runtime, an integer object is created somewhere in memory. We denote this integer’s memory location as its address.\n280\nx\n0x7fff492414a4\nvariable\naddress\nYou do not need to worry about the specific memory address of the integer above (since it was arbitrarily chosen). In fact, when you write a\nprogram, you are generally in control of the specific address locations that your objects are constructed in. Rather, the operating systemnot\ndecides the exact memory locations of each object during runtime.\n(&),Remark: If you want to get the address of an object in your program, you can use the ampersand symbol which is known as the\n&\"address-of\" operator. By placing an next to an existing object in your program, you can identify its address location in memory. Note that\nnotthis usage should be confused with the declaration of a reference, which also uses the ampersand symbol.\n1\nint main() {\n2\nint x = 280;\n3\nstd::cout << &x << std::endl; // sample output: 0x7fff492414a4\n4\n} // main()\n0x 0-9 a-f,Addresses are typically denoted in (base-16) notation, with a leading followed by the digits and the letters wherehexadecimal\na b c d e frepresents the value 10, represents the value 11, represents the value 12, represents the value 13, represents the value 14, and\nrepresents the value 15. The size of an address depends on the system you are using. If you are using a 64-bit machine, each memory address\ntakes up 64 bits (or 8 bytes). Alternatively, if you are using a 32-bit machine, each memory address takes up 32 bits (or 4 bytes).\ny x. xOn line 3, we initialize a variable with the contents of Here, the program takes the value stored in the memory associated with and\ny. y = x ycopies it to the memory allocated for Note that, in C++, the assignment of does assign to the same memory object referred to bynot\nx! y x x yInstead, a new object is created for with the value of copied over, and and refer to different objects in memory. This is a rather\nimportant distinction, which we will cover in detail a bit later in this section.\n280\nx\n0x7fff492414a4\n280\ny\n0x7ffdaddeb434\nx 281. x 280 281.On line 4, we assign the value of to be This updates the value stored in the memory of from to\n281\nx\n0x7fff492414a4\n280\ny\n0x7ffdaddeb434\ny x. xLastly, on line 5, we assign to the value of Here, the value stored in the memory object associated with is copied into the memory object\ny.associated with\n281\nx\n0x7fff492414a4\n281\ny\n0x7ffdaddeb434", "word_count": 569, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1d156ca6-3a08-5716-8f17-105e41d56f8c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 15, "real_page_number": null, "text": "1.2 Pointers and References\n3\n¸ 1.2.2\nReferences and Reference Semantics\ny = x yNotice that, when we executed in the previous examples, we ended up modifying the of the object that referred to. This is knownvalue\nas semantics, which is the default behavior of initialization and assignment in C++.value\ny = x\ny x)(value semantics: the value of is set to the value of\n281\nx\n0x7fff492414a4\n281\ny\n0x7ffdaddeb434\nAn alternative method of initialization and assignment is semantics, which changes the object the assigned variable refers to ratherreference\nthan the value of the object itself.\ny = x\ny x(reference semantics: the object that refers to is set to the object that refers to)\n281\nx\n0x7fff492414a4\ny\nIn C++, reference semantics is only supported during the initialization of a variable. To initialize a variable as a reference, place an ampersand\n(&) to the left of the variable name and assign it to the object you want the variable to refer to. An example is shown in the code below:\n1\nint main() {\n2\nint x = 280;\n3\nint& y = x; // y is a reference to x\n4\nx = 281;\n5\nstd::cout << y << std::endl; // prints 281\n6\n} // main()\nint& y = x. yHere, a reference is created on line 3 with the declaration You can think of this as creating an alternative name for the existing\nx. x yvariable Any changes made to either or will end up changing the exact same object in memory, regardless of which variable name is used\nx 281 y 281, x yto perform the change. For example, when is assigned to a new value of on line 4, the value of also becomes as and refer to\nthe same integer object in memory.\n280\nx,y\n0x7fff492414a4\nx = 281\n281\nx,y\n0x7fff492414a4\nIt is important to note that a reference in C++ cannot be reassigned once it is created! Once you initialize a variable as a reference, that variable\ncannot be associated with a different memory object until the reference variable goes out of scope. For instance, we would not be able to assign\ny x, yto another object once we initialized it as a reference to unless that variable goes out of scope (we will cover scope in more detail in a\nlater section of this chapter).\nThere are many instances where having a reference may be useful. One particularly important use case of references in 281 occurs when\nworking with function parameters. As an example, consider the following function, which attempts to increment an integer counter by one.\n1\nvoid increment(int counter) {\n2\ncounter += 1;\n3\n} // increment()\n4\n5\nint main() {\n6\nint c = 280;\n7\nincrement(c);\n8\nstd::cout << c << std::endl; // prints 280\n9\n} // main()\ncounter increment()However, since C++ uses value semantics by default, the variable within the scope of the function is actually\nc main() c increment()initialized as a of the variable in on line 7. As a result, a copy of is incremented in the function rather thancopy\nc increment() citself! Since the copy goes out of scope within the function without being returned, the original does not get modified at\n280.all, and line 8 prints out", "word_count": 565, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b419b467-938c-5ec4-888a-d5f53919dafa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 16, "real_page_number": null, "text": "4\nChapter 1. Programming Foundations\nTo ensure that the counter is actually modified in our function, we would need to pass it in by reference.\n1\nvoid increment(int& counter) { // pass by reference\n2\ncounter += 1;\n3\n} // increment()\n4\n5\nint main() {\n6\nint c = 280;\n7\nincrement(c);\n8\nstd::cout << c << std::endl; // prints 281\n9\n} // main()\ncounterNotice the ampersand that we placed on line 1 — this essentially indicates that the parameter should be a to whateverreference\nc increment() counter c,object is passed into that function. Therefore, when is passed into the function on line 7, is set as a toreference\ncounter c. counter 281, c 281,and any modifications to are also reflected in The value of is incremented to so the value of also becomes\nwhich gets printed out on line 8.\nIn general, if you want objects to be modified by a function, you must pass them in by reference. However, passing by reference is not\nlimited to this specific use case. Even if you do not want an object to be modified, it may still be prudent to pass the object in as a constant\nreference (as shown below). This is because passing an object by reference avoids the overhead of copying the object whenever it is passed into\nthe function, which can be inefficient if the object is large.\n1\nvoid func(const BigObject& obj) { // pass by const reference\n2\n// do work...\n3\n} // func()\n4\n5\nint main() {\n6\nBigObject my_big_object{};\n7\nfunc(my_big_object); // my_big_object is not copied into func() because it is passed by reference\n8\n} // main()\nRemark: When passing parameters into a function, the following steps provide a good framework for determining whether the parameters\nshould be passed by value or reference.\n1. If a parameter should be modified by the function, it should be passed by reference.\n2. Else, if a parameter is a primitive type (like an integer or character), pass it by value.\nconst3. Else, if it is an object, pass it by reference.\nNote that passing a primitive type (see section 1.3) by reference can be efficient than passing it by value. This is because passing anless\nobject by reference is akin to passing a pointer to that object behind the scenes. This not only adds a level of unnecessary indirection for\naccessing a primitive object (since we have to follow a pointer to where the object is located, instead of accessing it directly), but it also takes\nint, double,up additional memory on the function call stack because the sizes of primitive types (e.g., up to 4 bytes for 8 bytes for 1 byte\nchar bool)for and are typically no larger than the size of pointers (8 bytes on a 64-bit machine).\n¸ 1.2.3\nPointers\nWhen you create an object in C++, it is placed at some memory address during runtime. This specific address, however, is generally not under\nthe programmer’s control. If you run the same program more than once, its objects may be placed at different memory locations on each run.\nHowever, you can always query the address of an object after it has been created using the \"address-of\" operator, denoted using an ampersand.\n1\nint main() {\n2\nint x = 281;\n3\nstd::cout << &x << std::endl; // sample output: 0x7ffebd5f1fdc\n4\n} // main()\nx.The code above prints out the memory address of the integer However, addresses can also be stored in a category of objects known as\npointers. To declare a pointer, simply place an symbol to the left of the variable name during declaration. An example is shown below:*\n1\nint main() {\n2\nint x = 281;\n3\nint* ptr = &x; // ptr stores the address of x\n4\nstd::cout << ptr << std::endl; // sample output: 0x7ffeccc22c4c\n5\n} // main()\nint* int.Each data type in C++ has a corresponding pointer type. For instance, an would represent a pointer to an Note that pointers are\nint**,objects as well, and they themselves also have an address in memory. Therefore, it is perfectly valid to have a pointer with the type\nint* int).which would represent a pointer to an (or, in other words, a pointer to pointer to\n1\nint main() {\n2\nint x = 281;\n3\nint* y = &x; // y stores the address of x\n4\nint** z = &y; // z stores the address of y\n5\n} // main()\n281\nx\n0x4104\n0x4104\ny\n0x4108\n0x4108\nz\n0x4110", "word_count": 774, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "20700150-2b4a-5665-965b-6bb115cc9c13", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 17, "real_page_number": null, "text": "1.3 Address Space and Dynamic Memory\n5\nYou can access the object that a pointer points to by the pointer using the dereference operator, also denoted with an *. In thedereferencing\ny x, y *y) x.following code, is a pointer that stores the address of so applying the dereference operator to (i.e., would return the value of\n1\nint main() {\n2\nint x = 281;\n3\nint* y = &x; // y stores the address of x\n4\nstd::cout << *y << std::endl; // prints 281\n5\n} // main()\n281\nx\n0x4104\n0x4104\ny\n0x4108\n*y y xgets the value that points to, which is\nA few words of warning regarding pointers:\n1. If a pointer is not explicitly initialized, it is default initialized with an undefined value. Dereferencing a default-initialized pointer results\nin undefined behavior!\n1\nint main() {\n2\nint *x;\n3\n*x = 281; // undefined behavior\n4\n} // main()\nIt is possible for a pointer to outlive the object it is pointing to, which could also result in undefined behavior. For example, the2.\nget_address() x xfunction below returns a pointer to a copy of (because is not passed by reference). This local copy goes outlocal\nof scope after the function returns, so attempting to dereference its address results in undefined behavior.\n1\nint* get_address(int x) {\n2\nreturn &x; // returns pointer to x, but x goes out of scope after function returns\n3\n} // get_address()\n4\n5\nint main() {\n6\nint val = 280;\n7\nint* ptr = get_address(val);\n8\nstd::cout << *ptr << std::endl; // undefined behavior\n9\n} // main()\nIn C++, there is a special object known as a that can be used to designate pointers that do not point to a valid object. Any pointernull pointer\nnullptr(regardless of type) can be assigned to a null pointer value using the keyword, as shown in the code below.\n1\nint main() {\n2\nint* nullptr;x =\n3\ndouble* nullptr;y =\n4\nchar* nullptr;z =\n5\n} // main()\nnullptrSimilar to pointers that are default initialized without a value, dereferencing a also results in undefined behavior.\n1.3\nAddress Space and Dynamic Memory\nWhen you create a new object in a program, memory is allocated to store that object. The general location of this allocation depends on the\ntype of object that was created. Objects whose lifetimes are tied to a particular scope (i.e., local variables) are allocated in a different region of\nmemory than objects that are created dynamically by the programmer.\nEveryrunningprogram(knownasaprocess)hasitsownmemorylayout,independentfromotherprocesses(thisindependenceismaintained\nby the operating system using something known as memory, which is beyond the scope of the class). This memory layout designates thevirtual\naddress space of a process, which is the range of valid addresses in memory that the process can access. A typical layout for an address space is\nshown below.\nstack\nheap\nstatic\ntext\nhigh addresses\nlow addresses", "word_count": 515, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fa5cea97-2bbf-5aee-946f-14ed96f2d8a4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 18, "real_page_number": null, "text": "6\nChapter 1. Programming Foundations\nLocal variables are stored in the stack region of the address space. The stack grows downward with each allocation (i.e., the first allocation is\ngiven the highest address, and subsequent allocations are assigned to lower addresses). Every time a new local variable is declared, space is\nallocated on the stack to hold that variable. Whenever a local variable goes out of scope, its corresponding memory on the stack is deallocated.\nNote that allocation and deallocation for stack memory are both done automatically!\nOn the contrary, the heap region of the address space is used to store objects that are allocated dynamically. Dynamic memory is allocated\nby the programmer and is not confined to a particular scope. This type of memory must also be deallocated explicitly as well — failureexplicitly\nto do so would cause the memory to remain in the system indefinitely (until it is forceably wiped, such as from a computer shutdown). This is\nknown as a leak, which can hinder the performance of a system by exhausting the amount of memory available for use.memory\nnew deleteDynamic memory is allocated with the keyword and deallocated with the keyword. As such, the lifetime of a dynamically\nnew delete.allocated variable starts when it is created with and ends when it is deallocated with An example is shown in the code below:\n1\nint main() {\n2\nint* new int{281};ptr =\n3\nstd::cout << *ptr << std::endl; // prints 281\n4\ndelete ptr;\n5\n} // main()\nnewThe keyword does the following:\n1. Allocates space for an object of the given type on the heap.\n2. Initializes the object with the value of the given initialization expression.\n3. Evaluates to the address of the newly created object.\nnewIn the code above, line 2 creates a dynamically allocated integer on the heap, initialized with the value 281. Since is used, the expression to\nptr. ptrthe right of the equals sign evaluates to the memory address of this new integer, which is stored as Note that is a variable that islocal\nallocated on the program stack, and it points to a memory address on the heap! In memory, this would look something like this:\nptr 0x5370\nstack\nheap\n281\n0x5370\nptr. ptrLine 3 dereferences and prints out the value of The dereference (*) accesses the value that points to (i.e., the value at the address\n0x5370 281,in the illustration above). In this case, this is the integer so this value gets printed out.\ndelete ptr. delete ptr ptrLine 4 applies the keyword on It is important to note that deletes the object that points to, and not itself!\ndelete ptr 0x5370, 281. ptrIn our example above, calling on would deallocate the object at address or the integer The object still\ndeleteremains on the stack after the delete, since it is a local variable. It should also be noted that should only be applied to objects that are\nnew. delete newdynamically allocated using Attempting to an object that was not created using results in undefined behavior.\nRemark: One notable caveat to this process is the declaration of dynamically-allocated arrays. An array is a fixed sized collection of objects\nnew[]that share the same type, stored contiguously in memory. If you want to allocate an array on the heap, you will need to use and\ndelete[] with the additional square brackets. An example is shown below:\n1\nint main() {\n2\nint* new int[5];arr = // creates array of integers of size 5\n3\nfor (int i = 0; i < 5; ++i) {\n4\narr[i] = i; // fills up array with the values [0, 1, 2, 3, 4]\n5\n} // for\n6\ndelete[] arr; // delete array (notice the extra [])\n7\n} // main()\n1.4\nCompound Objects\n¸ 1.4.1\nPrimitive Data Types\nWhen you were first exposed to C++, you were probably introduced to the primitive data types: the building blocks of the language. One\n-231)(int),common primitive data type is the integer which is typically 4 bytes and can store integer values between -2,147,483,648 (equal to\n−1).1231 (char),and 2,147,483,647 (equal to Another primitive data type is the character which is a 1 byte data type that stores a number,\ncharwhere each number in the range from 0 to 127 represents an ASCII character (for example, the letter ‘a’ is represented using a value of 97\n16).2 double— this will be covered in more detail in chapter Lastly, a is a data type that can be used to store decimal values; this data type\ntypically requires 8 bytes of memory space. A table of common primitive types is shown below:\nType\nBytes\nMinimum Value\nMaximum Value\ndouble\n8\n×10−3082.22507\n×103081.79769\nint\n4\n-231)-2,147,483,648 (or\n2312,147,483,647 (or −1)\nchar\n1\n0\n127\nbool\n1\nN/A\nN/A\n1The intactualsizeofan canactuallydifferinsizefromsystemtosystem,butforourcasewewilltreatitas32bits,or4bytes. Ifyouwanttoensurethatan\nint32_tintegerobjectis 4bytes,youcanuse instead,whichwewilldiscussmomentarily.exactly\n2Althoughcharcansupportvaluesoutsidetherangefrom0to127,thesevaluesdonotrepresentmeaningfulcharacters.", "word_count": 894, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "191a84f2-0663-57ca-a817-f07d870b4b25", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 19, "real_page_number": null, "text": "1.4 Compound Objects\n7\nSome of these built-in data types can be further modified using a data type modifer that can be used to adjust the size or range of data the type\nunsigned unsigned intcan hold. For instance, the keyword can be used to ensure that a data type is always non-negative (e.g., an is\n232 short longnon-negative and can hold values from 0 to 4,294,967,295, or −1). The and keywords can be used to restrict or expand the\n-215 215short intrange that a data type can hold; for instance, a can only hold values between -32,768 and 32,767 (i.e., to −1), while a\n-263 263long long int can hold values from to −1. The tradeoff of being able to hold such a large number is that more memory is required\nlong long, short int.to store a compared to a or normal\nType\nBytes\nMinimum Value\nMaximum Value\nunsigned long\n8\n0\n26418,446,744,073,709,551,615 (or −1)\nlong\n8\n-263)-9,223,372,036,854,775,808 (or\n2639,223,372,036,854,775,807 (or −1)\nunsigned int\n4\n0\n2324,294,967,295 (or −1)\nint\n4\n-231)-2,147,483,648 (or\n2312,147,483,647 (or −1)\nunsigned short\n2\n0\n21665,535 (or −1)\nshort\n2\n-215)-32,768 (or\n21532,767 (or −1)\n<cstdint>.C++11 introduced the concept of types, defined in These types can be used to guarantee the size of an integerfixed-width integer\nint32_tobject. For instance, an is an integer that takes up exactly 32 bits (4 bytes). Fixed-width integers can also be unsigned; unsigned\nu uint32_tversions of these types have the letter prepended to the name of its corresponding signed integer type. For instance, is an unsigned\ninteger that takes up 32 bits. A few fixed-width integer types are shown below.\nType\nBytes\nMinimum Value\nMaximum Value\nuint64_t\n8\n0\n26418,446,744,073,709,551,615 (or −1)\nint64_t\n8\n-263)-9,223,372,036,854,775,808 (or\n2639,223,372,036,854,775,807 (or −1)\nuint32_t\n4\n0\n2324,294,967,295 (or −1)\nint32_t\n4\n-231)-2,147,483,648 (or\n2312,147,483,647 (or −1)\nuint16_t\n2\n0\n21665,535 (or −1)\nint16_t\n2\n-215)-32,768 (or\n21532,767 (or −1)\nuint8_t\n1\n0\n28255 (or −1)\nint8_t\n1\n-27)-128 (or\n27127 (or −1)\nsize_t, size_tAnother type you may encounter is which is an unsigned integer that is used to represent object sizes. You may often see\nsize_tbeing used for array indexing, loop counting, or size storing. The actual size of a object is architecture-dependent, as it is required to\nsize_tbe large enough to express the maximum size of any possible object on a given system. For instance, on a 32-bit system, will be at\nsize_t size_tleast 32 bits wide; similarly, on a 64-bit system, will be at least 64 bits wide. For our case, we will treat as a type that takes\nup 64 bits (8 bytes), but you should be aware that this is not always the case depending on the system you are using.\nType\nBytes\nMinimum Value\nMaximum Value\nsize_t\n8\n0\n26418,446,744,073,709,551,615 (or −1)\nLastly, when choosing what type of variable to use, think about the possible range of values that you may need to store. If you choose a type\nwith too small of a range, you might end up with or underflow, in which exceeding the maximum value (or going below the minimumoverflow\nvalue in the case of underflow) can cause a variable to wrap around to the minimum value (or maximum value for underflow). For instance, the\nint32_t int32_tlargest number that can be represented using an is 2,147,483,647. Thus, adding 1 to 2,147,483,647 would cause an to\nint32_twrap all the way around to -2,147,483,648, which is the smallest value the type can hold. On the other hand, if you choose a type\nwith too large of a range, you end up wasting memory by taking up additional space that you never end up needing.\n¸ 1.4.2\nStructs and Classes\nThe types discussed above are fundamental building blocks that can be used to construct bigger things. In many cases, you will need more\nthan these basic types when writing a program. C++ allows you to accomplish this easily with user-defined data types, or custom types that\nStudent Studentsyou can define to represent objects in your program. For instance, a programmer can define a object and create in their\nint char.program, much like how they would create an or This is possible using a concept known as a compound object, which include\nstructstructures and classes. An example of a is shown below:\n1\nstruct Student {\n2\nstd::string name;\n3\nstd::string uniqname;\n4\nint32_t age;\n5\nint32_t id;\n6\n};\nStudent Student stringHere, we have introduced the type, where each is defined with four member variables: a that stores the student’s\nstring int32_t int32_tname, a that stores the student’s uniqname, an that stores the student’s age, and an that stores the student’s\nStudent Studentstudent ID. Notice that this is just the of the object; it provides a blueprint that allows us to create objects in ourdefinition\nStudent Studentprogram. To instantiate an instance of a in memory, we would need to explicitly create the objects in our program:\n1\nint main() {\n2\nStudent s; // instantiates an instance of a Student\n3\n...\n4\n} // main()", "word_count": 885, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5cd6b6ea-0783-5060-96ef-1577b3911958", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 20, "real_page_number": null, "text": "8\nChapter 1. Programming Foundations\nStudentIn this case, we created a object, but we did not specify the contents of its member variables. Hence, the contents of the student’s\nname, uniqname, age, and student ID are initialized. This is not good practice, since you may end up with indeterminate values (in otherdefault\nwords, the student’s age could be default initialized to a junk value, which is likely not something you want). To initialize the members of the\nstruct upon initialization, an initializer list can be used, as shown below (both approaches are valid):\n1\nint main() {\n2\nStudent s1 = {\"Bobby Tables\", \"btables\", 21, 12345678};\n3\nStudent s2{\"Bobby Tables\", \"btables\", 21, 12345678};\n4\n...\n5\n} // main()\nstructThe ordering of the member variables in the initializer list is the same order as the ordering of members in the declaration. In this case,\nname uniqname, age, id.comes first, then then then\nYou can also use to initialize the contents of the object. An example of designated initialization is shown below,designated initializers\nwhere each initialization is specified using a period followed by a designator that names a non-static data member that should be initialized.\n1\nint main() {\n2\nStudent s{\n3\n.name = \"Bobby Tables\",\n4\n.uniqname = \"btables\",\n5\n.age = 21,\n6\n.id = 12345678\n7\n};\n8\n...\n9\n} // main()\nstructSimilar to initializer lists, the order in which members are designated must follow the ordering of members in the definition.\nstruct (.)Individual members of a can be accessed with the dot operator. For example, the following would assign the ID 87654321 to\nthe student and print out this value. This assignment changes the student’s ID from 12345678 to 87654321.\n1\nint main() {\n2\nStudent s{\"Bobby Tables\", \"btables\", 21, 12345678};\n3\ns.id = 87654321;\n// assigns 87654321 as ID of s\n4\nstd::cout << s.id << std::endl;\n// prints 87654321\n5\n...\n6\n} // main()\n->If we instead had a pointer to an object, we can use the operator to dereference and access a member of that object. For example, if the\nstudent were a pointer, the following would accomplish the same result as above:\n1\nint main() {\n2\nStudent s{\"Bobby Tables\", \"btables\", 21, 12345678};\n3\nStudent* s_ptr = &s;\n4\ns_ptr->id = 87654321;\n// assigns 87654321 as ID of s (what s_ptr points to)\n5\nstd::cout << s_ptr->id << std::endl;\n// prints 87654321\n6\n...\n7\n} // main()\nclass struct classA is another method for defining a custom type in C++. A common misconception is that a and a are fundamentally\nclass struct classdifferent, where objects support member functions while objects do not. However, this is not true; a C++ is based on\nstruct, class struct class struct:a C and a and a are essentially identical. There is only one big difference between a and a all\nclass private struct publicmembers in a are by default, while all members in a are by default.\nstruct classAs a result, you are able to use a and interchangeably by just modifying the access level of members. However, in\nstruct classcustomary usage, it is common to use a only when you want all members to be public, and a everywhere else. For example,\nString Stringconsider the following custom definition of a class. The contents of the object (such as the characters that make up the\nString) Stringare stored as variables, while functions that modify the are defined as functions.member member\n1\nclass String {\n2\npublic:\n// public member functions that the user can call on a string\n3\nclear();\n// clears the content of the string\n4\nerase(...);\n// erases portion of string\n5\ninsert(...);\n// inserts another string into the original string\n6\nreplace(...);\n// replaces a portion of the string with new contents\n7\n...\n8\nprivate:\n// private member variables that store the data of the string\n9\nchar* cstr;\n// c-string that stores the contents of the string object\n10\nsize_t size;\n// stores the size of the string object\n11\n...\n12\n};\npublic privateAs shown above, we can use the and keywords to specify different levels of access for the user of the class. Members that\npublic private privateare can be accessed outside the class, while members that are cannot. Thus, setting members as can prevent\nusers of the object from accessing these member variables directly. This is useful in cases where the class has an invariant that could be\nsize private Stringpotentially broken by the user. For instance, the variable is in the class above because we wouldn’t want to give\nString String’s Stringthe user of a the ability to modify the size! Otherwise, the could end up in an invalid state. For example, a user\nsizeshould not be able to change the size of the string \"apple\" to something like 563. The variable must be consistent with the contents of the\nStringstring itself, so its value should be handled internally by the class.", "word_count": 852, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9e62339c-3962-5c83-9cdb-0c4193365ee7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 21, "real_page_number": null, "text": "1.4 Compound Objects\n9\nClasses and structs play a major role in the implementation of abstract data types, or ADTs. Recall that abstraction is the process of separating\n(the interface) from (the implementation). That is, the user of an object is only provided the essentialwhat something does how it works\ninformation needed to use the object, and the lower-level implementation details are hidden.\nAs an example, suppose you wanted to drive a car. When you get in the car and start driving, you only need to know the basic steps required\nto operate the car. You should know that pressing the accelerator should increase the speed of the car, and that pressing the brakes should\nslow the car down. But you shouldn’t need to know how everything works behind the scenes in order to drive! You don’t need to know the\nmechanics by which pressing the accelerator increases the car’s speed; you just need to know that it does. In other words, the details about the\ncar’s implementation have been abstracted away.\nAn abstract data type (ADT) behaves in a similar fashion. With an ADT, a user can store data and perform operations on this data, but\ndoing so operations. In other words, ADTs provide the user with operations and theirrequires no knowledge of the implementation of these\nexpected behaviors on stored data, but they do not define implementation details for these operations. Throughout this course, you will be\nintroduced to several ADTs that can be used to perform different functionalities and solve different types of problems.\n¸ 1.4.3\nConstructors\nA can be used to initialize a class-type object, and it is called when an object is created. The syntax for declaring a constructor isconstructor\nvery similar to that of a member function, with two main differences: there is no return type, and the name of the constructor is the same as the\nStudentname of the class. Let’s consider a simplified version of the object defined previously:\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\n};\nTo build a constructor that takes in the intended member variables of the class as arguments, you can use a list. To do so,member-initializer\nsimply add a colon after the constructor body and list out the member variables in the order they are defined in the object. For each member\nvariable, the value it should be initialized to should be enclosed using parentheses or curly braces. An example is shown below:\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_tStudent(std::string name_in, age_in)\n5\n: name{name_in}, age{age_in} {}\n6\n};\nname_in age_in, name name_in ageThe above constructor takes in two values, and and initializes to the value of and to the value of\nage_in. Student name \"Alice\" age 21.Thus, the following would instantiate a object with a of and an of\nStudent s{\"Alice\", 21};\nIt is important to make sure that your variables are initialized before you do anything with them in your program, as failing to do so could cause\nyou to accidentally use undefined values! For atomic types, such as integers and doubles, default initialization doesn’t \"initialize\" the type to\nsome predetermined value; instead, it initializes to whatever junk previously existed in memory (i.e., the value is undefined).\nIf a class defines no constructors at all, the compiler provides an constructor, which default initializes each member variable.implicit default\nIf you declared a non-default constructor and want the object to have a default constructor, you would have to explicitly write one. An example\nof a default constructor is shown below — unlike the custom constructor defined above, the default constructor takes in no arguments.\nStudent()\n: name{\"Potato\"}, age{281} {}\nStudent, Student name \"Potato\",Here, if no argument is provided in the construction of a the object’s is automatically initialized to\nage 281.and its is automatically initialized to\n¸ 1.4.4\nMember Variable Organization\nConsider the following two student objects, each storing the same member variables, but in a different order:\n1\nstruct StudentA {\n2\nstd::string name;\n3\ndouble eecs_281_score;\n4\nint64_t enrollment_timestamp;\n5\nchar letter_grade;\n6\nbool has_passed;\n7\nbool has_applied_for_ia;\n8\n};\n1\nstruct StudentB {\n2\nbool has_passed;\n3\ndouble eecs_281_score;\n4\nchar letter_grade;\n5\nstd::string name;\n6\nbool has_applied_for_ia;\n7\nint64_t enrollment_timestamp;\n8\n};\nYou might expect these two objects to take up the same amount of space in memory. However, querying the sizes of these objects returns a\nStudentA StudentBpeculiar result: takes up 56 bytes, but takes up 72 bytes!\n1\nint main() {\n2\nsizeof(StudentA)std::cout << << std::endl;\n// prints 56\n3\nsizeof(StudentB)std::cout << << std::endl;\n// prints 72\n4\n} // main()", "word_count": 783, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5a7d822d-7131-59bd-901a-b80f1849348d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 22, "real_page_number": null, "text": "10\nChapter 1. Programming Foundations\nStudentB StudentAWhy does use 16 more bytes than despite storing the exact same information? This is due to something known as\nalignment, which is beyond the scope of this class (this is an EECS 370 concept). However, you can ensure that your object minimizes itsstruct\nmemory footprint by following this simple rule: when defining a custom object, you should list member variables in order of increasing or\ndouble intFor example, if you wanted to define an object that contains member variables of type (8 bytes), (4 bytes), anddecreasing size.\nchar double int, char(1 byte), you should list out the variables first, followed by then by (or vice versa):\n1\nstruct MyCustomType {\n2\n// doubles\n3\n// ints\n4\n// chars\n5\n};\n1\nstruct MyCustomType {\n2\n// chars\n3\n// ints\n4\n// doubles\n5\n};\nRemark: This advice is a bit of an oversimplication, since memory is not the only thing that you should consider when determining the\norder in which member variables should be listed. As an example, the ordering of members also impacts the order in which the member\nconstructors are run, so if one member must be initialized before another, then that member should be listed first. Style is also important to\nconsider; unless you are creating many instances of a custom object, the amount of memory you save typically is not worth it if you end up\nreducing the readability or maintainability of your custom object in the process.\n1.5\nOperator Overloading\noperatorIn C++, an is a symbol that can be used to complete a specific function. Every operator in the language has a function-like name\n+ operator+). int double,(e.g., the symbol used to add two objects has the name Although operators are predefined for types such as and\nthey need to be explicitly defined for custom types. The process of defining what an operator does for a custom type is known as operator\noverloading.\nOperator overloading is the process of defining the behavior of an operator symbol. Think of this like implementing a function that is\n+, <, >)invoked using a symbol (like instead of a standard function name. Some common operators that can be overloaded are:\noperator+•\noperator-•\noperator*•\noperator/•\noperator=•\noperator<•\noperator>•\noperator<=•\noperator>=•\noperator++•\noperator--•\noperator==•\noperator!=•\noperator+=•\noperator-=•\noperator[]•\noperator()•\noperator%•\noperatorˆ•\noperator<<•\noperator>>•\noperator+ +,operator<Tooverloadanoperatorforacustom-definedtype,youshoulddefinethecorrespondingoperatorfunction(e.g., for\n<) Pointfor using the appropriate arguments that the operator is applied on. Let’s look at a few examples using a object introduced below:\n1\nstruct Point {\n2\nint32_t x;\n3\nint32_t y;\n4\nPoint()\n// default constructor\n5\n: x{0}, y{0} {}\n6\nPoint(int32_t int32_tx_in, y_in)\n// custom constructor\n7\n: x{x_in}, y{y_in} {}\n8\n};\n+ PointFirst, let’s overload the operator, where adding two objects adds both their x- and y-coordinates, e.g.,\nPoint{3, 5} + Point{2, 6} = Point{5, 11}\n+ Point PointHere, the symbol works on two different objects and returns a with the sum of the two individual points’ x- and y-coordinates.\noperator operator+), +Since each operator has a function-like name (the word followed by the symbol, e.g., we can overload the operator\nPoint Point Pointon objects as a function that takes in two objects and returns another object that sums up the coordinates:\noperator+(const constPoint Point& lhs, Point& rhs);\n⏟⏟⏟\nreturntype\n⏟⏞⏞⏞⏞⏟⏞⏞⏞⏞⏟\noperatorname\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nobjectsthattheoperatoractsupon\nPoint lhs rhs,Now, we just need to implement this function to exhibit the behavior we want when two objects, and are added together\n(lhs rhs conststands for \"left-hand side\" and stands for \"right-hand side\"). We will cover the keyword later in this chapter, but just know it\nPointprevents the original objects from being modified.\n1\noperator+(const constPoint Point& lhs, Point& rhs) {\n2\n// return Point with x-coordinate lhs.x + rhs.x and y-coordinate lhs.y + rhs.y\n3\nreturn Point{lhs.x + rhs.x, lhs.y + rhs.y};\n4\n} // operator+()\n+ PointWith this, we are able to use the operator to add two objects together:\n1\nint main() {\n2\nPoint a{3, 5};\n// initialize Point a to (3, 5)\n3\nPoint b{2, 6};\n// initialize Point b to (2, 6)\n4\nPoint c = a + b;\n// c = (3, 5) + (2, 6) = (5, 11)\n5\n} // main()", "word_count": 770, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cb72c5e8-9a39-5d5e-96b9-cc87adbf5688", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 23, "real_page_number": null, "text": "1.5 Operator Overloading\n11\nHowever, this is not the only way to overload an operator for a custom-defined type. The usual custom is to overload operators within the type\ndefinition itself, if possible (an exception would be if the LHS object is not the same type as the class itself). If you overload the operator as a\nlhs) Pointmember function, the first argument (in this case, is implicit, and you are able to use the members of the type directly as if they\nlhs lhs.x x). operator+were part of (e.g., can just be written as For instance, the following overload of\n1\noperator+(const constPoint Point& lhs, Point& rhs) {\n2\n// return Point with x-coordinate lhs.x + rhs.x and y-coordinate lhs.y + rhs.y\n3\nreturn Point{lhs.x + rhs.x, lhs.y + rhs.y};\n4\n} // operator+()\nPointcan be implemented as a member function within the definition itself by making the following changes:\n1\nstruct Point {\n2\nint32_t x;\n3\nint32_t y;\n4\nPoint()\n// default constructor\n5\n: x{0}, y{0} {}\n6\nPoint(int32_t int32_tx_in, y_in)\n// custom constructor\n7\n: x{x_in}, y{y_in} {}\n8\n9\n// overloaded operator as a member function of Point\n10\n// lhs is implicit and does not have to be explicitly included (i.e., lhs == *this)\n11\noperator+(const constPoint Point& rhs) {\n12\nreturn Point{x + rhs.x, y + rhs.y};\n13\n} // operator+()\n14\n};\n+=,Some operators, such as may be used to modify the object it is called on. As a result, these operators require you to return an object by\nlhsreference. If the operator is defined outside the object (i.e., not a member function), you can simply return itself:\n1\noperator+=(Point& constPoint& lhs, Point& rhs) {\n2\n// adds the x- and y-coordinates of rhs to lhs and returns a reference to lhs\n3\nlhs.x += rhs.x;\n4\nlhs.y += rhs.y;\n5\nreturn lhs;\n6\n} // operator+=()\nPoint lhsHowever, if you define the overloaded function as a member function within the class, the value of is implicit. As a result, to\nlhs,return the value of what would have been you would have to return the current object that invoked the operator. To do so, your return\n*this, this thisvalue should be as is an internal pointer to the current instance of the class, and dereferencing would get you the current\noperator+= Pointobject itself. An example of overloading within the class is shown below:\n1\nstruct Point {\n2\nint32_t x;\n3\nint32_t y;\n4\nPoint()\n// default constructor\n5\n: x{0}, y{0} {}\n6\nPoint(int32_t int32_tx_in, y_in)\n// custom constructor\n7\n: x{x_in}, y{y_in} {}\n8\n9\n// overloaded operator as a member function of Point\n10\n// lhs is implicit and does not have to be explicitly included (i.e., lhs == *this)\n11\noperator+=(constPoint& Point& rhs) {\n12\nx += rhs.x;\n13\ny += rhs.y;\n14\nreturn *this;\n15\n} // operator+=()\n16\n};\n*this a, b,One advantage of returning by reference is that it allows you to chain multiple operations together. For example, given three points\nc, a += b += c a a + (b += c)).and the expression should be valid (and is set to\n(\"lhs\")As mentioned earlier, if the left-hand side parameter of an operator is not of the same type as the custom object the operator acts\nupon, then the operator overload cannot be implemented as a member function of that custom object. One example of this can occur if you\noperator<<. operator<<try to overload Overloading can be extremely useful, as it allows you to output your custom objects in any\nPoint \"(x, y)\" x y.format you want. For example, suppose you wanted to print out your objects in the form for member variables and The\nx y pt Pointcomplicated way to do this would be as follows (assuming that and are publicly accessible and is the object you want to print):\nstd::cout << \"(\" << pt.x << \", \" << pt.y << \")\" << std::endl;\noperator<<, PointIf you overload you can define this behavior for the object itself. This will allow you to get the same result by just\nPointoutputting the object to a stream:\nstd::cout << pt << std::endl;", "word_count": 719, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6916b7ae-f9c4-593d-aaaa-53c85b7df349", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 24, "real_page_number": null, "text": "12\nChapter 1. Programming Foundations\noperator<<, ostream ostreamTo overload you need to pass in a reference to and return the same by reference. The overloaded function\nalso takes in the object type whose output format you are trying to define. An example is shown below:\n1\noperator<<(std::ostream& conststd::ostream& os, Point& pt) {\n2\n// define the output format you want to print for this custom object\n3\nos << \"(\" << pt.x << \", \" << pt.y << \")\";\n4\nreturn os;\n5\n} // operator<<()\n\"(5, 11)\" operator<< PointAfter doing this, the following code would print since is now defined for objects (assuming that the\noperator+overloaded from before is still defined):\n1\nint main() {\n2\nPoint a{3, 5};\n// initialize Point a to (3, 5)\n3\nPoint b{2, 6};\n// initialize Point b to (2, 6)\n4\nstd::cout << a + b << std::endl;\n// prints \"(5, 11)\"\n5\n} // main()\n*this += ostreamSimilar to returning when overloading the operator, it is important to return a reference to the when overloading\noperator<<, operator<< cout <<since this allows multiple calls of to be chained on a single line. Without returning a reference,\na + b cout << a + b << endl operator<<would work, but would not, since is used multiple times in the same expression.\n1.6\nFunction Objects and Comparators\nThroughout this course, you may need to sort custom objects in your projects based on some defined ordering. For example, you may be tasked\nStudent Studentwith sorting a collection of objects by age. However, since a is a custom-defined type, we need to let the program know\nStudent agethat objects should be ordered based on their member variable. One way to accomplish this is to overload the less than (<)\nStudentoperator so that it compares two objects based on age. An example is shown below:\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_t id;\n5\nbool operator<(const constStudent& rhs) {\n6\nreturn age < rhs.age;\n7\n} // operator<()\n8\n};\n<However, what if you wanted to sort students by name for one step of an algorithm, and by age for another step? The operator can only be\noverloaded once per object, so it would not make sense to permanently define name or age as the only method of ordering. To address this,\noperator().you can use a comparator. A comparator is a function object (functor), which overloads the function call By overloading the\nfunction call operator, the object can be called as if it were an ordinary function.\noperator() (lhs)To create a comparator, create a custom object and overload its so that it takes in two objects and returns whether one\n(rhs).has a lesser value than the other\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_t id;\n5\n};\n6\n7\n// comparator for Student object\n8\nstruct StudentComparator {\n9\n// overload operator() so that this object can be used like a function\n10\nbool operator() (const const constStudent& lhs, Student& rhs) {\n11\nreturn lhs.age < rhs.age;\n12\n} // operator()()\n13\n};\nStudentComparator StudentWiththisdefinition,wecaninstantiatea objectanduseitasafunctiontocomparetwo objects. Anexample\nis shown below:\n1\nint main() {\n2\nStudent s1{\"Alice\", 21, 12345678};\n3\nStudent s2{\"Bob\", 22, 87654321};\n4\nStudentComparator comp;\n// create function object\n5\nstd::cout << comp(s1, s2) << std::endl;\n// prints whether s1 < s2\n6\n} // main()\nStudent age, comp(s1, s2) trueSince we defined our comparator to compare objects using evaluates to since 21 < 22.", "word_count": 615, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6e980fc2-eb55-52cc-85c2-02971f8b1468", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 25, "real_page_number": null, "text": "1.6 Function Objects and Comparators\n13\nHowever, defining the comparator based on age alone may not be enough. What if we have two students with the same age? How would we\norder these students? If we order students with the same age by name, how would we deal with students with the same name age? Luckily,and\nwe can build a comparator that takes in many levels of comparisons to determine the correct ordering of students. To illustrate this, suppose we\nwanted to build a comparator that enforces the following ordering:\n• Students are ordered by age first, with younger students coming before older students.\n• If two students have the same sage, ties are broken by name (ordered alphabetically).\n• If two students have the same name age, ties are broken by student ID (which is guaranteed to be unique).and\nLet’s look at our original comparator that only considers age:\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_t id;\n5\n};\n6\n7\nstruct StudentComparator {\n8\nbool operator() (const const constStudent& lhs, Student& rhs) {\n9\nreturn lhs.age < rhs.age;\n10\n} // operator()()\n11\n};\nWe want to modify this comparator so that, if two students have the same age, the comparator compares their names instead. This can be done\nif name Studentby adding an check so that is considered if the values of the two objects’ ages are the same.\n1\nstruct StudentComparator {\n2\nbool operator() (const const constStudent& lhs, Student& rhs) {\n3\nif (lhs.age == rhs.age) {\n4\nreturn lhs.name < rhs.name; // if ages are equal, use name to determine order\n5\n} // if\n6\nreturn lhs.age < rhs.age;\n7\n} // operator()()\n8\n};\nWe can add one final check if both name and age are the same. In this case, the comparator should compare the IDs of the students:\n1\nstruct StudentComparator {\n2\nbool operator() (const const constStudent& lhs, Student& rhs) {\n3\nif (lhs.age == rhs.age) {\n4\nif (lhs.name == rhs.name) {\n5\nreturn lhs.id < rhs.id; // if names are also equal, ID determines order\n6\n} // if\n7\nreturn lhs.name < rhs.name;\n8\n} // if\n9\nreturn lhs.age < rhs.age;\n10\n} // if\n11\n};\nStudent age name, id.We have successfully built a comparator that can order two objects, based on first, then then If we sorted a collection\nStudentof objects using this comparator, all the students would be first sorted in increasing order of age. Students with the same age would\nthen be sorted in alphabetical order, and students with the same name and age would be sorted in increasing order of student ID.\n1\nStudent s1{\"Alice\", 20, 12345678};\n2\nStudent s2{\"Alice\", 21, 23456789};\n3\nStudent s3{\"Alice\", 21, 34567890};\n4\nStudentComparator comp;\n5\nstd::cout << comp(s1, s2) << std::endl;\n// evaluates to true, since s1 < s3\n6\nstd::cout << comp(s2, s3) << std::endl;\n// evaluates to true, since s2 < s3\nFunction objects are also useful in ways beyond sorting a container of custom objects. Comparators and other functors can be integrated with\nmany common standard library functions (which we will cover in greater depth in chapter 11). For instance, suppose you are given a container\nof objects, and you wanted to remove all objects in the container that satisfy a given condition. To accomplish this, you can create a function\ntrue falseobject that returns if an object satisfies the condition required for removal and otherwise. Then, you can pass this function object\ninto a C++ standard library algorithm that can help remove all objects within a range that satisfy the condition.", "word_count": 615, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "360dead2-12c3-516d-9a93-475baa364cdf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 26, "real_page_number": null, "text": "14\nChapter 1. Programming Foundations\nstruct class,Because function objects can be implemented as a or they can be used to hold state through the use of member variables. For\ninstance, consider the following function object:\n1\nstruct Student {\n2\nstd::string name;\n3\ndouble exam_score;\n4\n};\n5\n6\nstruct ExamPass {\n7\nprivate:\n8\ndouble threshold;\n9\n10\npublic:\n11\nExamPass(double thres_in) : threshold{thres_in} {}\n12\n13\nbool operator() (const constStudent& s) {\n14\nreturn s.exam_score > threshold;\n15\n} // operator()()\n16\n};\ntrue falseThis function object returns if a student’s exam score is higher than some given threshold and otherwise. In this example, we are\nable to set the threshold to any value we want when instantiating the function object:\n1\nint main() {\n2\nStudent s{\"Alice\", 45};\n3\nExamPass pass{50};\n// threshold set to 50\n4\nstd::cout << pass(s) << std::endl;\n// prints 0 for false\n5\n} // main()\n1\nint main() {\n2\nStudent s{\"Alice\", 45};\n3\nExamPass pass{40};\n// threshold set to 40\n4\nstd::cout << pass(s) << std::endl;\n// prints 1 for true\n5\n} // main()\n1.7\nScope and Namespaces\n¸ 1.7.1\nScope\nWhen you declare a variable, function, or type in C++, where can you use it? The answer to this is determined by the scope of the named object\nin question. The scope of an object is the region of the program in which a named object can be used. An object’s scope is defined at compile\ntime, so attempting to use an object that is out of scope will result in a compile error.\nMost of the variables you will encounter in this class will have local, or block scope. The scope of a variable with local scope (which we\ncall a variable) begins when the variable is created, and it ends at the closing brace that concludes the block of code that the variable waslocal\nvaldefined in. For instance, the variable can only be used after the point of declaration, but within the curly braces that it is declared in (i.e.,\nafter line 4, but before the closing brace on line 6):\n1\nint main() {\n2\n// cannot use val here (not declared)\n3\n{\n4\nint32_t val = 281;\n5\n// can use val here (within scope)\n6\n}\n7\n// cannot use val here (out of scope)\n8\n} // main()\nIn a single scope, each variable name must uniquely represent a single entity. That is, there cannot be more than one object within the same\nscope that share the same name:\n1\nint main() {\n2\nint32_t val = 281;\n3\nint32_t val = 370; // not allowed!\n4\n} // main()\nHowever, if there are nested inner scopes within an outer scope, an inner scope can reuse a name that exists in its outer scope to reference a\ncompletely different entity. When this happens, the inner scope will only have access to the variable that it declared within its own scope, and\nnot the variable with the same name in an outer scope:\n1\nint main() {\n2\nint32_t val = 281;\n3\n{\n4\n// inner scope\n5\nint32_t val = 370;\n6\nstd::cout << val << std::endl;\n// prints 370\n7\n}\n8\nstd::cout << val << std::endl;\n// prints 281\n9\n} // main()", "word_count": 560, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0cedbc04-813e-5c0b-abc4-7c375fecaa57", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 27, "real_page_number": null, "text": "1.7 Scope and Namespaces\n15\nVariables that are initialized within the introduction of a statement (such as the variables created in a loop condition) are local to the block\ni for 281. ithey introduce. For example, the value of within the loop is equal to the running counter and not the value This is because the\nfor fordeclared in the introduction of the loop is local to the scope of the loop, which is the region between the curly braces on lines 3 and 5.\n\"0 1 2 3 4\" \"281 281 281 281 281\".Thus, the following code prints out instead of\n1\nint main() {\n2\nint32_t i = 281;\n3\nfor (int32_t i = 0; i < 5; ++i) {\n4\nstd::cout << i << \" \";\n5\n} // for i\n6\n} // main()\nclass struct.Scope also applies to items within a or The scope of any member of a class extends throughout the entire class, regardless of\nwhere the member is declared. Similar to block scope, each name within a class can only be used to represent a single member, regardless of\nwhether that member is a variable or a function.\n1\nclass MyClass {\n2\nint32_t sum;\n3\n4\n// not allowed, since sum is the name of another member of the same class\n5\nint32_t sum(int32_t int32_ta, b) {\n6\nreturn a + b;\n7\n} // sum()\n8\n};\nIf an object is declared outside any class or function, the object has global scope. The scope of a variable with global scope (which we call a\nvariable) begins when the variable is created and extends all the way to the end of the file in which it is declared. The scope of a globalglobal\nexternvariable can also be extended to other files in a project using the keyword (which you won’t need to know about for this class).\n1\nint32_t VAL = 281;\n// global variable\n2\n3\nvoid func() {\n4\nstd::cout << VAL << std::endl;\n5\n} // func()\n6\n7\nint main() {\n8\nVAL = VAL + 1;\n// VAL is now 282\n9\nfunc();\n// prints 282\n10\n} // main()\n¸ 1.7.2\n(✽)Namespaces\nBecause entities in the same scope cannot share the same name, you could have a problem if you have a large project that declares many different\nfunctions, variables, and types. This can be a nuisance if you are working with a codebase that depends on a lot of external libraries, since\nthere is a chance you will end up with multiple variables across different libraries that share the same name. For example, suppose you have a\ndo_something(),project that utilizes two different libraries, library A and library B. Library A has a global function called which you use\ndo_something()in your program. Now, suppose you upgrade to a new version of library B, which introduced its own function in its global\nnamespace. As a result, you have a naming conflict, and your entire project would no longer compile!\nTo resolve this issue, we can use to group entities that otherwise would have been part of the same global scope into smallernamespaces\nscopes. This smaller scope is known as a scope. An example is shown below:namespace\n1\nnamespace foo\n2\n{\n3\nvoid do_something() { /*...*/ }\n4\n} // namespace foo\n5\n6\nnamespace bar\n7\n{\n8\nvoid do_something() { /*...*/ }\n9\n} // namespace bar\n10\n11\nint main() {\n12\nfoo::do_something();\n// run version of do_something() in foo namespace\n13\nbar::do_something();\n// run version of do_something() in bar namespace\n14\n} // main()\nNamespaces are useful for preventing name collisions, as shown above. To specify a particular namespace scope, you can use the scope\nusing(::). Alternatively, you can avoid this by using the keyword:resolution operator\n1\nint main() {\n2\n{\n3\nusing namespace foo;\n4\ndo_something();\n// runs foo::do_something()\n5\n}\n6\n{\n7\nusing namespace bar;\n8\ndo_something();\n// runs bar::do_something()\n9\n}\n10\n} // main()", "word_count": 682, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab8e0ced-7ca0-5fde-9e80-250c4d3db238", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 28, "real_page_number": null, "text": "16\nChapter 1. Programming Foundations\nYou can also specify which names you want to take from each namespace:\n1\nint main() {\n2\n{\n3\nusing foo::do_something;\n4\ndo_something();\n// runs foo::do_something()\n5\n}\n6\n{\n7\nusing bar::do_something;\n8\ndo_something();\n// runs bar::do_something()\n9\n}\n10\n} // main()\nusing namespace std;You may have recognized namespaces from the term which is included at the top of many source files you have\nstdseen so far. This is because all files in the C++ standard library implement their functionality within the namespace! This is why you\nstd:: std::string std::vector<> stdneed to prepend in front of standard library entities such as and if you do not specify the\nboostnamespace otherwise. There are other namespaces that are available; for example, the Boost library can be found in the namespace.\nusing namespace std;Remark: You should avoid including the line in a header file! Even though it may save you some typing time,\nblanket including a namespace can be dangerous — if anyone else includes your header file in their code, you would essentially force them to\nstd.use all the namespaces you included, which could render their code unusable. This rule also applies for other namespaces, not just\n1.8\nCommon C++ Keywords\n¸ 1.8.1\nconstThe Keyword\nconstIn the previous few sections, the keyword was used to qualify certain variables and functions. A variable that is qualified using the\nconst keyword cannot be modified.\nconst int32_t MAX_VALUE = 281;\nMAX_VALUE = 5; // not allowed!\nconstThe keyword can also be used with pointers. Consider the following pointer declaration:\nint32_t* new int32_t{281};num =\nnumThere are two things we can do to modify this pointer. First, we could modify the content that the pointer points to (e.g., change so that it\n280 281):points to a value of instead of\n*num = 280;\nWe can also change the memory address that the pointer is pointing to:\nint32_t* new int32_t{280};num2 =\nnum = num2;\nconstIf the keyword is placed before the pointer symbol, you prevent the that the pointer points to from being modified. However,content\nchanging the pointer itself is allowed.\nconst int32_t* new int32_t{281};num =\n*num = 280; // not allowed!\nconstYou can also add the keyword after the pointer symbol. This would allow you to change the content that the pointer points to, but it\ndisallows you from reassigning the pointer to another address.\nint32_t* const new int32_t{281};num =\nnullptr;num = // not allowed!\nconstTo prevent modification of the content of the pointer and the pointer itself, you would need to add in both places:both\nconst int32_t* const new int32_t{281};num =\n*num = 280;\n// not allowed!\nnullptr;num = // not allowed!\nconstRemark: One of the biggest stylistic debates in C++ involves the placement of the modifier relative to the type that needs to be kept\nconstant. There are two primary camps: west and east const. West const (or const west) is the format used in the notes so far, where the\nconst qualifier is placed to the left of what it modifies:\n// const int (int cannot be modified)\nconst int32_t val = 281;\n// const int reference (int cannot be modified)\nconst int32_t& ref = val;\n// pointer to const int (int cannot be modified, but pointer can)\nconst int32_t* ptr1 = &val;\n// const pointer to int (pointer cannot be modified, but int can)\nint32_t* const ptr2 = &val;\n// const pointer to const int (neither point nor int can be modified)\nconst int32_t* const ptr3 = &val;", "word_count": 607, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f80cbacc-e6ff-53e0-a5f1-60ff8fa0a5cc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 29, "real_page_number": null, "text": "1.8 Common C++ Keywords\n17\nWest const is more widespread, but it is less consistent and requires you to remember rules regarding which object is const and which isn’t.\nconstThis is where east const comes into play. With east const, the keyword is placed to the right of what it modifies:\n// const int (int cannot be modified)\nint32_t const val = 281;\n// const int reference (int cannot be modified)\nint32_t const& ref = val;\n// pointer to const int (int cannot be modified, but pointer can)\nint32_t const* ptr1 = &val;\n// const pointer to int (pointer cannot be modified, but int can)\nint32_t* const ptr2 = &val;\n// const pointer to const int (neither point nor int can be modified)\nint32_t const* const ptr3 = &val;\nconstHere, thereisoneconsistentrule: thetypethatcannotbemodifiedcanalwaysbedeterminedbylookingatwhatisleftofthe keyword.\nint32_t const* int32_t int32_t const.For instance, indicates that the cannot be modified, since is what is left of Similarly,\nint32_t* const int32_t* const.indicates that the (i.e., the pointer) cannot be modified, since it is left of\nWhether you use west or east const is a matter of preference, and there are arguments for both sides. The recommended suggestion,\nhowever, is to be consistent with the style you use. If you are writing a program, try to avoid mixing and matching the two styles in your\ncode, since that typically makes your code less readable. The same applies when working with existing code in a large codebase; if you are\nmodifying an existing file, try to adhere to the same stylistic choices that are used (any style changes should be reflected in the entire file).\nFor these notes, west const will be used. This is not because west const is inherently superior; rather, it is because this style is more\nprevalent in existing codebases, so it is important for you to be familiar with how it works. However, feel free to use east const in your own\ncode if you feel that it is the better style.\nStudentLet’s go back and consider the following object that we defined earlier:\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_t id;\n5\nbool operator<(const constStudent& rhs) {\n6\nreturn age < rhs.age;\n7\n} // operator<()\n8\n};\noperator< const: rhs,Notice how our overloaded operator has two uses of one in the type definition of and one right before the starting\nconst?curly brace of the function definition. What is the purpose of each of these uses of\nStudent rhs Student operator<Here, we pass the object by reference. This allows us to access the object in our overloaded\nStudent Studentfunction without making a separate copy of a object (which would have happened had the been passed by value). If you\nare trying to pass a large object into a function, you generally want to pass it in by reference. This is because passing by value would require you\nrhsto make a copy of the object every time the function is called, which is inefficient in terms of both time and memory. By specifying that is\nconst Student, rhs operator<) rhs.a we indicate that the function is passed into (in this case, modify the contents ofcannot\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_t id;\n5\nbool operator<(const constStudent& rhs) {\n6\nrhs.age = 20; // not allowed!\n7\nreturn age < rhs.age;\n8\n} // operator<()\n9\n};\nconst constWhat about the second keyword that occurs after all the parameters? If this keyword is specified for a member function of an\nobject, it indicates that itself. In the case of the overloadedthe member function is not allowed to modify any member variables of the object\noperator< const operator< Studentfunction, the second indicates that the function cannot modify any member variables of the\nname, age, id).object that invokes it (i.e., and\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_t id;\n5\nbool operator<(const constStudent& rhs) {\n6\nage = 20; // not allowed!\n7\nreturn age < rhs.age;\n8\n} // operator<()\n9\n};", "word_count": 713, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "54a4ffea-3ca1-5e59-afe4-8dd5cf30e542", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 30, "real_page_number": null, "text": "18\nChapter 1. Programming Foundations\nIf you have a member function of an object that does not modify the member variables of the object itself, it is important to specify the member\nnon-constconst.function as This is important, not only because it prevents accidental changes to the object, but also because member\nnon-const constfunctions can only be called by objects! On the other hand, member functions can be called by any type of object,\nconst.regardless of whether the object itself is Consider the following code:\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_t id;\n5\nbool operator<(const Student& rhs) {\n6\nreturn age < rhs.age;\n7\n} // operator<()\n8\n};\n9\n10\nvoid print_smaller_age(const constStudent& s1, Student& s2) {\n11\nif (s1 < s2) {\n12\nstd::cout << s1.age << std::endl;\n13\n} // if\n14\nelse {\n15\nstd::cout << s2.age << std::endl;\n16\n} // else\n17\n} // print_smaller_age()\n18\n19\nint main() {\n20\nStudent s1{\"Alice\", 20, 12345678};\n21\nStudent s2{\"Bob\", 21, 87654321};\n22\nprint_smaller_age(s1, s2);\n23\n} // main()\nprint_smaller_age()The function prints out the smaller age of two students that are passed in. However, this code does not work\noperator< const. s1 s2 const Student operator<because is not defined as Since and are both objects, cannot be used to compare\noperator< const. operator<the two students since the function itself is not declared as The compiler doesn’t know whether will modify\nnon-const, const Student s1 operator<any member variables since it is defined as so when attempts to call its overloaded function,\ns1the compiler prevents it from happening since it cannot guarantee that will not be modified.\noperator< const, constTo fix this issue, simply define the overloaded function as so that the function can be called by both and\nnon-const Student objects.\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\nint32_t id;\n5\nbool operator<(const constStudent& rhs) { // specify function as const\n6\nreturn age < rhs.age;\n7\n} // operator<()\n8\n};\n9\n10\nint main() {\n11\nStudent s1{\"Alice\", 20, 12345678};\n12\nStudent s2{\"Bob\", 21, 87654321};\n13\nprint_smaller_age(s1, s2); // successfully prints 20\n14\n} // main()\n¸ 1.8.2\n(✽)constexprThe Keyword\nconstexpr,Another keyword that you may encounter is which stands for constant expression. This keyword should be applied to values\nconstexprthat are constant and compilation. When used appropriately, may improve the performance of a program, sinceknown during\nconstexprcomputing a value during compile time removes the computation work from runtime. There are also cases where is needed to\nperform functionality that is not allowed using runtime values, such as declaring the size of an array, as shown below.\n1\nint main() {\n2\nconstexpr size_t array_size = 281;\n3\nint32_t my_arr[array_size]; // array of size 281\n4\n} // main()\nconstexpr constexprThe keyword is particularly interesting when applied to functions. A function returns compile-time constants if the\narguments it is called with are also compile-time constants. Otherwise, if any one of its arguments has an unknown value during compilation,\ndegrees_to_radians()it behaves like a normal function and computes its result during runtime. For instance, consider the following\nconstexpr:function, which is qualified as\n1\nconstexpr double pi = 3.14159265358979323846;\n2\nconstexpr double degrees_to_radians(double angle) {\n3\nreturn angle (pi / 180.0);*\n4\n} // degrees_to_radians()", "word_count": 567, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3ba449d7-9876-506d-a132-3e9da1d242a8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 31, "real_page_number": null, "text": "1.8 Common C++ Keywords\n19\nangleIf the value of is known during compile time, the result of the function will also be calculated during compile time (i.e., when the\nangleprogram compiles). If the value of is not known during compile time, the result of the function will be calculated during runtime.\n1\nint main() {\n2\nconstexpr double radian1 = degrees_to_radians(57.296); // computed while compiling\n3\ndouble degree_from_file = read_value_from_file();\n4\ndouble radian2 = degrees_to_radians(degree_from_file); // computed during runtime\n5\n} // main()\nconstexpr.User-defined types can also be determined during compilation if their constructors and member functions are qualified as An\nexample of such a class declaration is shown below:\n1\nclass Foo {\n2\nint32_t i;\n3\ndouble j;\n4\npublic:\n5\nconstexpr Foo(int32_t doublei_in, j_in)\n6\n: i{i_in}, j{j_in} {}\n7\n8\nconstexpr int32_t constget_i() {\n9\nreturn i;\n10\n} // get_i()\n11\n12\nconstexpr double constget_j() {\n13\nreturn j;\n14\n} // get_j()\n15\n};\n16\n17\nconstexpr add_foos(const constFoo Foo& f1, Foo& f2) {\n18\nreturn Foo{f1.get_i() + f2.get_i(),\n19\nf1.get_j() + f2.get_j()};\n20\n} // add_foos()\n21\n22\nint main() {\n23\n// all done during compile time\n24\nconstexpr Foo f1{183, 2.8};\n25\nconstexpr Foo f2{281, 3.7};\n26\nconstexpr Foo f3 = add_foos(f1, f2); // f3 = Foo{464, 6.5};\n27\n} // main()\nFoo constexpr. FooHere, the constructor is marked as Thus, if the arguments of the constructor are known during compilation, a object\nconstexpr Foo::get_i()can be constructed at compile time if qualified as (as shown on lines 23 and 24). Similarly, because and\nFoo::get_j() constexpr constexpr Fooare functions, their return values are computed during compilation if they are invoked on a\nconstexpr Foo add_foos()object. This allows us to write functions that can return objects using these getter functions, as shown by the\nmain()function on line 17. Putting this all together, the above code in to be computed entirely while compiling the program (rather than\nduring program runtime)! In general, migrating computations from runtime to compile time will make a program run faster (and is often a\npreferable decision to make, even if it comes at the cost of longer compilation times).\n¸ 1.8.3\nstaticThe Keyword\nstaticThe keyword has three main use cases: it can be used to define\n1. a variable inside a function\nclass struct2. a variable inside a or definition\n3. a global variable that is defined in one file of a multifile program\nstatic staticA is a local variable declared with the keyword within a function. If a variable is declared as in astatic local variable\nfunction, there exists only one copy of the variable, and it remains in memory and maintains its value for the entire duration of the program. In\nfoo() counter.the example below, the function calls to all use the same instance of\n1\nint32_t foo() {\n2\nstatic int32_t counter = 0;\n3\ncounter += 1;\n4\nreturn counter;\n5\n} // foo()\n6\n7\nint main() {\n8\nstd::cout << foo() << std::endl; // prints 1\n9\nstd::cout << foo() << std::endl; // prints 2\n10\nstd::cout << foo() << std::endl; // prints 3\n11\n} // main()\nstaticIf is used to define a member variable of an object, that member variable has the same value for any instance of that object. In other\nstatic class structwords, without the keyword, each object of the or would have of the member variable. However, if aits own copy\nstatic,member variable is declared as its value is shared by all instances of the object.", "word_count": 610, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cd88648e-c0f3-5e37-aed5-001b2bcfb1b4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 32, "real_page_number": null, "text": "20\nChapter 1. Programming Foundations\nstatic (::)In order to access a member variable outside the class, the scope-resolution operator must be used. An example is shown below:\n1\nstruct Foo {\n2\nstatic int32_t counter;\n3\nvoid print_count() {\n4\ncounter += 1;\n5\nstd::cout << counter << std::endl;\n6\n} // print_count()\n7\n};\n8\n9\nint32_t Foo::counter = 0; // initializes counter for all Foo objects to 0\n10\n11\nint main() {\n12\nFoo f1;\n13\nFoo f2;\n14\nf1.print_count();\n// prints 1\n15\nf2.print_count();\n// prints 2\n16\n} // main()\nstatic staticLastly, it is also possible to use the keyword before a global variable. By doing so, the scope of the global variable is limited\nto the file it is defined in — code in other files cannot access the variable, even if they are part of the same project.\n¸ 1.8.4\nmutableThe Keyword\nconst,As mentioned previously, if a member function of an object is declared as it cannot modify any of the object’s own member variables.\nHowever, in some cases, we may want a member variable to be modified even if the member function we call promises not to change anything.\nConsider the following:\n1\nclass ProjectScoreTracker {\n2\ndouble project_average;\n// current project average\n3\n...\n// other data\n4\nint32_t request_counter;\n// counts number of times scores are requested\n5\npublic:\n6\nProjectScoreTracker() : request_counter{0} {}\n7\nvoid constrequest_scores() {\n8\n// prints out project score data\n9\n...\n10\n// update request counter\n11\n++request_counter;\n12\n} // request_scores()\n13\n};\nProjectScoreTracker request_counterThe class keeps track of data for an EECS project, and the member variable keeps track of\nrequest_scores()the number of times that project scores are fetched. However, this code does not compile, since is attempting to modify\nconst. consta class member variable even though the function is declared as If we try to remove the declaration from the function, then it\nProjectScoreTracker const.cannot be called if the we want to call the function on is\nmutable mutable, constOne way to fix this issue is to use the keyword. If you declare a member variable of an object as you give\nmember functions of that object permission to modify that member variable. The following code would compile:\n1\nclass ProjectScoreTracker {\n2\ndouble project_average;\n// current project average\n3\n...\n// other data\n4\nmutable int32_t request_counter;\n// counts number of times scores are requested\n5\npublic:\n6\nProjectScoreTracker() : request_counter{0} {}\n7\nvoid constrequest_scores() {\n8\n// prints out project score data\n9\n...\n10\n// update request counter\n11\n++request_counter;\n12\n} // request_scores()\n13\n};\nrequest_counter mutable, request_scores()Here, by declaring the member variable as we give the function permission to\nproject_average)modify it. The function, however, cannot modify any of the other member variables (such as since those variables are\nmutable.not declared as\nmutableObviously, this example is a bit contrived, and there are ways to get around this error without relying on the keyword. However,\ndepending the projects that are assigned during your semester, there may be instances where knowledge of this keyword will be useful. One\nsuch scenario occurs if you have a priority queue of custom objects, and you want to modify a member variable of the object at the top of the\npriority queue in a way that does not disrupt its priority (we will cover priority queues in chapter 10, so there is no need to worry about this\nconstinformation yet if you do not know what it is). However, the STL’s priority queue returns a reference to its top element, and you cannot\nmodify the element at the top without having to pop it first, modify it, and then push it back into the queue. By defining the member variable\nmutable,you want to modify as you can avoid having to pop and repush in the object you want to modify and instead update it in-place (but\nthis is only safe as long as your modification does not disrupt the existing priority of objects in your priority queue).\nmutableThe keyword can used for variables that do not keep track of the actual data of an object, but rather other useful information\nabout the data that can be modified. Because of this, if both the information about the data and the data itself are encapsulated within the same\nmutableclass, allows the former to be modified without breaking the invariant that the latter cannot.", "word_count": 757, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "957f3e3f-7abe-567c-bf42-f40dad953d63", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 33, "real_page_number": null, "text": "1.8 Common C++ Keywords\n21\n¸ 1.8.5\nexplicitThe Keyword\nIf an object has a constructor that takes in a single argument with a type from the type of the object itself, then the constructor is knowndifferent\nas a constructor. For example, consider the following:conversion\n1\nclass Foo {\n2\nint32_t foo_x;\n3\npublic:\n4\nFoo(int32_t input) : foo_x{input} {}\n5\n};\nfoo_x.Here, the constructor on line 4 takes in an integer and assigns its value to the member variable As a result, if you attempt to use the\nFoo foo_xconversion constructor to initialize a object from an integer, the constructor simply sets the value of to that integer:\nFoo myFoo{3}; // sets the value of foo_x to 3\nA conversion constructor is perfectly fine if the programmer wants to convert objects of one type to objects of another type. However, single-\nargument conversion constructors can be inappropriately used by the compiler to make scenes, or typeimplicit type conversions behind the\nconversions that are made by the compiler without prompt from the user. The compiler is not the programmer, so it shouldn’t make decisions on\nthe programmer’s behalf! There are cases where an implicit conversion goes against the intents of the programmer, and unwanted implicit\nconversions are often the toughest bugs to track down.\nThe following is an example of an implicit conversion. The programmer doesn’t directly call the constructor to make the conversion, but the\nFoocompiler does the conversion on the programmer’s behalf because it thinks the programmer intended to convert an integer to a object:\nFoo myFoo = 3; // compiler uses ctor to convert 3 to object of type Foo\nexplicitThe keyword can be used to remove control from the compiler and give it back to the programmer. If a single argument constructor\nexplicit,is defined as then the programmer must call that constructor if they want to perform a conversion from one type toexplicitly\nanother. In other words, the compiler cannot use the explicit constructor to make implicit conversions behind the scenes, without the approval of\nFoothe programmer. Using the above example, the following redefinition of the class prevents unintended implicit conversions:\n1\nclass Foo {\n2\nint32_t foo_x;\n3\npublic:\n4\nexplicit Foo(int32_t input) : foo_x{input} {}\n5\n};\nexplicitAfter the keyword is added, the following will be true:\nFoo myFoo{3};\n// okay since ctor explicitly called, so foo_x is set to 3\nFoo myFooToo = 3;\n// NOT okay: compiler error since implicit conversion\nexplicitBelow is an practical example where failing to include can cause a major problem. Consider the following class, which implements\na custom string object:\n1\nclass CustomString {\n2\n...\n// member variables\n3\npublic:\n4\nCustomString(int32_t n);\n// constructs CustomString with size n\n5\nCustomString(const char* p);\n// initialize CustomString with char*\n6\n...\n7\n};\n8\n9\nvoid print(CustomString c);\n// prints the CustomString\nCustomString CustomStringSuppose there are two ways to construct this object. If an integer is passed into the constructor, a is created\nconst char* CustomStringwith its size set to that integer (using the constructor on line 4). If a is passed into the constructor, a is created\nconst char* print() CustomStringwith the contents of the (using the constructor on line 5). The function prints the contents of the\nit is given. Now, suppose a programmer calls the following:\n1\n// print \"EECS 281\"\n2\nprint(\"EECS\");\n3\nprint(281);\nprint() \"EECS\" print()Whathappenshere? Thefirst linerunswithoutissue,and isprintedout. However,thesecond linecontainsabug:\nprint(\"281\") print()the programmer intended to call but forgot the quotation marks! The compiler notices that requires an argument of\nCustomString, int32_t.type but it instead received an argument of type But wait! There is a single-argument constructor defined on line\nCustomString explicit!4 that can construct a using an integer, and this constructor is not As a result, the compiler implicitly converts\n281 CustomString CustomString print()the into a using the constructor on line 4 and passes the resulting into the function. This\nCustomString CustomString \"281\",actually ends up constructing an empty with size 281 and not a initialized to so the program ends\nup printing an empty string of size 281. This was not the intent of the programmer, and the compiler does not issue any errors that may notify\nthe programmer of this bug.", "word_count": 736, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4e61efed-df79-5ca3-8210-84363ff136e3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 34, "real_page_number": null, "text": "22\nChapter 1. Programming Foundations\nexplicitDefining the constructor on line 4 as would prevent this from happening. Without this keyword, the compiler would be able to\nmake implicit conversions without any knowledge of the programmer, which can make debugging much more painful!\n1\nclass CustomString {\n2\n...\n// member variables\n3\npublic:\n4\nexplicit CustomString(int32_t n);\n// constructs CustomString with size n\n5\nCustomString(const char* p);\n// initialize CustomString with char*\n6\n...\n7\n};\n8\n9\nvoid print(CustomString c);\n// prints the CustomString\n1.9\nTemplates\n¸ 1.9.1\nTemplates\nC++ is a statically typed language, which means the types of variables must be known at compile time. The need for variable types to be fixed\nswap()at compile time brings up a notable issue: how can the same code be adapted for different types? Consider the following function:\n1\nvoid swap(int32_t& int32_t&a, b) {\n2\nint32_t temp = a;\n3\na = b;\n4\nb = temp;\n5\n} // swap()\ndoublesThis code allows you to swap the values of two integers that are passed in. However, what if you wanted to swap two instead? This\nint32_t double. swap()function would fail since a reference to cannot be set to a reference to As a result, this particular function would\nint32_t!be pretty useless on anything that is not an\nswap()Insteadofrewritingthe functiononallpossibletypes,wecaninsteadusetemplates. Templatesallowustodo programming:generic\nwriting code that applies to many different types. The compiler uses the templates to modify the code as needed for the type we want. To\nswap()illustrate how this works, let’s go back to the example we had before. To allow this function to work on all types, we can define the\nfollowing function template:\n1\ntemplate <typename T>\n2\nvoid swap(T& a, T& b) {\n3\nT temp = a;\n4\na = b;\n5\nb = temp;\n6\n} // swap()\nT THere, the letter represents the parameter. The compiler deduces what the type of actually is from the type that is passed into thetype\nTarguments of the function call, and it replaces the type parameter with what the type actually is. The compiler must see the template definition\nfirst, it is used in any code that you write! As an example, suppose you call the swap function with two integers:before\n1\nint32_t a = 5;\n2\nint32_t b = 6;\n3\nswap(a, b);\nint32_t swap() swap()The compiler sees from the function call that two objects of type were passed into the function, so it will use the\nT int32_t.template and replace all instances of with\n1\ntemplate <typename T>\n2\nvoid swap(T& a, T& b) {\n3\nT temp = a;\n4\na = b;\n5\nb = temp;\n6\n} // swap()\n1\n// Compiler deduces that T is int32_t\n2\nvoid swap(int32_t& int32_t&a, b) {\n3\nint32_t temp = a;\n4\na = b;\n5\nb = temp;\n6\n} // swap()\nswap()If you called with two doubles instead, the compiler would use the template to generate the following code:\n1\ntemplate <typename T>\n2\nvoid swap(T& a, T& b) {\n3\nT temp = a;\n4\na = b;\n5\nb = temp;\n6\n} // swap()\n1\n// Compiler deduces that T is double\n2\nvoid swap(double& double&a, b) {\n3\ndouble temp = a;\n4\na = b;\n5\nb = temp;\n6\n} // swap()\nA function template can have more than one type parameter, as shown below.\n1\ntemplate <typename typenameT1, T2>\n2\nvoid print_two_types(T1& a, T2& b) {\n3\nstd::cout << a << \" \" << b << std::endl;\n4\n} // print_two_types()\n5\n6\nint main() {\n7\nstd::string a = \"EECS\";\n8\nint32_t b = 281;\n9\nprint_two_types(a, b);\n10\n} // main()", "word_count": 652, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "af58921b-a639-5bd0-84b9-74ead944ce62", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 35, "real_page_number": null, "text": "1.9 Templates\n23\nT1 std::string T2 int32_t print_two_types()Here, the compiler deduces that is and is from the arguments of the function call.\n\"EECS 281\"It generates the code with the correct types using the template, and is printed.\nTemplates can also be used when defining classes. To create a class template, create a class with ordinary member variable data types, and\nchange them to type parameters in the same way as with function templates.\nCustomPairFor example, suppose we wanted to create a object that holds two objects of different types. Consider the following\nimplementation, which only supports objects where both items are integers:\n1\nstruct CustomPair {\n2\nint32_t first;\n3\nint32_t second;\n4\nCustomPair()\n5\n: first{}, second{} {}\n6\nCustomPair(int32_t int32_tfirst_in, second_in)\n7\n: first{first_in}, second{second_in} {}\n8\n};\nfirst secondIf we wanted the and member variables to support any type, we can use a class template. To do so, simply add a template\ndefinition before the class definition, and replace the types of the member variables with type parameters.\n1\ntemplate <typename typenameT1, T2>\n2\nstruct CustomPair {\n3\nT1 first;\n4\nT2 second;\n5\nCustomPair()\n6\n: first{}, second{} {}\n7\nCustomPair(T1 first_in, T2 second_in)\n8\n: first{first_in}, second{second_in} {}\n9\n};\nWhen instantiating an instance of a templated object, the types of the template type parameters need to be specified! For example, suppose you\nCustomPair,wanted to initialize a and you attempted to initialize one like this:\nCustomPair cp;\nCustomPair T1 T2Since is templated, there is no way for the compiler to deduce what and are! As a result, you will need to specify this\nCustomPair.information to create an instance of a To do so, add the correct types between angle brackets upon the object’s declaration, in\nthe same order as in the template definition. For instance, if the template definition is\ntemplate <typename typenameT1, T2>\nCustomPair T1 int32_t T2 double, CustomPairand you wanted to create a where is and is you would declare the as:\nCustomPair<int32_t, double> cp;\nCustomPair firstBy doing this, the compiler uses the provided template to generate the following definition of the object, where is of\nint32_t second double.type and is of type\nstruct CustomPair {\nint32_t first;\ndouble second;\nCustomPair()\n: first{}, second{} {}\nCustomPair(int32_t doublefirst_in, second_in)\n: first{first_in}, second{second_in} {}\n};\nClass templates are extremely useful for container classes, as it allows you to define generic containers that support many different object\nstd::vector<int>,types. For instance, when you declare a you are actually utilizing a class template to tell the vector that it should hold\nint.objects of type In fact, the C++ standard library (STL) — which you will be very familiar with after completing this course —template\nimplements many common data structures and algorithms in a templated manner.\n¸ 1.9.2\n(✽)Variadic Templates\nare templates that can take in a variable number of arguments. In C++, these templates are defined recursively: you processVariadic templates\nsome number of arguments, and then you recurse with the remaining elements. You will also need to implement a \"base case\" templated\nfunction call that will be invoked once there are no more arguments to recurse on. An example of variadic template syntax is shown below:\n1\ntemplate <typename T>\n2\nvoid print(T last_elt) {\n3\nstd::cout << last_elt << std::endl;\n4\n} // print()\n5\n6\ntemplate <typename typename...T, Args>\n7\nvoid print(T first_elt, Args... rem_elts) { // parameter pack\n8\nstd::cout << first_elt << \" \";\n9\nprint(rem_elts...); // parameter unpacking\n10\n} // print()\n11\n12\nint main() {\n13\nprint(\"a\", 5, 1 + 2, \"apple\"); // prints \"a 5 3 apple\"\n14\n} // main()", "word_count": 621, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c40960f5-931c-53d4-917b-bf2a9cb3087f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 36, "real_page_number": null, "text": "24\nChapter 1. Programming Foundations\nprint()Notice that we have two versions of the function: one that takes in a single argument (line 2) and one that takes in multiple arguments\nprint(), print()(line 7). If multiple arguments are passed into the second version of will be used, with each call processing the arguments\nprint(\"a\", 5, 1 + 2, \"apple\").one at a time. For example, let us consider the outcome of\n1\ntemplate <typename T>\n2\nvoid print(T last_elt) {\n3\nstd::cout << last_elt << std::endl;\n4\n} // print()\n5\n6\ntemplate <typename typename...T, Args>\n7\nvoid print(T first_elt, Args... rem_elts) { // parameter pack\n8\nstd::cout << first_elt << \" \";\n9\nprint(rem_elts...); // parameter unpacking\n10\n} // print()\nprint(), (\"a\") first_elt,Multiple arguments are passed into so the second function definition is chosen. The first argument becomes\n(5, 1 + 2, \"apple\") rem_elts. \"a\",while the remaining elements are packed in the variable Line 8 would therefore print out and line 9\nprint() 5, 1 + 2, \"apple\".would recursively call using the remaining arguments of and\n5\n// print(\"a\", 5, 1 + 2, \"apple\")\n6\ntemplate <typename typename...T, Args>\n7\nvoid print(T first_elt, Args... rem_elts) { // first_elt = \"a\", rem_elts = 5, 1 + 2, \"apple\"\n8\nstd::cout << first_elt << \" \"; // prints out \"a\"\n9\nprint(rem_elts...); // calls print(5, 1 + 2, \"apple\")\n10\n} // print()\nprint() 5,Since there are still multiple arguments remaining, the second definition of is called again. The next argument to unpack, becomes\nfirst_elt, 1 + 2 \"apple\" rem_elts. 5,and the remaining elements of and are stored in Line 8 prints out and line 9 recursively calls\nprint() 1 + 2 \"apple\".using the arguments of and\n5\n// print(5, 1 + 2, \"apple\")\n6\ntemplate <typename typename...T, Args>\n7\nvoid print(T first_elt, Args... rem_elts) { // first_elt = 5, rem_elts = 1 + 2, \"apple\"\n8\nstd::cout << first_elt << \" \"; // prints out 5\n9\nprint(rem_elts...); // calls print(1 + 2, \"apple\")\n10\n} // print()\n1 + 2, first_elt,This process repeats again for the two remaining arguments. The next argument to unpack, becomes and the remaining\n\"apple\", rem_elts. 3, print() \"apple\".argument, is stored in Line 8 prints out and line 9 recursively calls using the argument\n5\n// print(1 + 2, \"apple\")\n6\ntemplate <typename typename...T, Args>\n7\nvoid print(T first_elt, Args... rem_elts) { // first_elt = 3, rem_elts = \"apple\"\n8\nstd::cout << first_elt << \" \"; // prints out 3\n9\nprint(rem_elts...); // calls print(\"apple\")\n10\n} // print()\nprint() \"apple\"Notice that there is only one argument remaining! As a result, the first definition of (on line 2) is chosen, and is passed in\nlast_elt. \"apple\", \"a 5 3 apple\".as the value of Line 3 prints out and our initial function call is complete. The final output is\n0\n// print(\"apple\")\n1\ntemplate <typename T>\n2\nvoid print(T last_elt) { // last_elt = \"apple\"\n3\nstd::cout << last_elt << std::endl; // prints \"apple\"\n4\n} // print()\nThe \"base case\" function definition is important! Without it, the compiler would not know what to do when there are no more arguments to\nunpack, and your code would not compile.\nVariadic templated functions can also be used to process any number of arguments in a single call. The following is an example of a print\nfunction that prints out values in pairs:\n1\ntemplate <typename typenameT1, T2>\n2\nvoid print_pairs(T1 a, T2 b) {\n3\nstd::cout << a << \" \" << b << std::endl;\n4\n} // print_pairs()\n5\n6\ntemplate <typename typename typename...T1, T2, Args>\n7\nvoid print_pairs(T1 first, T2 second, Args... args) {\n8\nstd::cout << first << \" \" << second << std::endl;\n9\nprint_pairs(args...);\n10\n} // print_pairs()\n11\n12\nint main() {\n13\nprint_pairs(\"a\", 5, 1 + 2, \"apple\");\n14\n} // main()\nThe output of the above code is:\na 5\n3 apple", "word_count": 671, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "57f10368-cd86-543d-b3d8-6c488db3a3c7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 37, "real_page_number": null, "text": "1.10 Type Aliases\n25\nHowever, if you do handle multiple arguments at once, you need to make sure that there is always a matching call for each possible base case.\nprint_pairs()For instance, suppose we tried to call on an odd number of arguments:\n1\ntemplate <typename typenameT1, T2>\n2\nvoid print_pairs(T1 a, T2 b) {\n3\nstd::cout << a << \" \" << b << std::endl;\n4\n} // print_pairs()\n5\n6\ntemplate <typename typename typename...T1, T2, Args>\n7\nvoid print_pairs(T1 first, T2 second, Args... args) {\n8\nstd::cout << first << \" \" << second << std::endl;\n9\nprint_pairs(args...);\n10\n} // print_pairs()\n11\n12\nint main() {\n13\nprint_pairs(\"a\", 5, 1 + 2, \"apple\", \"sauce\");\n14\n} // main()\nprint_pairs()The above code would invoke the following versions of while unpacking the arguments:\nint, <int,print_pairs(std::string, std::string, std::string>);\n// second definition\nprint_pairs(int, std::string, <std::string>);\n// second definition\nprint_pairs(std::string);\n// no matching function call\nprint_pairs()There is no matching version of that takes in a single templated argument, so the third call would fail. The compiler would\ndetect this and issue an error if you tried to compile this code.\nRemark: As another example, below is a templated function that prints out the contents of a priority queue, which is a C++ standard library\ncontainer that allows you to access the highest priority element at any point in time (do not worry about priority queues yet if you are not\nfamiliar with them; this remark will make more sense after you have covered them in class). Since priority queues can support many template\nprint_priority_queue()parameters, you cannot template out just the element type if you want the method to accept any type of\npriority queue. Instead, you can use variadic templates to handle these additional template parameters, as shown:\n1\ntemplate <typename typename...Type, Params>\n2\nvoid constprint_priority_queue(std::ostream& os, std::priority_queue<Type, Params...>& pq) {\n3\nstd::priority_queue<Type, Params...> pq_to_print = pq;\n4\nwhile (!pq_to_print.empty()) {\n5\nos << pq_to_print.top() << \" \";\n6\npq_to_print.pop();\n7\n} // while\n8\n} // print_priority_queue()\n9\n10\nint main() {\n11\nstd::vector<int32_t> nums = {281, 280, 376, 203, 183, 370};\n12\nstd::vector<std::string> strs = {\"banana\", \"cherry\", \"apple\"};\n13\n14\nstd::priority_queue<int32_t> max_pq{nums.begin(), nums.end()};\n15\nstd::priority_queue<int32_t, std::vector<int32_t>, std::greater<int32_t>>\n16\nmin_pq{nums.begin(), nums.end()};\n17\nstd::priority_queue<std::string, std::deque<std::string>, std::greater<std::string>>\n18\nstr_pq{strs.begin(), strs.end()};\n19\n20\n// the templated print_priority_queue() method can be used on all three priority queues\n21\nprint_priority_queue(std::cout, max_pq);\n// prints \"376 370 281 280 203 183 \"\n22\nprint_priority_queue(std::cout, min_pq);\n// prints \"183 203 280 281 370 376 \"\n23\nprint_priority_queue(std::cout, str_pq);\n// prints \"apple banana cherry \"\n24\n} // main()\n1.10\nType Aliases\nallowsyoutoassignkeywordstodifferenttypesinC++. Usingaliasescanmakeprogrammingeasierandmoreefficient, especiallyTypealiasing\nwhen type names become extremely long.\nfirst int32_t) secondSuppose you have a pair, where the value stores a student’s ID (as an and the value stores a pair object\ncontaining the first and last names of this student:\nstd::pair<int32_t, std::pair<std::string, std::string>>\nThis would be nasty to type out every time you wanted to create an object of this type! To address this, type aliasing allows you to give an alias\nStudentPair,to the type. For example, if you wanted to give the above type the alias you would be able to declare objects of this type with\nits alias instead. In other words,\nstd::pair<int32_t, std::pair<std::string, std::string>> my_student;\ncould instead be initialized using\nStudentPair my_student;\nStudentPair std::pair<int32_t, std::pair<std::string, std::string>>.where is another name for", "word_count": 587, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "573cbf0f-f74a-5db7-abbd-f4afbb5c2265", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 38, "real_page_number": null, "text": "26\nChapter 1. Programming Foundations\nusing,To define an alias, simply use the keyword as follows:\nusing <ALIAS> = <ORIGINAL TYPE NAME>;\nStudentPairThefollowingwouldcreatethealias forthetype std::string>>.std::pair<int32_t, std::pair<std::string,\nusing std::pair<int32_t,StudentPair = std::pair<std::string, std::string>>;\nIt is perfectly fine to assign multiple aliases to a single type. For instance, you could do this:\nusing Uniqname = std::string;\nusing FirstName = std::string;\nusing LastName = std::string;\nstd::pair<Uniqname, std::pair<FirstName, LastName>> myStudent = ...\nUniqname, FirstName, LastName std::stringIn this case, and are all aliases for the type.\n1.11\nAutomatic Type Deduction\n¸ 1.11.1\nautoThe Keyword\nauto autoBeginning with C++11, you can declare a variable without specifying its type if you use the keyword in place of the type. If is\nautoused to declare a variable, the type of the variable is deduced from its initialization value. Because of this, a variable declared with must\nj int.also be initialized to a value. In the following example, the type of is deduced from the initialization value of 281, which is an\nint i = 281;\n// i is int\nauto j = 281;\n// j is int\nauto& k = i;\n// k is int&\nautoThe keywordcanbeadouble-edgedsword. Usingtypedeductioncan makeyourcodecleaner, butitcanalsomakeithardertounderstand,\nautoespecially if masks out a type that is not trivial to find (e.g., an ambiguous function return type defined in a distant section of code). In\ntype.3general, you should use type deduction to make your code clearer or safer, and not just to avoid the inconvenience of writing an explicit\nautoSome types (such as iterator types in a loop) can be replaced with without issue. However, you should refrain from such behavior for\nimportant types that may provide useful information for others, if the reason for doing so is for mere personal convenience.\n¸ 1.11.2\n(✽)Deducing Function Return Types\nSuppose you have a templated function whose return types depend on its parameters, as shown below. What should the return type be?\ntemplate <typename typenameT1, T2>\nreturn??? add(T1 a, T2 b) { a + b; }\nT1 + T2This is not apparent, since we do not know what the type of is. In C++11, we can address this problem by using the trailing return\ndecltypesyntax for defining a function, alongside the keyword. As you should be aware, the standard convention for defining a functiontype\nheader is to specify the return type the function name, as shown:before\nint32_t add(int32_t int32_t returnx, y) { a + b; };\nautoUsing the trailing return type syntax, we can preface the function name with the keyword and then specify the return type theafter\nfunction name and parameters:\nauto add(int32_t int32_t int32_t returnx, y) -> { a + b; };\nUnder this new syntax, we can elegantly express the return type of the function as follows:\ntemplate <typename typenameT1, T2>\nauto decltype(a returnadd(T1 a, T2 b) -> + b) { a + b; }\ndecltypeHere, is a specifier that can be used to query the type of an expression, and it is commonly used to signify types that depend on\ndecltype(a + b) a + b,templated parameters. In this case, represents the type of the expression which is used as the return type of the\ndecltype(a + b) a bfunction. Note that you cannot simply use as the return type using the old syntax, since the argument names and would\nnot be in scope (this is because the compiler parses the leading return type before the function parameters are declared).\n// doesn't compile, compiler doesn't know what a and b are when evaluating the function return type\ntemplate <typename typenameT1, T2>\ndecltype(a return+ b) add(T1 a, T2 b) { a + b; }\nThus, if you are using C++11, the trailing return type syntax is the cleanest way to handle a templated function whose return type depends on\nits parameters (there is a way to do it without the trailing return type, but it is convoluted and not as elegant). However, C++14 introduced\ndecltype(auto)the specifier, as well as automatic return type deduction without needing to explicitly specify the return type. By using\ndecltype(auto), the following code now compiles, and the return type of the templated function is automatically deduced:\n// this compiles now\ntemplate <typename typenameT1, T2>\ndecltype(auto) returnadd(T1 a, T2 b) { a + b; }\ndecltypeWe will not go into too much detail on how works here, but you are definitely encouraged to read more about it on your own.\n3FromGoogle’sC++styleguide: https://google.github.io/styleguide/cppguide.html#Type_deduction", "word_count": 792, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f6e9abd2-72a1-597a-b73e-3b0457b52861", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 39, "real_page_number": null, "text": "1.12 Preﬁx and Postﬁx Incrementation\n27\n1.12\nPreﬁx and Postﬁx Incrementation\noperator++ operator--.In C++, you can increment or decrement the value of a variable by using or However, it matters whether you\n(++i) (i++). ++put the operator a variable name or a variable name Putting in front of a variable (prefix incrementation) willbefore after\nincrement the value of the variable and then return the incremented value.\n1\nint32_t i = 281;\n2\nint32_t j = ++i;\n// increment i, then store in j\n3\nstd::cout << j << std::endl;\n// prints 282\n++Putting after a variable (postfix incrementation) will increment the value of the variable, but return the original value of the variable before it\nincremented. You can essentially treat this as if the incremention is done at the end.was very\n1\nint32_t i = 281;\n2\nint32_t j = i++;\n// store in j, then increment i\n3\nstd::cout << j << std::endl;\n// prints 281\noperator--.The same applies for decrementation using\nIs one method better than the other? The details are a bit nuanced, so you don’t need to know the full details for this course. However, if you\n(++i)do not need to use the previous value before a variable is incremented or decremented, it is generally good practice to prefer prefix over\n(i++).postfix Although many compilers can optimize away differences in many cases, this is not always guaranteed, and in cases where both\nvariations work equally, the prefix form is never less efficient than the postfix form.\n1.13\nLoops\n¸ 1.13.1\nFor and While Loops\nfor whileAt this point, you should probably be familiar with loops in C++, particularly loops and loops with the following usage:\n1\nfor (init; condition; update) {\n2\n// code\n3\n}\n4\n5\nwhile (condition) {\n6\n// code\n7\n}\nfor init conditionIn the loop, the statement is executed once before the execution of the code inside the loop, the statement identifies\nupdate conditionwhen the loop should be run, and the statement is executed every time the code in the loop has been executed. Once the\nwhilestatement is no longer true, the loop terminates. Similar logic can be applied to the loop, which loops through a block of code as long as\nits corresponding condition is true.\nfor while for whileYou can always convert between a loop and a loop, and vice versa. To convert a loop into a loop, add the\nwhile whileinitialization statement before the loop and the update statement to the end of the loop’s code block.\n1\n// for loop\n2\nfor (int32_t i = 0; i < 10; ++i) {\n3\n// code\n4\n} // for i\n5\n6\n// converted while loop\n7\nint32_t i = 0;\n8\nwhile (i < 10) {\n9\n// code\n10\n++i;\n11\n} // while\nwhile forTo convert a loop to a loop, you can omit the initialization statement.\n1\n// while loop\n2\nwhile nullptr)(*ptr++ != {\n3\n// code\n4\n++counter;\n5\n} // while\n6\n7\n// converted for loop\n8\nfor(; nullptr;*ptr != ++ptr, ++counter) {\n9\n// code\n10\n} // for\nIn the following sections, we will discuss two additional types of loops that you may encounter in this class.", "word_count": 557, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1c0c3da2-c57a-544d-87b8-2c936e5afd87", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 40, "real_page_number": null, "text": "28\nChapter 1. Programming Foundations\n¸ 1.13.2\nRange-Based For Loop\nfor forIf you want to loop through all elements in a given collection of elements, you can use the range-based loop. The range-based loop\nfollows the following format:\n1\nfor (individual element in collection : collection) {\n2\n// do stuff here\n3\n}\nFor example, the following two loops do the exact same thing:\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n2\n3\n// traditional for loop, uses index counter to visit each element\n4\nfor (size_t i = 0; i < vec.size(); ++i) {\n5\nvec[i] *= 2;\n6\n} // for i\n7\n8\n// range-based for loop, visits every element directly and stores value in x\n9\nfor (int32_t& x : vec) {\n10\nx *= 2;\n11\n} // for x\n12\n13\n// range-based for loop using auto, does the same thing as above\n14\nfor (auto& x : vec) {\n15\nx *= 2;\n16\n} // for x\nforIn the parentheses of the range-based loop, there are two things you need: a variable that stores the current element you are visiting, and the\nfor(int&x: vec)collectionofelementsyouwanttoiterateover. Thesetermsareseparatedwithacolon. Forinstance, loopsthroughevery\nvec, x vec = {1, 2, 3, 4, 5},element in the container where is the element that the loop is currently visiting. In the example where\nx 1 for 2 3the value of is on the first iteration of the range-based loop, on the second iteration, on the third iteration, and so on.\nfor (&)!If you are using the range-based loop to modify something in a container, make sure to declare the iteration variable as a reference\nforIf you don’t declare this variable as a reference, the range-based loop of each element in the search container and stores themakes a copy\nvec,copy in the iteration variable. If line 9 did not have a reference, line 10 would modify a copy of each element in rather than the actual\nvec vecelement in itself. This would leave the elements in unchanged, even after the loop completes).\n¸ 1.13.3\nDo While Loop\nThe is a post-test loop. The body of a do while loop always runs at least once, with the condition check of the test expression atdo while loop\nthe end. If the test expression is true, the body of the loop is executed again; otherwise, the do while loop is terminated. The do while loop\nfollows the following syntax:\n1\ndo {\n2\n// body of loop\n3\nwhile} (condition);\nAs an example, consider the following code:\n1\nint32_t num = 281;\n2\ndo {\n3\nstd::cout << \"EECS \" << num << std::endl;\n4\n++num;\n5\nwhile} (num < 280);\nnum 281, num < 280Noticehow the value of is initializedto so the conditionof is never true! However, sincethedo while loop isa post-test\nloop, the body of the loop runs is guaranteed to run once, regardless of whether the condition is true or not. In this case, the programat least\n\"EECS 281\" whileprints out on its first iteration of the loop. Only after this first iteration is complete does the program check to see if the\ncondition is met — in this case, it isn’t, so the loop terminates after the first iteration.\n¸ 1.13.4\nbreakThe Keyword\nbreak continue. breakTwo important keywords involved with loops are and When a statement is reached in a program, it terminates the\nsmallest enclosing loop. Execution of the program resumes immediately after the body of the terminated loop. This is illustrated below:\n1\nwhile (/* condition */) {\n2\n// do stuff\n3\nif (/* condition for break */) {\n4\nbreak;\n5\n} // if\n6\n// do more stuff\n7\n} // while\n8\n9\n// more code here\nbreakWhen the statement\nis reached, the while loop\nimmediately terminates.\nbreak breakEncountering a statement causes a loop to finish, even if its end condition is now fulfilled. Thus, can be used to end an otherwise\ninfinite loop, or to terminate a loop before its natural end.", "word_count": 715, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "35c9bbf4-d6c9-58f3-8547-558d990615e4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 41, "real_page_number": null, "text": "1.14 Ternary Operator\n29\nbreakNote that, if is encountered in a nested loop, you would only exit from the smallest enclosing loop that you are in. In the following code,\nbreak for whilefor example, the on line 4 causes the program to exit the loop defined on line 2, but not the loop defined on line 1.\n1\nwhile (true) {\n2\nfor (int32_t i = 0; i < 5; ++i) {\n3\nif (i == 3) {\n4\nbreak;\n5\n} // if\n6\nstd::cout << i << \" \";\n7\n} // for i\n8\nstd::cout << \"PotatoBot\" << std::endl;\n9\n} // while\n\"0 1 2 PotatoBot\"As a result, the output of would be printed indefinitely in this example.\n¸ 1.13.5\ncontinueThe Keyword\ncontinue break. continueThe statement is very similar to However, instead of terminating the loop, a statement forces the next iteration\nof the loop to take place, skipping any remaining code in the current iteration. This is illustrated below:\n1\nwhile (/* condition */) {\n2\n// do stuff\n3\nif (/* condition for continue */) {\n4\ncontinue;\n5\n} // if\n6\n// do more stuff\n7\n} // while\ncontinueWhen the statement is reached, the loop skips to the next\niteration, and the remaining code in the current iteration does not run.\ncontinueAn example of code that uses is shown below. This code prints out every integer from 1 to 5, except for 3.\n1\nfor (int32_t i = 1; i <= 5; ++i) {\n2\nif (i == 3) {\n3\ncontinue;\n4\n} // if\n5\nstd::cout << i << \" \";\n6\n} // for i\n1.14\nTernary Operator\nThe inC++(denotedbyaquestionmarkcharacter)providesanalternativewaytoexpressaconditionalstatement.ternary(conditional)operator\nThe syntax of a ternary operator is as follows:\n<CONDITION> ? <EXPRESSION IF TRUE> : <EXPRESSION IF FALSE>\nThe ternary operator can be used to shorten a standard if/else statement into one line. For example, consider the following function, which\nreturns whether a student received a passing or failing grade.\n1\nbool passed_class(int32_t score) {\n2\nif (score >= 70) {\n3\nreturn true;\n4\n} // if\n5\nelse {\n6\nreturn false;\n7\n} // else\n8\n} // passed_class()\nWith a ternary operator, we can condense the if/else onto a single line.\n1\nbool passed_class(int32_t score) {\n2\nreturn true false;score >= 70 ? :\n3\n} // passed_class()\n? (true) (false).Here, the statement before the dictates whether we return the value before the colon or the value after the colon If the\n≥70, true; false.given score is then the function returns otherwise, it returns Another example is shown below:\n1\nbool try_buy_item(double double double&price, willingness_to_pay, total_savings) {\n2\ndouble savings_from_purchase;\n3\nif (price < willingness_to_pay) {\n4\nsavings_from_purchase = willingness_to_pay - price;\n5\n} // if\n6\nelse {\n7\nsavings_from_purchase = 0.0;\n8\n} // else\n9\ntotal_savings += savings_from_purchase;\n10\nreturn price <= willingness_to_pay;\n11\n} // buy_item()\nUsing a ternary operator, this same function can be written as follows:\n1\nbool try_buy_item(double double double&price, willingness_to_pay, total_savings) {\n2\ndouble savings_from_purchase = price < willingness_to_pay ? willingness_to_pay - price : 0.0;\n3\ntotal_savings += savings_from_purchase;\n4\nreturn price <= willingness_to_pay;\n5\n} // buy_item()", "word_count": 563, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "95ac574f-8731-5b19-a1b7-44f220de0a37", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 42, "real_page_number": null, "text": "30\nChapter 1. Programming Foundations\n1.15\nInheritance and Polymorphism\n¸ 1.15.1\nInheritance\nObject-oriented programming (OOP) is a computer programming approach defined by solving complex problems using objects and their\nmethods. There are four main principles to object-oriented programming: encapsulation, abstraction, inheritance, and polymorphism.\nWe covered the first two of these principles in earlier sections. Encapsulation is the act of combining both data and methods into a single\nclassunit. This can be done using a in C++, where the state of an object is stored as private member variables, and methods that modify this\ninternal state are provided as public member functions. Abstraction is the idea that internal implementation details should be hidden from the\nuser, and that only relevant operations for usage should be revealed. For example, if you want to increase the speed of a car, you just need to\nknow to press the accelerator; you do not need to know the underlying details of how the car works! Similarly, if you want to remove an element\nstd::vector<>, std::vector::erase()from a you just need to call the member function; you do not need to understand how this\nmember function is implemented under the hood.\nIn this section, we will cover the concepts of inheritance and polymorphism. Inheritance is the ability for a class to reuse the interface or\nfunctionality of another class. The need for inheritance arises out of the fact that many objects in OOP design are very similar but not entirely\nthe same. Inheritance provides a means for similar objects to share common logic while also exhibiting logic that is individually distinct. Let’s\nclass,look at an example: suppose we define the following which represents a person at U-M. Every person at U-M has a name, a UMID, and\nan age. Since these member variables are private, the public getting functions starting on line 9 allow us to retrieve their values.\n1\nclass UMPerson {\n2\nstd::string name;\n3\nint32_t id;\n4\nint32_t age;\n5\npublic:\n6\nint32_t int32_tUMPerson(std::string name_in, id_in, age_in)\n7\n: name{name_in}, id{id_in}, age{age_in} {}\n8\n9\nconststd::string get_name() {\n10\nreturn name;\n11\n} // get_name()\n12\n13\nint32_t constget_id() {\n14\nreturn id;\n15\n} // get_id()\n16\n17\nint32_t constget_age() {\n18\nreturn age;\n19\n} // get_age()\n20\n};\nUMPersonHowever, not everyone at the university has the same traits! We can further break the type down into subtypes, such as students and\nUMPerson,professors. Both students and professors are considered as a and both groups have names, UMIDs, and ages. However, these\ngroups are also distinct from each other in several ways. For example, students have a GPA, a grade level, a list of classes taken, and a graduation\nyear. On the other hand, professors have information on a list of classes taught, the number of years they have been at the university, a salary,\nStudent Professor UMPersonand a rank or title. With inheritance, we can define both a and object that \"inherit\" shared traits from the\nclass but also behave slightly differently on their own.\nUMPerson\nProfessor\nStudent\nUMPerson,The class that contains the common functionality, in this case is known as a class. We can then make subclasses from thebase\nbase class that either change the functionality in subtle ways or introduce new functionality altogether — these subclasses are known as derived\nStudent Professor UMPerson.classes. In this example, the and classes are derived from the base class To create a derived class, add a\npublic,colon after the name of the derived class upon declaration, then the keyword then the name of the base class. The following defines a\nStudent UMPerson:class named that derives from the base class\n1\nclass publicStudent : UMPerson {\n2\nstd::vector<std::string> classes_taken;\n3\ndouble gpa;\n4\nint32_t grade_level;\n5\nint32_t graduation_year;\n6\n};\nStudent UMPerson publicBy doing this, the class has everything that the class has! Notice that we used the keyword when defining our\nStudentderived class. This allows our derived class to directly access any member functions or variables defined in the basenon-private\ninheritance).4UMPerson UMPersonclass (known as Note that members to are inherited but bypublic private cannot be directly accessed\nStudent UMPersonthe derived class, which is why we needed the getter functions in the interface.\n4There are other ways to inherit from a base class, such as private or protected inheritance, which can be used to change the level of access for non-private\ninheritedmembers. Therealternativeapproaches,however,arenotascommon.", "word_count": 759, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "54f332dc-a2c7-5f53-a165-ef326f6cb368", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 43, "real_page_number": null, "text": "1.15 Inheritance and Polymorphism\n31\nIn order to properly initialize a derived class, its constructor must properly initialize the base class it derives from. In our example, not only\ngpa, grade_level, graduation_year Student,do we need to initialize the and member variables upon creating a we also need to\nname, id, age UMPerson.initialize the and member variables that were inherited from Since these three inherited members are private in\nUMPerson Student Studentthe class, the class cannot initialize these member variables directly. Instead, the constructor initializes these\n(UMPerson)three members by invoking the base class constructor in the member-initializer list of its own constructor, as shown:\n1\nint32_t int32_tStudent(std::string name_in, id_in, age_in,\n2\ndouble int32_t int32_tgpa_in, grade_in, year_in)\n3\n: UMPerson{name_in, id_in, age_in}, gpa{gpa_in}, // init inherited members using base class ctor\n4\ngrade_level{grade_in}, graduation_year{year_in} {}\nThe constructor of a derived class will always invoke a constructor of the base class. If the constructor of the base class is not explicitly invoked\nin the member-initializer list of the derived class, a call to the base class’s default constructor is made implicitly.\nProfessor Student name, age, idThe following class follows the same vein as the class; it inherits the and member variables\nUMPersonfrom the class (since these variables are shared across all people at U-M), while having its own member variables unique to the\nProfessor salary department:class, such as and\n1\nclass UMPerson {\n2\nstd::string name;\n3\nint32_t id;\n4\nint32_t age;\n5\npublic:\n6\nint32_t int32_tUMPerson(std::string name_in, id_in, age_in)\n7\n: name{name_in}, id{id_in}, age{age_in} {}\n8\nconst returnstd::string get_name() { name; }\n9\nint32_t const returnget_id() { id; }\n10\nint32_t const returnget_age() { age; }\n11\n};\n12\n13\nclass publicProfessor : UMPerson {\n14\nstd::vector<std::string> classes_taught;\n15\nstd::string department;\n16\nstd::string title;\n17\ndouble salary;\n18\nint32_t years_taught;\n19\nbool tenured;\n20\npublic:\n21\nint32_t int32_tProfessor(std::string name_in, id_in, age_in, std::string dep_in,\n22\ndouble int32_t boolstd::string title_in, salary_in, years_in, tenured_in)\n23\n:\nUMPerson{name_in, id_in, age_in}, department{dep_in}, title{title_in},\n24\nsalary{salary_in}, years_taught{years_in}, tenured{tenured_in} {}\n25\n// other member functions specific to a Professor\n26\n...\n27\n};\n¸ 1.15.2\nPolymorphism\nThe fourth and final principle of object-oriented programming is polymorphism, which allows a derived class object to be used whenever a base\nexpected.5class object is In C++, you are allowed to use a reference or pointer of a base type to refer to an object of a derived type that publicly\nStudent my_student Professor my_professor.inherits from it. For instance, assume we had a object named and a object named\nUMPerson, UMPerson (UMPerson*) UMPersonSince both classes are derived from the base class we can use a pointer or a reference\n(UMPerson&) Student Professor.to refer to our and\n1\nUMPerson* student_ptr = &my_student;\n// okay, since Student is a UMPerson\n2\nUMPerson& student_ref = my_student;\n// okay, since Student is a UMPerson\n3\nUMPerson* prof_ptr = &my_professor;\n// okay, since Professor is a UMPerson\n4\nUMPerson& prof_ref = my_professor;\n// okay, since Professor is a UMPerson\nThe other way around is not implicitly allowed; you cannot use a reference or pointer of a derived type to refer to an object of the base type.\n1\nUMPerson myUMP{\"Bob\", 12345678, 21};\n// create a U-M Person\n2\nStudent* student_ptr = &myUMP;\n// not okay!\n3\nStudent& student_ptr = myUMP;\n// not okay!\n4\nProfessor* prof_ptr = &myUMP;\n// not okay!\n5\nProfessor& prof_ptr = myUMP;\n// not okay!\nUMPerson* UMPerson& Student.All students are people at U-M, so it would be safe to use a or to refer to a However, not all people at\nU-M are students, so it would be unsafe to convert the other way around! As a result, if you want to cast a base class to a derived class, you must\nstatic_cast<> dynamic_cast<>do so explicitly using or (which will be covered in more detail in the next section).\n5Thereareseveraltypesofpolymorphism,butthisdefinitionrefersto polymorphism,whichiswhattheterm\"polymorphism\"generallyusuallyrefersto.subtype", "word_count": 680, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d8d28160-b0b2-5cef-ad09-f8cf66d53402", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 44, "real_page_number": null, "text": "32\nChapter 1. Programming Foundations\nIt should also be mentioned that, although we can use a pointer or reference of a base class to refer to a derived class, the following would benot\nStudent UMPerson, Studentokay. This is because, even though an object of type derives from an object of type the value of a does not\nUMPersonfit into a object.necessarily\nUMPerson myUMP = myStudent;\nStudent UMPersonIn this case, trying to fit a object into an object of type would result in — only the members defined byobject slicing\nUMPerson Student UMPersonwould be copied, and the other member variables defined in but not in would be \"sliced off.\"\nPolymorphism allows us to pass a derived-class object into a function that expects a base-class object. There are many places where this\noperator<<behavior can be helpful. As an example, let’s revisit the overloaded that we wrote earlier.\n1\noperator<<(std::ostream& conststd::ostream& os, Point& pt) {\n2\n// define the output format you want to print for this custom object\n3\nos << \"(\" << pt.x << \", \" << pt.y << \")\";\n4\nreturn os;\n5\n} // operator<<()\noperator<< ostream ostreamNotice that the overload expects an object of type as its first argument. However, the type is actually a\nbase class for several other output stream types, as shown below:\nostream\nostringstream\nofstream\noperator<< ostringstreamBecause of this relationship, polymorphism allows us to use on an object of type (the derived class) even\nostreamthough an object of type (the base class) is expected.\n1\nint main() {\n2\nPoint pt{3, 4};\n3\nstd::ostringstream os;\n4\nos << pt;\n// operator<< works on ostringstream\n5\nstd::cout << os.str() << std::endl;\n// prints (3, 4)\n6\n} // main()\nPolymorphism is also helpful if you want to create a container of base class pointers, where each pointer points to a different derived class\nStudent Professorobject. As an example, let’s use a simplified version of the and classes covered previously.\n1\nclass UMPerson {\n2\npublic:\n3\nstd::string name;\n4\nUMPerson(std::string name_in) : name{name_in} {}\n5\nvoid constprint_name() { std::cout << \"Person \" << name << std::endl; }\n6\n};\n7\n8\nclass publicStudent : UMPerson {\n9\npublic:\n10\nStudent(std::string name_in) : UMPerson{name_in} {}\n11\nvoid constprint_name() { std::cout << \"Student \" << name << std::endl; }\n12\n};\n13\n14\nclass publicProfessor : UMPerson {\n15\npublic:\n16\nProfessor(std::string name_in) : UMPerson{name_in} {}\n17\nvoid constprint_name() { std::cout << \"Professor \" << name << std::endl; }\n18\n};\nUMPersonLet’s initialize several students and professors and add them all to a vector of pointers. Then, let’s go through each object in our\nprint_name()vector and call its member function.\n1\nint main() {\n2\nStudent a = Student{\"Alice\"};\n3\nStudent b = Student{\"Bob\"};\n4\nProfessor c = Professor{\"Cathy\"};\n5\nProfessor d = Professor{\"Dave\"};\n6\nstd::vector<UMPerson*> vec = { &a, &b, &c, &d };\n7\nfor (const auto& person : vec) {\n8\nperson->print_name(); // call print_name() of each object in vector\n9\n} // for\n10\n} // main()\nRunning this code ends up producing the following output.\nPerson Alice\nPerson Bob\nPerson Cathy\nPerson Dave", "word_count": 540, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b22428bb-3e43-58b3-9e7a-c2dda3445d5e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 45, "real_page_number": null, "text": "1.16 Casting\n33\nprint_name()Byreferringtoallofthederivedclassesinthevectorasapointerofthebaseclass, weendedupusingthebaseclass’s function\nStudent, print_name() print_name()for every object as well! Although Alice is a calling Alice’s function runs the function defined\nUMPerson Studentin the class, rather than the one defined in the class.\nvirtual virtualTo fix this, we will use the keyword. When a function is called, the compiler determines the correct version of the\nfunction to call based on the type of the object (a process known as binding) rather than the type of the object atduring runtime dynamic\nvirtualcompile time (which is known as binding). The keyword is only necessary in the base class; it is optional for derived classes.static\nvirtualFurthermore, it can only be prepended to function definitions within a class — if the function is defined outside of the class, the\nkeyword need not be applied.\nvirtual overrideAs mentioned, the keyword should be used for the base class. For derived classes, you should use the keyword,\nwhich lets the compiler know that the function intends to override a member function in the base class. This keyword can help detect bugs, as\noverridethe compiler would produce an error if a function declared with does not end up overriding anything. The following code is the\nvirtual overrideresult of adding the and keywords to our original member functions.\n1\nclass UMPerson {\n2\npublic:\n3\nstd::string name;\n4\nUMPerson(std::string name_in) : name{name_in} {}\n5\nvirtual void constprint_name() { std::cout << \"Person \" << name << std::endl; }\n6\n};\n7\n8\nclass publicStudent : UMPerson {\n9\npublic:\n10\nStudent(std::string name_in) : UMPerson{name_in} {}\n11\nvoid const overrideprint_name() { std::cout << \"Student \" << name << std::endl; }\n12\n};\n13\n14\nclass publicProfessor : UMPerson {\n15\npublic:\n16\nProfessor(std::string name_in) : UMPerson{name_in} {}\n17\nvoid const overrideprint_name() { std::cout << \"Professor \" << name << std::endl; }\n18\n};\nvirtual overrideIf we run our code again with the and keywords in place, we get the following output:\nStudent Alice\nStudent Bob\nProfessor Cathy\nProfessor Dave\nprint_name()This is exactly what we want, as the version of called matches the type of the object that calls it.\n1.16\nCasting\nCasting is the process of converting data from one type to another. In C++, there are two types of conversions: implicit and explicit conversions.\nAs mentioned earlier, implicit conversions are performed automatically by the compiler, without input from the programmer.\nint32_t a = 281;\ndouble b = a;\n// int implicitly converted to double\nOn the other hand, explicit conversions are explicitly specified by the programmer. In C++, there are four main types of explicit casts:\nstatic_cast<>•\ndynamic_cast<>•\nreinterpret_cast<>•\nconst_cast<>•\n¸ 1.16.1\nStatic Cast\nA is the standard, \"well-behaved\" cast that can be used to convert between two compatible types. This is the preferred method ofstatic cast\ndoublecastingforautomaticconversionsandnarrowingconversions(i.e., convertingfromatypewithawiderrangetoasmallerrange, suchas\nint).to NotA static cast should be the preferred method to use if you want to cast a variable to a different type, provided that it is possible!\nonly are static casts more readable and express clearer intent, they are also safe: if you attempt to perform an incompatible conversion using\nstatic_cast<>, the compiler would issue an error.\nValid static cast:\ndouble a = 281.99;\nint32_t static_cast<int32_t>(a);b =\n// casts a from double to int32_t, b is now 281\n// this is a narrowing conversion, which truncates decimals\nInvalid static cast:\nclass MyClass {\nchar a = 'a';\n} mc;\nchar static_cast<char>(mc);mc_a =\n// attempt to convert MyClass object to char, compile error", "word_count": 645, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "afd5aefc-1b3a-5933-9f83-36902f5c3bfc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 46, "real_page_number": null, "text": "34\nChapter 1. Programming Foundations\n¸ 1.16.2\n(✽)Dynamic Cast\nAdynamiccastcanbeusedtoperformtype-safedowncasting(e.g., convertingabaseclasspointer/referencetoaderivedclasspointer/reference).\nWhen you attempt a dynamic cast, the program actually performs a check to determine if the requested conversion is possible. Ifduring runtime\nnullptra dynamic cast fails, a is returned. In general, this cast is really only applicable if you are trying to downcast a polymorphic type. An\nUMPersonexample is shown below, using the example from the previous section.\n1\nvoid process_person(UMPerson& person) {\n2\nif (auto dynamic_cast<Student*>(&person))as_student = {\n3\nstd::cout << \"Received a student named \" << as_student->name << std::endl;\n4\n} // if\n5\nelse if (auto dynamic_cast<Professor*>(&person))as_professor = {\n6\nstd::cout << \"Received a professor named \" << as_professor->name << std::endl;\n7\n} // else if\n8\n} // process_person()\n9\n10\nint main() {\n11\nStudent a{\"Alice\"};\n12\nProfessor b{\"Bob\"};\n13\nprocess_person(a);\n// prints \"Received a student named Alice\"\n14\nprocess_person(b);\n// prints \"Received a professor named Bob\"\n15\n} // main()\n¸ 1.16.3\n(✽)Reinterpret Cast\nA reinterpret cast takes the memory representation of an object and reinterprets it as if it were the type you are trying to cast to. In other words,\nif you attempt to reinterpret cast an object, the underlying bits that represent the object in memory are treated as if they were of another type.\nExamples of reinterpret casts are shown below (you do not have to understand what is happening here):\n1\nstruct MyClass {\n2\nint16_t a;\n3\nint16_t b;\n4\n};\n5\n6\nint main() {\n7\ndouble d = 281.37;\n8\nuint64_t reinterpret_cast<uint64_t>(&d);ptr_addr =\n// casts pointer to integer\n9\nstd::cout << ptr_addr << std::endl;\n// prints out int representation of address\n10\n11\nint32_t num = 24248601;\n12\nreinterpret_cast<MyClass*>(&num);MyClass* mc =\n// casts int mem representation to MyClass\n13\nstd::cout << mc->a << std::endl;\n// prints 281 ...!? (depending on system)\n14\nstd::cout << mc->b << std::endl;\n// prints 370 ...!? (depending on system)\n15\n} // main()\nreinterpret_cast<>Because actually reinterprets physical bits in memory, it is a very dangerous cast that can cause misbehavior if you\ndo not know what you are doing. This cast is only mentioned here for completeness on the topic, but you really should not do this in your code.\nIf you ever need to make such a cast in the future, you will know (and it probably won’t be anytime soon, if ever).\n¸ 1.16.4\n(✽)Const Cast\nreinterpret_cast<> const_cast<>.Joining as a castthatyou should notuse in this class, we have A const cast can be usedto remove\nthe constness from a pointer or reference that refers to an object that is not const. An example is shown below:\n1\nint main() {\n2\nint32_t i = 0;\n3\nconst int32_t& i_ref = i;\n4\n// i_ref = 281;\n// not allowed, since i_ref is const\n5\nconst_cast<int32_t&>(i_ref) = 281;\n// const cast i_ref and assign it to 281\n6\nstd::cout << i_ref << std::endl;\n// prints 281\n7\nstd::cout << i << std::endl;\n// prints 281\n8\n// i_ref = 370;\n// not allowed, i_ref is still const\n9\n10\nint32_t j = 0;\n11\nconst int32_t* j_ptr = &j;\n12\n// *j_ptr = 281;\n// not allowed, since j_ptr is const\n13\n*const_cast<int32_t*>(j_ptr) = 281;\n// const cast j_ptr and assign its pointer value to 281\n14\nstd::cout << *j_ptr << std::endl;\n// prints 281\n15\nstd::cout << j << std::endl;\n// prints 281\n16\n// *j_ptr = 370;\n// not allowed, j_ptr is still const\n17\n} // main()\nAs before, this cast is only introduced here for completeness. This is not a cast that you should be frequently using, as it is not only dangerous if\nused incorrectly, but it is also indicative of poorly designed code.", "word_count": 655, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a57bca0a-f7e0-5235-8601-0d8a85f8d746", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 47, "real_page_number": null, "text": "1.17 Exception Handling\n35\n¸ 1.16.5\n(✽)C-Style Casts\nApart from static casts, another cast that you may see often is the standard C-style cast. To perform a C-style cast, simply add the type you want\nto convert to in parentheses before the object you want to cast. An example is shown below:\ndouble a = 281.99;\nint32_t (int32_t)a;b =\n// casts a from double to int32_t and assigns to b, b is now 281\nstatic_cast<>However, you should avoid C-style casts when developing in C++. If you want to perform a cast, you should generally use\ninstead. This is because a C-style cast can behave like a static cast, a const cast, or even a reinterpret cast depending on the types you are trying\nstatic_cast<>,to convert. This can lead to potentially dangerous or undefined behavior if you mistakenly try to cast the wrong type. With\nthe compiler will detect incompatible conversions and issue an error during compile time, making it a safer way to cast different types. Usage of\nstatic_cast<> is also more readable and makes it easier to identify places in your code where type conversions are occurring.\n1.17\nException Handling\nAn exception is an event that is issued during a program’s execution when erroneous conditions are encountered during runtime. Exceptions are\noften used to handle runtime errors in a program, and they provide several advantages over other traditional error handling mechanisms:\n• If you use exceptions to handle errors, youExceptions allow error handling code and normal program code to be organized separately.\ncan structure your program so that your error handling is implemented elsewhere (instead of in the middle of your normal program code).\nThis improves code readability and maintainability.\n• A method can throw manyExceptions are versatile, and different methods can pick and choose which exceptions it should handle.\nexceptions, but it can choose which ones it wants to handle. If an exception is not handled, it is able to propagate upward to another caller\nuntil it finds something that is able to handle it.\n• Thanks to inheritance, exception objects can be grouped into a hierarchy based on its type. ThisExceptions can be grouped together.\nmakes it easier to categorize different types of behaviors and allows similar errors to be handled together.\ntry, catch, throw. throwIn C++, exceptions are handled using three keywords: and The keyword can be used to trigger an error handling\ntryevent (we call this process \"throwing an exception\"). The code itself is segmented into blocks (which include code that could potentially\ncatchthrow an exception) and blocks (which include code to handle exceptions that are thrown). Try-catch blocks are placed around code that\ncould potentially encounter an error, as shown below:\n1\nint main() {\n2\ntry {\n3\n// code that could potentially throw an exception\n4\n}\n5\ncatch (const EXCEPTION_NAME& ex) {\n6\n// code that handles exceptions thrown within try block\n7\n}\n8\n}\ntry catchEach block can be followed by multiple blocks that each handle a different type of exception. When an exception is thrown within\ntry catcha block, the program looks at all of its associated blocks (from top to bottom), and program execution is transferred to the first\ncatch catchblock that is able to handle the exception that was thrown. After the code in this block runs to completion, execution proceeds\ntry-catch trybeyond the entire block, and any remaining code that was not run in the block is skipped.\ncatchIf there exists no block that can handle an exception, the exception propagates to the caller of the function that threw the exception,\ncatchand the program again attempts to look for a block that can potentially handle the exception. An uncaught exception continues\ncatch main()propagating to the caller as long as there is no block that can handle it; if it ever propagates beyond without being handled, the\nprogram terminates, and the user is informed that an error occurred.\ntry fav_classTechnically, anything can be thrown from a block. The following code throws the integer if it is not equal to 281:\n1\nint main() {\n2\ntry {\n3\nint32_t fav_class;\n4\nstd::cout << \"What is your favorite class?\" << std::endl;\n5\nstd::cin >> fav_class; // reads in fav_class from user input\n6\nif (fav_class == 281) {\n7\nstd::cout << \"Of course! Who doesn't love EECS 281?\" << std::endl;\n8\n} // if\n9\nelse {\n10\nthrow (fav_class);\n11\n} // else\n12\n} // try\n13\ncatch (int32_t wrong_class) {\n14\nstd::cout << \"You said your favorite class is \" << wrong_class << std::endl;\n15\nstd::cout << \"YOU ARE WRONG\" << std::endl;\n16\n} // catch\n17\n} // main()", "word_count": 794, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "16badbae-6719-5c52-b8ee-76d9ae0e1c14", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 48, "real_page_number": null, "text": "36\nChapter 1. Programming Foundations\nstd::exceptionIn this section, however, we will primarily focus on exceptions that inherit from the C++ standard library’s class. These\nexceptions are defined using the following inheritance hierarchy (as of C++17):\nstd::exception\nstd::bad_alloc\nstd::bad_cast\nstd::bad_exception\nstd::bad_function_call\nstd::bad_optional_access\nstd::bad_typeid\nstd::bad_variant_access\nstd::bad_weak_ptr\nstd::logic_error\nstd::runtime_error\nstd::bad_array_new_length\nstd::bad_any_cast\nlogic error exceptions\nstd::domain_error\nstd::future_error\nstd::invalid_argument\nstd::length_error\nstd::out_of_range\nruntime error exceptions\nstd::overflow_error\nstd::range_error\nstd::regex_error\nstd::system_error\nstd::underflow_error\nstd::ios_base::failure\nstd::filesystem::filesystem_error\nstd::exception what().Every exception in the language is derived from the base class, which implements a member function called\nwhat()The member function returns a message that identifies the exception.\n1\nint main() {\n2\ntry {\n3\nthrow std::runtime_error{\"descriptive message here\"};\n4\n} // try\n5\ncatch (const std::runtime_error& ex) {\n6\nstd::cerr << ex.what() << std::endl; // prints \"descriptive message here\"\n7\n} // catch\n8\n} // main()\nstd::bad_allocYou do not need to remember all these exceptions, but there are a few that you are likely to see again. One of these is the\nnewexception, which is thrown whenever the keyword fails to allocate the memory that was requested:\n1\nint main() {\n2\ntry {\n3\nchar* new char[STACK_SIZE];arr = // throws bad_alloc if not enough memory\n4\n} // try\n5\ncatch (const std::bad_alloc& ex) {\n6\nstd::cerr << \"bad_alloc caught: \" << ex.what() << std::endl;\n7\n} // catch\n8\n} // main()\nstd::exceptionIn addition, you can define custom exceptions that inherit from the base class. An example is shown below. Note that the\nwhat() member function is virtual, so you can override its implementation to return any message you want.\n1\nclass publicEECS281Exception : std::exception {\n2\nstd::string msg;\n3\npublic:\n4\nEECS281Exception(const std::string& msg_in)\n5\n: msg{msg_in} {}\n6\n7\nconst char* const noexcept overridewhat() {\n8\nreturn msg.c_str();\n9\n} // what()\n10\n};\n11\n12\nint main() {\n13\ntry {\n14\nint32_t fav_class;\n15\nstd::cout << \"What is your favorite class?\" << std::endl;\n16\nstd::cin >> fav_class; // reads in fav_class from user input\n17\nif (fav_class == 281) {\n18\nstd::cout << \"Of course! Who doesn't love EECS 281?\" << std::endl;\n19\n} // if\n20\nelse {\n21\nthrow EECS281Exception{\"YOU ARE WRONG\"};\n22\n} // else\n23\n} // try\n24\ncatch (const EECS281Exception& ex) {\n25\nstd::cout << e.what() << std::endl; // prints \"YOU ARE WRONG\"\n26\n} // catch\n27\n} // main()", "word_count": 401, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "be6f43f4-f31a-5e37-8036-d3719d67f744", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 49, "real_page_number": null, "text": "1.17 Exception Handling\n37\nwhat() noexceptRemark: On line 7 of the previous code, the function was declared using the keyword. What does this mean? The\nnoexcept noexceptkeyword is applied to functions that promise not to throw any exceptions. In other words, a function should never\nnewthrow an exception on its own, nor should it use to allocate memory, call another library function that could potentially throw an\nexception, perform arithmetic that could cause an overflow or underflow, or exhibit any other behavior that could cause an exception to be\nthrown. If something unexpected happens and the function somehow does throw an exception, the normal try-catch procedure is ignored and\nthe program is terminated immediately.\ncatch catchLastly, since clauses are checked in the order in which they appear (i.e., the first block that is able to handle an exception always\ncatchruns), blocks should be ordered so that any derived exception type is handled its base exception type. For example, if you have abefore\ncatch std::out_of_range catchblock that specifically handles the exception, and another block that handles all other logic errors, the\nstd::out_of_range std::logic_error.block that catches the exception should be listed before the block that catches the generic\n1\ntry {\n2\n/* ... code that could throw any type of logic error ... */\n3\n}\n4\ncatch (const std::out_of_range& ex) {\n5\n/* ... handles std::out_of_range exception ... */\n6\n}\n7\ncatch (const std::logic_error& ex) {\n8\n/* ... handles std::logic_error exception ... */\n9\n}\ncatch catch std::logic_errorIf you switch the order of these two statements, the block that handles exceptions would also end up\nstd::out_of_range std::out_of_range std::logic_error).handling exceptions (due to polymorphism, since is derived from\nstd::out_of_rangeAs a result, the catch block that handles exceptions would never run at all.\ncatch catchIt should also be noted that an ellipsis (…) can be used as a parameter of a statement, which enables the corresponding\ncatchblock to handle object that is thrown. Since this block catches everything, it should be placed below all other blocks if included.any\n1\ntry {\n2\n/* ... code that could throw anything ... */\n3\n}\n4\ncatch (const std::logic_error& ex) {\n5\n/* ... handles logic errors ... */\n6\n}\n7\ncatch (const std::runtime_error& ex) {\n8\n/* ... handles runtime errors ... */\n9\n}\n10\ncatch (...) {\n11\n/* ... handles anything else ... */\n12\n}\nExceptions are not a primary focus of this class, and you will not need to use them for your assignments. However, knowledge of how to handle\nexceptions may be important for future upper-level courses (and is a valuable skill overall). Even if you never throw any exceptions on your own\nin this class, you may still encounter them when you submit your code to the EECS 281 autograder! In the following autograder output, a test\nstd::bad_alloccase failed because it went over the maximum allowable memory limit, forcing the program to throw a exception.", "word_count": 506, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b53be1c4-6db9-5671-9302-9ca96154fa56", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 50, "real_page_number": null, "text": "38\nChapter 1. Programming Foundations\nChapter 1 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\nint32_t uint32_t?1. Which of the following statements is/are TRUE regarding the integer types of and\nuint32_t int32_tI. The data type cannot represent a negative number, while the data type can.\nint32_t uint32_tII. The largest integer value representable using the and data types are the same.\nuint32_t int32_t.III. Because a needs to cover a wider range of potential values, it takes up more bytes in memory than a\nA) I only\nB) II only\nC) III only\nD) I and II only\nE) I and III only\nstruct class2. Which of the following statements is/are TRUE regarding the difference between using a and a to create a custom type?\nstruct classA) A can support member functions, while a cannot\nclass structB) A can support member functions, while a cannot\nstruct public, class privateC) By default, access to members in a is while access to members in a is\nstruct private, class publicD) By default, access to members in a is while access to members in a is\nE) More than one of the above\n3. Consider the following code, which defines a derived class with several member variable types.\n1\nstruct ObjectA {\n2\nObjectA() { std::cout << \"Object A initialized! \"; }\n3\n};\n4\n5\nstruct ObjectB {\n6\nObjectB() { std::cout << \"Object B initialized! \"; }\n7\n};\n8\n9\nstruct ObjectC {\n10\nObjectC() { std::cout << \"Object C initialized! \"; }\n11\n};\n12\n13\nstruct Base {\n14\nObjectA a;\n15\n};\n16\n17\nstruct publicDerived : Base {\n18\nObjectB b;\n19\nObjectC c;\n20\n};\n21\n22\nint main() {\n23\nDerived my_derived_class;\n24\n} // main()\nstd::coutWhat is the output of this code? (Note: The statement prints a message to the console output.)\nObject A initialized!A)\nObject B initialized!\nObject C initialized!\nObject B initialized!B)\nObject C initialized!\nObject A initialized!\nObject A initialized!C)\nObject C initialized!\nObject B initialized!\nObject C initialized!D)\nObject B initialized!\nObject A initialized!\nE) The ordering of the constructor messages is indeterminate\nconst4. Consider the following code, which declares an integer using the keyword.\n1\nint main() {\n2\nint32_t v1 = 280;\n3\nint32_t v2 = 281;\n4\nint32_t* const ptr = &v1;\n5\n/* ??? */\n6\n} // main()\nWhich of the following expressions, if added on line 5, would result in a compilation error?\n++v1;A)\n++(*ptr);B)\n*ptr = v2;C)\nptr = &v2;D)\nE) More than one of the above", "word_count": 483, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "a9bf832b-35eb-5148-8a61-6cb2a2df3e3f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 51, "real_page_number": null, "text": "1.17 Exception Handling\n39\n5. Consider the following class:\n1\nclass Foo {\n2\nint32_t bar = 281;\n3\npublic:\n4\nint32_t process_values(int32_t constbaz) {\n5\n/* ??? */\n6\n} // process_values()\n7\n};\nWhich of the following statements is/are TRUE regarding this class definition?\nprocess_values() baz.I. The member function cannot change the value of the function argument\nprocess_values() bar.II. The member function cannot change the value of the member variable\nmutablebar mutable int32_t bar), process_values()III. If had been marked as on line 2 (i.e., then the member function\nbar.would be able to change the value of\nA) I only\nB) II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n6. Consider the following snippet of code:\n1\nclass Foo {\n2\nint32_t bar = 281;\n3\npublic:\n4\nvoid do_something() {\n5\n/* ??? */\n6\n} // do_something()\n7\n8\nvoid constdo_something_const() {\n9\n/* ??? */\n10\n} // do_something_const()\n11\n};\n12\n13\nint main() {\n14\nFoo f1{};\n15\nconst Foo f2{};\n16\n17\n} // main()\nWhich of the following lines of code, when added on line 16, would result in a compilation error?\nf1.do_something();A)\nf1.do_something_const();B)\nf2.do_something();C)\nf2.do_something_const();D)\nE) None of the above\nPerson7. Suppose you had the following one-argument constructor for a object:\n1\nclass Person {\n2\nint32_t age;\n3\npublic:\n4\nexplicit Person(int32_t age_in) : age{age_in} {}\n5\n};\nPersonWhich of the following lines of code would cause a compilation error in this case, but would have had therun without issue\nexplicit?constructor not been marked as\nPerson annie;A)\nPerson billy = billy(18);B)\nPerson cathy(17);C)\nPerson danny = 16;D)\nPerson eddie(15.62);E)\nstatic_cast<>8. Which of the following is/are valid advantages of using a to convert between two types instead of a C-style cast, provided\nthat it is possible to do so?\nI. Static casts are better able to identify incompatible type conversions at compile time.\nII. Static casts will always be significantly faster than C-style casts for larger objects.\nIII. Static casts are less prone to dangerous or undefined behavior.\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) I, II, and III", "word_count": 386, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9f608b5e-e9a1-55bd-8838-e79f7076d036", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 52, "real_page_number": null, "text": "40\nChapter 1. Programming Foundations\n9. What is the output of the following code?\n1\nstruct Compare {\n2\nbool operator() (int32_t int32_t consta, b) {\n3\nreturn a < b;\n4\n} // operator()()\n5\n};\n6\n7\ntemplate <typename T>\n8\nvoid conditional_print(const std::vector<int32_t>& int32_tvalues, conditional) {\n9\nT comp;\n10\nfor (const int32_t val : values) {\n11\nif (comp(conditional, val)) {\n12\nstd::cout << val << ' ';\n13\n} // if\n14\n} // for\n15\n} // conditional_print()\n16\n17\nint main() {\n18\nstd::vector<int32_t> values = {183, 203, 280, 281, 370, 376};\n19\nconditional_print<Compare>(values, 281);\n20\n} // main()\n183 203 280A)\n183 203 280 281B)\n183 203 280 281 370 376C)\n281 370 376D)\n370 376E)\nSuppose your friend wrote the following function, which takes in a vector of integers, multiplies every value by a constant multiplier, and10.\nprints out the modified values.\n1\nvoid multiply_and_print_values(std::vector<int32_t>& int32_tvec, multiplier) {\n2\nfor (auto val : vec) {\n3\nval *= multiplier;\n4\n} // for j\n5\n6\nfor (auto val : vec) {\n7\nstd::cout << val << ' ';\n8\n} // for k\n9\n} // multiply_and_print_values()\n\"1 2 3 4 5 \"To test this change, your friend ran the following function. However, to their surprise, the output of this code was\n\"2 4 6 8 10 \".instead of\n10\nint main() {\n11\nstd::vector<int32_t> values = {1, 2, 3, 4, 5};\n12\nmultiply_and_print_values(values, 2);\n13\n} // main()\nThere is a bug in the function that is causing this unexpected output. On what line is this bug located?\nA) Line 1\nB) Line 2\nC) Line 3\nD) Line 6\nE) None of the above\nfor11. Consider the following definition of a loop:\nfor (int32_t i = n; i >= 0; --i) { ... }\nforWhich of the following loop definitions exhibits the exact same behavior as the loop above?\nA) for (int32_t i = n; i-- > 0;) { ... }\nB) for (int32_t i = n + 1; i-- > 0;) { ... }\nfor (int32_t i = n + 1; i > 0; i--) { ... }C)\nfor (int32_t i = n; --i > 0;) { ... }D)\nfor (int32_t i = n + 1; --i > 0;) { ... }E)\n12. Which of the following statements best defines the concept of abstraction with regard to object-oriented programming?\nA) The act of hiding a component’s internal implementation details from a user and only exposing relevant information for usage\nB) The act of combining both data and methods into a single unit\nC) The act of reusing the interface of a component when developing the functionality of another component\nD) The act of substituting multiple components with shared behavior into the same position of a program’s implementation\nE) None of the above", "word_count": 491, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e41a1208-51f7-5fc2-ae96-8cf68c919e85", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 53, "real_page_number": null, "text": "1.17 Exception Handling\n41\nfor13. Consider the following loop, which is designed to print out all values from 281 down to 0 in descending order:\n1\nint main() {\n2\nfor (size_t i = 281; i >= 0; --i) {\n3\nstd::cout << i << std::endl;\n4\n} // for i\n5\n} // main()\nDoes this code work as intended? If not, what is the issue?\nsize_tA) This code does not compile, since you cannot decrement a\nmain()B) This code does not compile, since does not explicitly return a value\nfor size_t i >= 0C) The loop does not terminate, since is unsigned, which causes to always evaluate to true\nD) More than one of the above\nE) The code has no issues and correctly performs the intended behavior\nfoo(), int32_t:14. Consider the following function which takes in an argument of type\n1\nvoid foo(int32_t x) {\n2\n/* ...implementation here... */\n3\n} // foo()\nConsider the following ways this function can be invoked:\nI. void bar(double x) {\nfoo(x);\n} // bar()\nII. void baz(int16_t x) {\nfoo(x);\n} // baz()\nIII. void qux(uint32_t x) {\nfoo(x);\n} // qux()\nWhich of the above function invocations could result in a loss of precision?\nA) I only\nB) II only\nC) I and III only\nD) II and III only\nE) I, II, and III\nChapter 1 Exercise Solutions\nuint32_t1. The correct answer is (A). Statement I is true because is unsigned, so it cannot represent a negative number; meanwhile,\nint32_t uint32_tis signed, so it can represent a negative number. Statement II is false because the type has a larger maximum value,\nint32_tas it can use the same 32 bits to represent larger values beyond the largest value of a since it does not need to represent negative\nuint32_tvalues. Statement III is false for the same reason: even though is able to reach a larger maximum value, it is able to do so\nbecause it does not need to represent negative values, not because it uses more memory to represent these higher numbers.\nstruct class2. The correct answer is (C). In C++, a and a are essentially identical, with the main difference being the protection level\nstruct, public, class, private.of its members. For a member variables default to while for a member variables default to Both\nstruct classand are able to support member functions.\n3. The correct answer is (A). Members in a custom object are initialized in the order they are listed. Members in a base class are initialized\nbefore members in classes that derive from it.\nconst4. The correct answer is (D). The placement on line 4 indicates that the pointer cannot be modified, but the integer it stores can.\nOnly option (D) modifies the pointer and not the value it points to, which would cause a compilation error.\nconst5. The correct answer is (D). The that is applied to the member function on line 4 indicates that the member function is not allowed\nclass bar),to modify the member variables of the itself (in this case, the value of provided that the member variable is not marked\nmutable constas (which gives member functions the permission to modify a variable). By this logic, statements II and III are true.\nbaz const int32_t baz const int32_tStatement I is false because the function argument itself is not marked as (e.g., instead of\nbaz), so the member function is still able to change its value.\nNon-const non-const const6. The correct answer is (C). member functions can only be called by objects (a object cannot invoke a\nnon-const member function, since such a function does not guarantee it does not modify any of the object’s members). Thus, option (C)\nf2 const, do_something()would result in a compilation error, since is but is not.\nexplicit7. The correct answer is (D). The keyword prevents the compiler from using a one-argument constructor to construct an object\nPerson int32_t explicitfrom an implicit type conversion. The object takes an as its only argument, so without the keyword, the\nPerson int32_t int32_t Personassignment of a object with an would cause the to be implicitly converted into an object of type\nexplicitusing the one-argument constructor. Adding the keyword prevents this. Options (A), (C), and (E) do not involve an implicit\ntype conversion, and option (B) would not compile regardless.", "word_count": 743, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "9fa4e629-1528-5378-aefa-ae1fad55bbd1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 54, "real_page_number": null, "text": "42\nChapter 1. Programming Foundations\n8. The correct answer is (D). Static casts are better than C-style casts because they are better able to identify incompatible type conversions\nstatic_cast<>at compile time and are thereby less prone to undefined behavior. If you perform a cast incorrectly, using could catch\nthis mistake during program compilation. However, C-style casts could still compile and perform the cast during runtime, which may be\ndangerous.\n9. The correct answer is (E). The templated function here applies a comparator to each number in a container and prints it out if the\nComparecomparator returns true. On line 19, we can see that the comparator is used to conduct this comparison. With templates, we can\nCompare T conditional_print() operator()essentially substitute in place of in the function on line 8. Since the overloaded\nCompare a < b, conditional a ifof returns true if and is passed into the value of on line 11, the statement on line 11 only evaluates\nconditional < val. conditionalto true if In our example, has a value of 281, so the function invocation prints out all numbers in\nthe container that are larger than 281.\n10. The correct answer is (B). The issue with the code is that the container of numbers is never modified at all, despite the multiplication that\nis performed on line 3. This is because the loop that performs this multiplication is accessing each integer instead of by reference.by value\nAs a result, the multiplication is performed on a local copy of each integer, and not the actual integer in the original container itself. The\nauto auto&val val.bug is therefore located on line 2 — to fix this bug, should be replaced with\ni11. The correct answer is (B). The best way to solve this problem is to identify what the value of should be on the first and last iterations of\ni = n, i = 0.the loop body. We can see that the first iteration of the loop runs with and the last iteration of the loop runs with It should\nforalso be noted that the termination condition of the loop is checked before the body of the loop runs, so it is also important to keep\ni i ntrack of any changes that are made to there as well. For (A) and (D), the value of starts at but gets decremented in the termination\ni = n - 1,condition check before the body of the loop can run. As a result, the first iteration of the loop runs with so these options cannot\ni n + 1be correct. For (C), the value of starts at but does not get modified in the termination condition, so the first iteration of the loop\nn + 1, i =also begins at which is not what we want either. Only options (B) and (E) run the first iteration of the loop body with a value of\nn. (--i i--) iHowever, choice (E) is incorrect because it uses prefix decrementation instead of and subtracts from before comparing\ni = 0, --i > 0 i = 1.with 0. As a result, the loop in (E) does not run an iteration of the loop body with since evaluates to false when On\ni i =the other hand, the loop in (B) compares the value of with 0 before decrementing its value, so it is still able to run one iteration where\n0, i-- > 0 i = 1.since the evaluates to true when\n12. The correct answer is (A). Abstraction is the idea of hiding a component’s implementation from the user and only providing the relevant\ninformation needed to use the component. This essentially separates what a component does (the interface) from how it works (the\nimplementation), and removes the need for a user to understand all the internal details of what they want to use. Choice (B) defines\nencapsulation, choice (C) defines inheritance, and choice (D) defines polymorphism.\ni >= 013. The correct answer is (C). This is a common bug when working with loops involving unsigned values: does not work as a\ni >= 0termination condition because it is always true! To fix this, you can either cast the variable in the loop to a signed integer so that\nican become false, or you can check that does not wrap around to the maximum value of the unsigned type.\n14. The correct answer is (C). Precision loss may occur if you perform a conversion from one type to a more restrictive type. In this case,\nfoo() int32_t, foo() int32_t.takes in an so precision loss may occur if a value is passed into that cannot be represented as a This\ndouble int32_t, int32_tis true for I and III: if you pass a into a it gets truncated since values cannot handle floating point values\nuint32_t(all decimal information therefore gets lost). Similarly, an unsigned can store values that are larger than the maximum value\nint32_t,representable with a signed so precision loss may arise here as well. Option II does not result in precision loss since any value\nint16_t int32_t.that can be represented with an can also be represented with an", "word_count": 882, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "be5e246d-a71f-5401-84e9-d8a3a4029f23", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 55, "real_page_number": null, "text": "Chapter 2\nFile and Stream I/O\n2.1\nStandard I/O Channels\nA stream is an abstraction that allows programs to send and receive data sequentially. Data can be extracted from a stream using the extraction\noperator>>, operator<<.and data can be inserted into a stream using theoperator insertion operator\n<iostream> <iostream>The C++ library implements much of the functionality associated with streams. The library is object-\nstdoriented, so streams are defined as objects with the following inheritance hierarchy (all of these objects are part of the namespace, which is\nnot explicitly included in the diagram below due to space constraints):\nios_base\nios\nostream\nostringstream\nofstream\nistream\nistringstream\nifstream\niostream\nstringstream\nfstream\nstd::ios_base std::iostreamThe class serves as the base class of this library. Furthermore, the class inherits all the members of both\nstd::istream std::ostreamthe (input stream) and (output stream) objects, which allows it to perform both input and output operations.\n<ios>, <istream>, <ostream>Note that and are not explicitly included in most C++ programs, since they are automatically included by\n<iostream>.the header files of derived class libraries, such as", "word_count": 182, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3624d95a-5b3f-5f51-be14-36c292094be6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 56, "real_page_number": null, "text": "44\nChapter 2. File and Stream I/O\nstd::cin, std::cout, std::cerr.The most common streams that you will use in this class are and The first of these streams,\nstd::cin, std::istream cinis an object of type known as the for character input (in fact, the name is short forstandard input stream\nstd::cininput). The stream accepts input from the user via a standard input device such as a keyboard. To extract characters fromcharacter\nstd::cin operator>>the stream, the extraction operator should be used.\nstd::cinIn the following code, the user is prompted to enter a string into the console. When the user inputs the string, it is held in the\noperator>>. namestream and extracted using After the value is extracted from the string, it is stored in the variable.\nstd::string name;\nstd::cin >> name;\nThe extraction step would fail if the input from the user cannot be interpreted as the type of the variable that the data is being read into. For\ninstance, the following would fail if the user input cannot be interpreted as an integer.\nint32_t val;\nstd::cin >> val; // the next item in the stream must be an int\nstd::cout std::ostreamThe stream is an object of type known as the for character output (where the namestandard output stream\ncout std::coutis short for output). The object allows characters to be displayed on a standard output device, such as yourcharacter\ncout operator<<console. To insert characters into the stream for printing, the insertion operator should be used.\nstd::string text = \"This text is printed out as program output.\";\nstd::cout << text;\nstd::cerr std::ostreamLastly, the output stream is an object of type that represents the stream. The error output streamstandard error\nstd::cout,is distinct from the program output stream, which allows the programmer to work with these two outputs separately. Similar to\nstd::cerr operator<<.writing data to the stream requires use of the insertion operator\nstd::string text = \"This text is printed out as error message output.\";\nstd::cerr << text;\nAnother thing to note is that these operations work with Boolean values as integer values (0 and 1) instead of their text forms (true and false). In\nthe example below, 1 is printed to the console.\nbool true;b =\nstd::cout << b;\nstd::boolalpha trueTo work with Booleans in their text representations, use the keyword. The following would print out the word\nrather than the number 1.\nbool true;b =\nstd::cout << std::boolalpha << b;\nstd::boolalpha cinThe same keyword applies to reading input as well; using after extracting from would allow you to read the words\n\"true\" and \"false\" into a Boolean.\nbool b;\n// user inputs the word \"true\"\nstd::cin >> std::boolalpha >> b;\n// b is true (would not work without boolalpha)\nLastly, the insertion and extraction operators can be chained together in a single expression to insert or extract multiple objects. For example,\n\"EECS281\", std::coutthe following code would print as both the string and integer are inserted into the stream:\nstd::string str = \"EECS\";\nint32_t num = 281;\nstd::cout << str << num;\n2.2\nInput and Output Redirection\nIn this class, you will frequently be working with input files for your projects. Given a file, how can you read its contents into your program?\nstd::cinOne method that can be used is input redirection. The goal of input redirection is to redirect the contents of the input file into the\nstandard input stream and then extract the contents out of the stream in your program. As a result, if you use input redirection, you can read\nstd::cinfrom a file by directly extracting from (and treating the contents of the file as if they were inputted by the user on the console).\nmyProgramTo illustrate this process, suppose we compiled the following program into an executable named that takes in input from the\nstd::cin.standard input stream, If we were to run this program on the command line:\n./myProgram\nthe user would be prompted to give it input (in this case, item and price).", "word_count": 681, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dc4afe12-1f01-5005-b510-5419920ad270", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 57, "real_page_number": null, "text": "2.3 Reading Input in a Loop\n45\nstd::cin.In this example, we would manually enter each item and its price into the console, which the program reads in using However,\nsuppose we have all of our inputs in a separate file:\nInstead of manually inputting this information into the console one item at a time, we can use the input redirection command to send the entire\nstd::cininput file to the program via and treat its contents as user input:\n./myProgram < students.txt\n< students.txtHere, the symbol represents the operator. This operator redirects the contents of to the standard inputinput redirection\nstd::cin, operator>>stream so that we can extract its contents using in our program.\n>,Output redirection is a similar process that works the other way around. By using the output redirection operator, or we can send the\nstd::coutoutput of a program to a file. This is done by redirecting the contents of the standard output stream to a specified file on the\nmyProgram report.txt.command line. In the following command, the console output of the executable gets written to a file named\n./myProgram > report.txt\nstudents.txtYou can also combine input and output redirection in a single command. In the following command, the contents of will be\nreport.txt.used as input for the program, and the output of the program will be sent to a file named\n./myProgram < students.txt > report.txt\nstd::cin <By using redirection, everything you extract from will come from the file name that follows the symbol, and everything you print\nstd::cout >to will be sent to an output file with the file name that follows the symbol.\nstd::cin std::cout, std::cerr.Remark: In addition to and redirection can also be done on the standard error stream, To redirect\nstd::cerr (2>).the contents of the stream to a file, you can use the error redirection operator For example, the following redirects the\nmyProgram error_report.txt.error output of the executable to a file named\n./myProgram 2> error_report.txt\n2.3\nReading Input in a Loop\nstd::cin std::istreamOne interesting feature about and other objects of type is that they can be implicitly converted to a Boolean. If\ntrue.an input stream has no errors and is ready for extraction, the stream object can be implicitly converted to the value If the input stream is\nfalse.in an error state, the stream can be implicitly converted to a value of\nBecauseitispossibletoimplicitlyconvertbetweenastreamandaBoolean,youcanreadinanentirefileusinginputredirectionbyextracting\nstd::cin while std::cin falsefrom in the condition of a loop. Because implicitly converts to once it is in an error state, the loop will\nautomatically terminate once there is nothing left to read!\n1\nstd::string word;\n2\nwhile (std::cin >> word) {\n3\n// word stores the next word in the input stream\n4\n} // while\nwhileReading the contents of a stream inside a loop is the best way to guarantee that everything is read in correctly. This is especially true\nstd::cinwhen input from is redirected from an input file rather than the user’s keyboard.\nstd::cinWhen working with streams, you may encounter several member functions that can be used to check the status of an input\ncin.good(), cin.eof(), cin.bad() cin.fail(). whilestream, such as and These should not be used as the condition of a loop,\nfalseas these functions do not return until an operation has failed. This means that the loop would still have to run once after the inputafter\nbecomes invalid.\n100thTo illustrate this using an example, suppose we are redirecting input from a text file with 100 values. After we are done reading the\n101st 101ststd::cinvalue, we go back to the beginning of the iteration and check if is still good. There is no value, but the program doesn’t\n101ststd::cinknow that! At this point, hasn’t gone bad yet, so the loop continues. Only when the program attempts to read in the value does\ncin.good() falseit realize that this value does not exist, and the value of becomes (the other three functions behave similarly). However,\nat this point, it is too late: the program has already attempted to read in a non-existent value from the file, so it has already failed.", "word_count": 726, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "417a62eb-2b43-5b6c-a339-473c99d43b37", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 58, "real_page_number": null, "text": "46\nChapter 2. File and Stream I/O\n2.4\nStream Extraction\n¸ 2.4.1\nThe Extraction Operator\noperator>>.Thereareseveralwaystoextractthecontentsoutofaninputstream. Oneway, asmentionedpreviously, istheextractionoperator\nThe extraction operator leading whitespace and consumes as many items as possible (for the type being read into) until it encountersignores\nwhitespace or the flag (a special flag that denotes that there is no more data to read). The extraction operator also stops reading fromend-of-file\nthe stream if the stream is in an error state (which can happen if the next element extracted from the stream does not match the data type it is\nstd::istreambeing read into). Every time the extraction operator is called, it returns a reference to the object that it reads from (which can\nbe implicitly converted to a Boolean). Let’s look at the following example file.\ninput.txt\n∙∙∙here∙∙∙is∙∙∙a¶\n∙∙∙file∙∙∙to∙∙∙be¶\n¶\nread\nstd::cinHere, the \"∙\" character represents a space, and the \"¶\" character represents a newline character. Let’s redirect this file to and extract\noperator>>its contents using to see what happens.\n1\nstd::string word;\n2\nwhile (std::cin >> word) {\n3\nstd::cout << word;\n4\n} // while\nWhentheprogramfirstbeginsreadingfromthefile,theextractionoperatorignoresthewhitespacebeforetheword\"here\". Then,itconsumeseach\nstd::string,character of the word \"here\" until it reaches the whitespace after it (this is because we are trying to read into an object of type\noperator>> wordextracts everything until it reaches the next whitespace). Once the word \"here\" is stored in the variable, the program\nstd::cout.prints it to the standard output stream, or This continues for the rest of the file, where the extraction operator reads the next\nwordstring in the stream into the variable, and the newline character acts as a delimiter that separates the previous word from the next. Since\nwhitespace is ignored, the output of the above program is:\noutput.txt\nhereisafiletoberead\nThe extraction operator begins where you previously left off: if you extract something from the stream, you also remove it from the stream. As a\nresult, you can read in the first five values of a stream, do some other stuff, and then come back to extract the sixth value immediately after. You\ndo not have to start over from the beginning of the file whenever you extract information from a file using input redirection.\n¸ 2.4.2\nGetline\nstd::getline()Another method for extracting data from an input stream is the function.\ncharstd::istream& std::getline(std::istream& is, std::string& str, delim);\nis str delim delimExtracts characters from the input stream and stores them into the string until the delimitation character is found (if\n'\\n'is not specified, the newline character is used by default).\nstd::cin),The first parameter is the input stream that should be read from (such as the second parameter is destination of the input (the\nobject that the input value is stored in), and the third parameter is the delimiting character (the character to read up to). The third parameter is\nstd::getline()optional, and it’s perfectly okay to call on just two parameters:\nstd::istream& std::getline(std::istream& is, std::string& str);\n('\\n')If the third parameter is omitted, the newline character acts as the delimiter.\nstd::getline()The function consumes all characters — even whitespace — until it reaches the delimiting character. This delimiting\nstd::getline()character is discarded, and everything that was read up to it is stored. Like with the extraction operator, returns a reference\nfalse std::getline()to the input stream, which can be implicitly converted to once the stream goes bad. This means that can be\nwhileplaced as the condition of a loop if the function is being called in a loop. Let’s look at the same example file, but this time we will use\nstd::getline() to extract characters, using the newline character as a delimiter.\ninput.txt\n∙∙∙here∙∙∙is∙∙∙a¶\n∙∙∙file∙∙∙to∙∙∙be¶\n¶\nread\nThe following code will be used.\n1\nstd::string line;\n2\nint32_t counter = 1;\n3\nwhile (std::getline(std::cin, line)) {\n4\nstd::cout << \"line \" << counter++ << \": \" << line << std::endl;\n5\n} // while", "word_count": 701, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fdf49c57-ced1-540a-80d9-b9050d93544e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 59, "real_page_number": null, "text": "2.4 Stream Extraction\n47\nstd::getline()Here, the first call to consumes the entire first line up to the first newline character (¶) and stores the result in the variable\nline. line \"∙∙∙here ∙∙∙is ∙∙∙a\",Hence, during the first iteration of the loop, the string has a value of including the spaces (only the\nstd::getline()delimiting character is discarded). With each subsequent iteration, retrieves the part of the stream up to the next newline\nline.character, storing its contents into The final output of the above code is as follows:\noutput.txt\nline 1: ∙∙∙here∙∙∙is∙∙∙a\nline 2: ∙∙∙file∙∙∙to∙∙∙be\nline 3:\nline 4: read\nstd::getline()On the third iteration, does not retrieve any characters at all, since there are no characters between the second and third\nnewline characters. As such, the output for line 3 is empty.\nstd::getline()As mentioned previously, we can specify a custom delimiter with the function. For example, suppose we have the\nfollowing file (with no newlines):\ninput.txt\n0|12332|99:74:11:21:61|TCP|Connection attempt failed|149|36240|70:17:34:28:94|TCP|Connection lost|281|\n25313|11:30:32:34:70|OS|System failure\n'|' getline()To split this file using as the delimiting character, simply pass it as the third argument of each call:\n1\nstd::string line;\n2\nint32_t counter = 1;\n3\nchar delim = '|';\n4\nwhile (std::getline(std::cin, line, delim)) {\n5\nstd::cout << \"line \" << counter++ << \": \" << line << std::endl;\n6\n} // while\nThis code would produce the following output:\nline 1:\n0\nline 2:\n12332\nline 3:\n99:74:11:21:61\nline 4:\nTCP\nline 5:\nConnection attempt failed\nline 6:\n149\nline 7:\n36240\nline 8:\n70:17:34:28:94\nline 9:\nTCP\nline 10:\nConnection lost\nline 11:\n281\nline 12:\n25313\nline 13:\n11:30:32:34:70\nline 14:\nOS\nline 15:\nSystem failure\nstd::getline() operator>> std::getline()One common mistake with involves reading from an input stream using both and on\n>> std::getline()the same line of input. Because and both start from where you previously left off, both operations may end up reading\nfrom the same line! For example, consider this file, where \"∙\" represents a space and \"¶\" represents a newline character:\ninput.txt\n2¶\napple¶\nbanana¶\n>> getline()Let’s try to run the following code, which uses both the extraction operator and to extract from standard input. In this code,\nwe assume that the first line of the input file represents the number of fruits in the file, and we loop through these fruits and print them out:\n1\nint32_t num_fruits;\n2\nstd::cin >> num_fruits;\n3\nstd::string line;\n4\nfor (int32_t i = 1; i <= num_fruits; ++i) {\n5\nstd::getline(std::cin, line);\n6\nstd::cout << \"fruit \" << i << \": \" << line << std::endl;\n7\n} // for i\nHowever, this is the output we get:\nfruit 1:\nfruit 2:\napple", "word_count": 455, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d2a98a89-7431-5e94-925f-171b81337a75", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 60, "real_page_number": null, "text": "48\nChapter 2. File and Stream I/O\nstd::getline() \"banana\"We called twice after reading in the number on the first row, so should have been extracted. Why wasn’t it?\nstd::getline()Recall that reads until it reaches a newline character (or delimiting character, if otherwise specified) and then discards\n2that delimiting character. However, after extracting out of the input stream, the next character in line is a newline character; as a result,\nstd::getline() std::getline()would not read in anything at all! To fix this, you would have to run an additional time to remove\nthe residual input that was extracted on line 1. One possible approach is shown below:\n1\nint32_t num_fruits;\n2\nstd::cin >> num_fruits;\n3\nstd::string line;\n4\nstd::getline(std::cin, line);\n// clear out everything before next line\n5\nfor (int32_t i = 1; i <= num_fruits; ++i) {\n6\nstd::getline(std::cin, line);\n7\nstd::cout << \"fruit \" << i << \": \" << line << std::endl;\n8\n} // for i\nstd::istream::ignore() std::cin std::getline(),However, a cleaner way would be to use whenever you switch from to as\n<limits> std::numeric_limits<>).shown on line 3 below (include the library to use\n1\nint32_t num_fruits;\n2\nstd::cin >> num_fruits;\n3\nstd::cin.ignore(std::numeric_limits<std::streamsize>::max(), '\\n'); // remove whitespace and newline\n4\nstd::string line;\n5\nfor (int32_t i = 1; i <= num_fruits; ++i) {\n6\nstd::getline(std::cin, line);\n7\nstd::cout << \"fruit \" << i << \": \" << line << std::endl;\n8\n} // for i\n¸ 2.4.3\n(✽)Ignore\nstd::istream::ignore(),As shown, one useful feature of input streams is which can be used to discard undesired characters.\nintstd::istream& std::istream::ignore(std::streamsize n, delim);\nExtracts and discards either\nn• characters from the input stream\ndelim, delim• until and including if is encountered\n• until the end of the file or stream\nn delimwhichever comes first. If is not specified, then its value defaults to 1. If is not specified, then its value is set to a special end-of-file\nnidentifier (which indicates the end of a data source). If you do not care about and simply want to discard all characters up to a delimiter,\nn std::numeric_limits<std::streamsize>::max().you should set equal to\nstd::streamsize size_tNote: is essentially a signed version of (i.e., it can be positive or negative).\nThis ignore feature can be handy if you know that certain characters of your input should be discarded. For instance, suppose we had the\n\"Count:\"following input file, but with the label directly before the number of fruits specified.\ninput.txt\nCount:∙2¶\napple¶\nbanana¶\nIf we wanted to ignore this word before reading in the number of fruits, we could have used an additional junk variable to store this undesired\ninput, as shown in the code below:\n1\nstd::string junk;\n2\nstd::cin >> junk; // reads in \"Count:\"\n3\nint32_t num_fruits;\n4\nstd::cin >> num_fruits; // reads in 2\n5\nstd::string line;\n6\nfor (int32_t i = 1; i <= num_fruits; ++i) {\n7\nstd::getline(std::cin, line);\n8\n// get rid of the line if it is all whitespace\n9\nif (line.find_first_not_of(\" \") == std::string::npos) {\n10\n--i;\n11\ncontinue;\n12\n} // if\n13\nstd::cout << \"fruit \" << i << \": \" << line << std::endl;\n14\n} // for i", "word_count": 537, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fb2bc828-735c-5b2e-8173-7e0933271681", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 61, "real_page_number": null, "text": "2.4 Stream Extraction\n49\nignore()However, as mentioned earlier, a cleaner method would be to use to discard all characters up to the first space.\n1\n// ignore everything up to and including the first space character\n2\nstd::cin.ignore(std::numeric_limits<std::streamsize>::max(), ' ');\n3\nint32_t num_fruits;\n4\nstd::cin >> num_fruits; // reads in 2\n5\n// ignore everything up to the next newline\n6\nstd::cin.ignore(std::numeric_limits<std::streamsize>::max(), '\\n');\n7\nstd::string line;\n8\nfor (int32_t i = 1; i <= num_fruits; ++i) {\n9\nstd::getline(std::cin, line);\n10\nstd::cout << \"fruit \" << i << \": \" << line << std::endl;\n11\n} // for i\nAnother example is shown below, this time ignoring a specified number of characters instead of using a delimiter:\ninput.txt\n123456789\n1\nstd::cin.ignore(5); // extract and discard first five characters\n2\nint32_t num;\n3\nstd::cin >> num;\n4\nstd::cout << num << std::endl; // prints out 6789\n¸ 2.4.4\nSummary of Stream Operations\nSo far, we have discussed several methods that can be used to extract from a stream. The two most important of these are the extraction operator\n>> std::getline().and Illustrations depicting the functionality of these two operations are shown below:\noperator>>\nExtracts as many items out of the stream as possible for the type being read into, until whitespace/newline or end-of-file is reached.\ninput.txt\nabc∙def¶\nghi¶\na\nb\nc\n∙\nd\ne\nf\n¶\ng\nh\ni\ninput stream\nstd::string str;\nstd::cin >> str;\n\"abc\"\nstr\n∙\nd\ne\nf\n¶\ng\nh\ni\ninput stream\nextract from stream and store in string\nwhitespace ignored and discarded\nstd::getline()\nExtracts as many items out of the stream as possible and stores into a string, until a newline (or specified delimiter) or end-of-file is reached.\ninput.txt\nabc∙def¶\nghi¶\na\nb\nc\n∙\nd\ne\nf\n¶\ng\nh\ni\ninput stream\nstd::string str;\nstd::getline(std::cin, str);\n\"abc def\"\nstr\n¶\ng\nh\ni\ninput stream\nextract from stream and store in string\nnewline ignored and discarded", "word_count": 325, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5fed267d-3d50-5211-b17d-a7cca0cbdd8d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 62, "real_page_number": null, "text": "50\nChapter 2. File and Stream I/O\n2.5\nStream Buﬀers and Flushing (✽)\nWhen you send data to an output stream, the characters are not always immediately written to the console (or a file if you are using output\nredirection). Instead, depending on the stream type, the output values may be buffered before they are fully printed out.\nstd::cout,A buffer is a block of memory allocated for a stream object to hold temporary stream data. When you send data to for\ninstance, the data gets stored in this buffer instead of appearing immediately in an output console or file. This is done for efficiency reasons; it is\nmuch faster to write data in blocks rather than one character at a time.\nstd::flushAbufferusuallygetsemptied(orflushed)onceitisfilledtocapacity. However,bufferscanbemanuallyflushedbyapplyingthe\nmanipulator on its output stream.\n1\nstd::cout << \"Hello World\";\n// \"Hello World\" gets sent to cout output buffer\n2\nstd::cout << std::flush;\n// flushes buffer, \"Hello World\" written to console\nstd::coutThe stream is buffered, meaning that characters are stored in a buffer and written in chunks rather than one at a time. Theblock\nstd::cinsame is true for the input stream, which buffers input values so that they are read and processed in chunks. The one exception to this\nstd::cerr, std::cerrrule is the standard error stream which is (i.e., output sent to will appear in the destinationunbuffered by default\nstd::cerrlocation as soon as it is written). We will look at a reason why is unbuffered later in this section.\nThe fact that excessive flushing is inefficient has consequences regarding the printing of newlines. In previous classes, you likely used\nstd::endl '\\n'to print out a newline character. However, you can also print out the newline character using (the escape sequence\nrepresenting a newline character). Let’s consider the following two implementations, which do the exact same thing:\n1\nfor (int32_t i = 0; i < 100000; ++i) {\n2\nstd::cout << \"Hello World\" << std::endl;\n// use std::endl to print a newline\n3\n} // for i\n4\n5\nfor (int32_t i = 0; i < 100000; ++i) {\n6\nstd::cout << \"Hello World\\n\";\n// use '\\n' to print a newline\n7\n} // for i\n'\\n'Upon first glance, both loops should take roughly the same amount of time to complete. However, the second approach using is actually\nstd::endl. std::endlmuch faster than the first! How is this even possible? The reasoning lies in the implementation details of The\nstd::endloperation doesn’t just print out a newline — it also flushes the buffer. In other words, calling is equivalent to calling\nstd::cout << '\\n'<< std::flush;\n'\\n'In the first implementation, the output stream buffer is flushed at the end of of the loop. Meanwhile, does not invokeevery single iteration\nan immediate flush after every iteration, so you save time by buffering the output and printing it out in chunks. Therefore, if you do not intend\n'\\n' std::endlon writing output immediately to your destination, it is faster to use instead of to print a newline to the output stream.\nstd::endlThere are cases, however, where using is preferred. When output is buffered, and the program crashes before the buffer is\nflushed, you may end up losing the information in the buffer. Consider the following:\n280\n...\n281\nstd::cout << \"Reached line 281\\n\";\n282\n...\n310\n...\n311\nraise(SIGSEGV); // oops, there's a segfault on this line\n312\nstd::cout << \"Reached line 312\\n\";\n313\n...\nIf the output on line 281 was not flushed from the buffer before the program crash on line 311, it is lost. Running this program may result in the\nfollowing console output, which would lead one to believe that the program never made it to line 281:\nSegmentation fault (core dumped)\nHowever, if these debug lines were flushed immediately to the terminal, then the output \"Reached line 281\" would appear as soon as the code\nreached line 281! If we ran the following code instead:\n280\n...\n281\nstd::cout << \"Reached line 281\" << std::endl;\n282\n...\n310\n...\n311\nraise(SIGSEGV); // oops, there's a segfault on this line\n312\nstd::cout << \"Reached line 312\" << std::endl;\n313\n...\nthe following would be printed to console:\nReached line 281\nSegmentation fault (core dumped)\nstd::cerrNow, we know that the program made it to line 281, but crashed before making it to line 312. This is a reason why the error stream\nstd::cerr std::cout)!is unbuffered and prints to its destination immediately (and why it is preferred to send error output to instead of", "word_count": 780, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ff54efe9-9e21-5057-99f8-bd3fd9bc20ab", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 63, "real_page_number": null, "text": "2.6 C and C++ Stream Synchronization\n51\n2.6\nC and C++ Stream Synchronization (✽)\nstd::cin std::coutThe and streams are C++ objects. In C, however, things are a bit different. Unlike C++, the three standard streams in\nstdin, stdout, stderr. printf()C are and To print output to standard output in C, you would use the function instead of the insertion\nstd::cout. scanf()operator and To read input from standard input in C, you would use the function instead of the extraction operator and\nstd::cin. You do not need to know how to work with C in this class, but it is still worthwhile to know that these different I/O methods exist.\nBecause C and C++ I/O can be used in conjunction, there needs to be a way to ensure that streams in C and C++ are synchronized if a\nprogrammer uses both C and C++ style I/O in their program. For example, if you print output using C style I/O, and then print out additional\noutput using C++ style I/O, the output from the C I/O should be printed to the console the output from the C++ I/O. In C++, thisbefore\nstd::cout stdoutsynchronization is ensured by making C++ streams share a buffer with their corresponding C stream (e.g., and write to\nthe same buffer). As an illustration, consider the following example:\n1\nint main() {\n2\nprintf(\"EECS\");\n3\nstd::cout << \"281\";\n4\n} // main()\n\"EECS\" stdout \"281\" std::cout \"EECS\"Here, is sent to the C stream and is sent to the C++ stream. However, since is sent to an\n\"EECS\" \"281\".output stream first, you should expect to be printed before This process is known as synchronization.stream\nHowever, synchronization can slow down the process of I/O, since extra work is needed to maintain that the correct ordering of characters. It\nsync_with_stdio() std::ios_base:is possible to turn off synchronization between C and C++ streams using the member function in\nstd::ios_base::sync_with_stdio(false);\nstdioBy running this line of code, you turn off synchronization with (the name of the C standard input and output library). This can be used\nto speed up the execution time of your program, especially if you process a lot of input or output. However, it is important to note the side\neffects of turning off synchronization — this line should not be added if your program uses both C and C++ style I/O! If we run the following\ncode with synchronization turned off:\n1\nint main() {\n2\nstd::ios_base::sync_with_stdio(false);\n3\nprintf(\"EECS\");\n4\nstd::cout << \"281\";\n5\n} // main()\n\"EECS\" \"281\".there is no longer a guarantee that will be printed before Since there is no synchronization between C and C++ streams,\n\"EECS281\",the two are allowed to have their own independent buffers. If the C stream’s buffer flushes first, the output would be but if the\n\"281EECS\".C++ stream’s buffer flushes first, the output would be If both streams try to flush at the same time, you might even end up with\n\"E2E8C1S\"!something like Without synchronization, all bets are off.\nThat being said, you will not be doing C-style I/O in this class, so it is perfectly okay to turn synchronization off for the projects in this class.\nJust note that the improved performance is a side effect of this operation, and that you should not blindly include this line in every program you\nwrite, just because it might speed things up.\n2.7\nStringstreams (✽)\nstd::stringA stringstream allows you to treat a object as if it were a stream. That is, it gives you the ability to read/write to a string just\n<sstream>like how you would read/write a stream. To use stringstreams in your program, make sure to include the library.\nstd::string std::istringstream.Ifyouwanttotreata asan stream,youcandeclarean Anobjectofthistypeinheritsfrominput\nstd::istream std::istringstream std::istream(seethefigureinsection2.1),soan canbeusedwhereveraninputstreamoftype\nis expected (due to polymorphism).\nstd::istringstream std::stringAn example using an is shown below. In this example, a containing numbers is converted to\nstd::istringstream, >> std::istringstreaman and the extraction operator is used to extract the numbers out of the as integers.\nstr() \"4 66 30 46The member function can be used to set the contents of the stream. The following code outputs \".\n1\nstd::istringstream iss;\n2\nstd::string numbers = \"2 33 15 23\";\n3\niss.str(numbers); // initializes iss with the contents of numbers string\n4\nint32_t val;\n5\nfor (int32_t i = 0; i < 4; ++i) {\n6\niss >> val;\n// extracts number from iss and stores in val\n7\nstd::cout << val 2 << ' ';*\n// multiplies val by two and prints it out\n8\n} // for i\nstd::string std::ostringstream.If you want to write to a like an output stream, you can declare an An object of this type inherits\nstd::ostream, std::ostringstream std::ostreamfrom so an can be used wherever an output stream of type is expected (again\ndue to polymorphism).\nstd::ostringstream std::ostringstreamAn example using an is shown below. In this example, we initialize an and write\n<<. str()data to it using the insertion operator Then, we convert the contents of the stream into a string using the member function.\n1\nstd::ostringstream oss;\n2\noss << \"I really enjoy EECS 281\";\n// sends text to the ostringstream\n3\noss << \"!!!\";\n// sends text to the ostringstream\n4\nstd::string s = oss.str();\n// converts contents of oss to a string\n5\nstd::cout << s << '\\n';\n// prints the final string\n\"I really enjoy EECS 281!!!\".The output of the above code is", "word_count": 949, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a00a99ad-40a7-5452-bf4b-b2a32a8192aa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 64, "real_page_number": null, "text": "52\nChapter 2. File and Stream I/O\n2.8\nFile Streams (✽)\n<fstream>Input redirection is not the only way you can read or write data from files. Another method is to use file streams in the library.\nstd::ifstreamThere are three main types of file streams that we will cover in this section. An object, or input file stream, is a type of\nstd::ofstream,stream that allows you to read from a file. An or output file stream, is a type of stream that allows you to write to a file.\nstd::fstreamLastly, a generic object is a general file stream type that is able to do both file reading and file writing.\nstd::ifstream,To use an here are a few member functions that are important to know:\nFunction\nBehavior\nvoid open(const char* filename);\nfilenameOpens the file with the name and associates it with the input file stream object\nbool is_open();\nChecks to see if the input file stream is associated with a file that was successfully opened\nvoid close();\nCloses the file associated with the input file stream and disassociates it from the stream\noperator>>\nstd::istream)Extracts input from the input file stream (this is inherited from\nstd::ifstreamOnce an is associated with a file, you can use it in the same way as any other input stream. The code below creates an\nstd::ifstream input.txt.object that reads from a file named When working with file streams, it is good practice to check if the file\nis_open())was successfully opened (using before extracting from it.\n1\n// create ifstream object and associate it with input.txt\n2\nstd::ifstream fin;\n3\nfin.open(\"input.txt\");\n4\n5\n// check if file was successfully opened\n6\nif (!fin.is_open()) {\n7\nstd::cerr << \"Failed to open file.\" << std::endl;\n8\nexit(1);\n9\n} // if\n10\n11\n// read from file using ifstream\n12\nstd::string word;\n13\nwhile (fin >> word) {\n14\n// do stuff here\n15\n} // while\n16\n17\n// close file after done\n18\nfin.close();\nstd::ofstreamAn object has similar functions for opening and closing files. Instead of the extraction operator, it supports the insertion\noperator for writing output to a file.\nFunction\nBehavior\nvoid open(const char* filename);\nfilenameOpens the file with the name and associates it with the output file stream object\nbool is_open();\nChecks to see if the output file stream is associated with a file that was successfully opened\nvoid close();\nCloses the file associated with the output file stream and disassociates it from the stream\noperator<<\nostream)Inserts output into the output file stream (this is inherited from\nstd::ofstream \"output.txt\"The following code creates an object, associates it with the file, and then writes to the file using the\nstd::ofstream is_open()object. Much like before, the status of the output file stream should be checked with before writing to it.\n1\n// create ofstream object and associate it with output.txt\n2\nstd::ofstream fout;\n3\nfout.open(\"output.txt\");\n4\n5\n// check if file was successfully opened\n6\nif (!fout.is_open()) {\n7\nstd::cerr << \"Failed to open file.\" << std::endl;\n8\nexit(1);\n9\n} // if\n10\n11\nfout << \"This data is written to the file.\" << std::endl;\n12\n13\n// close file after done\n14\nfout.close();\nstd::fstream std::fstreamThe type supports both insertion and extraction, as well the other member functions listed above. Thus, an\nstd::ifstream std::ofstreamobject provides both read and write access to a file. However, it is good practice to use and instead of\nstd::fstream unless you absolutely need both read and write access to a file in a single object (which is not a common occurrence). This is\nstd::ifstream std::ostreambecause the and objects make it clear whether you are reading or writing, which minimizes the chance\nstd::fstreamfor human error compared to a generic object.", "word_count": 634, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f662276a-ead0-5e56-928c-23eb1572b090", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 65, "real_page_number": null, "text": "2.9 Reading Input Using Polymorphism\n53\n2.9\nReading Input Using Polymorphism (✽)\nTorecap, mostcommandlineshellsallowprogramstosendoutputtoorreadinputfromfiles. Thisisaccomplishedbyusingtheinputredirection\n(>) (<).operator or the output redirection operator Reading input from a file can be thought of as temporarily disconnecting the keyboard and\ngetting all input from a specified input file. This technique is known as input redirection, and replaces keyboard input with input from a file.\nOn a similar vein, writing output to a file can be thought of as temporarily disconnecting the screen and sending everything that would have\nbeen printed to the console directly to a file. The file does not need to exist before redirection; it will be created as necessary. If the file already\nexists, its original contents will be replaced with the new output. This technique is known as output redirection and sends text that would have\nbeen displayed on the screen directly to the specified file.\nstd::cerr.Redirection can be done on input, output, both, or neither, as well as on the standard error stream As previously mentioned,\n(2>) std::cerrthe error redirection operator can be used to redirect output in to a file.\nOften, a program will try to read from a file specified at the command line, but if one is not specified, it will try to read from standard input.\nPrograms that do this perform identically when invoked with the following two commands:\n./program < input.txt\n./program input.txt\ninput.txt programThe first variant redirects the contents of the file to the executable using input redirection, while the second variant\nspecifies the file name directly as an argument on the command line (and thus should be read in using a file stream). How can we write a\ninput.txtprogram that supports both methods of specifying an input file? That is, the program must be able to retrieve input from the file,\n< <).regardless of whether it is directly specified on the command line (no before the file name) or sent in via input redirection (using\nstd::ifstreamIt turns out this can be done with a few conditionals, thanks to polymorphism! Recall that the class inherits from the\nstd::istream std::istreamclass (see the diagram in section 2.1). Polymorphism thus allows an object of type to be assigned an object\nstd::ifstream, std::ifstream std::istream. std::istreamof type since is a subclass of As a result, we can create a single\nstd::cinobject in our program and assign it to either (if a file is passed in using input redirection) or an input file stream (if the file name is\nspecified as an argument on the command line).\n1\nstd::ifstream fin;\n2\nstd::string filename;\n// use argc and argv to retrieve file name from the\n3\nif (argc > 1) { ... }\n// command line if it is specified (see chapter 3)\n4\n5\nif (!filename.empty()) {\n6\nfin.open(filename);\n7\nif (!fin.is_open()) {\n8\nstd::cerr << \"Unable to open input file: \" << filename << std::endl;\n9\nexit(1);\n10\n} // if\n11\n} // if\n12\n13\nstd::istream& in = fin.is_open() ? fin : std::cin;\n14\n15\n// Read input by extracting from 'in' for the rest of the program (in >> ...)\n16\n// If a file was specified, 'in == fin' (extracts from file stream)\n17\n// If no file was specified, 'in == cin' (extracts from standard input)\n18\n19\nfin.close();\nfin.In this code, we first create an input file stream object named Then, we look on the command line to see if a file name was specified\n(command line parsing will be covered in the next chapter). On line 5, we check to see if we found a file name on the command line. If we\nfound a file name, we use the file stream to open it on line 6.\nstd::istreamLine 13 is where the polymorphism comes into play. We create a single object that is used to read input in the program. If\nfin in fin).the file stream was successfullyassociated with a file, then wewould readfrom the filestream(i.e., we would assign to Otherwise,\nin std::cin). inwe would read from standard input (i.e., we would assign to After line 13 runs, the stream will hold the input we want to\n(fin) (std::cin). inread, regardless of whether that input is sent via a file stream or standard input Note that is a to the stream itreference\nin, (in finis assigned; thus, when we read from we are always reading from the correct input stream is a reference to if the file is passed in\nstd::cinvia the command line, and a reference to if input redirection is used).", "word_count": 798, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "de8df682-ac04-59ee-abf4-70e425813de0", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 66, "real_page_number": null, "text": "54\nChapter 2. File and Stream I/O\nChapter 2 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\noperator>> std::getline()1. Which of the following statements is most accurate regarding and the default behavior of (without any\ncustom delimitation character specified)?\n>> std::getline()A) The extraction operator ignores leading whitespace, but does not\nstd::getline() >>B) ignores leading whitespace, but the extraction operator does not\n>> std::getline()C) Both the extraction operator and ignore leading whitespace\n>> std::getline()D) Neither the extraction operator and ignores leading whitespace\nE) None of the above\n2. Consider the following code, which reads from a file using input redirection:\n1\nint main() {\n2\nint32_t num = 0;\n3\nstd::string line;\n4\nint32_t counter = 0;\n5\nwhile (std::cin >> num) {\n6\nstd::getline(std::cin, line);\n7\n++counter;\n8\n} // while\n9\nstd::cout << \"num: \" << num << std::endl;\n10\nstd::cout << \"counter: \" << counter << std::endl;\n11\n} // main()\nSuppose this code were run on the following input file, where the \"∙\" character represents a space, and the \"¶\" character represents a new\nline. What is the output of this program?\ninput.txt\n∙∙∙∙∙17∙22∙∙∙∙14 19 10 13 15∙∙27∙∙∙12∙∙20¶ ¶¶∙¶∙∙¶ ¶ ¶ ¶\nA) num: 15\ncounter: 3\nB) num: 15\ncounter: 5\nC) num: 15\ncounter: 8\nD) num: 20\ncounter: 5\nE) num: 20\ncounter: 8\nwrite_wordle_words(),3. Consider the following function, which reads input words from an input stream and writes them to an output\nstream only if they have a length of five.\n1\nvoid write_wordle_words(_________________ input, _________________ output) {\n2\nstd::string word;\n3\nwhile (input >> word) {\n4\nif (word.length() == 5) {\n5\noutput << word << std::endl;\n6\n} // if\n7\n} // while\n8\n} // write_wordle_words\nYou want this method to be able to take in kind of input stream and kind of output stream. For example, both of the followingany any\nfunction calls should work:\n9\n// use file streams\n10\nstd::ifstream fin(\"input.txt\");\n11\nstd::ofstream fout(\"output.txt\");\n12\nwrite_wordle_words(fin, fout);\n13\n14\n// use standard streams\n15\nwrite_wordle_words(std::cin, std::cout);\nwrite_wordle_words()To accomplish this, how should the parameter types of the function be defined? Choose the best answer.\nstd::istream,A)\nstd::ostream\nstd::istream&,B)\nstd::ostream&\nstd::ifstream,C)\nstd::ofstream\nstd::ifstream&, std::ofstream&D)\nE) None of the above", "word_count": 432, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "b9b90b6d-c7cd-5ff6-9b34-d6653fd51916", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 67, "real_page_number": null, "text": "2.9 Reading Input Using Polymorphism\n55\n4. Which of the following statements regarding streams is/are TRUE?\n'\\n' std::endl,I. Using to represent a newline may be more efficient than using since the latter also flushes the stream buffer.\nstd::cinII. Input redirection can be used to direct the contents of an input file into the standard input stream.\nstd::stringstreamIII. A allows you to read and write data into a string as if it were a stream.\nA) I only\nB) II only\nC) III only\nD) II and III only\nE) I, II, and III\nnumbers.txt:5. You are given the following file,\n13\n95\n22\n51\n18\n82\nWrite a short program that takes in this file as standard input via input redirection and prints out the sum of these numbers to(a)\nstandard output.\nexec.(b) Suppose you compiled this program into an executable named Write out the command you would type in the terminal if you\nnumbers.txt sum.txt.wanted to read in the numbers in as input and print out the sum in a text file named\nwhile6. Consider the following code, which reads input within a loop:\n1\nint main() {\n2\nstd::string word;\n3\nwhile (true) {\n4\nif (std::cin.good()) {\n5\nstd::cin >> word;\n6\n} // if\n7\nelse {\n8\nstd::cout << \"reached end of file\" << std::endl;\n9\n} // else\n10\n} // while\n11\n} // main()\nWhat could go wrong with this implementation, and how could this issue be fixed?\n7. Consider the following code:\n1\nint main() {\n2\nint32_t num_lines = 0;\n3\nstd::string line;\n4\nstd::cin >> num_lines;\n5\nfor (int32_t i = 0; i < num_lines; ++i) {\n6\nstd::getline(std::cin, line);\n7\nstd::cout << line << '\\n';\n8\n} // for i\n9\n} // main()\nSuppose this code were run on the following input file, where \"∙\" represents a space, and \"¶\" represents a new line:\n∙∙∙∙∙5∙∙∙∙∙¶\napple¶\nbanana¶\ncherry¶\ndragonfruit¶\neggfruit¶\nDoes the code work as intended? If not, make a fix to the original code so that it works properly. You may assume that all input files that\nare run with this program follow the same structure as the file detailed above.", "word_count": 379, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dc40a328-c653-5661-b525-435b12956bd8", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 68, "real_page_number": null, "text": "56\nChapter 2. File and Stream I/O\nChapter 2 Exercise Solutions\n>> std::getline()1. The correct answer is (A). The extraction operator ignores leading whitespace, but the does not (instead,\nstd::getline() '\\n'reads all characters up to the next delimitation character, which is by default).\nstd::cin operator>>2. The correct answer is (B). When you read something from the standard input stream using either or\nstd::getline(), std::cinyou also extract it from the stream. There are two places where is being read: the extraction op-\nwhile std::getline() operator>>erator in the loop on line 5, and the call to on line 6. ignores leading whitespace, while\nstd::getline() consumes everything up to the next newline character. With this information, we will walk through the input file and\nobserve what happens when it is redirected via standard input:\noperator>> 17 num.• First, we use to read in the first value from the input stream. This reads the number into After this first\nextraction, the stream now looks like this:\nnum=17\nline=\"\"\ninput.txt\n∙22∙∙∙∙14 19 10 13 15∙∙27∙∙∙12∙∙20¶ ¶¶∙¶∙∙¶ ¶ ¶ ¶\nstd::getline()• Next, extracts everything up to the next newline, and then discards the newline character.\nnum=17\nline=\"∙22∙∙∙∙14\"\ninput.txt\n19 10 13 15∙∙27∙∙∙12∙∙20¶¶∙¶∙∙¶ ¶ ¶ ¶\ncounter 1.• is incremented to\noperator>> num, 19.• The next call to reads the next number into or\nnum=19\nline=\"∙22∙∙∙∙14\"\ninput.txt\n10 13 15∙∙27∙∙∙12∙∙20¶¶∙¶∙∙¶ ¶ ¶ ¶\nstd::getline()• then reads in all characters up to the next newline, and then discards this newline. However, there is nothing\nstd::getline()before the next newline, so does not extract anything.\nnum=19\nline=\"\"\ninput.txt\n10 13 15∙∙27∙∙∙12∙∙20¶∙¶∙∙¶ ¶ ¶ ¶\ncounter 2.• is incremented to\noperator>> num, 10• The next call to reads the next number into or (all whitespace is ignored up to this point).\nnum=10\nline=\"\"\ninput.txt\n13 15∙∙27∙∙∙12∙∙20¶ ¶ ¶", "word_count": 364, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "55b3910f-6844-5b62-8a6b-ae037e418ced", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 69, "real_page_number": null, "text": "2.9 Reading Input Using Polymorphism\n57\nstd::getline()• then reads in all characters up to the next newline, and then discards this newline. However, there is nothing\nstd::getline()before the next newline, so does not extract anything.\nnum=10\nline=\"\"\ninput.txt\n13 15∙∙27∙∙∙12∙∙20¶ ¶\ncounter 3.• is incremented to\noperator>> num, 13.• The next call to reads the next number into or\nnum=10\nline=\"\"\ninput.txt\n15∙∙27∙∙∙12∙∙20¶ ¶\nstd::getline()• then reads in all characters up to the next newline, and then discards this newline. However, there is nothing\nstd::getline()before the next newline, so does not extract anything.\nnum=10\nline=\"\"\ninput.txt\n15∙∙27∙∙∙12∙∙20¶\ncounter 4.• is incremented to\noperator>> num, 15.• The next call to reads the next number into or\nnum=15\nline=\"\"\ninput.txt\n∙∙27∙∙∙12∙∙20¶\nstd::getline()• then reads in all characters up to the next newline, and then discards this newline.\nnum=15\nline=\"∙∙27∙∙∙12∙∙20\"\ninput.txt\ncounter• is incremented to 5.\nstd::cin >> num false while num• The stream is now empty, so evaluates to and the loop terminates. therefore ends with a\n15, counter 5.value of and ends with a value of\nstd::istream std::ostream3. The correct answer is (B). The and serve as the base classes of input and output streams, so if you\nwant a function to accept any kind of input or output stream, you can use these types. Additionally, we want to write directly to the stream\nwe pass into the function instead of making a copy, so we will pass the stream by reference instead of by value.\nstd::endl4. The correct answer is (E). All of the statements are true. Statement I is true because using also flushes the stream buffer in\naddition to inserting a newline into a stream. Statement II is true because that is the definition of input redirection: we can treat the contents\nstd::stringstreamof an input file as if it were a stream. Statement III is true because provides stream-like functionality for strings.", "word_count": 367, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e50ac37f-63bd-54b7-8c9c-f004c1aa9b7b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 70, "real_page_number": null, "text": "58\nChapter 2. File and Stream I/O\nstd::cin,5. (a) Since we are reading in this input file via input redirection, we can simply read the contents of this file using and then write\nstd::cout. std::cin whilethe output using To identify when we are done reading from the input file, we can extract from within a\nfalseloop, which evaluates to once there is no more input left to read. This is shown in the code below:\n1\nint main() {\n2\nint32_t num = 0, sum = 0;\n3\nwhile (std::cin >> num) {\n4\nsum += num;\n5\n} // while\n6\nstd::cout << sum << '\\n';\n7\n} // main()\n<.(b) To redirect input from a file, you can simply use the input redirection operator To redirect output into a file, you can use the output\n>.redirection operator In this case, the command would be:\n./exec < numbers.txt > sum.txt\nstd::cin.good() while .good(),6. The issue with this implementation is that it is using within a loop. Although methods such as\n.eof(), .bad(), .fail()and canbeusedtodeterminewhetheraninputstreamisareadablestate, itshouldnotbeputinthecondition\nfalseof the loop. This is because these methods return only an extraction from the stream has already failed, and the loop would stillafter\nwhilerun one more iteration after the stream becomes invalid. This can be fixed by extracting from the stream directly in the condition —\nwhile(std::cin >> word).i.e.,\nstd::getline() 5This code does not work as intended because the first ends up reading in the five spaces directly after the number7.\nstd::getline()on the first line of the example input file (remember that does not ignore whitespace and reads everything up to the\nstd::cin 5).next newline, and the extraction from on line 4 would only extract from the string up to the There are two ways to address\nstd::getline() std::cinthis. One way is to invoke an additional call after the extraction on line 4 to clear out any additional\nwhitespace before the next newline in the stream. This is shown below:\n1\nint main() {\n2\nint32_t num_lines = 0;\n3\nstd::string line;\n4\nstd::cin >> num_lines;\n5\nstd::getline(std::cin, line);\n// clear out everything before next line\n6\nfor (int32_t i = 0; i < num_lines; ++i) {\n7\nstd::getline(std::cin, line);\n8\nstd::cout << line << '\\n';\n9\n} // for i\n10\n} // main()\nstd::cin.ignore(),Another option is to use which can be used to ignore everything up to the next newline character after the first\nstd::cinextraction from on line 4. This is shown below:\n1\nint main() {\n2\nint32_t num_lines = 0;\n3\nstd::string line;\n4\nstd::cin >> num_lines;\n5\nstd::cin.ignore(std::numeric_limits<std::streamsize>::max(), '\\n');\n6\nfor (int32_t i = 0; i < num_lines; ++i) {\n7\nstd::getline(std::cin, line);\n8\nstd::cout << line << '\\n';\n9\n} // for i\n10\n} // main()", "word_count": 496, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ba985366-4fa7-5c06-884a-023ec912a480", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 71, "real_page_number": null, "text": "Chapter 3\nCommand Line Parsing\n3.1\nargc argvand\nWhen running a program, you may need to pass in parameters via the command line to dictate how your program will run. These parameters are\nknown as command line arguments. For example, consider the following command line input:\n./game --level 25 --difficulty 10 --multiplayer\n\"./game\" gameHere, the first term runs an executable named in the current directory. The subsequent arguments that begin with a dash are\nknown as command line options, and they can be used to specify certain features of the program. These options follow the executable name on\nthe command line. For example, the above command may have been used to start the game at level 25, with a difficulty level of 10, and with\nmultiplayer mode on. In this chapter, we will discuss mechanisms that allow you to handle command line options in your program.\nmain()If you want your program to support command line options, you should pass the following arguments into your function.\nint main(int char*argc, argv[]);\n—or—\nint main(int char**argc, argv);\nargc, gameThe first argument, stores the number of arguments that are passed into the program from the command line. In the example, there\n\"./game\", \"--level\", \"25\", \"--difficulty\", \"10\", \"--multiplayer\". argv,are six arguments: and The second argument, is an\nargcarray of size that holds pointers to character arrays (also known as C-strings), where each C-string stores the contents of a command line\nargvargument. The name of the program itself is included as the first argument, and it is located at index 0 of the array.", "word_count": 269, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fedc51c0-6935-5541-a563-ccc232419ad0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 72, "real_page_number": null, "text": "60\nChapter 3. Command Line Parsing\nargc argvA visualization of and for the example command is shown below.\n./game --level 25 --difficulty 10 --multiplayer\nint main(int\nchar**,argc\nargv );\n6\n.\n/\ng\na\nm\ne\n\\0\n-\n-\nl\ne\nv\ne\nl\n\\0\n2\n5\n\\0\n-\n-\nd\ni\nf\nf\n...\ny\n\\0\n1\n0\n\\0\n-\n-\nm\nu\nl\nt\n...\nr\n\\0\nNULL\nargv[0]\nargv[1]\nargv[2]\nargv[3]\nargv[4]\nargv[5]\nargv[6]\nargv,All the arguments in including the numbers, are stored as C-strings. A C-string is a character array that ends with a null character, or\n'\\0' argv,(this will be covered in more detail in chapter 16). Thus, if you want to retrieve the integer value of an argument in you must\nint. atoi()first convert it from a C-string to an This can be done using the function, which accepts a C-string and returns the integer it\natoi() atoi(argv[2]) 25 int.represents (if the C-string cannot be interpreted as an integer, returns 0). For example, returns as an\nargc argv.Command line arguments related to input redirection are included as a part of and Consider the following example, whichnot\nruns the same command as before, but with input and output files specified:\n./game --level 25 --difficulty 10 --multiplayer < input.txt > output.txt\nargc argv. argcIn this example, the arguments pertaining to input and output redirection are not considered as a part of and The value of is\n6, argv \"<\", \"input.txt\", \">\", \"output.txt\". argc argvstill and the array will store or Simply put, the values of and will be thenot\nsame as it was before, when redirection was not specified.\n3.2\nSwitch Statements and Enumerated Types\n¸ 3.2.1\nSwitch Statements\nBefore we move forward, we will go over the switch statement in C++, which will be used to parse command line options in the next section\n(and is a programming concept that is quite useful in general). Switch statements are essentially an alternative if-else structure for primitive\ntypes. A switch statement is not only neater than a long if-else chain, but depending on the cases involved, the compiler may also be able to\noptimize a switch statement to improve performance. The basic structure of a switch statement is shown below:\n1\nswitch (variable_to_test)\n2\n{\n3\ncase potential_value_1:\n4\n{\n5\n/* code to run */\n6\nbreak;\n7\n}\n8\ncase potential_value_2:\n9\n{\n10\n/* code to run */\n11\nbreak;\n12\n}\n13\n...\n14\ndefault:\n15\n{\n16\n/* code to run */\n17\n}\n18\n}\nThe variable you want to check goes into the switch statement.\nJust like the argument of a traditional if statement, the\nvalue of this variable dictates what code gets run.\nThese are potential values that the variable you want to test can\npotential_value_1,take on. If the value of the variable is\nfor example, the code under the first case statement runs.\nbreakThe statements terminates the switch statement. If not\nincluded, the program falls through and runs code in the next case.\nYou can have any number of cases, or values\nthat the variable you’re testing can take on.\nIf the value of the variable matches none of the provided cases,\ndefaultthe code associated with the case label runs\n(this is kind of like the else case of an if-else statement).\nTo write a switch statement, pass in the variable whose value should be checked. This variable will be tested for equality against all the values\nspecified in the subsequent case statements. If the value of the variable matches a case, the code associated with that case runs.", "word_count": 616, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1f4dfea9-4fa7-5f2d-9b1b-9677f3375082", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 73, "real_page_number": null, "text": "3.2 Switch Statements and Enumerated Types\n61\nYou can have as many cases as you want in a switch statement, as long as\n1. the variable in the switch matches the data type of the cases\n2. the cases are constant expressions that can be interpreted as integer values\ncaseEach case statement starts with the keyword, followed by the value that the switch variable should be compared to, followed by a colon.\nThe order of cases in a switch statement does not matter. This is different from an if-else chain, where the order of checks may influence the\nprogram’s performance (note: for if-else chains, it would be preferable to list the most likely branches first).\nbreak breakWhen the code associated with a case runs, the case will execute until a statement is reached. When a statement is reached,\nbreakthe switch statement terminates and the program continues to the code after the switch. However, if no is encountered at the end of a\nbreakcase, the program to subsequent cases until a is found — in other words, if the value of the switch variable matches a case,falls through\nbreakthe program will execute that case, as well that come after the match until a statement is reached.all cases\n[[fallthrough]]Remark: C++17 introduced the attribute, which can be used to explicitly indicate that a fallthrough is intentional.\nbreakThis is because fallthroughs are typically not expected behavior, and they are often caused by forgetting the statement at the end\nof a case. As a result, certain compilers will issue a warning if a fallthrough case is encountered in a switch statement. The inclusion of\n[[fallthrough]] suppresses this error, and it is also good style if fallthrough behavior is actually intended.\ndefaultA switch statement may also have an optional case at the very end. This case covers the condition where the switch variable being\ndefault breakchecked does not match with any of the provided cases. Since the case is placed at the very end, there is no need for a\nstatement because there are no cases that follow it (and thus no risk of unintended fallthrough).\n¸ 3.2.2\nEnums\nOne disadvantage of a switch statement is that they can only be applied on variables that can be interpreted as an integer. As a result, something\nstd::stringlike this would work, since it is impossible to interpret a object as an integer:not\n1\nstd::string day_of_week;\n2\nstd::cin >> day_of_week;\n3\nswitch (day_of_week)\n4\n{\n5\ncase \"Monday\":\n6\n{\n7\nstd::cout << \"It's Monday!\\n\";\n8\nbreak;\n9\n}\n10\ncase \"Tuesday\":\n11\n{\n12\nstd::cout << \"It's Tuesday!\\n\";\n13\nbreak;\n14\n}\n...\n35\ncase \"Sunday\":\n36\n{\n37\nstd::cout << \"It's Sunday!\\n\";\n38\nbreak;\n39\n}\n40\ndefault:\n41\n{\n42\nstd::cout << \"Invalid day!\\n\";\n43\n}\n44\n}\nOne way to get around this is to use an integer to represent each string. For example, we could let 1 represent Monday, 2 represent Tuesday, …,\nand 7 represent Sunday:\n1\nint32_t day_of_week;\n2\nstd::cin >> day_of_week;\n3\nswitch (day_of_week)\n4\n{\n5\ncase 1:\n6\n{\n7\nstd::cout << \"It's Monday!\\n\";\n8\nbreak;\n9\n}\n10\ncase 2:\n11\n{\n12\nstd::cout << \"It's Tuesday!\\n\";\n13\nbreak;\n14\n}\n...\n35\ncase 7:\n36\n{\n37\nstd::cout << \"It's Sunday!\\n\";\n38\nbreak;\n39\n}\n40\ndefault:\n41\n{\n42\nstd::cout << \"Invalid day!\\n\";\n43\n}\n44\n}\nHowever, this approach is complicated, messy, and prone to errors. For example, does the number 1 represent Monday or Sunday? Should\nSunday be represented as 0 or 7? Not only is the code ambiguous, it is also unreadable. For complicated switch statements with many cases,\nother programmers may have no idea what the integers refer to and can easily mess something up.\nA better approach would be to use something known as an enumerated type, or enum. An enum is a user-defined type that is restricted to a\ncertain set of values, and it allows you to use a condition’s \"name\" in the switch statement. For example, an enum would allow you to describe\nthe switch cases using the actual names of the days of the week (Monday, Tuesday, etc.) instead of integers (1, 2, etc.). This is allowed because\nan enum associates each \"name\" with an integer that is used for the switch cases under the hood. To define an enum, use the following syntax:\nenum class <NAME OF ENUM> { ... VALUES OF ENUM ... }", "word_count": 752, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3ffce47a-ef4a-5768-8f9b-ba20e2a6983c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 74, "real_page_number": null, "text": "62\nChapter 3. Command Line Parsing\nDayFor example, the following would define an enum called that takes on the values of Monday, Tuesday, Wednesday, Thursday, Friday,\nSaturday, and Sunday:\nenum class Day { Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday };\nThis allows you do things like this:\n1\nDay day_of_week = Day::Friday;\n2\nif (day_of_week == Day::Friday) {\n3\nstd::cout << \"It's Friday!\\n\";\n4\n} // if\nday_of_weekHere, is an integer behind the scenes, and not a string. However, the readability of the program is maintained since the full\nname of each weekday is used instead of an arbitrary number.\nDay::Friday,What integer does each weekday actually represent? If we were to print out the value of for example, the number 4 is\nDay::Fridayprinted out. Why is equal to 4 upon initialization? It turns out that the construction of an enum follows two rules:\n• If the first enum value is not explicitly assigned a value, it is assigned an integer value of 0.\n• For any enum that follows, if it is not explicitly assigned a value, its value is set to a value one greater than the value of the previous enum.\nThus, in the following enum definition:\nenum class Day { Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday };\nMonday gets assigned the integer 0, Tuesday gets assigned the integer 1, Wednesday gets assigned the integer 2, Thursday gets assigned the\ninteger 3, Friday gets assigned the integer 4, Saturday gets assigned the integer 5, and Sunday gets assigned the integer 6.\nHowever, this is customizable: you can explicitly assign the value of an enum using an initializer in its definition. For instance, if you\nwanted Monday to be assigned the integer 1, Tuesday to be assigned the integer 2, and so on, you can explicitly assign these values as follows:\nenum class Day { Monday = 1, Tuesday = 2, Wednesday = 3,\nThursday = 4, Friday = 5, Saturday = 6, Sunday = 7 };\nHowever, via the second rule of enum initialization, only Monday needs to be explicitly set to 1 if you want the above configuration. This\nis because each unassigned enum is assigned the value of the previous enum plus one (so Tuesday gets automatically assigned to 1 + 1 = 2,\nWednesday to 2 + 1 = 3, etc.). The definition below does the same thing as the one above:\nenum class Day { Monday = 1, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday };\nIt is possible for two enum values can be assigned to the same integer, and they can be compared just like integers.\nYou can also change the underlying integer type of an enum. For instance, the following would store the days of the week as integers of\nint8_ttype (and would take up 1 byte of memory instead of 4 bytes):\nenum class int8_tDay : { Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday };\ncharSince an object of type is actually a 1-byte number behind the scenes, you can assign each enum to a character and use them in the cases\nof switch statements as well.\nenum class charDay : { Monday = 'm', Tuesday = 't', Wednesday = 'w',\nThursday = 'h', Friday = 'f', Saturday = 's', Sunday = 'u' };\nPutting this all together, you can use enumerated types to build a cleaner, more readable switch statement, such as the one below.\n1\nenum class Day { Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday };\n2\nDay day_of_week = ...;\n3\nswitch (day_of_week)\n4\n{\n5\ncase Day::Monday:\n6\n{\n7\nstd::cout << \"It's Monday!\\n\";\n8\nbreak;\n9\n}\n10\ncase Day::Tuesday:\n11\n{\n12\nstd::cout << \"It's Tuesday!\\n\";\n13\nbreak;\n14\n}\n...\n35\ncase Day::Sunday:\n36\n{\n37\nstd::cout << \"It's Sunday!\\n\";\n38\nbreak;\n39\n}\n40\ndefault:\n41\n{\n42\nstd::cout << \"Invalid day!\\n\";\n43\n}\n44\n}", "word_count": 654, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b1b2c089-fb8b-5169-b976-a7aa4d4906c7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 75, "real_page_number": null, "text": "3.2 Switch Statements and Enumerated Types\n63\nbreakAs mentioned, switch cases allow individual cases to fall through if a statement is not specified in a case. This can be useful if you want\nto group different cases together, such as with the following example.\n1\nenum class Suit { Diamonds, Hearts, Clubs, Spades };\n2\nSuit current_suit = ...;\n// assign value to suit\n3\nswitch (current_suit)\n4\n{\n5\ncase Suit::Diamonds:\n6\n{\n7\n[[fallthrough]];\n8\n}\n9\ncase Suit::Hearts:\n10\n{\n11\nstd::cout << \"You got a red card!\\n\";\n12\nbreak;\n13\n}\n14\ncase Suit::Clubs:\n15\n{\n16\n[[fallthrough]];\n17\n}\n18\ncase Suit::Spades:\n19\n{\n20\nstd::cout << \"You got a black card!\\n\";\n21\nbreak;\n22\n}\n23\ndefault:\n24\n{\n25\nstd::cout << \"I don't know what your card is.\\n\";\n26\n}\n27\n}\nDiamonds, Diamonds breakHere, if the suit ends up being the code associated with the case runs. Since there is no statement in the\nDiamonds Heartscase, the program falls through to the next case and runs the code associated with the case as well (printing \"You got a\nbreak Clubs Spades.red card!\"). Only then does the code encounter a statement, so it exits the switch without running the code for or\nHere’s another example of a switch statement, which gives a prognosis of the current day’s weather conditions when given one of four\npossible weather values.\n1\nenum class Weather { partly_cloudy, sunny, rainy, overcast };\n2\nWeather current_weather = ...;\n// assign value to current weather\n3\nswitch (current_weather)\n4\n{\n5\ncase Weather::partly_cloudy:\n6\n{\n7\nstd::cout << \"There will be clouds in the sky.\\n\";\n8\n[[fallthrough]];\n9\n}\n10\ncase Weather::sunny:\n11\n{\n12\nstd::cout << \"The sun will be visible in the sky.\\n\";\n13\nbreak;\n14\n}\n15\ncase Weather::rainy:\n16\n{\n17\nstd::cout << \"It will rain.\\n\";\n18\n[[fallthrough]];\n19\n}\n20\ncase Weather::overcast:\n21\n{\n22\nstd::cout << \"The sun will likely not be visible in the sky.\\n\";\n23\nbreak;\n24\n}\n25\ndefault:\n26\n{\n27\nstd::cout << \"Cannot identify weather\\n\";\n28\n}\n29\n}\ncurrent_weather partly_cloudy, partly_cloudyIf were set to the switch statement would run code from the case (line 5) to the\nbreakfirst encountered (line 12). Thus, the following would be printed:\nThere will be clouds in the sky.\nThe sun will be visible in the sky.\ncurrent_weather sunny, breakIf were set to the switch statement would run code from line 9 to the first encountered, or line 12. The\nfollowing would be printed:\nThe sun will be visible in the sky.", "word_count": 436, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d7efdf6d-7b71-5e71-9f09-c9e2e44fbb87", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 76, "real_page_number": null, "text": "64\nChapter 3. Command Line Parsing\ncurrent_weather rainy, breakIf were set to the switch statement would run code from line 14 to the first encountered, or line 21. The\nfollowing would be printed:\nIt will rain.\nThe sun will likely not be visible in the sky.\ncurrent_weather overcast, breakIf were set to the switch statement would run code from line 18 to the first encountered, or line 21.\nThe following would be printed:\nThe sun will likely not be visible in the sky.\nLastly, it should be mentioned that curly braces are not always necessary for each case. The following two versions of code do the same thing\nand would both work:\n1\ncase Day::Monday:\n2\n{\n3\nstd::cout << \"It's Monday!\\n\";\n4\nbreak;\n5\n}\n6\ncase Day::Tuesday:\n7\n...\n1\ncase Day::Monday:\n2\nstd::cout << \"It's Monday!\\n\";\n3\nbreak;\n4\ncase Day::Tuesday:\n5\n...\nHowever, the version without the curly braces does allow you to initialize variables within the code for a case! Of the two implementationsnot\nbelow, only the one of the left would work — the one on the right would not compile.\n1\ncase Day::Monday:\n2\n{\n3\nint num_classes = 5; // OK\n4\nstd::cout << \"It's Monday!\\n\";\n5\nbreak;\n6\n}\n7\ncase Day::Tuesday:\n8\n...\n1\ncase Day::Monday:\n2\nint num_classes = 5; // ERROR\n3\nstd::cout << \"It's Monday!\\n\";\n4\nbreak;\n5\ncase Day::Tuesday:\n6\n...\nenum class enum.Remark: There is a difference between an and a traditional Technically, both of the following approaches can be used\nto define a brand new enum:\nenum class <NAME OF ENUM> { ... VALUES OF ENUM ... }\nenum <NAME OF ENUM> { .. VALUES OF ENUM ... }\nclassHowever, you should always include the keyword when creating a new enum. This is because enum classes address several bug\nprone issues that traditional enums do not:\n1. Traditional enums implicitly convert to integers, which may not be desired by the programmer.\nexplicitThe compiler is not the programmer, so it should not be making decisions on the programmer’s behalf (similar to why the\nkeyword is necessary). Thus, you would want to prevent implicit conversions between an enum type and its underlying integer value, which\nis a protection that only enum classes provide.\n1\nenum Color { Red, Green, Blue, Yellow };\n// traditional enum\n2\n3\nint main() {\n4\nColor enum_value = Color::Blue;\n5\nif (enum_value == 2) {\n// implicit conversion, bad!\n6\nstd::cout << \"blue\" << std::endl;\n7\n} // if\n8\n} // main()\n1\nenum class Color { Red, Green, Blue, Yellow };\n// enum class\n2\n3\nint main() {\n4\nColor enum_value = Color::Blue;\n5\nif (enum_value == 2) {\n// compiler error, good!\n6\nstd::cout << \"blue\" << std::endl;\n7\n} // if\n8\n} // main()\n1\nenum class Color { Red, Green, Blue, Yellow };\n// enum class\n2\n3\nint main() {\n4\nColor enum_value = Color::Blue;\n5\nif (static_cast<int32_t>(enum_value) == 2) {\n// okay, because explicitly cast to integer\n6\nstd::cout << \"blue\" << std::endl;\n7\n} // if\n8\n} // main()", "word_count": 526, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "79e47649-869c-5d7b-8808-b885334d3a3b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 77, "real_page_number": null, "text": "3.3 Getopt Long\n65\n2. Traditional enums export their names to the surrounding scope, which could cause clashes with other variable names.\nUnlike enum classes, the enum names used in a traditional enum cannot be used in the same scope as the enum, or in any other enum. This is\nnot ideal behavior, as shown below:\n1\nenum Color { Red, Green, Blue, Yellow };\n// traditional enum\n2\nenum Color2 { Red, Orange, Pink, Purple };\n// not allowed, \"Red\" already used by 1st Color enum\n3\n4\nint main() {\n5\nenum Color3 { Brown, Indigo, Cyan, Teal };\n6\nstd::string Brown = \"brown\";\n// not allowed, \"Brown\" already used by enum\n7\n} // main()\n1\nenum class Color { Red, Green, Blue, Yellow };\n// enum class\n2\nenum class Color2 { Red, Orange, Pink, Purple };\n// okay\n3\n4\nint main() {\n5\nenum class Color3 { Brown, Indigo, Cyan, Teal };\n6\nstd::string Brown = \"brown\";\n// okay\n7\n} // main()\n3.3\nGetopt Long\n¸ 3.3.1\nCommand Line Options\nargc argv,In the first section of this chapter, we introduced and which can be used to retrieve information from the command line. However,\nonce we get these command line options, we need a way to parse them and adjust our program’s behavior based on which options were entered.\nLet’s return to our original command:\n./game --level 25 --difficulty 10 --multiplayer\nThis command tells us that we should run level 25 of the game, with difficulty set to 10, and multiplayer mode on. A tempting (but incorrect)\nargv[2], argv[4], argcapproachwouldbetoretrievethelevelnumberfrom thedifficultyfrom andcheck todeterminewhethermultiplayer\nmode is turned on. However, this would fail if the options were given in a different order!\n./game --multiplayer --level 25 --difficulty 10\nargv --levelA better approach would be to go through the array and check for each of the options. If is specified, check the following\n--multiplayerindex for the number of the level. If is specified, turn on multiplayer. This approach is more likely to produce the desired\n--level --multiplayerbehavior, but it also has its faults. What if the user forgot to input the level number after the option? What if the\noption could be followed by a number that represents the number of players?\nTo further complicate things, command lines typically support short form options as well. A is an abbreviated form ofshort form option\n--levelan option, and it is preceded with a single dash rather than two dashes (the fully typed out options we currently have, such as and\n--difficulty, --difficulty -d,are known as options). For example, the short form option of may be and the short formlong form\n--multiplayer -m.option of may be In other words, the command\n./game --level 25 --difficulty 10 --multiplayer\ncan be rewritten like this for the same outcome:\n./game -l 25 -d 10 -m\nIn most programs, short forms and long forms can be mixed together, and short forms may be combined. As long as the difficulty follows the\n--difficulty -d, --level -l,long form or the short form and the level follows the long form or the short form the command should be\nvalid. For instance, the following commands should all exhibit the same behavior:\n./game --level 25 --difficulty 10 --multiplayer\n./game -l 25 -d 10 -m\n./game --difficulty 10 -m -l 25\n./game -md 10 --level 25\nAs you can see here, command line parsing can be get quite complicated if you are forced to do so by hand! While it might be feasible to\naccount for all possible combinations and orderings of options when the number of possible options are low, this becomes much harder as the\nnumber of options increases. The total number of possible orderings also makes it difficult to handle errors in the command line.", "word_count": 653, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "09492a00-13e9-5ed6-b7f9-0402eb40251d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 78, "real_page_number": null, "text": "66\nChapter 3. Command Line Parsing\n¸ 3.3.2\nUsing Getopt Long\ngetopt_long() <getopt.h>To simplify the process of parsing command line options, we can use the GNU’s function, found in the\ngamelibrary. A possible implementation for the example is shown below:\n1\n#include <iostream>\n2\n#include <getopt.h>\n3\nusing namespace std;\n4\n5\n// sets the difficulty of the game\n6\nset_game_difficulty(int32_t difficulty);\n7\n// sets the level of the game\n8\nset_game_level(int32_t level);\n9\n// turns multiplayer mode on\n10\nset_multiplayer();\n11\n12\nint main(int char**argc, argv) {\n13\nint opt;\n14\nint opt_index = 0;\n15\nstatic struct option long_opts[] = {\n16\n{ \"difficulty\", required_argument, nullptr, 'd' },\n17\n{ \"level\", required_argument, nullptr, 'l' },\n18\n{ \"multiplayer\", no_argument, nullptr, 'm' },\n19\n{ nullptr, 0, nullptr, '\\0' }\n20\n};\n21\n22\nwhile ((opt = getopt_long(argc, argv, \"d:l:m\", long_opts, &opt_index)) != -1) {\n23\nswitch (opt)\n24\n{\n25\ncase 'd':\n26\n{\n27\n// runs if 'd' or \"difficulty\" is specified on the command line\n28\nstd::cout << \"difficulty set to \" << atoi(optarg) << '\\n';\n29\nset_game_difficulty(atoi(optarg));\n30\nbreak;\n31\n}\n32\ncase 'l':\n33\n{\n34\n// runs if 'l' or \"level\" is specified on the command line\n35\nstd::cout << \"level set to \" << atoi(optarg) << '\\n';\n36\nset_game_level(atoi(optarg));\n37\nbreak;\n38\n}\n39\ncase 'm':\n40\n{\n41\n// runs if 'm' or \"multiplayer\" is specified on the command line\n42\nstd::cout << \"multiplayer mode turned on\\n\";\n43\nset_multiplayer();\n44\nbreak;\n45\n}\n46\ndefault:\n47\n{\n48\nstd::cout << \"unrecognized option\" << std::endl;\n49\nexit(1);\n50\n}\n51\n} // switch\n52\n} // while\n53\n} // main()\nThere is quite a bit of information to break down here. In the following pages, we will break this code down into segments and explain the\ngetopt_long() long_opts[]details of one segment at a time. To start off, let’s look at the array:\n1\nstatic struct option long_opts[] = {\n2\n{ \"level\", required_argument, nullptr, 'l' },\n3\n{ \"difficulty\", required_argument, nullptr, 'd' },\n4\n{ \"multiplayer\", no_argument, nullptr, 'm' },\n5\n{ nullptr, 0, nullptr, '\\0' }\n6\n};\nlong_opts[] option optionThe array is an array of structs, where each is defined as follows:\n1\nstruct option {\n2\nconst char* name;\n3\nint has_arg;\n4\nint* flag;\n5\nint val;\n6\n};", "word_count": 397, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dfe1750a-c28c-528f-9f2e-bf939d136fa8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 79, "real_page_number": null, "text": "3.3 Getopt Long\n67\n-d, -l, optionEach valid option that the user can enter into the command line (e.g., etc.) is treated as a separate object. As shown above,\noption optioneach has four member variables. Consider the first struct, defined on line 16.\n\"difficulty\",{ required_argument, nullptr, 'd' }\nnameThe member variable stores the long version of the option as a C-string. For example, this variable would store the word \"difficulty\" as a\n--difficultyC-string for the command line option (as shown above).\nrequired_argument,{ \"difficulty\", nullptr, 'd' }\nhas_argThe member variable stores an integer that represents whether the option needs to be followed by an argument. There are three types\nof options: ones that have a required argument, ones that have no argument, and ones that have an optional argument.\n• An option with a must be followed by an additional argument if it is included in the command line. For example,required argument\n--level --level 25).would be an option with a required argument, since it must be followed by the level you want to go to (e.g.,\n--multiplayer• An option with must be run on its own if specified on the command line. For example, requires nono argument\nargument, since there is no need to include anything along with it.\n• An option with an may or may not be followed by an additional argument if specified on the command line.optional argument\nUnlike options with required arguments, this additional argument is for the command to be valid. For example, if thenot necessary\n--multiplayer option defaults to 2 players, but the number of players could be customized with an additional argument (e.g.,\n--multiplayer 3 for 3 players), then this option would have an optional argument. For optional arguments, it is important to note\nthat the argument following the option is the thing that is optional, and not the actual option itself!\nhas_argThe value of can take on three possible values, no_argument, required_argument, or optional_argument. All three are\nenums that correspond to the integers 0, 1, and 2, respectively. Make sure you specify the correct option type for each option, as mistakes here\nrequired_argument no_argumentBecause these are enums, you can simply type out the word or in thecan be difficult to track down!\ninitializer list, as shown in the provided code.\nnullptr, 'd'{ \"difficulty\", required_argument, }\nflag val optionNext, we have the and member variables of the object. There are two different behaviors that can happen, depending on\nflag. flagthe value assigned to If you want a command line option to set the value of an integer variable, you can set to the address of this\nvalinteger variable and to the value you want to set the integer to. Consider the following example:\n1\nstatic int num_players = 1;\n2\nint main(int char**argc, argv) {\n3\n...\n4\nstatic struct option long_opts[] = {\n5\n{ \"multiplayer\", no_argument, &num_players, 2 },\n6\n...\n7\n};\n8\n...\n9\n}\nflag --multiplayerHere, the number of players is initially set to 1. However, because the value of for the option was set to the address of\nnum_players val 2, num_players 2 --multiplayerand was set to the value of will be changed to if the option is ever seen on the\ncommand line. You will not be using this method in EECS 281.\nIn this class, you will be using the second behavior, which allows you to specify a short form for each option. To accomplish this, set the\nflag nullptr valvalue of to and the value of to the character that should be used as the option’s short form. For example, the following\n\"difficulty\" 'd'.indicates that the option can be represented in short form using the letter\nnullptr, 'd'{ \"difficulty\", required_argument, }\noption getopt_long()The last item in the array must be an with all of its members set to zero. This item tells that there are no more\noption choices remaining:\n{ nullptr, 0, nullptr, '\\0'}\noption {0, 0, 0, 0},You can also initialize this last using which does the same thing.\nwhileNow, let’s look at the loop after the array definition.\n22\nwhile ((opt = getopt_long(argc, argv, \"d:l:m\", long_opts, &opt_index)) != -1) {\ngetopt_long() optHere, the function parses the command line and assigns the short form of the parameter into the variable (declared on\n-1, whileline 13). If there are no more command line arguments to read, this function would return and the loop would terminate.\ngetopt_long()The third argument of the function is the string, which tells the function whether it should expect anshort options\ngame --leveladditional argument after the option. In the example, we had to tell the program what level we wanted to go to if we specify\n--level 25 --level.on the command line. in this case is our option, but the that follows is an additional argument that must always follow\nIn the short options string, you would use a colon to indicate whether a short option should be followed by an argument. If an option has a\nrequired argument, its short option must be followed by a colon in the short options string. If an option has no argument, its short option is\nfollowed by nothing. If an option has an optional argument, its short option must be followed by two colons in the short options string.", "word_count": 911, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "890a6e2a-99bc-54b1-ac0d-a8487133c4af", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 80, "real_page_number": null, "text": "68\nChapter 3. Command Line Parsing\n\"d:l:m\",The example in the sample code, indicates that the\nd (difficulty) d• The option has a required argument (since is followed by a colon).\nl (level) l• The option has a required argument (since is followed by a colon).\nm (multiplayer) m• The option has no argument (since is followed by nothing).\n\"l:d:m\", \"md:l:\", \"ml:d:\", \"l:md:\"The order of the options in the short options string does not matter. The short option strings and\nd lall work as alternatives. You just need to ensure that and are followed by a colon, since these options require an additional argument.\n23\nswitch (opt)\n24\n{\n25\ncase 'd':\n26\n{\n27\n// runs if 'd' or \"difficulty\" is specified on the command line\n28\nstd::cout << \"difficulty set to \" << atoi(optarg) << '\\n';\n29\nset_game_difficulty(atoi(optarg));\n30\nbreak;\n31\n}\n32\ncase 'l':\n33\n{\n34\n// runs if 'l' or \"level\" is specified on the command line\n35\nstd::cout << \"level set to \" << atoi(optarg) << '\\n';\n36\nset_game_level(atoi(optarg));\n37\nbreak;\n38\n}\n39\ncase 'm':\n40\n{\n41\n// runs if 'm' or \"multiplayer\" is specified on the command line\n42\nstd::cout << \"multiplayer mode turned on\\n\";\n43\nset_multiplayer();\n44\nbreak;\n45\n}\n46\ndefault:\n47\n{\n48\nstd::cout << \"unrecognized option\" << std::endl;\n49\nexit(1);\n50\n}\n51\n}\nwhileIn the body of the loop, you will have a switch statement that determines what your program does when each of the options are\nencountered on the command line. Each case denotes the short option of a command (as shown on lines 25, 32, and 39), and the body of each\ncase implements the actions that are taken when the option is seen.\noptarg.In the case where there is an additional argument after the option, the argument is stored in a global C-string called For example,\n--level 25 25 optarg,if is seen on the command line, the is stored in a variable called which can be used in your program (an example is\noptarg optargshown on line 28). It is important to note that is a C-string, so you will have to convert to the correct type before you use it.\ndefault defaultFor error handling, it may be useful to add an additional case at the end of the switch statement. The code in the\nlong_opts[]section is run when the option encountered does not match any of the options that are defined in the array. In the example, the\ndefault case is included on line 46, and exits the program if an unrecognized option is provided on the command line.\ngetopt_long() -1Lastly, as mentioned before, the function returns if there are no more options to read from the command line.\nHowever, what happens if there are more arguments that need to be read after the final option? Consider the following command:\n./game --multiplayer --level 25 --difficulty 10 input.txt\ninput.txt, input.txtHere, we added an input file, that needs to be read in by the program. However, is not associated with a command\ngetopt_long() -1line option, so would return before processing this last term.\nargv[6],How do we know where the input file is located? In the above example, it is located at so we would want to start processing any\nargvadditional arguments starting at index 6 of the array. Finding this final index though is not trivial; the command below is equally valid,\nargv[5].and the input file for this command is located at\n./game -ml 25 -d 10 input.txt\ngetopt_long() optindLuckily, keeps track of this information for us. There is a global variable called that stores the index of the\nargv getopt_long()next element in the array that needs to be processed. Once finishes processing all of the option terms, the value of\noptind optindcan be used to determine where the remaining non-option terms begin. In the first command, would have a value of 6 after\ngetopt_long() input.txt, argv[6]. optindfinishes running, since the first non-option term, is located at In the second command,\nargv[5].would have a value of 5 since the first non-option term is located at\ngetopt_long(),In general, to check if there are non-option terms that need to be processed after running you can compare the value of\noptind argc. (argc > optind) true,with the value of If evaluates to then there are additional terms that you will need to parse.", "word_count": 750, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e0895e6b-710d-5f78-ade9-9b58b5e0c1db", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 81, "real_page_number": null, "text": "3.4 Boost Program Options\n69\n3.4\nBoost Program Options (✽)\n¸ 3.4.1\n(✽)Using Boost Program Options\ngetopt_long()If you ever do C++ development in industry, there is a good chance that you will not be using to handle command line\nboost::program_options boost boostarguments. Instead,apopularalternativeis inthe library. The library,however,isbannedinthe\nboost::program_optionsclass, material. Thatbeingsaid,thissectionisincludedbecausenotsoyouwill beresponsibleforknowingthis\nis a useful resource, and it would not hurt to be aware of its existence as a developer.\nboost::program_optionsAshortsnippetofcommandlineparsingcodeusing isshownbelow, withthesamegameexamplecovered\nin the previous section. You do not need to understand everything in this code for now; we will go over the details shortly.\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nint main(int char**argc, argv) {\n12\nboost::program_options::options_description command_line_options{\"Program Options\"};\n13\ncommand_line_options.add_options()\n14\nboost::program_options::value<int32_t>(),(\"difficulty,d\", \"Game difficulty\")\n15\nboost::program_options::value<int32_t>(),(\"level,l\", \"Starting level\")\n16\n(\"name,n\", boost::program_options::value<std::string>(), \"Player name\")\n17\n(\"multiplayer,m\", \"Starts game in multiplayer mode\")\n18\n(\"help,h\", \"Prints out help message\");\n19\n20\nboost::program_options::variables_map options_map;\n21\nint32_t difficulty{};\n22\nint32_t level{};\n23\nstd::string name;\n24\nbool false;is_multiplayer =\n25\n26\ntry {\n27\nboost::program_options::parsed_options parsed_result =\n28\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n29\nboost::program_options::store(parsed_result, options_map);\n30\n31\nif (options_map.count(\"help\")) {\n32\nstd::cout << command_line_options << '\\n'; // prints out options in pretty format\n33\nreturn 0;\n34\n} // if help\n35\nif (options_map.count(\"difficulty\")) {\n36\noptions_map[\"difficulty\"].as<int32_t>();difficulty = // sets difficulty to provided value\n37\n} // if difficulty\n38\nif (options_map.count(\"level\")) {\n39\noptions_map[\"level\"].as<int32_t>();level = // sets level to provided value\n40\n} // if level\n41\nif (options_map.count(\"name\")) {\n42\nname = options_map[\"name\"].as<std::string>(); // sets name to provided value\n43\n} // if name\n44\nif (options_map.count(\"multiplayer\")) {\n45\ntrue;is_multiplayer =\n46\n} // if multiplayer\n47\n48\nboost::program_options::notify(options_map);\n49\n} // try\n50\ncatch (boost::program_options::error& e) {\n51\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n52\n// error handling here\n53\n} // catch\n54\n} // main()\nboost::program_options::options_descriptionThe class(createdonline12)canbeusedtoregisterthecommandlineoptions\ndifficulty, level, multiplayerthat your program can handle. Lines 14-18 register the command line options of and (like in the\nexample at the end of section 3.3), as well as an option to print a help message and specify a player’s name.\nadd_options() options_descriptionOn line 13, the member of the class can be used to define the available options for a\nprogram. If you want to specify a short option for a command line option, add a comma and the letter you want to use after the long name of the\n\"difficulty,d\" --difficulty -doption (i.e., to allow and to be used interchangeably). A short option must only consist of one letter.\n(difficulty level), (multiplayerNoticethatsomeoftheoptionslistedaboveinvolvethreeparameters and whileothersinvolvetwo\nhelp). no_argument getopt_long()).and Forthelattertwo,thismeansthatnoargumentcanbesettoeachoftheseflags(i.e.,the casewith\nboost::program_options::value<>On the other hand, options that require an argument can be specified using with the expected\ndifficulty level int32_t).type for the argument (as shown with and on lines 14 and 15, which both expect an argument of type", "word_count": 587, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8fa3fa4d-7851-55e4-98fd-b460964a9a6a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 82, "real_page_number": null, "text": "70\nChapter 3. Command Line Parsing\nboost::program_options::options_description \"Program Options\"On line 12, we created an instance using the string\noptions_descriptionin the constructor. This string is known as a caption, and it is printed out if you try to print the contents of an\nobject (which is done on line 32). The output from line 32 is shown below:\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --help\nOutput:\nProgram Options:\n-d [ --difficulty ] arg\nGame difficulty\n-l [ --level ] arg\nStarting level\n-n [ --name ] arg\nPlayer name\n-m [ --multiplayer ]\nStarts game in multiplayer mode\n-h [ --help ]\nPrints out help message\noptions_descriptionThe caption is optional, and you can omit it while constructing an object. If no caption is provided, printing out\noptions_descriptionthe would only print out the list of valid commands, as shown:\n11\nint main(int char**argc, argv) {\n12\nboost::program_options::options_description command_line_options; // no caption\n13\ncommand_line_options.add_options()\n14\nboost::program_options::value<int32_t>(),(\"difficulty,d\", \"Game difficulty\")\n15\nboost::program_options::value<int32_t>(),(\"level,l\", \"Starting level\")\n16\n(\"name,n\", boost::program_options::value<std::string>(), \"Player name\")\n17\n(\"multiplayer,m\", \"Starts game in multiplayer mode\")\n18\n(\"help,h\", \"Prints out help message\");\nOutput:\n-d [ --difficulty ] arg\nGame difficulty\n-l [ --level ] arg\nStarting level\n-n [ --name ] arg\nPlayer name\n-m [ --multiplayer ]\nStarts game in multiplayer mode\n-h [ --help ]\nPrints out help message\nAfter you are done specifying which arguments your program accepts, you will need to parse the arguments that are actually provided on the\nboost::program_options::variables_map,command line. This is done using a which stores the options that are supplied on the\noptarg getopt_long()).command line with their associated values (similar to in the case of An example is shown on lines 27-29:\n27\nboost::program_options::parsed_options parsed_result =\n28\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n29\nboost::program_options::store(parsed_result, options_map);\nparse_command_line() argc,argv, options_descriptionThe methodtakesin andan objectthatholdsthelistofvalidcommand\nstore()line arguments. Then, the return value of this method is passed into the method, which stores the parsed results in a provided\nvariables_map (in this case, the options map that was initialized on line 20).\nifAfter line 29, we have a series of checks that determine whether an option was provided on the command line. To access the argument\noptarg getopt_long()), options_mapof a provided option (i.e., the value in the context of we can use square brackets on our in the\nformat shown on lines 36, 39, and 42.\n31\nif (options_map.count(\"help\")) {\n32\nstd::cout << command_line_options << '\\n'; // prints out options in pretty format\n33\nreturn 0;\n34\n} // if help\n35\nif (options_map.count(\"difficulty\")) {\n36\noptions_map[\"difficulty\"].as<int32_t>();difficulty = // sets difficulty to provided value\n37\n} // if difficulty\n38\nif (options_map.count(\"level\")) {\n39\noptions_map[\"level\"].as<int32_t>();level = // sets level to provided value\n40\n} // if level\n41\nif (options_map.count(\"name\")) {\n42\nname = options_map[\"name\"].as<std::string>(); // sets name to provided value\n43\n} // if name\n44\nif (options_map.count(\"multiplayer\")) {\n45\ntrue;is_multiplayer =\n46\n} // if multiplayer", "word_count": 509, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1bb6d8a8-b733-5ff3-ad14-17c5d55df679", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 83, "real_page_number": null, "text": "3.4 Boost Program Options\n71\noptions_mapPreviously, we initialized variables on lines 21-24 and assigned them while looping through our on lines 31-46. However, this\nis not the only way to assign variables to values provided on the command line. You can also perform an assignment by passing a pointer to the\noptions_descriptionvariable you want to assign into the value argument of the initial object. This is shown below on lines 19-21:\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nint main(int char**argc, argv) {\n12\nint32_t difficulty{};\n13\nint32_t level{};\n14\nstd::string name;\n15\nbool false;is_multiplayer =\n16\n17\nboost::program_options::options_description command_line_options{\"Program Options\"};\n18\ncommand_line_options.add_options()\n19\nboost::program_options::value<int32_t>(&difficulty),(\"difficulty,d\", \"Game difficulty\")\n20\nboost::program_options::value<int32_t>(&level),(\"level,l\", \"Starting level\")\n21\n(\"name,n\", boost::program_options::value<std::string>(&name), \"Player name\")\n22\n(\"multiplayer,m\", \"Starts game in multiplayer mode\")\n23\n(\"help,h\", \"Prints out help message\");\n24\n25\nboost::program_options::variables_map options_map;\n26\n27\ntry {\n28\nboost::program_options::parsed_options parsed_result =\n29\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n30\nboost::program_options::store(parsed_result, options_map);\n31\n32\nif (options_map.count(\"help\")) {\n33\nstd::cout << command_line_options << '\\n'; // prints out options in pretty format\n34\nreturn 0;\n35\n} // if help\n36\nif (options_map.count(\"multiplayer\")) {\n37\ntrue;is_multiplayer =\n38\n} // if multiplayer\n39\n40\nboost::program_options::notify(options_map);\n41\n42\nstd::cout << \"Difficulty: \" << difficulty << '\\n';\n43\nstd::cout << \"Level: \" << level << '\\n';\n44\nstd::cout << \"Name: \" << name << '\\n';\n45\n} // try\n46\ncatch (boost::program_options::error& e) {\n47\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n48\n// error handling here\n49\n} // catch\n50\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --difficulty 10 --level 25 --name Dario\nOutput:\nDifficulty: 10\nLevel: 25\nName: Dario", "word_count": 307, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6d52df83-f6e3-5b4c-8988-8e3231fca7a5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 84, "real_page_number": null, "text": "72\nChapter 3. Command Line Parsing\n¸ 3.4.2\n(✽)Switch Arguments\nis_multiplayer multiplayerWe can also use the previous process to assign the Boolean. This can be done by converting the option\ninto a argument. A switch argument is a command line argument that takes in no value, but can be used to switch on or off someswitch\nfunctionality if specified. The format for specifying a switch argument is shown below on line 22:\n17\nboost::program_options::options_description command_line_options{\"Program Options\"};\n18\ncommand_line_options.add_options()\n19\nboost::program_options::value<int32_t>(&difficulty),(\"difficulty,d\", \"Game difficulty\")\n20\nboost::program_options::value<int32_t>(&level),(\"level,l\", \"Starting level\")\n21\n(\"name,n\", boost::program_options::value<std::string>(&name), \"Player name\")\n22\nboost::program_options::bool_switch(&is_multiplayer)->default_value(false),(\"multiplayer,m\",\n23\n\"Starts game in multiplayer mode\")\n24\n(\"help,h\", \"Prints out help message\");\nAdding this change to our previous code, we would get the following:\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nint main(int char**argc, argv) {\n12\nint32_t difficulty{};\n13\nint32_t level{};\n14\nstd::string name;\n15\nbool is_multiplayer;\n16\n17\nboost::program_options::options_description command_line_options{\"Program Options\"};\n18\ncommand_line_options.add_options()\n19\nboost::program_options::value<int32_t>(&difficulty),(\"difficulty,d\", \"Game difficulty\")\n20\nboost::program_options::value<int32_t>(&level),(\"level,l\", \"Starting level\")\n21\n(\"name,n\", boost::program_options::value<std::string>(&name), \"Player name\")\n22\nboost::program_options::bool_switch(&is_multiplayer)->default_value(false),(\"multiplayer,m\",\n23\n\"Starts game in multiplayer mode\")\n24\n(\"help,h\", \"Prints out help message\");\n25\n26\nboost::program_options::variables_map options_map;\n27\n28\ntry {\n29\nboost::program_options::parsed_options parsed_result =\n30\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n31\nboost::program_options::store(parsed_result, options_map);\n32\n33\nif (options_map.count(\"help\")) {\n34\nstd::cout << command_line_options << '\\n'; // prints out options in pretty format\n35\nreturn 0;\n36\n} // if\n37\n38\nboost::program_options::notify(options_map);\n39\n40\nstd::cout << \"Difficulty: \" << difficulty << '\\n';\n41\nstd::cout << \"Level: \" << level << '\\n';\n42\nstd::cout << \"Name: \" << name << '\\n';\n43\nstd::cout << \"Multiplayer: \" << std::boolalpha << is_multiplayer << '\\n';\n44\n} // try\n45\ncatch (boost::program_options::error& e) {\n46\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n47\n// error handling here\n48\n} // catch\n49\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --difficulty 10 --level 25 --name Dario --multiplayer\nOutput:\nDifficulty: 10\nLevel: 25\nName: Dario\nMultiplayer: true", "word_count": 357, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3d977014-b44e-5540-9e75-0d445ca1cbd5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 85, "real_page_number": null, "text": "3.4 Boost Program Options\n73\n¸ 3.4.3\n(✽)Notiﬁers\nare an additional feature that you can use when parsing program options. A notifier allows you to call a function whenever youNotifiers\nnotifier()encounter a specific command line option. To use a notifier, add the method to the value type of an option’s description and\n\"Hello <NAME>!\"pass in the function you want to invoke when that option is encountered. For example, the following prints out when\n<NAME> --nameis passed into the command line with the option:\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nvoid greet_user(const std::string& name) {\n12\nstd::cout << \"Hello \" << name << \"!\\n\";\n13\n} // greet_user()\n14\n15\nint main(int char**argc, argv) {\n16\nboost::program_options::options_description command_line_options{\"Program Options\"};\n17\ncommand_line_options.add_options()\n18\n(\"name,n\", boost::program_options::value<std::string>()->notifier(greet_user), \"Player name\");\n19\n20\nboost::program_options::variables_map options_map;\n21\n22\ntry {\n23\nboost::program_options::parsed_options parsed_result =\n24\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n25\nboost::program_options::store(parsed_result, options_map);\n26\n27\nboost::program_options::notify(options_map);\n28\n} // try\n29\ncatch (boost::program_options::error& e) {\n30\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n31\n// error handling here\n32\n} // catch\n33\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --name Dario\nOutput:\nHello Dario!", "word_count": 229, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0621d1d1-ce71-5b9e-ab72-19e846c18357", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 86, "real_page_number": null, "text": "74\nChapter 3. Command Line Parsing\nNotifiers make it easy to use lambda functions to set the values of variables. The following reads in configs in the command line and stores them\nin a configuration class (you do not need to know what lambda functions are for now, but we will cover them in chapter 11).\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nclass GameConfig {\n12\nstd::string name;\n13\nint32_t difficulty{};\n14\npublic:\n15\nstd::string get_name() {\n16\nreturn name;\n17\n} // get_name()\n18\n19\nvoid set_name(const std::string& name_in) {\n20\nname = name_in;\n21\n} // set_name()\n22\n23\nint32_t get_difficulty() {\n24\nreturn difficulty;\n25\n} // get_difficulty()\n26\n27\nvoid set_difficulty(int32_t difficulty_in) {\n28\ndifficulty = difficulty_in;\n29\n} // set_difficulty()\n30\n};\n31\n32\nint main(int char**argc, argv) {\n33\nGameConfig config;\n34\nboost::program_options::options_description command_line_options{\"Program Options\"};\n35\ncommand_line_options.add_options()\n36\nboost::program_options::value<int32_t>()->notifier((\"difficulty,d\",\n37\n(int32_t[&config] provided_difficulty) { config.set_difficulty(provided_difficulty); }\n38\n), \"Game difficulty\")\n39\n(\"name,n\", boost::program_options::value<std::string>()->notifier(\n40\n(const[&config] std::string& provided_name) { config.set_name(provided_name); }\n41\n), \"Player name\");\n42\n43\nboost::program_options::variables_map options_map;\n44\n45\ntry {\n46\nboost::program_options::parsed_options parsed_result =\n47\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n48\nboost::program_options::store(parsed_result, options_map);\n49\n50\nboost::program_options::notify(options_map);\n51\n52\nstd::cout << \"Name: \" << config.get_name() << '\\n';\n53\nstd::cout << \"Difficulty: \" << config.get_difficulty() << '\\n';\n54\n} // try\n55\ncatch (boost::program_options::error& e) {\n56\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n57\n// error handling here\n58\n} // catch\n59\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --name Dario --difficulty 10\nOutput:\nName: Dario\nDifficulty: 10\nGameConfigIn this code, lines 37 and 40 are lambda functions that initialize the contents of a object directly from the program arguments.\nnotify()If you paid close attention to the example code provided so far in this section, you may noticed that there is always a call to\noptions_mapon the constructed (line 50 of the above code). This call is very important to have after you are done parsing the command\nnotify(),By calling you trigger any actions that are required after the value of an option is determined. This includesline arguments!\nvalue<>notifier functions like in the example above, as well as any variable initializations within the object of the options description (i.e., if\nvalue<int32_t>(&difficulty) difficulty notify()is specified, is only assigned to its command line value is invoked).after", "word_count": 428, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4e842e21-7d05-5215-a507-05c99b9af39e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 87, "real_page_number": null, "text": "3.4 Boost Program Options\n75\n¸ 3.4.4\n(✽)Required Arguments\nrequired()If you want to require an argument to be specified on the command line, you can use the method, as shown. In this code, the\nuser must specify a difficulty, level, and name. Failure to specify any of these arguments would result in an error, which is issued when the\nnotify() function is invoked.\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nint main(int char**argc, argv) {\n12\nint32_t difficulty{};\n13\nint32_t level{};\n14\nstd::string name;\n15\n16\nboost::program_options::options_description command_line_options{\"Program Options\"};\n17\ncommand_line_options.add_options()\n18\nboost::program_options::value<int32_t>(&difficulty)->required(),(\"difficulty,d\",\n19\n\"Game difficulty\")\n20\nboost::program_options::value<int32_t>(&level)->required(),(\"level,l\", \"Starting level\")\n21\n(\"name,n\", boost::program_options::value<std::string>(&name)->required(), \"Player name\")\n22\n(\"help,h\", \"Prints out help message\");\n23\n24\nboost::program_options::variables_map options_map;\n25\n26\ntry {\n27\nboost::program_options::parsed_options parsed_result =\n28\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n29\nboost::program_options::store(parsed_result, options_map);\n30\n31\nif (options_map.count(\"help\")) {\n32\nstd::cout << command_line_options << '\\n';\n33\nreturn 0;\n34\n} // if\n35\n36\nboost::program_options::notify(options_map); // issues error if required arguments missing\n37\n} // try\n38\ncatch (boost::program_options::required_option& req) {\n39\nstd::cout << \"Missing required argument: \" << req.get_option_name() << '\\n';\n40\n// error handling here\n41\n} // catch\n42\ncatch (boost::program_options::error& e) {\n43\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n44\n// error handling here\n45\n} // catch\n46\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --name Dario --difficulty 10\nOutput:\nMissing required argument: --level", "word_count": 267, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cf963dbf-c1ae-5127-99fc-a62cada147a7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 88, "real_page_number": null, "text": "76\nChapter 3. Command Line Parsing\n¸ 3.4.5\n(✽)Default and Implicit Values\ndefault_value()The method can be used to specify a default value if a given option is not provided on the command line. In the following\ndifficulty levelcode, has a default value of 10 if not specified, and has a default value of 1 if not specified.\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nint main(int char**argc, argv) {\n12\nint32_t difficulty{};\n13\nint32_t level{};\n14\nstd::string name;\n15\n16\nboost::program_options::options_description command_line_options{\"Program Options\"};\n17\ncommand_line_options.add_options()\n18\nboost::program_options::value<int32_t>(&difficulty)->default_value(10),(\"difficulty,d\",\n19\n\"Game difficulty\")\n20\nboost::program_options::value<int32_t>(&level)->default_value(1),(\"level,l\", \"Starting level\")\n21\n(\"name,n\", boost::program_options::value<std::string>(&name), \"Player name\")\n22\n(\"help,h\", \"Prints out help message\");\n23\n24\nboost::program_options::variables_map options_map;\n25\n26\ntry {\n27\nboost::program_options::parsed_options parsed_result =\n28\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n29\nboost::program_options::store(parsed_result, options_map);\n30\n31\nif (options_map.count(\"help\")) {\n32\nstd::cout << command_line_options << '\\n';\n33\nreturn 0;\n34\n} // if\n35\n36\nboost::program_options::notify(options_map);\n37\n38\nstd::cout << \"Difficulty: \" << difficulty << '\\n';\n39\nstd::cout << \"Level: \" << level << '\\n';\n40\nstd::cout << \"Name: \" << name << '\\n';\n41\n} // try\n42\ncatch (boost::program_options::error& e) {\n43\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n44\n// error handling here\n45\n} // catch\n46\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --name Dario\nOutput:\nDifficulty: 10\nLevel: 1\nName: Dario", "word_count": 258, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d3e71b7b-c9c5-58fe-ae9b-e6d83133df55", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 89, "real_page_number": null, "text": "3.4 Boost Program Options\n77\nBoost program options also support optional arguments, which occur if an option is specified on the command line without a value (for example,\n--difficulty -dor without a number after it). If you want to set a value for an option that accepts a value but is not supplied one, you can\nimplicit_value() difficultyuse the method. In the following code, if is specified on the command line without a value, then it is\nlevelimplicitly set to value of 15. Similarly, if is specified on the command line without a value, it is implicitly set to value of 5.\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nint main(int char**argc, argv) {\n12\nint32_t difficulty{};\n13\nint32_t level{};\n14\nstd::string name;\n15\n16\nboost::program_options::options_description command_line_options{\"Program Options\"};\n17\ncommand_line_options.add_options()\n18\nboost::program_options::value<int32_t>(&difficulty)->implicit_value(15),(\"difficulty,d\",\n19\n\"Game difficulty\")\n20\nboost::program_options::value<int32_t>(&level)->implicit_value(5),(\"level,l\", \"Starting level\")\n21\n(\"name,n\", boost::program_options::value<std::string>(&name), \"Player name\")\n22\n(\"help,h\", \"Prints out help message\");\n23\n24\nboost::program_options::variables_map options_map;\n25\n26\ntry {\n27\nboost::program_options::parsed_options parsed_result =\n28\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n29\nboost::program_options::store(parsed_result, options_map);\n30\n31\nif (options_map.count(\"help\")) {\n32\nstd::cout << command_line_options << '\\n';\n33\nreturn 0;\n34\n} // if\n35\n36\nboost::program_options::notify(options_map);\n37\n38\nstd::cout << \"Difficulty: \" << difficulty << '\\n';\n39\nstd::cout << \"Level: \" << level << '\\n';\n40\nstd::cout << \"Name: \" << name << '\\n';\n41\n} // try\n42\ncatch (boost::program_options::error& e) {\n43\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n44\n// error handling here\n45\n} // catch\n46\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --name Dario --difficulty --level\nOutput:\nDifficulty: 15\nLevel: 5\nName: Dario\ndifficultyYou can also chain multiple methods together. For instance, the following option description indicates that is set to 10 if it is not\nspecified on the command line at all, and 15 if it is specified but without a value.\n1\n(\n2\n\"difficulty,d\",\n3\nboost::program_options::value<int32_t>()->default_value(10)->implicit_value(15),\n4\n\"Game difficulty\"\n5\n)", "word_count": 357, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8d344174-72f7-5fad-bab5-48b8aa146684", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 90, "real_page_number": null, "text": "78\nChapter 3. Command Line Parsing\n¸ 3.4.6\n(✽)Multitoken and Composing Arguments\nmultitoken()The method can be used to specify a command line option that can take in multiple arguments. For instance, suppose you\nwanted to specify a list of levels that you want to play for your game. One potential approach for doing this is to pass in all the levels after the\n--level option, as shown:\n./game --level 3 10 12 19\nboost::program_options, multitoken()Tohandlethisusing youcanusethe methodonthevaluetypewhendefiningyourcommand\nstd::vector<>.line options. When parsing an option of this type, the corresponding arguments are returned in the form of a\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nint main(int char**argc, argv) {\n12\nstd::vector<int32_t> levels_to_play;\n13\n14\nboost::program_options::options_description command_line_options{\"Program Options\"};\n15\ncommand_line_options.add_options()\n16\nboost::program_options::value<std::vector<int32_t>>(&levels_to_play)->multitoken(),(\"level,l\",\n17\n\"Levels to play\")\n18\n(\"help,h\", \"Prints out help message\");\n19\n20\nboost::program_options::variables_map options_map;\n21\n22\ntry {\n23\nboost::program_options::parsed_options parsed_result =\n24\nboost::program_options::parse_command_line(argc, argv, command_line_options);\n25\nboost::program_options::store(parsed_result, options_map);\n26\n27\nif (options_map.count(\"help\")) {\n28\nstd::cout << command_line_options << '\\n';\n29\nreturn 0;\n30\n} // if\n31\n32\nboost::program_options::notify(options_map);\n33\n34\nfor (int32_t level : levels_to_play) {\n35\nstd::cout << level << \" \";\n36\n} // for level\n37\n} // try\n38\ncatch (boost::program_options::error& e) {\n39\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n40\n// error handling here\n41\n}\n42\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --level 3 10 12 19\nOutput:\n3 10 12 19", "word_count": 288, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4f959ee8-4acc-5bad-b807-49b2464d1d5f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 91, "real_page_number": null, "text": "3.4 Boost Program Options\n79\nAnother option would be to allow an option to be specified more than once:\n./game --level 3 --level 10 --level 12 --level 19\ncomposing()To handle this type of option, you can use the method, which also parses all of the option’s arguments into a vector. If we\ncomposing() multitoken(),replace lines 16 and 17 of the previous code to use instead of we would get the exact same output if the\n--level option was specified more than once.\n14\nboost::program_options::options_description command_line_options{\"Program Options\"};\n15\ncommand_line_options.add_options()\n16\nboost::program_options::value<std::vector<int32_t>>(&levels_to_play)->composing(),(\"level,l\",\n17\n\"Levels to play\")\n18\n(\"help,h\", \"Prints out help message\");\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --level 3 --level 10 --level 12 --level 19\nOutput:\n3 10 12 19\nAgain, multiple attributes can be chained together, so it is perfectly valid for an option to support both multitoken and composing arguments.\n1\n(\n2\n\"level,l\",\n3\nboost::program_options::value<std::vector<int32_t>>()->multitoken()->composing(),\n4\n\"Levels to play\"\n5\n)\nzero_tokens()Lastly, the method can be used to specify that no arguments need to be included with a multitoken option (for example, if\n--level zero_tokens()the user wanted to play no levels at all, they could specify the option with no arguments if is used).\n¸ 3.4.7\n(✽)Positional Arguments\nare options on the command line that are not associated with a name. For example, consider the following command:Positional arguments\n./game input1.txt input2.txt input3.txt\nThe input files at the end of the command line are positional arguments. To handle these, you should first add an entry (with a name) into\noptions_descriptionthe object, much like any other argument type (you still need a name, since this is how positional arguments are\ncategorized into groups). The type of the value should be a vector of the positional arguments that you want to read in. An example is shown\nbelow:\n1\nboost::program_options::options_description command_line_options;\n2\ncommand_line_options.add_options()\n3\n(\"input-files\", boost::program_options::value<std::vector<std::string>>(), \"Input files\")\nboost::program_options::position_options_descriptionHowever,thisoptiontypemustalsobeaddedtoanobjectoftype\nto specify that it is a positional argument. This is shown below:\n1\nboost::program_options::positional_options_description positional_options;\n2\npositional_options.add(\"input-files\", -1);\n-1The specifies that an unlimited number of options on the command line should be considered as input files. You can change this value to\npositional_optionsadjust the number of positional arguments that you want to group within a given category. For instance, calling\n.add(\"input-files\", 5) would indicate to the program that the next five positional arguments should be considered as input files, but\nnot anything else after it. This allows you to easily group positional arguments into individual categories:\n1\nboost::program_options::positional_options_description positional_options;\n2\npositional_options.add(\"player-names\", 2); // two player names\n3\npositional_options.add(\"input-files\", -1); // rest of positional arguments are input files\nAfter specifying the positional arguments, you should include them while parsing the command line as shown. Note that we are using the more\ncommand_line_parser() parse_command_line(),advanced method to parse our options instead of which we used before. This is\nparse_command_line()because is a simplified method that does not support positional arguments.\n1\nboost::program_options::variables_map options_map;\n2\nboost::program_options::parsed_options parsed_result =\n3\nboost::program_options::command_line_parser(argc, argv)\n4\n.options(command_line_options)\n5\n.positional(positional_options)\n6\n.run();\n7\nboost::program_options::store(parsed_result, options_map);", "word_count": 527, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5b8a471d-1820-596b-9c5c-1a32dfd493f0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 92, "real_page_number": null, "text": "80\nChapter 3. Command Line Parsing\nA full example using positional arguments is shown below:\n1\n#include <iostream>\n2\n3\n#include <boost/program_options.hpp>\n4\n#include <boost/program_options/errors.hpp>\n5\n#include <boost/program_options/options_description.hpp>\n6\n#include <boost/program_options/parsers.hpp>\n7\n#include <boost/program_options/positional_options.hpp>\n8\n#include <boost/program_options/value_semantic.hpp>\n9\n#include <boost/program_options/variables_map.hpp>\n10\n11\nint main(int char**argc, argv) {\n12\nint32_t difficulty{};\n13\nint32_t level{};\n14\nstd::vector<std::string> player_names;\n15\nstd::vector<std::string> input_files;\n16\n17\nboost::program_options::options_description command_line_options{\"Program Options\"};\n18\ncommand_line_options.add_options()\n19\nboost::program_options::value<int32_t>(&difficulty),(\"difficulty,d\", \"Game difficulty\")\n20\nboost::program_options::value<int32_t>(&level),(\"level,l\", \"Starting level\")\n21\n(\"player-names\", boost::program_options::value<std::vector<std::string>>(&player_names),\n22\n\"Player names\")\n23\n(\"input-files\", boost::program_options::value<std::vector<std::string>>(&input_files),\n24\n\"Input files\")\n25\n(\"help,h\", \"Prints out help message\");\n26\n27\nboost::program_options::positional_options_description positional_options;\n28\npositional_options.add(\"player-names\", 2); // two player names\n29\npositional_options.add(\"input-files\", -1); // rest of positional arguments are input files\n30\n31\nboost::program_options::variables_map options_map;\n32\n33\ntry {\n34\nboost::program_options::parsed_options parsed_result =\n35\nboost::program_options::command_line_parser(argc, argv)\n36\n.options(command_line_options)\n37\n.positional(positional_options)\n38\n.run();\n39\nboost::program_options::store(parsed_result, options_map);\n40\n41\nif (options_map.count(\"help\")) {\n42\nstd::cout << command_line_options << '\\n';\n43\nreturn 0;\n44\n} // if\n45\n46\nboost::program_options::notify(options_map);\n47\n48\nstd::cout << \"Difficulty: \" << difficulty << '\\n';\n49\nstd::cout << \"Level: \" << level << '\\n';\n50\n51\nstd::cout << \"Player Names:\\n\";\n52\nfor (const std::string& name : player_names) {\n53\nstd::cout << name << '\\n';\n54\n} // for name\n55\nstd::cout << '\\n';\n56\n57\nstd::cout << \"Input Files:\\n\";\n58\nfor (const std::string& file : input_files) {\n59\nstd::cout << file << '\\n';\n60\n} // for file\n61\nstd::cout << '\\n';\n62\n} // try\n63\ncatch (boost::program_options::error& e) {\n64\nstd::cout << \"Error while parsing command line; Message=\" << e.what() << '\\n';\n65\n// error handling here\n66\n} // catch\n67\n} // main()\nCompile and run program:\ng++ -std=c++1z -O3 -Wall -pedantic -pthread main.cpp -lboost_program_options -o game\n./game --difficulty 10 --level 25 Dario Paoluigi input1.txt input2.txt input3.txt\nOutput:\nDifficulty: 10\nLevel: 25\nPlayer Names: Dario Paoluigi\nInput Files: input1.txt input2.txt input3.txt", "word_count": 312, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3192cf93-5118-5232-a085-0be4d8b454ab", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 93, "real_page_number": null, "text": "3.4 Boost Program Options\n81\nChapter 3 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\nargc?1. Consider the following statement on the command line. What is the value of\n./letter -q -c -l -p -o M -b apple -e orange < dictionary.txt\nA) 7\nB) 10\nC) 11\nD) 12\nE) 13\n2. You are writing a command line program that can be used to fetch historical weather conditions for locations in the United States. When the\n(./weather), --zipuser runs the executable for this program they are able to pass in a numerical zip code using the flag (short form\n-z), --date -d), --verbosea numerical date in YYYYMMDD format using the flag (short form and an optional flag that can be used\n-v).to display the results in verbose mode (short form The verbose flag does not take in any subsequent arguments. An example is shown\nbelow (this displays the weather details for zip code 48109 on May 9, 2023 in verbose mode):\n./weather --zip 48109 --date 20230509 --verbose\noption getopt_long()Which of the following objects for best depicts each of these command line options?\nA) {\"zip\", no_argument, nullptr, 'z'},\n{\"date\", no_argument, nullptr, 'd'},\n{\"verbose\", optional_argument, nullptr, 'v'}\nB) {\"zip\", required_argument, nullptr, 'z'},\n{\"date\", required_argument, nullptr, 'd'},\n{\"verbose\", no_argument, nullptr, 'v'}\nC) {\"zip\", required_argument, nullptr, 'z'},\n{\"date\", required_argument, nullptr, 'd'},\n{\"verbose\", optional_argument, nullptr, 'v'}\nD) {\"zip\", optional_argument, nullptr, 'z'},\n{\"date\", optional_argument, nullptr, 'd'},\n{\"verbose\", required_argument, nullptr, 'v'}\nE) None of the above\n3. Consider the same program as described in question 2. Which of the following strings is the correct short options string for use with\ngetopt_long()?\nzdvA)\nz:d:v:B)\nzd:vC)\nzdv:D)\nz:d:vE)\n4. Your friend is implementing a command line program that can be used to rank users on a class forum by their number of posts. This program\n--rank -r) --help -h)accepts two options on the command line: (short form followed by a required integer argument, and (short form\nlong_opts[]followed by no argument. This is the array that they came up with:\n1\nstatic struct option long_opts[] = {\n2\n{ \"rank\", required_argument, nullptr, 'r' },\n3\n{ \"help\", no_argument, nullptr, 'h' }\n4\n};\nDoes this work as intended? If not, what is the issue?\nlong_opts staticA) No, the array cannot be declared as\nrank \"r:\" 'r'B) No, the fourth member of the option on line 2 should be instead of\nlong_optsC) No, the last item of the must be an option with all its members set to zero\nD) No, for more than one of the reasons above\nlong_optsE) Yes, this array works as intended", "word_count": 482, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "11bcf825-94b3-5898-8382-2be21ae4b107", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 94, "real_page_number": null, "text": "82\nChapter 3. Command Line Parsing\noptarg getopt_long()5. What is the purpose of the variable in the function?\noptargA) stores the value of the short options string\noptargB) keeps track of how many arguments there are to process\noptargC) stores the value of the supplied argument for options that accept arguments\noptargD) holds the integer values corresponding to the short options present in the command line\nE) None of the above\nlong_opts6. Consider the following array:\n1\nstatic struct option long_opts[] = {\n2\n{ \"apple\", no_argument, nullptr, 'a' },\n3\n{ \"banana\", required_argument, nullptr, 'b' },\n4\n{ \"orange\", no_argument, nullptr, 'o' },\n5\n{ \"grape\", required_argument, nullptr, 'g' },\n6\n{ \"pear\", no_argument, nullptr, 'p' },\n7\n{ nullptr, 0, nullptr, '\\0' }\n8\n};\nexec.Which of the following commands would NOT be valid? Assume the exectuable is named\n./exec -abg 12 --orange < fruits.txtA)\n./exec --apple -pb fruits.txt < fruits.txtB)\n./exec -o -g 13 -aob pineapple > fruits.txtC)\n./exec -pao --grape 281 --banana grapeD)\nE) More than one of the above\nswitch7. Consider the following statement:\n1\nint32_t num;\n2\nstd::cin >> num;\n3\nswitch (num) {\n4\ncase 1:\n5\nstd::string str = \"yes\";\n6\nstd::cout << str << '\\n';\n7\nbreak;\n8\ncase 0:\n9\nstd::string str = \"no\";\n10\nstd::cout << str << '\\n';\n11\nbreak;\n12\ndefault:\n13\nbreak;\n14\n}\nWhich of the following statements is TRUE regarding this code?\nA) The code compiles, and prints \"yes\" if 1 is passed into standard input, and \"no\" if 0 is passed into standard input\nB) The code compiles, but nothing gets printed if 1 or 0 is passed into standard input\nC) The code does not compile, since it is impossible to declare variables within a switch statement\nD) The code does not compile, since variables declared in one case are still visible in another case since they are part of the same scope\nE) The code does not compile, since cases must be defined in ascending order", "word_count": 347, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c313c6cb-5d35-5723-991d-4f85402f6cb3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 95, "real_page_number": null, "text": "3.4 Boost Program Options\n83\n'l' \"level\"8. Consider the following code, which is partially completed. There are two valid command line options. The or option has\ninitial_level. 'h'a required argument in the form of an integer, and the value of this argument is assigned to the variable The or\n\"hard\" hard_mode true. 'h'option turns on, setting the bool to The option has no argument. Assuming all commands are valid,\natoi()complete the code below (you may use the method to convert a valid C-string into an integral number):\nclass Game {\nint initial_level = 0;\nbool false;hard_mode =\npublic:\nvoid get_options(int char**argc, argv);\n/* ... other members ... */\n};\nvoid Game::get_options(int char**argc, argv) {\nint option = 0;\nint index = 0;\n// TODO: Fill out the option struct\nstatic struct option long_opts[] = {\n};\n// TODO: Complete the while loop\nwhile ((option = getopt_long(\n,\n,\n, long_opts, &index)) != -1) {\nswitch (option) {\ncase 'l':\ncase 'h':\n} // switch\n} // while\n} // get_options()", "word_count": 177, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d8ae379f-6378-52db-ade7-98093dacfe3a", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 96, "real_page_number": null, "text": "84\nChapter 3. Command Line Parsing\nChapter 3 Exercise Solutions\n\"<\" \"dictionary.txt\" argc argv.1. The correct answer is (C). The input redirection components of and do not contribute to and\nargcThus, is the number of other items in the command line, which comes out to 11.\nzip date2. The correct answer is (B). Both and require additional arguments to be provided if they are specified, so these two command\nrequired_argument. verboseline options should be defined with a On the other hand, does not take in any subsequent arguments\nno_argument. optional_argumentif it is specified, so it should be defined with Notice that indicates that an argument is optional\nspecified, which does not apply here.if the option is\nz d)3. The correct answer is (E). Options that require arguments (in this case, and should be followed by a colon.\nlong_opts getopt_long()4. The correct answer is (C). The last item of the array should be an option with all zeros, so that knows\nstatic,when there are no more option choices remaining. The options array can be defined as and the additional colon is needed for the\ngetopt_long()short options string within the function call, but not in the option declaration itself.\noptargThecorrectansweris(C).Forcommandlineoptionsthatacceptarguments, isaC-stringthatstoresthevaluesofthesearguments.5.\nbanana/b grape/g6. The correct answer is (A). If or is specified on the command line, they must be followed with an additional\napple/a, orange/o, pear/pargument. If or is specified, they must be followed with no additional argument. The only command for\nb 12 g).which this is not true is (A), since is specified without an argument after it (the is the argument for Note that there is no restriction\ngetopt_long().on including an option more than once for\nswitch7. The correct answer is (D). The statement here does not compile because the newly created variables are not scoped properly —\nthe lifetime of the string instantiated on line 5 extends past its specific case, which is not allowed. To fix this, curly braces can be added to\neach case, as shown:\n1\nint32_t num;\n2\nstd::cin >> num;\n3\nswitch (num) {\n4\ncase 1:\n5\n{\n6\nstd::string str = \"yes\";\n7\nstd::cout << str << '\\n';\n8\nbreak;\n9\n}\n10\ncase 0:\n11\n{\n12\nstd::string str = \"no\";\n13\nstd::cout << str << '\\n';\n14\nbreak;\n15\n}\n16\ndefault:\n17\n{\n18\nbreak;\n19\n}\n20\n}\n'l' initial_level optarg 'h'8. The completed code is shown below. The case sets to the value of as an integer, and the case sets\nhard_mode to true.\n1\nclass Game {\n2\nint initial_level = 0;\n3\nbool false;hard_mode =\n4\npublic:\n5\nvoid get_options(int char**argc, argv);\n6\n/* ... other members ... */\n7\n};\n8\n9\nvoid Game::get_options(int char**argc, argv) {\n10\nint option = 0;\n11\nint index = 0;\n12\nstatic struct option long_opts[] = {\n13\nnullptr,{ \"level\", required_argument, 'l' },\n14\nnullptr,{ \"hard\", no_argument, 'h' },\n15\nnullptr, nullptr,{ 0, '\\0' }\n16\n};\n17\n18\nwhile ((option = getopt_long(argc, argv, \"l:h\", long_opts, &index)) != -1) {\n19\nswitch (option) {\n20\ncase 'l':\n21\ninitial_level = atoi(optarg);\n22\nbreak;\n23\ncase 'h':\n24\ntrue;hard_mode =\n25\nbreak;\n26\n} // switch\n27\n} // while\n28\n} // get_options()", "word_count": 581, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "fd5db202-1ff8-5cc3-b324-2bd407fae591", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 97, "real_page_number": null, "text": "Chapter 4\nComplexity Analysis\n4.1\nAsymptotic Runtime\nIn this class, we will focus a lot on the of algorithms. However, what exactly is efficiency, and how do we measure it? Upon firstefficiency\nglance, it might seem appropriate to measure the efficiency of an algorithm by measuring its runtime. Suppose that two students are given\ntwo algorithms, A and B, that do the exact same thing, and they want to determine which algorithm is more time-efficient. Student 1 prepares\nalgorithm A on their machine, and student 2 prepares algorithm B on their machine. Both students begin running their algorithms at the same\ntime, and whichever algorithm finishes first is the one that is deemed more efficient.\nHowever, this approach does not work because there are that could affect the runtime of the algorithms. Whattoo many confounding factors\nif student 2 had a faster computer than student 1? Even if algorithm A was truly better, the fact that student 2 had a faster machine may lead us\nto incorrectly conclude that algorithm B is superior.\nWhat if we remove this confounding variable of machine performance by limiting the experiment to a single machine? Suppose student\n1 runs both algorithm A and algorithm B on their machine, one after another, and uses the runtime results to determine which algorithm is\nbetter. While this approach removes the discrepancy across machines, it is still not perfect. What if more intensive processes were running in\nthe background when the student was running function A? There is still a chance that the inferior algorithm can come up on top.\nThus, while it may seem intuitive to base the time-efficiency of a program off its runtime, pure runtime is actually an imperfect metric!\nThis is because the runtime of an algorithm can be confounded by factors other than the efficiency of the algorithm itself. Variables such as\nCPU speed, the compiler and compiler flags used, and other programs running in parallel can influence the runtime of a program, even if these\nfactors are not relevant to the algorithm itself. As a result, simply using program runtime to measure efficiency does not fully work! We want to\nmeasure efficiency in a way that removes these confounds and better reflects the efficiency of a program on its own.\nTo do this, we will be using input size instead of actual runtime to determine the relative efficiency of a program. That is, instead of asking\nourselves when measuring efficiency, we ask ourselves\"How long does this program take to run?\" \"How does the program’s runtime scale\nsize?\". If we double the size of the input, does the runtime also double? Does it quadruple? Or does it stay the same? Given twowith input\nalgorithms that perform the same task, if the runtime of one algorithm grows much slower than the other as input size increases, then that\nalgorithm is considered to be better than the other when it comes to time efficiency.", "word_count": 493, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "84ca890e-b013-5eb8-ac8f-1d763c6fb124", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 98, "real_page_number": null, "text": "86\nChapter 4. Complexity Analysis\nWhy is this approach better? As mentioned before, the actual runtime of a program may be confounded by factors such as CPU speed, compiler\nflags, and processes running the background. However, for input sizes that are large enough, the rate of growth of runtime with respect to\nRegardless of how fast the CPU is, which compiler flags are used, or what programs areinput size is independent of most external factors!\nrunning in the background, the rate of growth of an algorithm should stay relatively consistent as you increase its input size. This idea forms the\nbasis of asymptotic runtime, which describes how the runtime of an algorithm grows as the size of its input grows. We use asymptotic runtime\nto measure and compare the time-efficiency of algorithms.\nAsymptotic runtime and actual runtime are not the same thing! For example, if program A completes in 2.81 seconds, the number 2.81\nrepresents the of the program in seconds. However, if program A’s runtime doubles when input size doubles, triples when input sizeruntime\ntriples, and quadruples when input size quadruples, etc., we say there is a linear relationship between the input size and the runtime of the\nprogram. This linear relationship describes the of the program. Unlike runtime, which is expressed as a unit of time (e.g.,asymptotic runtime\nseconds), the asymptotic runtime of an algorithm describes the relationship between input size and runtime, and it is expressed using something\nknown as big-O notation. This will be covered in the next section.\n4.2\nComplexity Classes and Big-O Notation\nIn the previous section, we introduced the concept of asymptotic runtime. In this section, we will go over how the asymptotic runtime of an\nalgorithm can be computed. To do so, we will first need to conceptualize how we want to measure our input size.\nWhat counts as\"one unit\" of input? Ultimately, it depends on what the algorithm is designed to do: you want to select a unitof measurement\nthat is related to the functionality of your algorithm. For example, if your algorithm sorts an array of integers, it would make sense to treat the\ninput size as the number of integers in the array.\nOn a similar vein, if you had a graph with 𝑉vertices and 𝐸edges, the way you define the input size should be related to what your algorithm\ndoes. If your algorithm only deals with the number of vertices in the graph (and ignores the edges), it would be sufficient to define input size in\nterms of the number of vertices 𝑉. However, if your algorithm does work on the vertices and edges of the graph, it may be better to defineboth\ninput size in terms of both 𝑉and 𝐸. The more information you include when defining your input size, the more information you can potentially\nget out from time complexity analysis (for example, measuring asymptotic runtime using both 𝑉and 𝐸can give you an idea of which variable\nhas a greater impact on the performance of an algorithm).\nAfter deciding what counts as a unit of input, you can vary the amount of input you feed into the algorithm, measure the amount of time it\ntakes to run the algorithm, and plot out the points on a graph to visualize a relationship between input size and runtime. To ensure that this\nIf the input size isrelationship can be clearly determined by the results, make sure that the input sizes you experiment with are large enough!\ntoo small, your results may be obfuscated by variance, and you may not be able to clearly see how runtime scales with the input size.\nExample 4.1 Consider the following algorithm, which sums up all the numbers in an array of integers. Determine the asymptotic runtime\ncomplexity of this algorithm by measuring runtime with respect to input size.\n1\nint32_t sum_array(int32_t size_tarr[], size) {\n2\nint32_t total = 0;\n3\nfor (size_t i = 0; i < size; ++i) {\n4\ntotal += arr[i];\n5\n} // for i\n6\nreturn total;\n7\n} // sum_array()\nOur first step is to determine how we want to measure our input size. Since we are summing an array, it would make sense for input size to be the\nsize of the input array. Now that we know our unit of input, let’s measure the runtime of the program with different input sizes. The following\nsum_array()data was collected by running the function for array sizes of 10000, 20000, 30000, 40000, 50000, 100000, and 250000.\nInput Size\nRuntime (ms)\n10000\n2.5\n20000\n2.9\n30000\n3.2\n40000\n3.5\n50000\n3.9\n100000\n5.7\n250000\n11.8\nGraphing this out, we can see that there is a relationship between the input size and the runtime. That is, as the size of the input sizelinear\nincreases, the runtime increases linearly (in a straight line).", "word_count": 813, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d231c734-42e4-5537-8fd8-b237d91e7da1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 99, "real_page_number": null, "text": "4.2 Complexity Classes and Big-O Notation\n87\nThe time complexity of an algorithm is a way to express the relationship between runtime and input size. If there is a linear relationship between\nruntime and input size, then the algorithm has a complexity.linear time\nLinear time is just one of many time complexity classes that an algorithm can exhibit. If there exists a quadratic relationship between input\nsize and runtime, the algorithm has a complexity. If the runtime stays relatively constant regardless of the input size, then thequadratic time\nalgorithm has a complexity. Examples of these two time complexity classes are shown below:constant time\nBig-O notation can be used to express these time complexities mathematically. Instead of describing time complexity classes using words like\nconstant, linear, and quadratic, we describe them using a notation that follows the form 𝑂(𝑓(…)), where is a function that depends on an𝑓(…)\nalgorithm’s input size variables.\nsum_array()Let’s revisit the graph of the function, shown previously in example 4.1. How can we express its asymptotic runtime using\nbig-O notation? To do so, we would need to determine a function 𝑓(𝑛), where 𝑛is the size of the array we want to sum. We can determine 𝑓(𝑛)\nby following these steps:\n1. Express the runtime as a function of the input size variables.\n2. Identify the fastest growing terms in the equation calculated in step 1 and drop all lower order terms.\n3. Drop the coefficients of the fastest growing terms.\nAfter doing this, you will be left with the term that is used for the big-O notation. Let’s try an example:\nExample 4.2 sum_array()Express the time complexity class of the function using big-O notation.\nTo do this, we can follow the three steps above.\n1. Express the runtime as a function of the input size variables. We can use the points shown on the previous page to determine the line\nof best fit. By doing so, we can express the runtime as the function of the input size 𝑛using the equation:𝑇(𝑛)\n(4×10−5)𝑛+2𝑇(𝑛)=\n2. Identify the fastest growing terms in the equation calculated in step 1 (and drop all lower order terms). The fastest growing term is\nthe term that grows the fastest as the input size (in this case, 𝑛) increases without bound. In the equation above, the constant term 2 does\n(4×10−5)𝑛grows 4×10−5𝑛.not grow at all as 𝑛increases, while the term linearly as 𝑛increases. The fastest growing term is thus\nIn many cases, the fastest growing term is the term with the largest exponent. For example, if the relationship between input size and\n5𝑛3+2𝑛7+16𝑛6+9𝑛2+55𝑛4 2𝑛7,runtime were +23, the fastest growing term would be since this is the term with the largest𝑇(𝑛)=\nexponent, 7. See the following page for orders of growth.\n4×10−5𝑛, (4×10−5).3. Dropthecoefficientsofthefastestgrowingterms. Sincethefastestgrowingtermis wecanremovethecoefficient\nWe are left with the term 𝑛, which is our value of 𝑓(𝑛).\nsum_array()The time complexity of is thus 𝑂(𝑛). Let us consider another example.\nExample 4.3 return_zero()You are given the following function. Express the time complexity of the function using big-O notation:\n1\nint32_t return_zero(int32_t size_tarr[], size) {\n2\nint32_t zero = 0;\n3\nreturn zero;\n4\n} // return_zero()\nAfter running this function with arrays of many different sizes and measuring its runtime, we get the following:\nInput Size\nRuntime (ms)\n10000\n1.7\n25000\n1.7\n50000\n1.7\n75000\n1.7\n100000\n1.7", "word_count": 589, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3c16734f-8213-5170-bdb0-2e26410b300f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 100, "real_page_number": null, "text": "88\nChapter 4. Complexity Analysis\nThe runtime of the function is plotted below:\nWe can use this information to follow the three steps on the previous page.\nExpress the runtime as a function of the input size variables. We can express the runtime as a function of the input size variable1. 𝑇(𝑛)\n𝑛using the equation:\n𝑇(𝑛) 1.7=\nIdentify the fastest growing terms in the equation calculated in step 1 (and drop all lower order terms). There is only one term here2.\n(1.7), so it is by default the fastest growing term.\n1.7𝑛0.3. Drop the coefficients of the fastest growing terms. Although it may not be obvious, the term 1.7 can actually be written as Thus,\n𝑛0,the coefficient of the fastest growing term is 1.7 — removing this, we end up with just or 1.\nreturn_zero()The time complexity of is thus 𝑂(1).\nWhy are we allowed to drop coefficients and lower order terms when dealing with time complexities? Recall that the time complexity of\nan algorithm only deals with how its runtime with respect to input size. Thus, it does not matter what the coefficients and lowerscales\n0.5𝑛2order terms are, as they are not directly responsible for an algorithm’s rate of growth. For example, the functions and𝑇(𝑛) +13𝑛+5=\n14𝑛2 both experience the same quadratic growth with respect to input size, even though they have different coefficients and𝑇(𝑛) +3𝑛+9=\n𝑂(𝑛2).lower order terms; it would therefore be more meaningful to express both functions with a complexity of\nRemark: Just because coefficients are dropped does mean that they are not important! An algorithm whose runtime can be expressednot\n999𝑛2 0.01𝑛3using the function is slower than an algorithm whose runtime can be expressed using for most reasonable input𝑇(𝑛) 𝑇(𝑛)= =\nsizes 𝑛, even if it technically has a better time complexity.\nThis is where knowing the difference between runtime and asymptotic runtime is crucial. When dealing with time complexities, we only\nwork with the rate of growth and not the actual runtime itself. Because of this, a better time complexity does not guarantee a faster runtime.\nAs shown with the example above, it is entirely possible for an algorithm with a better asymptotic time complexity to run slower — this is\nbecause time complexity only indicates how fast runtime size, not the actual runtime itself!scales with input\nThe following table lists a few common complexity classes, in order of rate of growth:\nName of Complexity Class\nBig-O Notation\nConstant\n𝑂(1)\nLogarithmic\n𝑂(log(𝑛))\nLinear\n𝑂(𝑛)\nLoglinear/Linearithmic\n𝑂(𝑛log(𝑛))\nQuadratic\n𝑂(𝑛2)\nCubic\n𝑂(𝑛3)\nPolynomial (includes quadratic and cubic)\n𝑂(𝑛𝑑) where 𝑑is a constant\nExponential\n𝑂(𝑐𝑛) where 𝑐is a constant\nFactorial\n𝑂(𝑛!)\nDoubly Exponential\n𝑂(22𝑛)\nA visualization of each of these complexity classes is provided on the graph at the top of the next page.\nTo summarize this section so far, we introduced the concept of big-O notation and how we can derive the time complexity of an algorithm\nusing experimentation. However, if the chapter ended up here, you should not be satisfied! First of all, experimentation is not always feasible\n(for example, you will not be able to run code on an exam!). And, in the cases where experimentation can be done, it is not always easy. Even\nwith data, how can we derive an equation that best relates runtime to input size? How large will our input size have to be to derive a precise\nestimation that is not subject to noise and variation? Furthermore, if we have multiple variables at play (such as the number of vertices (𝑉) and\nedges (𝐸) in a graph), how can we take all of these variables into account and distinguish among them in our measurements?\nThe good news is that you do not need a computer or any experimentation to determine the time complexity of an algorithm; simply looking\nat the code itself is often more than enough to calculate its time complexity. This can be done by analyzing the steps that the algorithm takes\nand measuring how the (rather than its runtime) scales with the input size.number of steps taken", "word_count": 690, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0637e95b-d0f9-5199-a5b8-ca7d97551f24", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 101, "real_page_number": null, "text": "4.3 Measuring Complexity by Counting Steps\n89\n4.3\nMeasuring Complexity by Counting Steps\nIn many cases, you will not be able to measure the runtime of an algorithm via experimentation. Luckily, experimentation is not the only way to\ncalculate an algorithm’s time complexity. An alternative approach is to count the that an algorithm takes and express the resultnumber of steps\nas a function of the input size. This is the approach you will be expected to know on exams.\nWhy does this work? As long as the runtime of a single step is independent of input size and can be done in constant time, a function that\nexpresses the relationship between the number of steps and the input size should have the as a function that expresses thesame order of growth\nrelationship between runtime and input size. In other words, the number of steps that an algorithm takes is directly related to its runtime! For\nexample, suppose that there is a linear relationship between the number of steps an algorithm takes and its input size. In this case, increasing\nthe input size causes a linear growth in the step count needed to complete the algorithm. Since each step takes constant time, increasing the\nstep count linearly would also increase the runtime linearly. Similarly, if there is a quadratic relationship between the number of steps that an\nalgorithm takes and its input size, a quadratic blowup in the number of steps would also lead to a quadratic blowup in runtime.\nWhat counts as a step in a program? In this class, we will consider the following operations as a single step. These operations all take a\nconstant amount of time and are dependent on the size of the input. For example, the time it takes to multiply two fixed-size numbers in annot\narray should remain roughly the same regardless of whether the input array has a size of ten or ten million.\n1. variable assignment (e.g. 𝑎= 5)\n2. arithmetic operation (e.g. 𝑎+𝑏)\n3. comparison operation (e.g. 𝑏)𝑎<\narr[i])4. array indexing (e.g.\n*ptr)5. pointer reference (e.g.\n6. function call\n7. function return\nWith this in mind, we can use the number of steps an algorithm takes to determine its time complexity. The following procedure can be used to\ncomplete this process (this is very similar to the procedure outlined in section 4.2):\n1. Express the as a function of the input size variables.step count\n2. Identify the fastest growing terms in the equation calculated in step 1 and drop all lower order terms.\n3. Drop the coefficients of the fastest growing terms.", "word_count": 438, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "22823051-10ac-5caa-8634-15aa77b54498", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 102, "real_page_number": null, "text": "90\nChapter 4. Complexity Analysis\nLet’s consider the example in the previous section, this time using a step-counting approach:\nExample 4.4 sum_array()Express the time complexity of the function using big-O notation. Let input size 𝑛be the size of the array.\n1\nint32_t sum_array(int32_t size_tarr[], size) {\n2\nint32_t total = 0;\n3\nfor (size_t i = 0; i < size; ++i) {\n4\ntotal += arr[i];\n5\n} // for i\n6\nreturn total;\n7\n} // sum_array()\nsize,In this example, we will count the number of steps needed to complete this function. The input size 𝑛is the value of the variable so we\nsizewill use both and 𝑛interchangeably. Let’s look at the function line-by-line and analyze the number of steps required.\n• On line 1, we have the function call definition. For our examples, we will ignore this line and attribute the work required to call the\nsum_array() sum_array() main()function to the caller of the function instead of the function itself. In other words, if calls\nsum_array(), main(), sum_array().then line 1 executes — however, the work for line 1 will be counted for and not\nsum_array()• Line 2 is where the implementation of begins. We have a variable assignment, which takes one step.\nfor (i• Lines 3 and 4 contain a loop. Line 3 itself has an initialization assignment = 0) which is done once, a comparison that is done\ntimes (one comparison for every time the body of the loop runs, plus one additional comparison for the final check to determine𝑛+1\n(++i)that the loop should end), and an arithmetic operation that is run 𝑛times (once for each time the body of the loop runs).\nfor• Line 4 contains an arithmetic operation, which takes one step. However, since this is run inside a loop that runs 𝑛times, the total\nnumber of steps taken is 𝑛.\n• Line 6 returns from the function, which takes one step.\nWith this information, we can express the number of steps as a function of input size.\n1. Express the step count as a function of the input size variables. The total number of steps taken is:\n• 1 step on line 2\n• 1 + (𝑛+ 1) + 𝑛steps on line 3\n• 𝑛steps on line 4\n• 1 step on line 6\nwhich totals to steps.1+1+𝑛+1+𝑛+𝑛+1 3𝑛+4=\nThus, our equation that expresses the step count as a function of input size 𝑛is:𝑆(𝑛)\n𝑆(𝑛) 3𝑛+4=\n2. Identify the fastest growing terms in the equation calculated in step 1 (and drop all lower order terms). The fastest growing term in\nthe expression is 3𝑛.\n3. Drop the coefficients of the fastest growing terms. Removing the coefficeint from 3𝑛leaves us with 𝑛.\nsum_array()The time complexity of is thus 𝑂(𝑛).\nWe got the same answer that we did when we measured the complexity using the more tedious method of experimentation. However, we are\n+=still doing a bit more work than necessary. For example, consider the operator on line 4. In the example, we listed it as one step. However,\n+= ++iis actually two separate steps: an addition followed by an assignment. A similar argument can be made for the operation on line 3: the\n++ operator first increments, then assigns, resulting in two steps rather than one. Thus, the equation is not fully accurate.3𝑛+4\n+=So, why did we provide an example with an \"inaccurate\" analysis? It turns out that debating whether counts as one step or two steps does\nnot matter! Since coefficients and lower order terms are removed anyway, small differences like these will not affect the time complexity class. If\n+= ++we had treated and as two steps instead of one in the previous example, we would have gotten the equation 5𝑛+4. However, the𝑆(𝑛)=\ngrowth of this function is still linear! The difference between the 3 and the 5 does not change the overall time complexity class fo the algorithm.\nThis little detail actually makes our job easier. Why? It does not matter if a line of code takes 1, 2, 3, or even 999 steps; as long as the\nnumber is a constant, the difference in steps does not affect the answer. Because these details do not matter, we do not need to calculate the\nexact number of steps when deriving the complexity class of an algorithm. Instead, it would suffice to just determine the asymptotic complexity\nof each line instead — any coefficients or lower order terms would eventually be dropped regardless.", "word_count": 763, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "370fac03-d1e6-5322-bd3e-b6044c3ad66a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 103, "real_page_number": null, "text": "4.3 Measuring Complexity by Counting Steps\n91\nIn other words, if a line of code involves a constant number of steps, we assign the line with the complexity 𝑂(1), regardless of what the constant\nis. Similarly, if the number of steps required to execute a line of code has a linear relationship with the input size, we would assign it with the\ncomplexity 𝑂(𝑛). This leads us to the following procedure for determining the time complexity of an algorithm, given its code:\nCalculating the Time Complexity of a Function By Counting Steps\n1. Start at the first line of the function, not including the function header. Walk through the code line by line, assigning each line with a\nvalue using the following rules (it may be easy to jot it down to the side).\nfor(...) while(...),a. If the line is the definition of a loop (e.g. or etc.), assign it with the number of times the loop\nexecutes (you may ignore coefficients and lower order terms).\nb. If the line is not a loop definition, assign it with the time complexity required to execute the line.\n2. After all lines have been assigned a value, identify the loops in the function. For non-nested loops, all time complexities inadd\nthe body of the loop, and then this result with the number of times the loop runs. That is, to determine the overall timemultiply\ncomplexity of a loop, multiply the time complexity of its body with the number of times the loop runs.\nFor nested loops (loops within a loop), start from the innermost loop and work outwards, calculating the total complexity at each step.3.\nThis can be done by starting with the innermost loop and running the procedure from step 2 on that loop. Then, use the value you get\nto repeat step 2 on the next innermost loop, repeating until you reach the outermost loop.\n4. After all the loops are taken care of, the remaining complexities to get the final time complexity. Remember to remove coefficientsadd\nand lower order terms.\nLet’s try a few examples.\nExample 4.5 sum_array()Express the time complexity of the function using big-O notation. Let input size 𝑛be the size of the array.\n1\nint32_t sum_array(int32_t size_tarr[], size) {\n2\nint32_t total = 0;\n3\nfor (size_t i = 0; i < size; ++i) {\n4\ntotal += arr[i];\n5\n} // for i\n6\nreturn total;\n7\n} // sum_array()\nTo approach this problem, walk through each line of the function code and assign it with the time required to execute it:\n• Line 2: this is an assignment, which takes constant time, so line 2 gets assigned 𝑂(1).\nsize size• Line 3: this is a loop definition, so we assign it with the number of times it runs. This loop runs times. Since is the input\nsizesize 𝑛, we can substitute with 𝑛, so line 3 gets assigned 𝑂(𝑛).\n• Line 4: this is an addition and an assignment, which takes constant time, so line 4 gets assigned 𝑂(1).\n• Line 6: this is a return statement, which takes constant time, so line 5 gets assigned 𝑂(1).\nforAt this point, we should have the following (the box represents the scope of the loop):\n1\nint32_t sum_array(int32_t size_tarr[], size) {\n2\nint32_t total = 0;\n3\nfor (size_t i = 0; i < size; ++i) {\n4\ntotal += arr[i];\n5\n} // for i\n6\nreturn total;\n7\n} // sum_array()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n𝑂(1)\n𝑂(𝑛):\n𝑂(1)\n𝑂(1)\nNow that we have listed out all the time complexities, we will now combine the loops. Here, we only have one loop defined on line 3. To\ndetermine the overall time complexity of running this loop, we can multiply the body of the loop (which takes time, as indicated on line 4)𝑂(1)\nwith the number of times the loop runs: 𝑂(𝑛).𝑂(𝑛)×𝑂(1)=\n1\nint32_t sum_array(int32_t size_tarr[], size) {\n2\nint32_t total = 0;\n3\nfor (size_t i = 0; i < size; ++i) {\n4\ntotal += arr[i];\n5\n} // for i\n6\nreturn total;\n7\n} // sum_array()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n𝑂(1)\nO(n)\n𝑂(1)\nforthe loop runs a 𝑂(1)\noperation times, so the𝑂(𝑛)\noverall time complexity of the\nloop is 𝑂(𝑛)×𝑂(1), or 𝑂(𝑛).\nNow that we have dealt with all the loops, we can sum up the remaining time complexities to get the overall time complexity of the function:\n𝑇(𝑛) 𝑂(1)+𝑂(𝑛)+𝑂(1)=\nsum_array()The highest order term is 𝑂(𝑛), so the time complexity of the function is 𝑂(𝑛).", "word_count": 792, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1be102e5-bdb1-5378-9b31-0e1b9c01337d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 104, "real_page_number": null, "text": "92\nChapter 4. Complexity Analysis\nExample 4.6 foo()Express the time complexity of the function using big-O notation. Let the input size 𝑛be the size of the array. The\nsum_array() function is the same as the one in the previous example.\n1\nvoid foo(int32_t size_tarr[], size) {\n2\nint32_t static_cast<int32_t>(size);num =\n3\nwhile (num > 0) {\n4\nfor (size_t j = 0; j < size; ++j) {\n5\nfor (size_t k = size - 1; k > (size / 2); --k) {\n6\nstd::cout << arr[k] << '\\n';\n7\nsum_array(arr, size);\n8\n} // for k\n9\n} // for j\n10\n--num;\n11\n} // while\n12\nfor (size_t i = 0; i < size; ++i) {\n13\nstd::cout << arr[i] << '\\n';\n14\n} // for i\n15\nreturn;\n16\n} // foo()\nLet’s walk through each line of this function and assign it with the time required to execute the line:\n• Line 2: this is an assignment, which takes constant time, so line 2 gets assigned 𝑂(1).\nnumLine 3: this is a loop definition, so we assign it with the number of times it runs. Notice how this loop runs as long as is greater•\nnum size while size,than 0, where is initialized to and is decremented after each run of the loop. Thus, the loop runs or 𝑛times.\nsize• Line 4: this is a loop definition, so we assign it with the number of times it runs. Because this loop runs from 0 to and increments\nby 1 per iteration, it runs 𝑛times.\n• Line 5: this is a loop definition, so we assign it with the number of times it runs. This loop runs from to 𝑛∕2, or roughly times.𝑛−1 𝑛∕2\nThis can be represented using the complexity class 𝑂(𝑛).\n• Line 6: this is a print statement with array indexing — the runtime of this operation is independent of input size, so this line gets assigned\nconstant time, or 𝑂(1).\nsum_array()• Line 7: the function runs in linear time, from the previous example. Thus, this line gets assigned with 𝑂(𝑛).𝑂(𝑛)\n• Line 10: this is a constant time operation, so it gets assigned 𝑂(1).\nsize• Line 12: this is a loop definition, so we assign it with the number of times it runs. This loop runs from 0 to and increments by 1\nper iteration, so it runs 𝑛times.\n• Line 13: this is a constant time operation, so it gets assigned 𝑂(1).\n• Line 15: this is a constant time operation, so it gets assigned 𝑂(1).\nAt this point, we have the following (the boxes represent the scope of the loops):\n1\nvoid foo(int32_t size_tarr[], size) {\n2\nint32_t static_cast<int32_t>(size);num =\n3\nwhile (num > 0) {\n4\nfor (size_t j = 0; j < size; ++j) {\n5\nfor (size_t k = size - 1; k > (size / 2); --k) {\n6\nstd::cout << arr[k] << '\\n';\n7\nsum_array(arr, size);\n8\n} // for k\n9\n} // for j\n10\n--num;\n11\n} // while\n12\nfor (size_t i = 0; i < size; ++i) {\n13\nstd::cout << arr[i] << '\\n';\n14\n} // for i\n15\nreturn;\n16\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n10\nx\n11\nx\n12\nx\n13\nx\n14\nx\n15\nx\n16\nx\n𝑂(1)\n𝑂(𝑛):\n𝑂(𝑛):\n𝑂(𝑛):\n𝑂(1)\n𝑂(𝑛)\n𝑂(1)\n𝑂(𝑛):\n𝑂(1)\n𝑂(1)\nNow, we will walk through the code and condense the loops. The first loop we encounter is a triple nested loop spanning lines 3-11, consisting\nwhile forof one loop and two loops. Since this loop is nested, we start from the innermost loop and work outwards. We will first begin by\nforanalyzing the loop defined on line 5 (the innermost loop):\n3\nwhile (num > 0) {\n4\nfor (size_t j = 0; j < size; ++j) {\n5\nfor (size_t k = size - 1; k > (size / 2); --k) {\n6\nstd::cout << arr[k] << '\\n';\n7\nsum_array(arr, size);\n8\n} // for k\n9\n} // for j\n10\n--num;\n11\n} // while\n3\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n10\nx\n11\nx\n𝑂(𝑛):\n𝑂(𝑛):\n𝑂(𝑛):\n𝑂(1)\n𝑂(𝑛)\n𝑂(1)", "word_count": 732, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "02d8b5fa-adea-5518-b439-6598fb7d23eb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 105, "real_page_number": null, "text": "4.3 Measuring Complexity by Counting Steps\n93\nWhen dealing with a non-nested loop, we first compute the complexity class of the loop body (lines 6-7). The time complexity of lines 6\nand 7 is 𝑂(𝑛), which can be simplified to after dropping lower order terms. Thus, the body of the loop (lines 6 and 7) runs in𝑂(1) 𝑂(𝑛)+\nfortime. Since lines 6-7 run times inside the innermost loop defined on line 5, the time complexity of the innermost loop thus is𝑂(𝑛) 𝑂(𝑛)\n𝑂(𝑛2). 𝑂(𝑛2),After condensing this loop into a single complexity class we get the following:𝑂(𝑛)×𝑂(𝑛)=\n3\nwhile (num > 0) {\n4\nfor (size_t j = 0; j < size; ++j) {\n5\nfor (size_t k = size - 1; k > (size / 2); --k) {\n6\nstd::cout << arr[k] << '\\n';\n7\nsum_array(arr, size);\n8\n} // for k\n9\n} // for j\n10\n--num;\n11\n} // while\n𝑂(𝑛2)running this entire loop takes time\n3\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n10\nx\n11\nx\n𝑂(𝑛):\n𝑂(𝑛):\nO(n2)\n𝑂(1)\n𝑂(𝑛2)Now, we move on to the next innermost loop, defined on line 4. This loop runs 𝑛times, and the body of the loop runs in time. Thus, the\n𝑂(𝑛)×𝑂(𝑛2) 𝑂(𝑛3).combined time complexity of the two loops is Lines 4-9 have now been condensed into a single time complexity class:=\n3\nwhile (num > 0) {\n4\nfor (size_t j = 0; j < size; ++j) {\n5\nfor (size_t k = size - 1; k > (size / 2); --k) {\n6\nstd::cout << arr[k] << '\\n';\n7\nsum_array(arr, size);\n8\n} // for k\n9\n} // for j\n10\n--num;\n11\n} // while\n𝑂(𝑛3) timerunning this entire loop takes\n3\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n10\nx\n11\nx\n𝑂(𝑛):\nO(n3)\n𝑂(1)\nwhileWe have one loop left in this nested loop (the loop defined on line 3). The body of this loop has an overall time complexity of\n𝑂(𝑛3)+𝑂(1) 𝑂(𝑛3), 𝑂(𝑛)×𝑂(𝑛3) 𝑂(𝑛4).whileanditisrunatotalof times. Thus, theoveralltimecomplexityofthe looponline3is𝑂(𝑛)= =\n𝑂(𝑛4),The nested loop on lines 3-11 has been condensed into a single time complexity of so now we can move on to the next loop on line 12.\n1\nvoid foo(int32_t size_tarr[], size) {\n2\nint32_t static_cast<int32_t>(size);num =\n3\nwhile (num > 0) {\n4\nfor (size_t j = 0; j < size; ++j) {\n5\nfor (size_t k = size - 1; k > (size / 2); --k) {\n6\nstd::cout << arr[k] << '\\n';\n7\nsum_array(arr, size);\n8\n} // for k\n9\n} // for j\n10\n--num;\n11\n} // while\n12\nfor (size_t i = 0; i < size; ++i) {\n13\nstd::cout << arr[i] << '\\n';\n14\n} // for i\n15\nreturn;\n16\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n10\nx\n11\nx\n12\nx\n13\nx\n14\nx\n15\nx\n16\nx\n𝑂(1)\n𝑂(𝑛4)\n𝑂(𝑛):\n𝑂(1)\n𝑂(1)\nThe loop on line 12 runs a operation times, so the complexity of this loop is 𝑂(𝑛).𝑂(1) 𝑂(𝑛) 𝑂(𝑛)×𝑂(1)=\n1\nvoid foo(int32_t size_tarr[], size) {\n2\nint32_t static_cast<int32_t>(size);num =\n3\nwhile (num > 0) {\n4\nfor (size_t j = 0; j < size; ++j) {\n5\nfor (size_t k = size - 1; k > (size / 2); --k) {\n6\nstd::cout << arr[k] << '\\n';\n7\nsum_array(arr, size);\n8\n} // for k\n9\n} // for j\n10\n--num;\n11\n} // while\n12\nfor (size_t i = 0; i < size; ++i) {\n13\nstd::cout << arr[i] << '\\n';\n14\n} // for i\n15\nreturn;\n16\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n10\nx\n11\nx\n12\nx\n13\nx\n14\nx\n15\nx\n16\nx\n𝑂(1)\n𝑂(𝑛4)\nO(n)\n𝑂(1)\nAfter dealing with all the loops, we have managed to condense this function into four sequential steps: an assignment that takes time,𝑂(1)\n𝑂(𝑛4)while fora triple-nested loop that runs in time, and loop that runs in time, and a return statement that takes time. Since𝑂(𝑛) 𝑂(1)\nfoo()these steps happen one after another, we can add their complexities together to obtain the overall time complexity of the function:\n𝑂(1)+𝑂(𝑛4)+𝑂(𝑛)+𝑂(1) 𝑂(𝑛4).=\nfoo()Note that lower order terms can be ignored, and that the time complexity of the entire function is governed by the term with the\n𝑂(𝑛4)largest time complexity class. Here, the nested loop that spans lines 3-11 runs in time, which dominates all the other components of the\nfoo()function. As a result, this loop is a bottleneck that determines the rate of growth for the entire function.", "word_count": 839, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8712f598-ffc3-55da-a82a-751decf44ece", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 106, "real_page_number": null, "text": "94\nChapter 4. Complexity Analysis\n4.4\nLogarithmic Runtime\n¸ 4.4.1\nAnalyzing Logarithmic Runtime\nIn the previous two examples, we dealt with loops that ran 𝑛iterations, where 𝑛is the input size. An example of such a loop is shown below.\nfor (int32_t i = 1; i < input_size; ++i) { ... }\nHowever, not all loops behave like this. What if we changed the update condition so that the loop counter is rather than incremented?doubled\nfor (int32_t i = 1; i < input_size; i *= 2) { ... }\niHow many times does the loop run now? On the first iteration, the loop counter is 1. Then, it becomes 2, then 4, then 8, then 16, doubling\ninput_size,with each iteration until it reaches the value of or 𝑛. Thus, the number of times the loop runs is equal to the number of times we\ni input_size.can double before it hits In mathematical terms, this can be represented as:\n2𝑘=𝑛\ni input_sizewhere 𝑘represents the number of times the loop can run before exceeds or 𝑛. Solving for 𝑘, we get:\n𝑘=log2(𝑛)\nfor iThus, the second loop (where is doubled with each iteration) runs on the order of times, or times. For example,log2(𝑛) 𝑂(log2(𝑛))\ni isuppose the input size is 64, and the loop counter doubles with each iteration. The loop would run six times: when equals 1, 2, 4, 8, 16, and\ni32. After the sixth iteration runs, gets doubled to 64, and the loop exits. This makes sense with our analysis, since is indeed 6. If thelog2(64)\ninput size were a non-power of two like 65, the loop would run seven times, which is still on the order of log2(𝑛).\nDivision works similarly. Consider the following loop:\nfor (int32_t i = input_size; i > 1; i /= 2) { ... }\niHere, the loop counter is halved at each iteration of the loop. Now, we want to calculate the number of times we can halve the input size\nbefore we hit 1, since this is the number of times the loop runs. Once again, we can write this in mathematical terms as:\n𝑛\n(\n1\n2\n)𝑘\n=1\nSolving for 𝑘, we can rewrite this as:\n𝑘=log2(𝑛)\nDivision gives us the same result as multiplication. This leads us to an interesting conclusion: if multiplication or division is involved in the\nupdate of a loop, there is a good chance that a logarithmic term is in play. Furthermore, if the search space of a problem gets cut in half (or by\nsome other factor) with each iteration, a logarithmic term is likely involved.\nHere is an example of a logarithmic algorithm in practice: suppose you wanted to find someone in a phone book, where the phone book has\na total of 𝑛pages. If you looked through every page of the phone book until you find the person you want, that would be an algorithm.𝑂(𝑛)\nHowever, if the phone book is alphabetical by last name, you can use this fact to reduce the work you have to do! For example, if you wanted to\nfind a last name starting with the letter \"P\", you could completely ignore the first half of the book since you know that the letter \"P\" must be in\nthe second half. You can continue this process by flipping to the middle page of the second half and checking if \"P\" comes before or after. This\ni /= 2eliminates half of the remaining pages at each step (similar to the update of the loop), leading to a logarithmic algorithm with complexity\n𝑂(log2(𝑛)). In other words, you would only need to check at most pages of a phone book with 𝑛pages to find any person you want!𝑂(log2(𝑛))\nExample 4.7 foo()Express the time complexity of the function using big-O notation. Let the input size 𝑛be the size of the array.\n1\nint32_t foo(int32_t size_tarr[], size) {\n2\nint32_t total = 0;\n3\nfor (size_t i = 1; i < size; i *= 2) {\n4\ntotal += arr[i];\n5\n} // for i\n6\nreturn total;\n7\n} // foo()\nWalking through the code and assigning each line with its complexity results in the following:\n1\nint32_t foo(int32_t size_tarr[], size) {\n2\nint32_t total = 0;\n3\nfor (size_t i = 1; i < size; i *= 2) {\n4\ntotal += arr[i];\n5\n} // for i\n6\nreturn total;\n7\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n𝑂(1)\n𝑂(log2(𝑛)):\n𝑂(1)\n𝑂(1)", "word_count": 773, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6fea465e-9243-5db5-81d4-22133ce52dd0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 107, "real_page_number": null, "text": "4.4 Logarithmic Runtime\n95\niHere, the loop counter is multiplied by 2 with each iteration, so the number of times the loop runs is 𝑂(log2(𝑛)). Since the body of the loop\nforruns in time, the total time complexity of the loop on line 3 is 𝑂(log2(𝑛)).𝑂(1) 𝑂(log2(𝑛))×𝑂(1)=\n1\nint32_t foo(int32_t size_tarr[], size) {\n2\nint32_t total = 0;\n3\nfor (size_t i = 1; i < size; i *= 2) {\n4\ntotal += arr[i];\n5\n} // for i\n6\nreturn total;\n7\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n𝑂(1)\nO(log2(n))\n𝑂(1)\nfoo()The overall time complexity of the function is thus 𝑂(1)+𝑂(log2(𝑛))+𝑂(1), or simply after dropping lower order terms.𝑂(log2(𝑛))\nExample 4.8 foo()Express the time complexity of the function using big-O notation, in terms of the input value 𝑛.\n1\nvoid foo(int32_t n) {\n2\nfor (int32_t i = 1; i < n; ++i) {\n3\nfor (int32_t j = 1; j < log2(n); j *= 2) {\n4\nfor (int32_t k = 1; k < n; k *= 2) {\n5\nstd::cout << \"Potato!\\n\";\n6\n} // for k\n7\n} // for j\n8\n} // for i\n9\n} // foo()\nAs before, we will walk through each line of this function and assign it with the time required to execute the line:\n• Line 2: this loop runs from 1 to 𝑛, incrementing the counter by one with each iteration, so it gets assigned with 𝑂(𝑛).\nLine 3: this loop runs from 1 to log2(𝑛), multiplying the counter by two with each iteration. Thus, the number of times this loop runs is•\nequal to 𝑘in the following equation:\n2𝑘=log2(𝑛)\n𝑘=log2(log2(𝑛))\n• Line 4: this loop runs from 1 to 𝑛, multiplying the counter by two with each iteration. The number of times this loop runs is 𝑂(log2(𝑛)).\n• Line 5: this is a constant time operation, so it gets assigned 𝑂(1).\n1\nvoid foo(int32_t n) {\n2\nfor (int32_t i = 1; i < n; ++i) {\n3\nfor (int32_t j = 1; j < log2(n); j *= 2) {\n4\nfor (int32_t k = 1; k < n; k *= 2) {\n5\nstd::cout << \"Potato!\\n\";\n6\n} // for k\n7\n} // for j\n8\n} // for i\n9\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n𝑂(𝑛):\n𝑂(log2(log2(𝑛))):\n𝑂(log2(𝑛)):\n𝑂(1)\nTheinnermostlooponline4runsa operation times,sotheoveralltimecomplexityofthisloopis 𝑂(log2(𝑛)).𝑂(1) 𝑂(log2(𝑛)) 𝑂(log2(𝑛))×𝑂(1)=\n1\nvoid foo(int32_t n) {\n2\nfor (int32_t i = 1; i < n; ++i) {\n3\nfor (int32_t j = 1; j < log2(n); j *= 2) {\n4\nfor (int32_t k = 1; k < n; k *= 2) {\n5\nstd::cout << \"Potato!\\n\";\n6\n} // for k\n7\n} // for j\n8\n} // for i\n9\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n𝑂(𝑛):\n𝑂(log2(log2(𝑛))):\nO(log2(n))\nThe loop on line 3 runs the loop on line 4 times, so its complexity is 𝑂(log2(log2(𝑛))×log2(𝑛)).𝑂(log2(log2(𝑛)) 𝑂(log2(log2(𝑛))×𝑂(log2(𝑛))=\n1\nvoid foo(int32_t n) {\n2\nfor (int32_t i = 1; i < n; ++i) {\n3\nfor (int32_t j = 1; j < log2(n); j *= 2) {\n4\nfor (int32_t k = 1; k < n; k *= 2) {\n5\nstd::cout << \"Potato!\\n\";\n6\n} // for k\n7\n} // for j\n8\n} // for i\n9\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n𝑂(𝑛):\nO(log2(log2(n)) log2(n))×\nThe loop on line 2 runs the loop on line 3 times, so its complexity is 𝑂(𝑛×log2(log2(𝑛))×log2(𝑛)).𝑂(𝑛) 𝑂(𝑛)×𝑂(log2(log2(𝑛))×log2(𝑛))=\nfoo()This is also the overall time complexity of the function: 𝑂(𝑛×log2(log2(𝑛))×log2(𝑛)).\n1\nvoid foo(int32_t n) {\n2\nfor (int32_t i = 1; i < n; ++i) {\n3\nfor (int32_t j = 1; j < log2(n); j *= 2) {\n4\nfor (int32_t k = 1; k < n; k *= 2) {\n5\nstd::cout << \"Potato!\\n\";\n6\n} // for k\n7\n} // for j\n8\n} // for i\n9\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\nO(n log2(log2(n)) log2(n))× ×", "word_count": 761, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "35e1eadc-3637-586a-a293-88128a4c446a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 108, "real_page_number": null, "text": "96\nChapter 4. Complexity Analysis\nExample 4.9 power_sum()Express the time complexity of the function using big-O notation, in terms of the input value 𝑛.\n1\nint32_t power_sum(int32_t n) {\n2\nint32_t total = 0;\n3\nint32_t i = 1;\n4\nwhile (i < n) {\n5\ntotal += i;\n6\ni *= 2;\n7\n} // while\n8\nreturn total;\n9\n} // power_sum()\nLines 2, 3, 5, 6, and 8 are all constant time operations. Thus, the time complexity of this function will be determined by the number of times the\nwhile i n, iloop on line 4 runs. The loop runs as long as is less than and is doubled with each iteration of the loop. Thus, the number of\nwhile whiletimes the loop runs is 𝑂(log2(𝑛)). Since the body of the loop runs in time, the time it takes for the entire loop to execute is𝑂(1)\niterations work per iteration 𝑂(log2(𝑛)).𝑂(log2(𝑛)) 𝑂(1)× =\n1\nint32_t power_sum(int32_t n) {\n2\nint32_t total = 0;\n3\nint32_t i = 1;\n4\nwhile (i < n) {\n5\ntotal += i;\n6\ni *= 2;\n7\n} // while\n8\nreturn total;\n9\n} // power_sum()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n𝑂(1)\n𝑂(1)\n𝑂(log2(𝑛)):\n𝑂(1)\n𝑂(1)\n𝑂(1)\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n𝑂(1)\n𝑂(1)\nO(log2(n))\n𝑂(1)\npower_sum()The overall time complexity of the function is 𝑂(log2(𝑛)).𝑂(1)+𝑂(1)+𝑂(log2(𝑛))+𝑂(1)=\n¸ 4.4.2\nDoes the Base of a Logarithm Matter?\nWhen dealing with logarithmic complexity terms, we often ignore the base of the logarithm. It turns out that it does not matter whether an\nalgorithm is or 𝑂(log3(𝑛)); we treat both as part of the same complexity class 𝑂(log(𝑛)). This is because we can always convert a𝑂(log2(𝑛))\nlog with one base to a log with another base by multiplying it with a constant, thanks to the for logarithms:change of base formula\nlog𝑎(𝑛)log𝑏(𝑛)=\nlog𝑎(𝑏)\nTo change a logarithmic expression with base 𝑎to an identical expression with base 𝑏, we simply have to multiply the base-𝑎log with the\nconstant\n1\nlog𝑎(𝑏). Because of this identity, the base does not matter! We will go over an example of this in the following example.\nExample 4.10 Show that is both and 𝑂(log3(𝑛)).log2(𝑛) 𝑂(log2(𝑛))\nis by definition also 𝑂(𝑓(𝑛)), so must be 𝑂(log2(𝑛)). To show that is also 𝑂(log3(𝑛)), we will use the change of base formula:𝑓(𝑛) log2(𝑛) log2(𝑛)\nlog3(𝑛)log2(𝑛)=\nlog3(2)\nlog3(𝑛)In other words, can be rewritten aslog2(𝑛)\nlog3(2). This means that\nlog2(𝑛) 𝑂(log2(𝑛)) 𝑂= =\n(log3(𝑛)\nlog3(2)\n)\n𝑂=\n(\n1\n×log3(𝑛)log3(2)\n)\nSince we can ignore constants when expressing asymptotic time complexity, we can ignore the\n1\nterm, since it is a constant (≈1.58):log3(2)\nlog2(𝑛) 𝑂=\n(\n1\n×log3(𝑛)log3(2)\n)\n𝑂(log3(𝑛))=\nThus, we have shown that is also 𝑂(log3(𝑛)), and that and must be a part of the same complexity class! Thelog2(𝑛) 𝑂(log2(𝑛)) 𝑂(log3(𝑛))\nsame applies for any base. Because of the change of base formula, two logarithms with different bases only differ by a factor. Asconstant\nshown above, the difference between and is a constant factor of approximately 1.58. As a result, if you are in a logarithmiclog3(𝑛) log2(𝑛)\ncomplexity class, it does not matter if the base is 2 or 3 or 999. While a algorithm does run faster than a algorithm for largelog3(𝑛) log2(𝑛)\nenough input sizes, the speed difference should only differ by a constant amount that grows. Therefore, the basedoes not grow as the input size\nof the logarithm does not affect the growth of an algorithm, and thus is ignored in big-O notation.\n𝑂(2𝑛) 𝑂(8𝑛)Note that this does NOT apply to exponents! The base of an exponent does matter. For example, and are not part of the same\n8𝑛=2𝑛×22𝑛, 22𝑛iscomplexity class. This is because and not a constant factor!", "word_count": 677, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "606f994e-139f-5382-99e7-f6072cae1c6e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 109, "real_page_number": null, "text": "4.4 Logarithmic Runtime\n97\n¸ 4.4.3\nLogarithmic and Power Identities\nThe following identities may be useful for working with time complexities that involve logarithms:\nLog Identities\nPower Identities\n1) log𝑎(𝑥𝑦) log𝑎(𝑥)+log𝑎(𝑦)=\n2) log𝑎\n(\n𝑥\n𝑦\n)\nlog𝑎(𝑥)−log𝑎(𝑦)=\nlog𝑎(𝑥𝑟)3) 𝑟log𝑎(𝑥)=\n4) log𝑎\n(\n1\n𝑥\n)\n−log𝑎(𝑥)=\nlog(𝑥)5) log𝑎(𝑥)=\nlog(𝑎)\n6) log𝑎(𝑎)=1\n7) log𝑎(1)=0\n𝑎𝑛+𝑚=𝑎𝑛𝑎𝑚1)\n𝑎𝑛𝑎𝑛−𝑚=2)\n𝑎𝑚\n(𝑎𝑛)𝑚=𝑎𝑛𝑚3)\n1𝑎−𝑛=4)\n𝑎𝑛\nExample 4.11 An algorithm has a runtime that can be expressed using the following equation. Express the asymptotic time complexity of\nthis algorithm using big-O notation.\n×3𝑛+5)log(𝑛)+log(𝑛17𝑇(𝑛)=\n×3𝑛+5) log(𝑛17)+log(3𝑛+5).log(𝑛17We will use the identities above to simplify the expression. First, using log identity 1, we can simplify to\nThis leaves us with\nlog(𝑛)+log(𝑛17)+log(3𝑛+5)𝑇(𝑛)=\nlog(𝑛17)Using log identity 3, we can further simplify to 17log(𝑛). This leaves us with\nlog(𝑛)+17log(𝑛)+log(3𝑛+5)𝑇(𝑛)=\nlog(3𝑛+5) log(3𝑛×35). log(3𝑛×35) log(3𝑛)+log(35).Usingpoweridentity1,wecansimplify to Wecanthenuselogidentity1againtosimplify to\nThis leaves us with\nlog(𝑛)+17log(𝑛)+log(3𝑛)+log(35)𝑇(𝑛)=\nlog(3𝑛)We can use log identity 3 again to simplify into 𝑛log(3). This leaves us with\nlog(𝑛)+17log(𝑛)+𝑛log(3)+log(35)𝑇(𝑛) 18log(𝑛)+𝑛log(3)+log(243)= =\nAfter dropping all coefficients and lower order terms in this equation, we can see that the highest order term here is 𝑛(from the 𝑛log(3), but\nis a constant coefficient, so it is dropped). Thus, 𝑂(𝑛).𝑇(𝑛)log(3) =\nExample 4.12 foo()Express the time complexity of the function using big-O notation, in terms of 𝑚and 𝑛, where 𝑚and 𝑛are positive\nintegers greater than 1.\n1\nint32_t foo(int32_t int32_tm, n) {\n2\nint32_t c = 0;\n3\nwhile (m > n) {\n4\nm = m / 2;\n5\nfor (int32_t i = 1; i < n; i *= 3) {\n6\n++c;\n7\n} // for i\n8\n} // while\n9\nreturn c;\n10\n} // foo()\nfor iLines 2, 4, 6, and 9 are all constant time operations. We can tell that the inner loop on line 5 runs a total of times since is tripledlog3(𝑛)\nwhilewith each iteration. However, how many times does the outer loop on line 3 run, in terms of 𝑚and 𝑛?\nwhile whileTo solve this, first imagine that the loop condition were instead of 𝑛. How many times would the loop run?𝑚> 𝑚>1\nwhileSince 𝑚is halved with each iteration, the outer loop would run times.log2(𝑚)\nHowever, we are stopping the loop once 𝑚reaches 𝑛, not when it reaches 1. As a result, we do not fully run the loop times. Instead,log2(𝑚)\nwe iterations if the loop is stopped when 𝑚reaches 𝑛instead of when 𝑚reaches 1. Let’s use an example to visualize this. Supposelog2(𝑛)lose\n𝑚starts off with a value of 256. If we allowed the loop to run as long as 1, the loop would run a total of times:𝑚> log2(256)=8\n256\n128\n64\n32\n16\n8\n4\n2\nwhileHowever, let’s choose the value of 𝑛to be 32. How many iterations does the loop run now? In this case, we only end up running the first\nthree iterations, since the loop now terminates when 𝑚reaches 32:\n256\n128\n64\n32\n16\n8\n4\n2", "word_count": 553, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "891969c9-f446-560c-9030-83caccaa7ec3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 110, "real_page_number": null, "text": "98\nChapter 4. Complexity Analysis\nWe lost iterations of the loop when we set 32, and only loop iterations were run. In fact, for any 𝑚and 𝑛,𝑛=log2(32)=5 log2(256)−log2(32)\nwhilethe total number of times the loop runs is equal to log2(𝑚)−log2(𝑛), which can be rewritten as using log identities.(𝑚∕𝑛)log2\n1\nint32_t foo(int32_t int32_tm, n) {\n2\nint32_t c = 0;\n3\nwhile (m > n) {\n4\nm = m / 2;\n5\nfor (int32_t i = 1; i < n; i *= 3) {\n6\n++c;\n7\n} // for i\n8\n} // while\n9\nreturn c;\n10\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n10\nx\n𝑂(1)\n𝑂(log(𝑚∕𝑛)):\n𝑂(1)\n𝑂(log(𝑛)):\n𝑂(1)\n𝑂(1)\nwhileThis outer loop runs the\nforinner loop times.𝑂(log(𝑚∕𝑛))\nforThis inner loop runs a\noperation times.𝑂(1) 𝑂(log(𝑛))\nSince the inner loop executes in time, and the outer loop runs the inner loop times, the overall time complexity of the𝑂(log(𝑛)) 𝑂(log(𝑚∕𝑛))\nfoo() function is 𝑂(log(𝑚∕𝑛)×log(𝑛)).\nRemark: A more direct way to solve this problem is to notice that 𝑚is halved at each iteration in the outer loop, which means that the value\n𝑘th 𝑚×(1∕2)𝑘.of 𝑚at the iteration of this loop must be Since our loop stops once 𝑚becomes equivalent to 𝑛, the total number of iterations\nwhilerun by the loop can be expressed as 𝑘in the following equation:\n𝑚×(1∕2)𝑘=𝑛\nwhile forSolving for the number of iterations 𝑘, we get log2(𝑚∕𝑛). Therefore, the outer loop runs the inner loop times.𝑘= log2(𝑚∕𝑛)\nSince the complexity of the inner loop is Θ(log(𝑛)), the overall time complexity of the function is 𝑂(log(𝑚∕𝑛)×log(𝑛)).\n4.5\nLoop Dependencies\nsize.Consider the following function. What is its time complexity? Assume that the input size 𝑛is the variable\n1\nint32_t foo(int32_t size_tarr[], size) {\n2\nint32_t count = 0;\n3\nfor (size_t i = size; i > 1; i /= 2) {\n4\nfor (size_t j = 0; j <= i; ++j) {\n5\n++count;\n6\n} // for j\n7\n} // for i\n8\nreturn count;\n9\n} // foo()\nThis problem may look relatively straightforward upon first glance. You might be tempted to say that the time complexity is 𝑂(𝑛log(𝑛)), since\nthe outer loop appears to run times, and the inner loop appears to run times. However, this is not accurate! This is because𝑂(log(𝑛)) 𝑂(𝑛)\nithe terminating condition of the loop depends on a value that is changing with the loop. Notice that the value of used toinner outer\nforterminate the inner loop on line 4 depends on the current iteration of the outer loop on line 3; as a result, the inner loop runs a different\nnumber of steps each time. Because of this, the interaction between the two loops cannot be determined if each loop is analyzed individually. To\napproach the problem, you must consider the entire loop dependency together and count the number of steps that occur with each iteration of the\noutermost loop in the dependency. This procedure is shown in the following examples.\nRemark: For solving loop dependency problems, it may be useful to know the equation for the sum of a geometric series. Given a sequence\nwhere the number of terms is 𝑛, the first term is 𝑎, and the common ratio between terms is 𝑟, the sum of the first 𝑛terms can be expressed\nusing the following equation:\n𝑎(1−𝑟𝑛)𝑆𝑛=\n1−𝑟\n𝑟≠1,\nExample 4.13 foo()Express the time complexity of the function using big-O notation. Let the input size 𝑛be the size of the array.\n1\nint32_t foo(int32_t size_tarr[], size) {\n2\nint32_t count = 0;\n3\nfor (size_t i = size; i > 1; i /= 2) {\n4\nfor (size_t j = 0; j <= i; ++j) {\n5\n++count;\n6\n} // for j\n7\n} // for i\n8\nreturn count;\n9\n} // foo()\nfor i forAs mentioned previously, there is a loop dependency in this function: the inner loop on line 4 depends on the value of in the outer\nforloop on line 3. Thus, we will analyze both loops together and count the total number of steps that occur with each iteration of the outer\niloop. On the first iteration of the outer loop, the value of is 𝑛, or the input size. Therefore, the inner loop would also run 𝑛times on the first\niiteration of the outer loop. On the second iteration of the outer loop, the value of is halved to 𝑛∕2, so the inner loop will also run times.𝑛∕2", "word_count": 787, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "663bc210-5164-5772-ae4a-d7e7ef7aa0e9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 111, "real_page_number": null, "text": "4.5 Loop Dependencies\n99\niWe can see that the value of is halved with each iteration, which also halves the number of times the inner loop is able to run. As a result, the\nruntime of the outer loop can be expressed as:\n𝑛𝑇(𝑛) 𝑛+=\n𝑛+2\n𝑛+4\n𝑛+8\n𝑛+16\n+…32\nsince the inner loop runs 𝑛times on the first iteration, times on the second iteration, times on the third iteration, and so on.𝑛∕2 𝑛∕4\niSince the outer loop halves the value of until it reaches a value of 1, the inner loop must run a total of times. After factoring outlog2(𝑛)\nthe 𝑛from our expression, we get the following:\n𝑇(𝑛) 𝑛=\n(\n11+\n1+2\n1+4\n1+8\n1+16\n+…32\n)\n𝑛=\nlog2(𝑛)\n∑\n𝑘=0\n(1\n2\n)𝑘\nTo solve this, we can use the equation for the sum of a geometric series.\n𝑎(1−𝑟𝑛)𝑆𝑛=\n1−𝑟\n𝑟≠1,\nHere, the number of terms is log2(𝑛)+1, the common ratio is 1∕2, and the first term is 1:\n𝑇(𝑛) 𝑛=\nlog2(𝑛)\n∑\n𝑘=0\n(1\n2\n)𝑘\n≈𝑛\n⎡\n⎢\n⎢\n⎢\n⎢⎣\n1\n(\n1−\n(\n1\n2\n)log2(𝑛)+1)\n1−1\n2\n⎤\n⎥\n⎥\n⎥\n⎥⎦\n𝑛=\n[\n2\n(\n1−\n(1\n2\n)log2(𝑛)+1)]\n𝑛=\n[\n2−2\n((1\n2\n)log2(𝑛)+1)]\n2−2((1∕2)log2(𝑛)+1)We can see that the coefficient is a constant value that is roughly equal to 2, since the term that is subtracted approaches\n𝑛(2−2((1∕2)log2(𝑛)+1))zero as 𝑛gets large. Thus, we can conclude that 𝑂(𝑛), and that the nested loop therefore takes time.≈2𝑛= 𝑂(𝑛)\n1\nint32_t foo(int32_t size_tarr[], size) {\n2\nint32_t count = 0;\n3\nfor (size_t i = size; i > 1; i /= 2) {\n4\nfor (size_t j = 0; j <= i; ++j) {\n5\n++count;\n6\n} // for j\n7\n} // for i\n8\nreturn count;\n9\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n𝑂(1)\n𝑂(𝑛)\n𝑂(1)\nThe number of steps that this nested loop\ntakes can be expressed using the summation\n𝑛(1+1∕2+1∕4+1∕8+1∕16+...)≈2𝑛\nThus, the complexity of this nested loop is 𝑂(𝑛).\nforSince lines 2 and 8 take constant time, the nested loop on line 3 takes the most time and is thus the dominant term of the entire𝑂(𝑛)\nfoo()function. The time complexity of is therefore 𝑂(𝑛).\nExample 4.14 foo()Express the time complexity of the function using big-O notation, in terms of the input size 𝑛.\n1\nvoid foo(int32_t n) {\n2\nfor (int32_t i = 0; i < n; ++i) {\n3\nfor (int32_t j = 1; j < n; j *= 3) {\n4\nfor (int32_t k = 0; k < j; ++k) {\n5\nstd::cout << \"Potato!\\n\";\n6\n} // for k\n7\n} // for j\n8\n} // for i\n9\n} // foo()\nj,Here, we have a loop dependency on lines 3 and 4. The number of times the loop on line 4 runs depends on the value of which changes in the\nfor loop on line 3. Thus, we will need to consider these two loops together.\nOn the first iteration of the line 3 loop, the line 4 loop runs 1 time. On the second iteration of the line 3 loop, the line 4 loop runs 3 times.\njOn the third iteration of the line 3 loop, the line 4 loop runs 9 times. This continues until the value of reaches 𝑛. From this, we can see that the\nline 3 loop runs the following number of iterations (using the geometric series formula, where 1, 3, and log3(𝑛)+1):𝑎= 𝑟= 𝑛=\n1+3+9+27+…+𝑛=\nlog3(𝑛)\n∑\n𝑘=0\n3𝑘=\n1(1−3log3(𝑛)+1)\n1−3\n=\n1−3(3log3(𝑛))\n1−3\n1−3𝑛=\n3𝑛−1=1−3\n2\n𝑂(𝑛)=\nThus, the loop on line 3 runs in time:𝑂(𝑛)\n1\nvoid foo(int32_t n) {\n2\nfor (int32_t i = 0; i < n; ++i) {\n3\nfor (int32_t j = 1; j < n; j *= 3) {\n4\nfor (int32_t k = 0; k < j; ++k) {\n5\nstd::cout << \"Potato!\\n\";\n6\n} // for k\n7\n} // for j\n8\n} // for i\n9\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n𝑂(𝑛):\n𝑂(𝑛)\nThe number of steps that this nested loop\ntakes can be expressed using the summation\n1+3+9+27+81+…+𝑛≈1.5𝑛\nThus, the complexity of this nested loop is 𝑂(𝑛).\nWe can see that the loop on line 2 runs 𝑛times, so the loop on line 3 must be executed 𝑛times as well. Thus, the time complexity of this function\n𝑂(𝑛2),is since the loop on line 3 is run 𝑛times in the loop on line 2.𝑂(𝑛)×𝑂(𝑛) 𝑂(𝑛)=", "word_count": 822, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ac834b2b-a052-545e-a7f4-a7d0d3bc94a0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 112, "real_page_number": null, "text": "100\nChapter 4. Complexity Analysis\n4.6\nBig-O, Big-Θ, and Big-Ω\nIn the previous few sections, we measured asymptotic time complexities using big-O. In fact, this is the notation that you will likely see in\nindustry. However, for this course and in academia, the letter \"O\" on its own may not be specific enough to express certain time complexities. In\nthis section, we will expand upon the definition of big-O notation by introducing two new notations: big-Θ and big-Ω.\nEven though we used big-O as our only notation in the previous sections, the O-notation is actually used to denote an asymptotic upper\nbound. The formal definition of big-O is as follows:\n≤𝑐𝑔(𝑛) 𝑛≥𝑛0.A function is if there exist positive constants 𝑐and such that for all input sizes𝑓(𝑛) 𝑂(𝑔(𝑛)) 𝑛0 𝑓(𝑛)\nBig-O can often be used to denote a worst-case time complexity. If an algorithm has a time complexity of 𝑂(𝑔(𝑛)), it means that the algorithm’s\n𝑂(𝑛2)runtime growth will be no worse than for input size 𝑛. For example, a algorithm will scale quadratically with respect to input size𝑔(𝑛) at\n≤𝑔(𝑛)worst. Big-O notation is similar to a relationship. If 𝑂(𝑔(𝑛)), as input size 𝑛grows without bound.𝑇(𝑛) 𝑇(𝑛)less than or equal to =\nIn addition to big-O, we also have big-Ω, which represents an asymptotic lower bound. The formal definition of big-Ω is as follows:\n≥𝑐𝑔(𝑛) 𝑛≥𝑛0.A function is if there exist positive constants 𝑐and such that for all input sizes𝑓(𝑛) Ω(𝑔(𝑛)) 𝑛0 𝑓(𝑛)\nBig-Ω can often be used to denote a best-case time complexity. If an algorithm has a time complexity of Ω(𝑔(𝑛)), it means that the algorithm’s\nΩ(𝑛2)runtime growth will be no better than for input size 𝑛. For example, a algorithm will scale quadratically with respect to input size𝑔(𝑛) at\n≥𝑔(𝑛)best. Big-Ω notation is similar to a relationship. If Ω(𝑔(𝑛)), as 𝑛grows without bound.𝑇(𝑛) 𝑇(𝑛)greater than or equal to =\nNotice that, while big-O and big-Ω are upper and lower bounds, they are bounds. If someone came to you and asked younot always tight\nhow many students were enrolled in EECS 281 this semester, it would not be wrong to say \"less than one million\" or \"more than five.\" The\nsame thing applies to big-O and big-Ω. Because big-O specifies an upper bound, any algorithm that grows slower than can be treated as a𝑔(𝑛)\n𝑂(2𝑛),algorithm. You could assign a constant time algorithm a time complexity of and it would not be wrong because grows𝑂(𝑔(𝑛)) 𝑂(1)\n𝑂(2𝑛)!slower than However, much like the \"less than one million\" response, this does not provide any useful information. The same applies for\nthe \"more than five\" response for big-Ω. An algorithm that runs in exponential time is technically also Ω(1), but saying that this algorithm will\nnot run faster than constant time does not provide any meaningful analysis.\nTo address this issue, we use something called big-Θ to denote an asymptotic tight bound. Big-Θ is the tightest bound we can assign to an\n𝑂(𝑛2)algorithm. For example, an algorithm with a complexity will not experience anything worse than quadratic growth, and an algorithm with\nΩ(𝑛2) Θ(𝑛2)a complexity will not experience anything better than quadratic growth. However, if you are given a algorithm, you can expect its\nruntime to scale quadratically as the input size gets larger. The formal definition of big-Θ is defined as follows:\n≤𝑓(𝑛)≤𝑐2𝑔(𝑛) 𝑛≥𝑛0.A function is if there exist positive constants 𝑐1, and such that for all input sizes𝑓(𝑛) Θ(𝑔(𝑛)) 𝑐2 𝑛0 𝑐1𝑔(𝑛)\nBig-Θ is similar to an relationship. If Θ(𝑔(𝑛)), we can expect and to have the same order of growth. In this course,𝑇(𝑛) 𝑇(𝑛) 𝑔(𝑛)equal to =\nwe will typically use the tightest possible bound to denote the asymptotic complexity of an algorithm. (In fact, the complexities from the\nearlier examples could have been more accurately denoted using big-Θ rather than big-O.)\n≤, ≥, ≤𝑔(𝑛)By definition, if a function is both and Ω(𝑔(𝑛)), it is also Θ(𝑔(𝑛)). Like with the and operations, if and𝑇(𝑛) 𝑂(𝑔(𝑛)) 𝑇(𝑛)=\n≥𝑔(𝑛) are both true, then must also be true.𝑇(𝑛) 𝑇(𝑛) 𝑔(𝑛)=\nComplexity Rules: The following rules can be used when dealing with asymptotic time complexities.\n• Rules of transitivity:\n1. AND IMPLY𝑓(𝑛) Θ(𝑔(𝑛)) 𝑔(𝑛) Θ(ℎ(𝑛)) 𝑓(𝑛) Θ(ℎ(𝑛))= = =\nIf grows at the same rate as 𝑔(𝑛), and grows at the same rate of ℎ(𝑛), then must grow at the same rate as– 𝑓(𝑛) 𝑔(𝑛) 𝑓(𝑛)\nℎ(𝑛).\n2. AND IMPLY𝑓(𝑛) 𝑂(𝑔(𝑛)) 𝑔(𝑛) 𝑂(ℎ(𝑛)) 𝑓(𝑛) 𝑂(ℎ(𝑛))= = =\n– If grows slower than or equal to 𝑔(𝑛), and grows slower than or equal to ℎ(𝑛), then must grow slower than𝑓(𝑛) 𝑔(𝑛) 𝑓(𝑛)\nor equal to ℎ(𝑛).\n3. AND IMPLY𝑓(𝑛) Ω(𝑔(𝑛)) 𝑔(𝑛) Ω(ℎ(𝑛)) 𝑓(𝑛) Ω(ℎ(𝑛))= = =\n– If grows faster than or equal to 𝑔(𝑛), and grows faster than or equal to ℎ(𝑛), then must grow faster than or𝑓(𝑛) 𝑔(𝑛) 𝑓(𝑛)\nequal to ℎ(𝑛).\n• Rules of reflexivity (i.e., a function is big-O of itself):\n1. 𝑓(𝑛) Θ(𝑓(𝑛))=\n2. 𝑓(𝑛) 𝑂(𝑓(𝑛))=\n3. 𝑓(𝑛) Ω(𝑓(𝑛))=\n• Rules of symmetry and transpose symmetry:\n1. if and only if𝑓(𝑛) Θ(𝑔(𝑛)) 𝑔(𝑛) Θ(𝑓(𝑛))= =\n– If grows at the same rate as 𝑔(𝑛), then must grow at the same rate as 𝑓(𝑛).𝑓(𝑛) 𝑔(𝑛)\n2. if and only if𝑓(𝑛) 𝑂(𝑔(𝑛)) 𝑔(𝑛) Ω(𝑓(𝑛))= =\n– If grows slower than or equal to 𝑔(𝑛), and must grow faster than or equal to 𝑓(𝑛).𝑓(𝑛) 𝑔(𝑛)\nWhen working with complexities, you may also encounter two other notations: little-𝑜and little-𝜔. These two notations are similar to their\ncounterparts big-𝑂and big-Ω, except that they are used to describe an upper or lower bound that is tight (i.e., less than or greater).not strictly\nMore formally:\n𝑛≥𝑛0.• A function is if there exist positive constants 𝑐and such that for all input sizes𝑓(𝑛) 𝑜(𝑔(𝑛)) 𝑛0 𝑓(𝑛)<𝑐𝑔(𝑛)\n𝑛≥𝑛0.• A function is if there exist positive constants 𝑐and such that for all input sizes𝑓(𝑛) 𝜔(𝑔(𝑛)) 𝑛0 𝑓(𝑛)>𝑐𝑔(𝑛)\nHowever, you do not need to worry about these two notations for this class.", "word_count": 1037, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "938bdbc7-baa4-5f71-9dcb-81c22ae70f17", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 113, "real_page_number": null, "text": "4.7 Complexity Metrics\n101\nExample 4.15 Show that Θ(𝑛log(𝑛)).log(𝑛!)=\nThis is an interesting question, as it is not intuitive that taking the log of a factorial term would result in something with the same complexity\nclass as 𝑛log(𝑛). However, with an understanding of log identities, we can prove that this is in fact true. For our proof, we will show that log(𝑛!)\nis both and — if both of these are true, then by definition, must also be Θ(𝑛log(𝑛)).𝑂(𝑛log(𝑛)) Ω(𝑛log(𝑛)) log(𝑛!)\nBefore we begin, the of a non-negative integer 𝑛(denoted as 𝑛!) is defined as the product of all positive integers less than or equalfactorial\nto 𝑛. In other words, 𝑛×(𝑛−1)×(𝑛−2)×…×1. By definition, the special case of is equal to 1.𝑛!= 0!\nFirst, we will show that 𝑂(𝑛log(𝑛)), or that will never grow at a rate that exceeds a constant multiple of 𝑛log(𝑛). To dolog(𝑛!) log(𝑛!)=\nthis, we can separate the individual terms of the factorial, as follows:\nlog(𝑛!) log(𝑛×(𝑛−1)×…×2×1)=\nUsing our log identities, we can rewrite this as:\nlog(𝑛!) log(𝑛)+log(𝑛−1)+…+log(2)+log(1)=\nWe know that this sum is less than 𝑛log(𝑛), which proves that 𝑂(𝑛log(𝑛)).log(𝑛!)=\n≤log(𝑛)+log(𝑛)+…+log(𝑛)+log(𝑛)log(𝑛)+log(𝑛−1)+…+log(2)+log(1) 𝑛log(𝑛)=\nNow, we will show that Ω(𝑛log(𝑛)), or that will always grow at least as fast as a constant multiple of 𝑛log(𝑛). Once again, welog(𝑛!) log(𝑛!)=\nwill start by using log identities to expand the factorial term:\nlog(𝑛!) log(𝑛)+log(𝑛−1)+…+log(2)+log(1)=\n𝑛However, this time we will remove the lower half of these terms from this sum. That is, we will remove all terms wherelog(𝑘) 𝑘<\n(the terms2\nthat are grayed out below are removed):\nlog(𝑛!) log(𝑛)+log(𝑛−1)+…+log=\n(\n𝑛\n+12\n)\n+log\n(\n𝑛\n2\n)\n+log\n(\n𝑛\n−12\n)\n+…+log(2)+log(1)\n≥log(𝑛)+log(𝑛−1)+…+loglog(𝑛!)\n(\n𝑛\n+12\n)\n+log\n(\n𝑛\n2\n)\n+log\n(\n𝑛\n−12\n)\n+…+log(2)+log(1)\n𝑛We know that the other terms must be greater than or equal to\nlog2\n(\n𝑛\n2\n)\n, which is what we get if we replaced all remaining terms with log\n(\n𝑛\n2\n)\n:\n≥log(𝑛)+log(𝑛−1)+…+loglog(𝑛!)\n(\n𝑛\n+12\n)\n+log\n(\n𝑛\n2\n)\n≥log\n(\n𝑛\n2\n)\n+log\n(\n𝑛\n2\n)\n+…+log\n(\n𝑛\n2\n)\n+log\n(\n𝑛\n2\n)\n=\n𝑛\nlog2\n(\n𝑛\n2\n)\nThus, we have shown that log(𝑛!)=Ω\n(\n𝑛\nlog2\n(\n𝑛\n2\n))\n𝑛Now, we will need to show that\nlog2\n(\n𝑛\n2\n)\nΘ(𝑛log(𝑛)), which would prove that islog(𝑛!)=\nalso Ω(𝑛log(𝑛)). To do so, we can rewrite log\n(\n𝑛\n2\n)\n𝑛as using our log identities.log(𝑛)−log(2)\nlog2\n(\n𝑛\n2\n)\ncan thereby be rewritten as:\n𝑛\nlog2\n(\n𝑛\n2\n)\n=\n𝑛\n(log(𝑛)−log(2)) =2\n𝑛\nlog(𝑛)−𝑛2\nlog(2) =2\n1\n(𝑛log(𝑛))−log(2)2\n2\n(𝑛)\n1Since coefficients and lower-order terms can be dropped,\n(𝑛log(𝑛))−log(2)2\n2\nΘ(𝑛log(𝑛)).(𝑛)=\nSince log(𝑛!)=Ω\n(\n𝑛\nlog2\n(\n𝑛\n2\n))\n𝑛and\nlog2\n(\n𝑛\n2\n)\nΘ(𝑛log(𝑛)), we can conclude that Ω(𝑛log(𝑛)).log(𝑛!)= =\nWe have successfully shown that is both and Ω(𝑛log(𝑛)). Therefore, Θ(𝑛log(𝑛)).log(𝑛!) 𝑂(𝑛log(𝑛)) log(𝑛!)=\n4.7\nComplexity Metrics\nThere are three primary ways we can express the complexity of an algorithm. Given an algorithm and a fixed input size 𝑛, we can either calculate\nits best, worst, or average-case time complexity.\nThe best-case time complexity is the time complexity of an algorithm in the very best case. This is the lower bound on the running time of\nan algorithm, and it describes the algorithm’s behavior under optimal conditions.\nThe worst-case time complexity is the time complexity of an algorithm in the very worst case. This is the upper bound on the running time\nof an algorithm, and it describes the worst rate of runtime growth that could occur over all possible inputs. The worst-case time complexity is\na common metric you will see, as it provides valuable insight into how bad an algorithm’s runtime may be. However, this metric does not\ntake into account the frequency of the worst-case scenario happening, so it is not always the only metric that should be considered. In cases\nwhere the worst-case occurs very rarely, the worst-case time complexity may not accurately reflect the true performance of an algorithm (this is\naddressed using a concept known as complexity, which is discussed in chapter 12).amortized\nThe average-case time complexity is the average time complexity of an algorithm over all possible inputs of size 𝑛. Much like finding an\naverage in a given set of values, the average-case time complexity can be determined by summing up the resource usage of all possible inputs of\nsize 𝑛and dividing this sum by the total number of inputs of size 𝑛.\nWhen analyzing the best- and worst-caseNotice that the input size we feed into an algorithm is fixed for all three complexity metrics.\nscenarios, we can only change the contents of the input and not its size. For example, it would be misleading to claim that the best-case scenario\nfor a sorting algorithm occurs when the size of the array is set to 1, since that messes with the size of the input. It would be correct to claim,\nhowever, that the best case occurs when the input is already sorted, since that does not mess with the input size.", "word_count": 914, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "15ff93f7-459a-5f63-b8b7-c08e86ea76f3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 114, "real_page_number": null, "text": "102\nChapter 4. Complexity Analysis\nExample 4.16 You are given an array of 𝑛integers, and you want to find the position of a number that you know is in the array. You have\nan algorithm that conducts a linear search through this array to see if the number you want to find is at each position (i.e., it looks at the\nfirst element, then the second, then the third, and so on until it finds the number). What is the best-case, worst-case, and average-case time\ncomplexity of this algorithm in terms of 𝑛?\nThe best-case scenario occurs if the number you want to find is the very first number in the array. The algorithm would check the first element,\nsee that it matches the number you want, and return. This would take constant time! Thus, the best-case time complexity of this algorithm is\nΘ(1). The following example illustrates the best-case scenario. Suppose you want to find the number 5 in this array — the linear search would\nfind 5 on its very first try, so it only has to do a constant amount of work.\n5\n2\n3\n9\n6\n...\n23\n17\n8\n1\n0\n1\n2\n3\n4\n...\n𝑛−4\n𝑛−3\n𝑛−2\n𝑛−1\nThe worst-case scenario occurs if the number you want to find is the very last number in the array (or if it does not exist). This would force the\nalgorithm to check every element in the array. Since there are 𝑛elements in total, the worst-case time complexity of this algorithm is Θ(𝑛). The\nfollowing example illustrates the worst-case scenario. Suppose you want to find the number 1 in this array (and assuming that 1 only appears\nonce) — the linear search would have to check all 𝑛elements before it reaches 1, which takes time.Θ(𝑛)\n5\n2\n3\n9\n6\n...\n23\n17\n8\n1\n0\n1\n2\n3\n4\n...\n𝑛−4\n𝑛−3\n𝑛−2\n𝑛−1\nNow, let’s find the average-case time complexity. To do so, we will compute the average time complexity over all possible cases when given an\ninput size of 𝑛. For the sake of simplicity, we will assume that all cases are uniformly distributed (i.e., the likelihood of the target element\nending up at index 0, index 1, …, index is equal).𝑛−1\n1stFirst, we have the case where the element we want to find is the element in the array. The number of elements our algorithm would have\n2ndto check in this case is 1. Second, we have the case where the element we want to find is the element in the array. The number of elements\nour algorithm would have to check in this case is 2.\n3rdWe can continue this process, keeping track of the number of values our algorithm would need to check if the element we want is the in\n4th 5th 𝑛ththe array, the in the array, the in the array, , up until the in the array. After doing this, we would find that:…\n1st• 1 comparison operation is needed in the case where the element we want is the element in the array\n2nd• 2 comparison operations is needed in the case where the element we want is the element in the array\n• …\n𝑛th• 𝑛comparison operations is needed in the case where the element we want is the element in the array\nTo calculate the average-case time complexity, we would need find the average number of operations out of all of these possible cases. Using\nthe definition of an average, we get:\ntotal number of operations over all possible cases=average number of operations\nnumber of possible cases\n1+2+…+𝑛=\n𝑛\n=\n𝑛(𝑛+1)\n2\n𝑛\n𝑛+1=\n2\n= Θ(𝑛)\nAfter carrying out the division, we find that the average number of operations scales with a complexity of Θ(𝑛). Since each operation takes\nconstant time, the runtime must also scale with a complexity of Θ(𝑛). Thus, the average-case time complexity of this algorithm is Θ(𝑛).\nFor those with a statistics background, here is a formal proof of the above calculation. We know that the input is uniformly distributed,\nand that the size of the array is 𝑛. If we define 𝑋as a random variable that represents the number of comparison operations needed before the\n1target value is found, then the probability that we do 𝑥comparisons is 𝑃(𝑋=𝑥)=\n𝑛(since the distribution is uniform). Using the property of\nexpectation, we can determine the expected number of comparison operations as follows:\n𝐸[𝑋]=\n𝑛\n∑\n𝑥=1\n𝑥𝑃(𝑥)=\n𝑛\n∑\n𝑥=1\n𝑥\n1𝑛=\n2𝑛+\n𝑛𝑛+…+\n𝑛+1𝑛=\n2\nΘ(𝑛)=\nIn this class, however, you likely will not have to calculate the average-case time complexity from scratch. You will typically be given the\naverage-case time complexities of important algorithms, without asking you to derive these results on your own. However, you should still\nunderstand what an average-case analysis measures and how to calculate it if you ever need to do so.\n4.8\nAdditional Rules for Determining Complexity\nIt is possible for two functions to have neither a big-O, big-Θ, nor big-Ω relationship. A good example would be the trigonometric functions.\nFor instance, if and cos(𝑛), then is NOT 𝑂(𝑔(𝑛)),Θ(𝑔(𝑛)), nor Ω(𝑔(𝑛)). This is because there is no way to strictly𝑓(𝑛) sin(𝑛) 𝑔(𝑛) 𝑓(𝑛)= =\nbound one function by another, as shown by the graph on the next page. There is no constant that we can multiply with to have it alwayscos(𝑛)\ngrow faster or slower than beyond some input size 𝑛0. That being said, runtimes that follow a trigonometric pattern are relatively rare.sin(𝑛)", "word_count": 947, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8d21e22b-14fd-56b9-ba9c-120c582dae50", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 115, "real_page_number": null, "text": "4.8 Additional Rules for Determining Complexity\n103\nTo show that a function is 𝑂(𝑔(𝑛)), the following condition can be used. This condition is but necessary. In other words, the𝑓(𝑛) sufficient not\n≠𝑂(𝑔(𝑛)).following rule be used to prove that 𝑓(𝑛)cannot\nIf lim𝑛→∞\n(𝑓(𝑛)\n𝑔(𝑛)\n)\n𝑓(𝑛)∞, then 𝑂(𝑔(𝑛)). In other words, if the limit of𝑑< 𝑓(𝑛)= =\nas 𝑛grows toward infinity is a constant whose value is𝑔(𝑛)\nless than ∞, then is 𝑂(𝑔(𝑛)).𝑓(𝑛)\nIn cases where 𝑛appears in both and 𝑔(𝑛), L’Hopital’s rule may have to be used:𝑓(𝑛)\nL’Hopital’s Rule: If and are differentiable functions and𝑓(𝑛) 𝑔(𝑛) lim𝑛→𝑐\n(𝑓(𝑛)\n𝑔(𝑛)\n)\n0=\n∞or0\n∞, then\nlim\n𝑛→𝑐\n(𝑓(𝑛)\n𝑔(𝑛)\n)\n= lim\n𝑛→𝑐\n(𝑓′(𝑛)\n𝑔′(𝑛)\n)\n𝑓′(𝑛) 𝑔′(𝑛)where and represent the derivatives of and 𝑔(𝑛), respectively.𝑓(𝑛)\nExample 4.17 Prove that is using the approach introduced in this section.log2(𝑛) 𝑂(𝑛)\nWe can use the condition and L’Hopital’s rule to complete this proof. Let and 𝑛. Using the limit definition, we have:𝑓(𝑛) log2(𝑛) 𝑔(𝑛)= =\nlim\n𝑛→∞\n(log2(𝑛)\n𝑛\n)\n∞=\n∞\n∞Since we get a result of\n∞, we can use L’Hopital’s rule. The derivative of islog2(𝑛)\n1\n𝑛ln(2), and the derivative of 𝑛is (don’t worry about1\ncalculating derivatives in this class — anything you might need will be provided, unless specified otherwise).\nL’Hopital’s rule gives us the following:\nlim\n𝑛→∞\n(log2(𝑛)\n𝑛\n)\n= lim\n𝑛→∞\n⎛\n⎜\n⎜⎝\n𝑑\n𝑑𝑛\n[log2(𝑛)]\n𝑑\n𝑑𝑛[𝑛]\n⎞\n⎟\n⎟⎠\n= lim\n𝑛→∞\n(\n1\n𝑛ln(2)\n)\n<=0 ∞\nSince 0 is a constant that is less than ∞, must be 𝑂(𝑛).log2(𝑛)\nExample 4.18 Prove that isln(𝑛) 𝑂\n(\nln\n(\n𝑛\n281\n))\nusing the approach introduced in this section.\nWecanusethesameprocessasabovetocompletethisproof. Here, and𝑓(𝑛) ln(𝑛) 𝑔(𝑛)= =ln\n(\n𝑛\n281\n)\n. Wecansimplifyln\n(\n𝑛\n281\n)\ntoln(𝑛)−ln(281)\nusing logarithm identities. Using the limit definition, we have\nlim\n𝑛→∞\n(\nln(𝑛)\nln(𝑛)−ln(281)\n)\n∞=\n∞\n∞Since we get a result of\n1∞, we can use L’Hopital’s rule. The derivative of isln(𝑛)\n𝑛.\nL’Hopital’s rule gives us the following:\nlim\n𝑛→∞\n(\nln(𝑛)\nln(𝑛)−ln(281)\n)\n= lim\n𝑛→∞\n⎛\n⎜\n⎜⎝\n𝑑\n𝑑𝑛[ln(𝑛)]\n𝑑\n𝑑𝑛[ln(𝑛)]−𝑑\n𝑑𝑛[ln(281)]\n⎞\n⎟\n⎟⎠\n= lim\n𝑛→∞\n⎛\n⎜\n⎜⎝\n𝑑\n𝑑𝑛[ln(𝑛)]\n𝑑\n𝑑𝑛[ln(𝑛)]−0\n⎞\n⎟\n⎟⎠\n= lim\n𝑛→∞\n(1∕𝑛\n1∕𝑛\n)\n<=1 ∞\nSince 1 is a constant that is less than ∞, must beln(𝑛) 𝑂\n(\nln\n(\n𝑛\n281\n))\n.", "word_count": 448, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7cb092bc-475c-5510-950c-7ed184c011f7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 116, "real_page_number": null, "text": "104\nChapter 4. Complexity Analysis\n4.9\nSpace Complexity and Auxiliary Space\nIn this chapter, we mainly focused on complexity, which deals with how runtime scales with input size. However, runtime is not the onlytime\nthing that can be expressed using big-O notation. In fact, we can use the notations introduced in this chapter to express the growth of any\nwith respect to input size.computational resource\nIn this section, we will introduce the concept of space complexity (also known as memory complexity), which can be used to describe an\nalgorithm’s memory usage. The concept of complexity here is still the same — instead of measuring how runtime scales with input size, we\nwill look at how memory usage scales with input size.\nSpace complexity is the amount of memory used by an algorithm to solve a problem, with respect to input size.\nHowever, there is a caveat with this definition — since the space complexity looks the total memory usage of an algorithm, it also includes the\nmemory used to store any input values themselves. This is not always something we want! If we do not consider the memory used for the input\nvalues themselves in our analysis, we are measuring the of the algorithm instead.auxiliary space\nAuxiliary space is the temporary space allocated by an algorithm to solve a problem, with respect to input size. Auxiliary space only\nconsiders the additional memory used by the algorithm, and it does not include the memory used by the input values themselves.\nFor this class, we will mostly be dealing with space. If an algorithm uses auxiliary space, the amount of additional memoryauxiliary Θ(1)\nneeded to run the algorithm stays constant and does not depend on the input size. If an algorithm uses auxiliary space, the amount ofΘ(𝑛)\nΘ(𝑛2)additional memory needed to run the algorithm grows linearly with the size of the input. Similarly, if an algorithm uses auxiliary space,\nincreasing the input size results in a quadratic increase in the amount of additional memory that is needed.\nWhendealingwithspacecomplexity,itisimportanttoconsiderallpossiblememorysources. Forexample,ifafunctionacceptsanargument\nthat is passed in by value (rather than by reference), a copy of that argument is made, which may contribute to auxiliary space. Furthermore, if\nan algorithm uses recursion, stack frames may be used with each recursive call. If the number of recursive calls depends on input size, the\nnumber of stack frames you need may depend on the input size as well! Because stack frames use up memory, an algorithm that does not\nexplicitly allocate memory may still have an auxiliary space complexity worse than Θ(1).\nExample 4.19 What is the auxiliary space complexity of the function foo()? The input size 𝑛is the size of the input array.\n1\nint32_t foo(int32_t size_tarr[], size) {\n2\nint32_t* new int32_t[size];bar =\n3\nint32_t counter = 0;\n4\nfor (size_t i = 0; i < size; ++i) {\n5\nbar[i] = arr[size - 1 - i];\n6\n} // for i\n7\nfor (size_t j = 0; j < size; ++j) {\n8\ncounter += (bar[j] + arr[j]);\n9\n} // for j\n10\ndelete[] bar;\n11\nreturn counter;\n12\n} // foo()\nTo determine the auxiliary space of this function, we need to identify where the function allocates memory and express this additional memory\nusage in terms of the input size. In this case, new memory is allocated on lines 2, 3, 4, and 7. The allocations on lines 3, 4, and 7 instantiate an\ninteger, whose memory usage does not depend on the size of the input array. Thus, we say these lines use auxiliary space. The allocationΘ(1)\nbaron line 2, however, does depend on the size of the input, as the size of the array is set to the size of the input array. Hence, as input size\nbargrows, the amount of memory needed for also grows. This growth is linear, so the allocation on line 2 uses auxiliary space.Θ(𝑛)\n1\nint32_t foo(int32_t size_tarr[], size) {\n2\nint32_t* new int32_t[size];bar =\n3\nint32_t counter = 0;\n4\nfor (size_t i = 0; i < size; ++i) {\n5\nbar[i] = arr[size - 1 - i];\n6\n} // for i\n7\nfor (size_t j = 0; j < size; ++j) {\n8\ncounter += (bar[j] + arr[j]);\n9\n} // for j\n10\ndelete[] bar;\n11\nreturn counter;\n12\n} // foo()\n1\nx\n2\nx\n3\nx\n4\nx\n5\nx\n6\nx\n7\nx\n8\nx\n9\nx\n10\nx\n11\nx\n12\nx\nΘ(𝑛)\nΘ(1)\nΘ(1)\nΘ(1)\nSimilar to before, we can remove coefficients and lower order terms when expressing space complexity. Because the memory allocation onΘ(𝑛)\nfoo()line 2 dominates the allocations elsewhere in the function, the overall auxiliary space used by is Θ(𝑛).Θ(1)\nbar size bar sizeRemark: If the size of did not depend on (e.g., if the size of were set to 500 instead of on line 2), then the auxiliary\nbar foo()memory used by would be Θ(1). If this were the case, the auxiliary space used by the entire function would also be Θ(1),500=\nsince the additional memory needed to execute the function would not depend on input size (i.e., would not change as the input size grows).", "word_count": 903, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c4c4d98a-029f-5b46-a23f-da456b176fa7", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 117, "real_page_number": null, "text": "4.9 Space Complexity and Auxiliary Space\n105\nChapter 4 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Which of the following statements provides an explanation for why it may be better to measure a program’s relative efficiency by analyzing\nhow its runtime scales by input size rather than its pure runtime?\nI. If you compare two algorithms based on pure runtime, the superior algorithm may end up being measured as less efficient if more\nprocesses were running in the background when it was run.\nII. For input sizes that are large enough, the rate of growth of a program’s runtime with respect to input size is independent of most\nexternal factors, making it a consistent benchmark for comparing the growth of different algorithms.\nIII. Runtimemeasuredwithrespecttoinputsizecanbeexpressedmathematicallyusingbig-Onotation,whichmakesiteasiertocategorize\nand compare different algorithms with different orders of growth.\nA) I only\nB) II only\nC) I and II only\nD) II and III only\nE) I, II, and III\n2. Consider the following four statements regarding algorithm complexities:\nΘ(𝑛2)I. An algorithm with a time complexity will always run faster than an algorithm with a time complexity.Θ(𝑛log(𝑛))\nΘ(𝑛2)II. An algorithm with a time complexity will always run faster than an algorithm with a time complexity.Θ(𝑛log(𝑛))\nΘ(𝑛2)III. An algorithm with a time complexity will always run faster than an algorithm with a time complexity.Θ(𝑛!)\nΘ(𝑛2)IV. An algorithm with a time complexity will always run faster than an algorithm with a time complexity.Θ(𝑛!)\nHow many of these statements is/are TRUE?\nA) 0\nB) 1\nC) 2\nD) 3\nE) 4\n3. Given a function 𝑓, which of the following best defines using big-O notation?𝑂(𝑓)\nA) is the set of functions that grow at least as slowly as𝑂(𝑓) 𝑓\nB) is the set of functions that grow at least as quickly as𝑂(𝑓) 𝑓\nC) is the set of functions that grow exactly as quickly as𝑂(𝑓) 𝑓\nD) is the set of functions that do not grow more slowly than𝑂(𝑓) 𝑓\nE) is the set of functions that do not grow more quickly than𝑂(𝑓) 𝑓\n4. You are measuring the asymptotic time complexity of a function by counting steps. Suppose you determine that the total number of stepsfoo\ntaken by with respect to input size 𝑛can be expressed as:foo\n𝑇(𝑛) 53log(𝑛)+79𝑛+12=\nWhat of the following statements is TRUE?\nA) has a time complexity of Θ(53log(𝑛))foo\nB) has a time complexity of Θ(log(𝑛))foo\nC) has a time complexity of Θ(𝑛)foo\nD) has a time complexity of Θ(𝑛log(𝑛))foo\nE) More than one of the above\n5. You are given five different algorithms, each with a different time complexity:\nI. Algorithm A has a time complexity of Θ(𝑛!)\nΘ(2𝑛)II. Algorithm B has a time complexity of\nΘ(𝑛3)III. Algorithm C has a time complexity of\nΘ(𝑛3IV. Algorithm D has a time complexity of log(𝑛))\nΘ(𝑛𝑛)V. Algorithm E has a time complexity of\nWhich of the following correctly ranks these algorithms in order of increasing time complexity?\nA) D, C, B, A, E\nB) B, C, D, A, E\nC) B, D, C, A, E\nD) C, D, B, A, E\nE) C, D, B, E, A", "word_count": 598, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "bd024629-ed8c-5b08-8374-4d48f4f85cb7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 118, "real_page_number": null, "text": "106\nChapter 4. Complexity Analysis\n6. You have 𝑛billiard balls, all of which have the same weight except for one that is heavier than all the others. Which of the following is a\nsolution for finding the billiard ball with a heavier weight using only a single balance?Θ(log(𝑛))\nA) Randomly pick two balls at a time and place them on each side of the balance until you find an imbalance\nB) Place one ball on one side of the balance, and compare its weight with each of the other balls using the other side of the balance\nC) Split the balls into two halves, place one half on one side of the balance and the other half on the other, and check to see which half is\nheavier; continue this process with the balls on the heavier side until the target ball is found\nD) Split the balls into three groups, place one third on one side and another third on the other, and check which group the heavier ball is\nin (if the two groups on the scale have the same weight, then the heavier ball must be in the group not on the scale); continue this\nprocess with the group that includes the heavier ball until the target ball is found\nE) More than one of the above\n7. Youjustcompletedaprojectafteralongdayofwork,andgoodnews: yourcodeappearstoruncorrectlywiththeprovidedtestfiles! However,\nwhen you plot out runtimes with your own test files, you notice that there exists a relationship even though you had implemented aΘ(𝑛)\nalgorithm. Which of the following could be a reason for this discrepancy?Θ(log(𝑛))\nA) There were too many programs running in parallel with your program\nB) algorithms can also have a time complexity ofΘ(𝑛) Θ(log(𝑛))\nC) The tests that you used were too large for the actual relationship to be revealed\nD) The tests that you used were too small for the actual relationship to be revealed\nE) None of the above\nΘ(𝑛2)8. Your friend is working on a project, which they implemented using a algorithm. However, when they ran a test file on their program,\nthey noticed that the program instead ran in time. All else equal, which of the following is NOT a possible reason for this discrepancy?Θ(𝑛)\nA) Your friend accidentally ran the test case with a different algorithm\nB) Your friend incorrectly analyzed the time complexity of their algorithm\nC) Your friend’s test inputs exposed worst-case behavior\nD) Your friend’s test case input size was too small\nE) None of the above\n9. Suppose you have three functions 𝑓, 𝑔, and ℎ. Consider the following statements:\nI. 𝑓must either be 𝑂(𝑔), Θ(𝑔), or Ω(𝑔).\nII. If 𝑓+𝑔is 𝑂(ℎ), then either 𝑓or 𝑔must also be 𝑂(ℎ).\nIII. If 𝑓×𝑔is 𝑂(ℎ), then either 𝑓or 𝑔must also be 𝑂(ℎ).\nIV. There exists no 𝑓such that 𝑓is Ω(𝑓×𝑔), where 𝑓×𝑔is the product of two functions 𝑓and 𝑔.\nHow many of these statements is/are TRUE?\nA) 0\nB) 1\nC) 2\nD) 3\nE) 4\n10. Which of the following statements is FALSE?\nlog(𝑛39𝑛) Θ(9𝑛)A) is\nlog(𝑛543𝑛)B) is Θ(𝑛)\nC) islog(𝑛!) Θ(𝑛log(𝑛))\nD) islog3(𝑛) Θ(log2(𝑛))\nlog(𝑛2)E) is Θ(log(𝑛))\n11. Which of the following strategies can be used to improve the worst-case time complexity of an algorithm?asymptotic\nA) Reducing the size of the input that is passed into the algorithm\nB) Increasing the size of the input that is passed into the algorithm\nC) Running the algorithm on a faster machine\n-O3D) Compiling the algorithm using the compiler flag\nE) None of the above", "word_count": 614, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "94d5b4a9-4be1-5703-a742-3d74280db021", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 119, "real_page_number": null, "text": "4.9 Space Complexity and Auxiliary Space\n107\n12. Consider the following function implementation:\n1\nint32_t foo(int32_t n) {\n2\nint32_t a = helper_a(n);\n3\nint32_t b = helper_b(n);\n4\nint32_t c = helper_c(n);\n5\nreturn a + b + c;\n6\n} // foo()\nΘ(𝑛2),helper_a() helper_b()For an input size of 𝑛, the function has an asymptotic time complexity of the function has an\nhelper_c()asymptotic time complexity of Θ(𝑛), and the function has an asymptotic time complexity of Θ(𝑛log(𝑛)). Which of the\nfoo()?following modifications would the asymptotic time complexity class of the functionchange\nΘ(𝑛2)helper_a()A) Improving the asymptotic time complexity of from to Θ(𝑛log(𝑛))\nΘ(𝑛2) Θ(0.5𝑛2)helper_a()B) Improving the asymptotic time complexity of from to\nhelper_b()C) Improving the asymptotic time complexity of from toΘ(𝑛) Θ(1)\nhelper_c()D) Improving the asymptotic time complexity of from toΘ(𝑛log(𝑛)) Θ(log(𝑛))\nE) More than one of the above\n13. You are given three algorithms 𝐴, 𝐵, and 𝐶, each with runtimes that can be modeled using the functions 𝐴(𝑛), 𝐵(𝑛), for input size 𝑛.𝐶(𝑛)\nYou know that and Ω(𝐶(𝑛)). Which of the following statements CANNOT be true?𝐴(𝑛) 𝑂(𝐵(𝑛)) 𝐴(𝑛)= =\nA) 𝐵(𝑛) Θ(𝐶(𝑛))=\nB) 𝐶(𝑛) Ω(𝐴(𝑛))=\nC) 𝐵(𝑛) Ω(𝐶(𝑛))=\nD) 𝐵(𝑛) 𝑂(𝐶(𝑛))=\nE) None of the above (i.e., all of the above statements can be true)\n14. You have four algorithms that accomplish the same task, and you want to know which algorithm to use. Memory is not an issue, so you\nwant to make your decision based on time efficiency alone. To make your decision, you run each algorithm once on the same machine at\nthe exact same time. The same input size is used for all the algorithms. You obtain the following results from this experiment:\nAlgorithm\nRuntime (ms)\nA\n0.011\nB\n0.015\nC\n0.006\nD\n0.019\nUsing just the information collected from this experiment, which algorithm should you select?\nA) Algorithm A\nB) Algorithm B\nC) Algorithm C\nD) Algorithm D\nE) The algorithm you should select should not be determined using the results of this experiment\nfoo(),15. You are given a function and you graph its runtime with respect to different input sizes. After running multiple experiments with\ndiffering inputs, you end with the following relationship between runtime and input size:\nfoo()What is the asymptotic time complexity of with respect to input size 𝑛?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nD) Θ(𝑛log(log(𝑛)))\nΘ(𝑛2E) log(𝑛))", "word_count": 412, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "713b39e9-6e5f-5d47-95af-d848446dd711", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 120, "real_page_number": null, "text": "108\nChapter 4. Complexity Analysis\n16. You are given three strictly increasing functions 𝐴(𝑛), 𝐵(𝑛), and for input size 𝑛. Suppose you know that is and is𝐶(𝑛) 𝐴(𝑛) 𝑂(𝐵(𝑛)) 𝐴(𝑛)\nΩ(𝐶(𝑛)). Which of the following statements must also be TRUE?\nA) is𝐵(𝑛) 𝑂(𝐶(𝑛))\nB) is𝐵(𝑛) Ω(𝐶(𝑛))\nC) is𝐵(𝑛) Θ(𝐶(𝑛))\nD) is𝐵(𝑛) Θ(𝐴(𝑛))\nE) None of the above\n17. Which of the following correctly orders these complexity classes in order from slowest to fastest growth?\nI. Θ\n(√\nlog(𝑛)\n)\nII. Θ(log(𝑛))\nΘ(log2(𝑛))III.\nIV. Θ(log(log(𝑛)))\nA) I, IV, II, III\nB) I, IV, III, II\nC) I, II, IV, III\nD) IV, I, II, III\nE) IV, I, III, II\nΘ(𝑛218. Suppose you are given a function that you know is log(𝑛)). Which of the following statements must also be TRUE about 𝑓(𝑛)?𝑓(𝑛)\nΘ(𝑛2)A) is𝑓(𝑛)\nΘ(𝑛2B) is𝑓(𝑛) +𝑛)\nΩ(𝑛2C) is𝑓(𝑛) +log(𝑛))\n𝑂(𝑛2)D) is𝑓(𝑛)\nE) More than one of the above\n19. For which of the following pairs of and is Θ(𝑔(𝑛))? Note: 𝑒is a mathematical constant with approximate value 2.71828…𝑓(𝑛) 𝑔(𝑛) 𝑓(𝑛)=\n22𝑛I. 𝑓(𝑛)=\n2𝑛𝑔(𝑛)=\n2𝑛log2(𝑛)II. 𝑓(𝑛)=\n𝑛𝑛𝑔(𝑛)=\nIII. 𝑓(𝑛) 2𝑒=\n𝑛∕2\n𝑒𝑛𝑔(𝑛)=\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) II and III only\n20. Which one of the following statements is TRUE?\nA) islog(𝑛+𝑚) 𝑂(log(𝑛)+log(𝑚))\nB) is𝑛! 𝑂((𝑛−1)!)\n2𝑛+𝑚is 𝑂(2𝑛+2𝑚)C)\nD) issin(𝑛) 𝑂(cos(𝑛))\nE) More than one of the above\n21. If and are real-valued functions such that𝑓(𝑛) 𝑔(𝑛) lim𝑛→∞\n𝑓(𝑛)\n∞, which of the following conclusions are valid?=𝑔(𝑛)\nA) is𝑓(𝑛) 𝑂(𝑔(𝑛))\nB) is𝑓(𝑛) Θ(𝑔(𝑛))\nC) is𝑓(𝑛) Ω(𝑔(𝑛))\nD) More than one of the above\nE) None of the above conclusions can be made from the given information", "word_count": 315, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7b1784de-c702-5ad1-af0a-ca6c8f05e04a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 121, "real_page_number": null, "text": "4.9 Space Complexity and Auxiliary Space\n109\n𝐶𝑛for22. You are given an algorithm whose runtime can be expressed in the form input size 𝑛and some unknown constant 𝐶. From𝑓(𝑛)=\nanalysis, you discover that it takes 𝑇time for this algorithm to solve a problem with input size 𝑆, and 9𝑇time for this algorithm to solve a\nproblem with input size 𝑆+6. Knowing this, what is the value of the constant 𝐶?\nA)\n3√\n3\nB)\n√\n3\nC)\n3∕2\nD) 2\nE) 3\n23. You are given an algorithm whose runtime can be expressed in the form for input size 𝑛. If it takes 𝑇time to solve a problem𝑓(𝑛) log2(𝑛)=\nwith input size 𝑆, a problem of what input size can be solved in time 𝑇+1?\nA) log2(𝑆)\nB) 𝑆+log2(𝑆)\nC) 2𝑆\nD) 2log2(𝑆)\nE) 2𝑆+2log2(𝑆)\n𝑛224. You are given an algorithm whose runtime can be expressed in the form for input size 𝑛. If it takes 𝑇time to solve a problem𝑓(𝑛)=\nwith input size 𝑆, a problem of what input size can be solved in time 8𝑇?\nA) 2𝑆\nB) (2\n√\n2)𝑆\nC) 3𝑆\nD) 4𝑆\nE) (4\n√\n2)𝑆\n25. Consider the following function implementation:\n1\nvoid foo(int32_t int32_tm, n) {\n2\nfor (int32_t i = 0; i < m; ++i) {\n3\nstd::cout << \"EECS 281\\n\";\n4\n} // for i\n5\n6\nfor (int32_t j = 0; j < n; ++j) {\n7\nstd::cout << \"EECS 281\\n\";\n8\n} // for j\n9\n} // foo()\nWhat is the time complexity of this function in terms of 𝑚and 𝑛?\nA) Θ(𝑚)\nB) Θ(𝑛)\nC) Θ(𝑚+𝑛)\nD) Θ(𝑚𝑛)\nE) None of the above\n26. Consider the following function implementation:\n1\nvoid foo(int32_t int32_tm, n) {\n2\nfor (int32_t i = 0; i < m; ++i) {\n3\nfor (int32_t j = 0; j < n; ++j) {\n4\nstd::cout << \"EECS 281\\n\";\n5\n} // for j\n6\n} // for i\n7\n} // foo()\nWhat is the time complexity of this function in terms of 𝑚and 𝑛?\nA) Θ(𝑚)\nB) Θ(𝑛)\nC) Θ(𝑚+𝑛)\nD) Θ(𝑚𝑛)\nE) None of the above", "word_count": 365, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6e4c9e95-5aeb-5d6f-b5f3-947899d7e84e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 122, "real_page_number": null, "text": "110\nChapter 4. Complexity Analysis\n27. Consider the following function implementation:\n1\nvoid foo(int32_t int32_t int32_tm, n, p) {\n2\nfor (int32_t k = 0; k < p; ++k) {\n3\nfor (int32_t i = 0; i < m; ++i) {\n4\nstd::cout << \"EECS 281\\n\";\n5\n} // for i\n6\n7\nfor (int32_t j = 0; j < n; ++j) {\n8\nstd::cout << \"EECS 281\\n\";\n9\n} // for j\n10\n} // for k\n11\n} // foo()\nWhat is the time complexity of this function in terms of 𝑚, 𝑛, and 𝑝?\nA) Θ(𝑚𝑝)\nB) Θ(𝑛𝑝)\nC) Θ(𝑚𝑝+𝑛𝑝)\nD) Θ(𝑚𝑛𝑝)\nE) None of the above\n28. Consider the following function implementation:\n1\nint32_t foo(int32_t n) {\n2\nint32_t count = 0;\n3\nfor (int32_t i = 0; i < n; ++i) {\n4\nfor (int32_t j = n; j > 0; --j) {\n5\n++count;\n6\n} // for j\n7\n} // for i\n8\n9\nreturn count;\n10\n} // foo()\nWhat is the time complexity of this function in terms of 𝑛?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛2)D)\nE) None of the above\n29. Consider the following function implementation:\n1\nint32_t foo(int32_t n) {\n2\nint32_t count = 0;\n3\nint32_t static_cast<int32_t>(std::floor(std::sqrt(n)));root =\n4\nfor (int32_t i = n / 2; i < n; ++i) {\n5\nfor (int32_t j = 1; j < n; j *= 2) {\n6\nfor (int32_t k = 0; k < n; k += root) {\n7\n++count;\n8\n} // for k\n9\n} // for j\n10\n} // for i\n11\n12\nreturn count;\n13\n} // foo()\nstd::floor(x)What is the time complexity of this function in terms of 𝑛? Note that computes the largest integer value not greater than\nx, std::sqrt(x) x.and computes the square root of\nA) Θ(\n√\n𝑛log(𝑛))\nB) Θ(𝑛log(𝑛))\nC) Θ(𝑛\n√\n𝑛log(𝑛))\nΘ(𝑛2D) log(𝑛))\nΘ(𝑛2√E)\n𝑛log(𝑛))", "word_count": 327, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fcc23b12-fe07-5233-a789-7a6a0549cb89", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 123, "real_page_number": null, "text": "4.9 Space Complexity and Auxiliary Space\n111\n30. Consider the following function implementation:\n1\nint32_t foo(int32_t n) {\n2\nint32_t count = 0;\n3\nwhile (n > 1) {\n4\nn /= 2;\n5\n++count;\n6\n}\n// while\n7\n8\nreturn count;\n9\n} // foo()\nWhat is the time complexity of this function in terms of 𝑛?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛2)D)\nE) None of the above\n31. Consider the following function implementation:\n1\nvoid foo(int32_t n) {\n2\nint32_t k = 0;\n3\nwhile (n > 1) {\n4\nfor (int32_t i = 0; i <= n; ++i) {\n5\n++k;\n6\n} // for i\n7\n8\nn /= 2;\n9\n} // while\n10\n} // foo()\nWhat is the time complexity of this function in terms of 𝑛? Note: you may find the equation for the sum of a geometric series helpful, where\n𝑎is the first term and 𝑟is the common ratio:\n𝑎(1−𝑟𝑛)𝑆𝑛=\n1−𝑟\n𝑟≠1,\nΘ(log2(𝑛))A)\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛2)D)\nE) None of the above\n32. Consider the following function implementation:\n1\nvoid foo(int32_t int32_tm, n) {\n2\nint32_t k = 0;\n3\nwhile (n > m) {\n4\nfor (int32_t i = 0; i < m; i *= 3) {\n5\n++k;\n6\n} // for i\n7\n8\nn /= 2;\n9\n} // while\n10\n} // foo()\nWhat is the time complexity of this function in terms of 𝑚and 𝑛?\nA) Θ(log(𝑚∕𝑛)log(𝑛))\nB) Θ(log(𝑛∕𝑚)log(𝑚))\nC) Θ(𝑛log(𝑚))\nD) Θ(log(𝑚)log(𝑛))\nE) None of the above", "word_count": 262, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "048659eb-bc22-5209-8d9c-179da4021c7c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 124, "real_page_number": null, "text": "112\nChapter 4. Complexity Analysis\n33. Consider the following function, which identifies whether an array has any duplicate values. Let 𝑛represent the size of the array.\n1\nbool has_duplicates(int32_t size_tarr[], n) {\n2\nfor (size_t i = 0; i < n; ++i) {\n3\nfor (size_t j = i + 1; j < n ; ++j) {\n4\nif (arr[i] == arr[j]) {\n5\nreturn true;\n6\n} // if\n7\n} // for j\n8\n} // for i\n9\n10\nreturn false;\n11\n} // has_duplicates()\nWhat is the best and worst-case time complexity of this function, in terms of 𝑛? Note: you may find the following identity useful:\n𝑛∑\n𝑥=1\n𝑥= 1+2+3+4+…+𝑛=\n𝑛(𝑛+1)\n2\nA) Best-Case: Θ(1),\nWorst-Case: Θ(𝑛)\nB) Best-Case: Θ(𝑛),\nWorst-Case: Θ(𝑛)\nC) Best-Case: Θ(1),\nΘ(𝑛2)Worst-Case:\nD) Best-Case: Θ(𝑛),\nΘ(𝑛2)Worst-Case:\nΘ(𝑛2), Θ(𝑛2)E) Best-Case: Worst-Case:\n34. Consider the following function implementation:\n1\nint32_t foo(int32_t n) {\n2\nint32_t count = 0;\n3\nfor (int32_t i = 1; i < n; ++i) {\n4\nfor (int32_t j = 1; j < i; ++j) {\n5\n++count;\n6\n} // for j\n7\n} // for i\n8\n9\nreturn count;\n10\n} // foo()\nWhat is the time complexity of the this function in terms of 𝑛? Note: you may find the following identity useful:\n𝑛∑\n𝑥=1\n𝑥= 1+2+3+4+…+𝑛=\n𝑛(𝑛+1)\n2\nA) Θ(𝑛)\nB) Θ(𝑛log(𝑛))\nΘ(𝑛2)C)\nΘ(𝑛3)D)\nE) None of the above\n35. Consider the following function implementation:\n1\nint32_t foo(int32_t n) {\n2\nint32_t count = 0;\n3\nfor (int32_t i = 1; i < n; ++i) {\n4\nfor (int32_t j = 1; j < i; j *= 2) {\n5\n++count;\n6\n} // for j\n7\n} // for i\n8\n9\nreturn count;\n10\n} // foo()\nWhat is the time complexity of the this function in terms of 𝑛?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛2)D)\nE) None of the above", "word_count": 327, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a35a90a7-0024-5e71-a122-75eee7e4874c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 125, "real_page_number": null, "text": "4.9 Space Complexity and Auxiliary Space\n113\n36. Consider the following function implementation:\n1\nint32_t foo(int32_t n) {\n2\nint32_t count = 0;\n3\nfor (int32_t i = 1; i < n; i *= 2) {\n4\nfor (int32_t j = 1; j <= i; ++j) {\n5\n++count;\n6\n} // for j\n7\n} // for i\n8\n9\nreturn count;\n10\n} // foo()\nWhat is the time complexity of the this function in terms of 𝑛? Note: you may find the equation for the sum of a geometric series helpful,\nwhere 𝑎is the first term and 𝑟is the common ratio:\n𝑎(1−𝑟𝑛)𝑆𝑛=\n1−𝑟\n𝑟≠1,\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛2)D)\nE) None of the above\n37. Consider the following function implementation:\n1\nint32_t foo(int32_t n) {\n2\nint32_t count = 0;\n3\nfor (int32_t i = 0; i < n; ++i) {\n4\nfor (int32_t j = n; j > i; j /= 2) {\n5\nfor (int32_t k = j / 2; k < j; ++k) {\n6\nfor (int32_t m = 1; m < n; m *= 2) {\n7\n++count;\n8\n} // for m\n9\n} // for k\n10\n} // for j\n11\n} // for i\n12\n13\nreturn count;\n14\n} // foo()\nWhat is the time complexity of this function in terms of 𝑛? Note: you may find the following identity useful:\n𝑛∑\n𝑥=1\n1\n𝑥=\n(\n11+\n1+2\n1+3\n1+…+4\n𝑛\n)\nΘ(log(𝑛))=\nA) Θ(𝑛log(𝑛))\nΘ(𝑛2B) log(𝑛))\nΘ(𝑛log2(𝑛))C)\nlog2(𝑛))Θ(𝑛2D)\nE) None of the above\n38. Consider the following function implementation:\n1\nint32_t sum(int32_t size_tarr[], n) {\n2\nint32_t sum = 0;\n3\nfor (int32_t i = 0; i < n; ++i) {\n4\nsum += arr[i];\n5\n} // for i\n6\n7\nreturn sum;\n8\n} // foo()\narr,What is the and used by this function, in terms of the size of 𝑛?space complexity auxiliary space\nA) Space Complexity: Θ(1),\nAuxiliary Space: Θ(1)\nB) Space Complexity: Θ(𝑛),\nAuxiliary Space: Θ(1)\nC) Space Complexity: Θ(1),\nAuxiliary Space: Θ(𝑛)\nD) Space Complexity: Θ(𝑛),\nAuxiliary Space: Θ(𝑛)\nE) None of the above", "word_count": 372, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "31dec78c-cd61-5ae6-b1af-f13c1ffd33cd", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 126, "real_page_number": null, "text": "114\nChapter 4. Complexity Analysis\nChapter 4 Exercise Solutions\n1. The correct answer is (E).All ofthe statements aretrue. For statement I,pure runtime is animperfect metricforcomparingtwo algorithms\ndue to the presence of confounding factors like background processes and CPU usage. For statement II, a program’s runtime with respect\nto input size is indeed independent of most external factors, congruent with the claim in statement I. For statement III, when we express\nruntime in terms of input size, we can more easily categorize and compare them using big-O notation (e.g., its easier to see that an Θ(𝑛)\nΘ(𝑛2)algorithm has a lower order of growth than an algorithm).\n2. The correct answer is (A). None of the statements are true. The asymptotic efficiency of algorithms deals with how runtime increases\nΘ(𝑛2)with the size of the input as the size of the input increases bound. For instance, as the size of the input grows, we can reason that a\nalgorithm will see its runtime increase much more drastically than a algorithm. However, it does not guarantee that an algorithmΘ(𝑛log(𝑛))\nΘ(𝑛2)with a time complexity will always run faster than an algorithm with a time complexity, since there are other factorsΘ(𝑛log(𝑛))\nat play (e.g., the algorithm with the larger complexity class may have been run in a best-case scenario or may have better constants and\nlower-order terms that make it faster for a given input size).\n3. The correct answer is (E). Big-O represents an asymptotic upper bound, where is if serves as an upper bound for 𝑎(𝑛).𝑎(𝑛) 𝑂(𝑏(𝑛)) 𝑏(𝑛)\nThe set of functions that comprise for a given function 𝑓are the set of functions that are bounded above by 𝑓. This matches option𝑂(𝑓)\n(E) — for a function to be bounded above by 𝑓, it cannot grow quicker than 𝑓.\n4. The correct answer is (C). Lower order terms can be ignored in big-O, and the term with the highest order is 79𝑛. Coefficients can be\nignored as well, so the complexity of can be simplified to Θ(𝑛).foo\nΘ(𝑛3 Θ(𝑛3)5. The correct answer is (D). A algorithm grows faster than a algorithm due to the presence of an additional termlog(𝑛)) log(𝑛)\nΘ(2𝑛) Θ(𝑛𝑛) Θ(2𝑛)𝑛3.that is multiplied with A algorithm grows faster than a algorithm, and a algorithm grows faster than both aΘ(𝑛!)\nand a algorithm. You can also see this by testing a few values of 𝑛, as shown below:Θ(𝑛!)\nn\n2\n4\n8\n2𝑛\n4\n16\n256\n𝑛!\n2\n24\n40,320\n𝑛𝑛\n4\n256\n16,777,216\n6. The correct answer is (E). Algorithms involving complexities often involve splitting work over and over again and doing a constantlog(𝑛)\nnumber of operations for each split. This is true for both (C) and (D).\n7. The correct answer is (D). If the input size is too small, the relationship may not be revealed. Plotting out runtimes is most revealing when\nthe input size is large.\n8. The correct answer is (C). The program ran better than expected, so it could not have exposed worst-case behavior if all else went to plan.\n9. The correct answer is (A). None of the statements are true. The best way to disprove these statements is to use trigonometric functions,\nwhich do not adhere to many of the rules that are associated with traditional functions. Statement I is false when and cos(𝑥).𝑓=sin(𝑥) 𝑔=\nStatement II is false when sin(𝑥), cos(𝑥), and cos(𝑥). Statement III is false when sin(𝑥), cos(𝑥), and𝑓= 𝑔= ℎ= sin(𝑥) 𝑓= 𝑔=+\n𝑥2sin(𝑥)cos(𝑥). Statement IV is false when and 1∕𝑥.ℎ= 𝑓= 𝑔=\nlog(𝑛39𝑛) log(𝑛3)+log(9𝑛).10. The correct answer is (A). Using log rules, can be rewritten as We can move the exponents in front of the log\nto get 3log(𝑛)+𝑛log(9). Both 3 and are constants, so they can be ignored, leaving us with log(𝑛)+𝑛. The lower order term of log(𝑛)log(9)\nΘ(9𝑛). log(𝑛543𝑛)can also be dropped, so the final complexity is Θ(𝑛), not (A) is false. The other options are true. For (B), can be rewritten\nlog(𝑛5)+log(43𝑛)as Θ(𝑛). For (C), is (see example 4.15). For (D), the base of a log does not5log(𝑛)+3𝑛log(4) log(𝑛!) Θ(𝑛log(𝑛))= =\nlog(𝑛2)matter when working with big-O notation, so is Θ(log2(𝑛)). For (E), is using log rules, which is Θ(log(𝑛)).log3(𝑛) 2log(𝑛)\n11. The correct answer is (E). Asymptotic runtime deals with how an algorithm’s runtime scales with respect to input size; the rate of growth\nof an algorithm is not impacted by the input size, machine performance, or compiler flags.\nΘ(𝑛2 Θ(𝑛2),foo() helper_a()12. The correct answer is (A). The overall time complexity of is where the runtime of+𝑛+𝑛log(𝑛))=\nhelper_b() helper_c()contributes to the highest order term. Therefore, only improving or does not change the overall time\nfoo().complexity of This eliminates options (C) and (D). For option (B), the coefficient does not matter, so the time complexity of\nΘ(𝑛2). Θ(𝑛2)foo() foo()would still be Only option (A) improves the overall time complexity of the function from to Θ(𝑛log(𝑛)).\n13. The correct answer is (E). All of statements can be true in the equal case, where 𝐴(𝑛), and have the same order of growth. Note𝐵(𝑛) 𝐶(𝑛)\nthat big-𝑂, Θ, and can also apply to a tight bound.Ω\n14. The correct answer is (E). This is not a good experiment for determining which algorithm is the most efficient due to the presence of\nconfounding factors, especially since each algorithm is only timed once. Plus, the conditions of the experiment are not well defined enough\nto come to a reasonable conclusion (e.g., what input size are you running on, compilation flags used, etc.).\n15. The correct answer is (A). This graph represents a logarithmic relationship between input size and runtime. None of options (B) through\n(E) are correct since they all grow at least as fast as linear, which is not true for the relationship in the provided graph.\n16. The correct answer is (B). If is bounded above by and bounded below by 𝐶(𝑛), and all three functions are strictly increasing,𝐴(𝑛) 𝐵(𝑛)\nthen must also be bounded below by (otherwise, it could not be an upper bound to 𝐴(𝑛), which is also bounded below by 𝐶(𝑛)).𝐵(𝑛) 𝐶(𝑛)\nThis matches option (B). Note that the other three options are all possible, but only (B) is true in all cases.\n17. The correct answer is (D). Logarithmic functions grow slower than polynomial functions. To make this problem easier to understand, one\nΘ(𝑥1∕2) Θ(𝑥2).strategy is to replace with another variable, let’s say 𝑥. We know that Thus, it must also belog(𝑛) Θ(log(𝑥))< <Θ(𝑥)<\nΘ(log1∕2(𝑛)) Θ(log2(𝑛)),true that which matches option (D).Θ(log(log(𝑛)))< <Θ(log(𝑛))<", "word_count": 1134, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "53d85ad4-b3bb-5415-9e7d-910f7d881788", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 127, "real_page_number": null, "text": "4.9 Space Complexity and Auxiliary Space\n115\nΘ(𝑛2 Θ(𝑛2).18. The correct answer is (C). Options (A) and (B) are incorrect because is part of a different complexity class as Optionlog(𝑛))\n𝑛2 𝑛2, 𝑛2 𝑛2(D) is incorrect because grows faster than so cannot be an upper bound. Only option (C) is correct, since islog(𝑛) +log(𝑛)\nΘ(𝑛2), 𝑛2which does serve as a lower bound for log(𝑛).\n(2𝑛)2 𝑔(𝑛)2,19. The correct answer is (B). Only II is true. Statement I is false because so and are not part of the same𝑓(𝑛) 𝑓(𝑛) 𝑔(𝑛)= =\n𝑒𝑛∕2 𝑒𝑛arecomplexity class. Statement III is false because and part of different complexity classes (one is the square root of the other, so\n2𝑛log2(𝑛) (2log2(𝑛))𝑛=𝑛𝑛.they cannot grow at the same rate as input size grows). Statement II is true because =\n20. The correct answer is (A). Statement (A) is true because log(𝑛𝑚), which is an upper bound on log(𝑛+𝑚). Statement (B)log(𝑛)+log(𝑚)=\n2𝑛+𝑚=2𝑛2𝑚,is false because is a factor of 𝑛smaller than 𝑛!, so it cannot be an upper bound on 𝑛!. Statement (C) is false because(𝑛−1)!\n2𝑛+2𝑚.which is not bounded above by Statement (D) is false sine cannot be strictly bounded by cosine, and vice versa.\n21. The correct answer is (C). If lim𝑛→∞\n𝑓(𝑛)\n∞, that means that must have a strictly larger rate of growth than 𝑔(𝑛). This means that𝑓(𝑛)=𝑔(𝑛)\nmust be a lower bound on 𝑓(𝑛), which matches option (C). Note that you can also use the notation to indicate that is𝑔(𝑛) 𝜔(𝑔(𝑛)) 𝑓(𝑛)\nstrictly larger than 𝑔(𝑛), but you do not need to worry about that here.\n𝐶𝑆.The correct answer is (A). The time to solve a problem with input size 𝑆can be expressed as The time to solve a problem with input22.\n𝐶𝑆+6. 𝐶𝑆and 𝐶𝑆+6.size is From this, we know that Combining these equations, we can conclude that:𝑆+6 𝑇= 9𝑇=\n9𝑇\n𝐶𝑆+6𝑇=\n𝐶𝑆\n𝐶𝑆+6−𝑆=𝐶69=\n𝐶=\n6√\n9=\n6√\n32 =\n3√\n3\n23. The correct answer is (C). The time to solve a problem with input size 𝑆can be expressed as log2(𝑆). Therefore, the input that can be\nsolved in time is 1 + (since log2(𝑆)). Using log rules, we can express the following:𝑇+1 log2(𝑆) 𝑇=\n𝑇+1 1+log2(𝑆)=\n𝑇+1 log2(2)+log2(𝑆)=\n𝑇+1 log2(2𝑆)=\nis the time needed to solve a problem of input size 2𝑆. Therefore, we can solve a problem of input size 2𝑆in time 𝑇+1.log2(2𝑆)\n𝑆2.24. The correct answer is (B). The time 𝑇it takes to solve a problem of input size 𝑆is Thus, in terms of 𝑆, 8𝑇can be equivalently\n8𝑆2. 8𝑆2? 𝑆′ 8𝑆2expressed as What input size can be solved in a time of Let be the size of problems that can be solved in time. Solving\n𝑆′,for we get a solution of (2\n√\n2)𝑆.\n(𝑆′)2 8𝑆2=\n𝑆′ =\n√\n8𝑆2\n𝑆′ =(2\n√\n2)𝑆\n25. Thecorrectansweris(C).Theloopfromlines2-4takes time, andtheloopfromlines6-8takes time. TheseloopsaresequentialΘ(𝑚) Θ(𝑛)\nfoo()(one happens directly after the other), so their time complexities should be added to get the overall complexity of the function —\nthis comes out to Θ(𝑚+𝑛).\n26. The correct answer is (D). The loop from lines 3-5 takes time, and this loop is run 𝑚times in the outer loop on line 2. Since this is aΘ(𝑛)\nnested loop, we multiply by 𝑚to get the overall time complexity of Θ(𝑚𝑛).Θ(𝑛)\n27. The correct answer is (C). The loop from lines 3-5 takes time, and the loop from lines 7-9 takes time. Since these two loopsΘ(𝑚) Θ(𝑛)\nare sequential, the combined time complexity of lines 3-9 is Θ(𝑚+𝑛). These two loops are then run 𝑝times in an outer loop on line 2, so\nfoo().we multiply by 𝑝to get a final time complexity of for the functionΘ(𝑚+𝑛) Θ(𝑚𝑝+𝑛𝑝)\n28. The correct answer is (D). The inner loop starting on line 4 runs in time, and it is run 𝑛times in the outer loop defined on line 3. AllΘ(𝑛)\nΘ(𝑛2).other work in the function takes constant time. Thus, the total time complexity is 𝑛×Θ(𝑛)=\n29. The correct answer is (C). The inner loop on line 6 iterates from 0 to 𝑛, incrementing by\n√\n𝑛each time. This means that 𝑛∕\n√\n𝑛=\n√\n𝑛\niterations must be run in this innermost loop, each of which performs constant work. The middle loop on line 5 increments from 1 to 𝑛,\ndoublingwitheachiteration—thisruns iterations. Thus, theloopsonline5and6haveacombinedtimecomplexityoflog(𝑛) Θ(\n√\n𝑛log(𝑛)).\nLastly, the outermost loop on line 4 runs from to 𝑛, which involves iterations. Each of these iterations performs a𝑛∕2 𝑛∕2 𝑛∕2 Θ(\n√\n𝑛log(𝑛))\nnestedloop,sotheoveralltimecomplexityofthetriple-nestedloopis(𝑛∕2)×Θ(\n√\n𝑛log(𝑛)) Θ(𝑛=\n√\nafterdroppingallcoefficients.𝑛log(𝑛)))\nSince all other work takes constant time, which is a lower order term that can be dropped in our big-O notation, the overall time complexity\nfoo()of is therefore Θ(𝑛\n√\n𝑛log(𝑛))).\nwhile30. The correct answer is (A). The dominant work within this function can be attributed to the loop on line 3, which divides the input\nsize 𝑛by half with every iteration, terminating when 𝑛reaches a value less than one. Thus, the time complexity of the loop is Θ(log(𝑛)).\nfor31. The correct answer is (B). There is a loop dependency here, where the number of times the loop runs on line 4 is determined by the\nvalue of 𝑛, which is being halved within the outer loop defined on line 3. During the first iteration, the loop on line 4 runs 𝑛times; during\nthe second iteration, the loop on line 4 runs times, and so on. Since each iteration of the inner loop takes a constant amount of work,𝑛∕2\nthe overall time complexity of the nested loop is using the formula for a geometric series.𝑛+𝑛∕2+𝑛∕4+… Θ(𝑛)=\n32. The correct answer is (B). See example 4.12.", "word_count": 1043, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e2adf6bb-5f7e-59f2-b8f4-eef06cc90012", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 128, "real_page_number": null, "text": "116\nChapter 4. Complexity Analysis\n33. The correct answer is (C). In the best case, the first two numbers in the array are duplicates, which would cause the function to return in\nconstant time (since only two values would be checked). In the worst-case, there are no duplicates, which would require the entire nested\nΘ(𝑛2).loop to run. Using the provided identity, the time complexity in the worst case would be\n34. The correct answer is (C). There is a loop dependency here, so we will consider both nested for loops in tandem. During the first iteration\nof the outer loop, the inner loop runs 0 times; during the second iteration of the outer loop, the inner loop runs 1 time; during the third\niteration of the outer loop, the inner loop runs 2 times, and so on. Using the provided identity, the time complexity of the function comes\nΘ(𝑛2).out to 0+1+2+…+(𝑛−1)=\n35. The correct answer is (C). There is a loop dependency here, so we will consider both nested for loops in tandem. The analysis for this\nproblemissimilartothepreviousproblem, withtheexceptionthattheamountofworkcanbeapproximatedas0+log(1)+log(2)+log(3)+\njinstead of because the value of is doubled instead of incremented within the inner loop. Using…+log(𝑛−1) 0+1+2+3+…+(𝑛−1)\nlog rules, we can conclude that the total work done by the nested loop is log((𝑛−1)!). Furthermore, we knowlog(1×2×3×…×(𝑛−1))=\nthat Θ(𝑛log(𝑛)), which allows us to conclude thatlog(𝑛!) log((𝑛−1)!) Θ(log(𝑛!∕𝑛)) Θ(log(𝑛!)−log(𝑛)) Θ(𝑛log(𝑛)−log(𝑛))= = = = =\nafter dropping lower order terms.Θ(𝑛log(𝑛))\n36. The correct answer is (B). There is a loop dependency here, so we will consider both nested for loops in tandem. The first iteration of the\nouter loop runs 1 time, the second iteration of the outer loop runs 2 times, the third iteration of the outer loop runs 4 times, the fourth\niteration of the outer loop runs 8 times, and so on. This follows the form of a geometric series, where the iteration count is doubled with\neach iteration of the outer loop, and the outer loop runs times. Using the provided summation, we can conclude that:log2(𝑛)\n1+2+4+8+…+𝑛=\nlog2(𝑛)\n∑\n𝑘=0\n2𝑘=\n1(1−2log2(𝑛)+1)\n1−2\n=\n1−2(2log2(𝑛))\n1−2\n1−2𝑛=\n2𝑛−1=1−2\n1\nΘ(𝑛)=\nj i, k j),37. The correct answer is (B). This function involves a triply-nested loop dependency (where depends on and depends on so we will\nn,consider the three loops on lines 3-5 together. Notice that the innermost loop on line 6 always iterates from 1 to doubling with each\niteration, so the time complexity of this innermost loop is always Θ(log(𝑛)).\n(i = 0):• On the first iteration of the outermost for loop\n(j = n) k = n/2 k = n,– The first iteration of the second nested loop runs the third loop from to which performs workΘ(log(𝑛))\nkon each iteration. Since the loop is run times, the total work done on this first iteration is (𝑛∕2)log(𝑛).𝑛∕2\nj k n/4 n/2,– The second iteration of the loop runs the loop from to where work is performed each iteration. SinceΘ(log(𝑛))\nkthe loop is run times here, the total work done on this second iteration is (𝑛∕4)log(𝑛).𝑛∕4\nj j n i= 0, j i– The loop halves from to so the loop runs times during this first iteration of the loop. Continuing thelog(𝑛)\n𝑛iprevious pattern, the total work done on the first iteration of the loop is\n𝑛log(𝑛)+2\n𝑛log(𝑛)+4\nlog(𝑛)+…+8\n𝑛\nlog(𝑛).2log2(𝑛)\n(i = 1):• On the second iteration of the outermost for loop\nj (j = n) k k = n k = n/2,– The first iteration of the loop runs the loop times from to which performs work with𝑛∕2 Θ(log(𝑛))\neach iteration. The total work done on this iteration is (𝑛∕2)log(𝑛).\nj (j = n/2) k k = n/4 k = n/2,– The second iteration of the loop runs the loop times from to which performs𝑛∕4 Θ(log(𝑛))\nwork with each iteration. The total work done on this iteration is (𝑛∕4)log(𝑛).\ni j– The only difference between the second and first iteration of the outer loop is the number of times the loop runs. During this\nj n 1 0,iteration, is halved from to instead of so the loop runs times instead of times, which means the totallog2(𝑛−1) log2(𝑛)\n𝑛iwork done on the second iteration of the loop is\n𝑛log(𝑛)+2\n𝑛log(𝑛)+4\nlog(𝑛)+…+8\n𝑛\nlog(𝑛).2log2(𝑛−1)\n𝑛(i = 2),• On the third iteration of the outermost for loop the total work done is\n𝑛log(𝑛)+2\n𝑛log(𝑛)+4\nlog(𝑛)+…+8\n𝑛\nlog(𝑛).2log2(𝑛−2)\n• This pattern continues, until the final iteration of the outermost for loop.\nPutting this all together, the total work done by the nested loop can be expressed as:\n𝑇(𝑛)=\n(\n𝑛\n𝑛log(𝑛)+2\nlog(𝑛)+…+4\n𝑛\nlog(𝑛)2log2(𝑛)\n)\n+\n(\n𝑛\n𝑛log(𝑛)+2\nlog(𝑛)+…+4\n𝑛\nlog(𝑛)2log2(𝑛−1)\n)\n+\n(\n…+\n𝑛\nlog(𝑛)2log2(1)\n)\n1=\n2𝑛log(𝑛)\n[(\n11+\n1+2\n+…+4\n1\n2log2(𝑛)\n)\n+\n(\n11+\n1+2\n+…+4\n1\n2log2(𝑛−1)\n)\n+…+\n(\n…+\n1\n2log2(1)\n)]\n1=\n2𝑛log(𝑛)\n⎡\n⎢\n⎢⎣\nlog2(𝑛)\n∑\n𝑦=0\n(1\n2\n)𝑦\n+\nlog2(𝑛−1)\n∑\n𝑦=0\n(1\n2\n)𝑦\n+…+\nlog2(1)\n∑\n𝑦=0\n(1\n2\n)𝑦⎤\n⎥\n⎥⎦\n1=\n2𝑛log(𝑛)\n𝑛\n∑\n𝑥=1\n⎛\n⎜\n⎜⎝\nlog2(𝑥)\n∑\n𝑦=0\n(1\n2\n)𝑦⎞\n⎟\n⎟⎠\n1=\n2𝑛log(𝑛)\n𝑛\n∑\n𝑥=1\n⎛\n⎜\n⎜\n⎜⎝\n1−\n(\n1\n2\n)log2(𝑥)\n1−1\n2\n⎞\n⎟\n⎟\n⎟⎠\n1=\n2𝑛log(𝑛)\n𝑛\n∑\n𝑥=1\n(\n2\n(\n1−\n(1\n2\n)log2(𝑥)))\n𝑛log(𝑛)=\n𝑛\n∑\n𝑥=1\n(\n1−\n(1\n2\n)log2(𝑥))\n𝑛log(𝑛)=\n( 𝑛\n∑\n𝑥=1\n1−\n𝑛\n∑\n𝑥=1\n1\n2log2(𝑥)\n)\n𝑛log(𝑛)=\n(\n𝑛−\n𝑛\n∑\n𝑥=1\n1\n𝑥\n)\nΘ(𝑛2𝑛log(𝑛)×(𝑛−Θ(log(𝑛))) 𝑛log(𝑛)×Θ(𝑛) log(𝑛))= = =\n38. The correct answer is (B). The space complexity is the the amount of memory used by an algorithm to solve a problem with respect to\ninput size. On the other hand, auxiliary space only considers the additional memory used by the algorithm but does not include the memory\nsum()used by the input values themselves. In this example, the function acts on an array of size 𝑛that is stored in memory, so the total\nspace complexity is Θ(𝑛). However, this array is an input, so although the function uses its memory, it does not actually allocate it. Beyond\nsum()the memory passed in as an input, the only initializes a constant amount of additional space, so its auxiliary space usage is Θ(1).", "word_count": 1141, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "109c1284-840c-5e0c-91f0-60d30fdc9352", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 129, "real_page_number": null, "text": "Chapter 5\nRecursion and Recurrence Relations\n5.1\nRecursion\nIn the previous chapter, we introduced the concept of time complexity and several strategies that can be used to compute the time complexity of\na given algorithm. One such strategy is to count the number of steps a program takes with respect to input size. However, this is not always\neasy! With an iterative algorithm, we can analyze loops to get an accurate estimate for time complexity in most cases. However, this becomes\nmuch trickier if recursion is involved.\nA function is recursive if it calls itself in its own implementation. A recursive function is if each invocation of the functionlinear recursive\ncan only make at most one recursive call, and if each invocation can make more than one recursive call. Recursion is useful if thetree recursive\nsolution to a smaller subproblem (that is of the same type as the original problem) can be used to solve for the solution to a larger subproblem.\nTo solve problems using recursion, we\n1. assume that our recursive function already works and make a recursive call to a smaller subproblem to get its solution (this is known as\nthe faith).recursive leap of\n2. then, use the solution for this smaller subproblem to solve our original problem.\nThe soundness of recursion as an algorithmic approach rests on the assumption that, if a function calls itself over and over again using smaller\nand smaller input sizes, it will eventually reach an input size that is small enough for the problem to be solved trivially. This case is known as\nthe base case of a recursive algorithm, and it allows the solution to build back up to the original input size.\nThe concept of recursion can be difficult to understand at first, so let’s use the following real life example. Suppose you are waiting in a\nvery long line, and you want to know what position you are in. Since you cannot leave the line without losing your position, you make the\nassumption that the person in front of you already knows what position they are in, and you ask them for their position. The person in front of\nyou turns out does not know either, so they make the same assumption that the people in front of them know their positions, and they ask the\nsame question to the person in front of them.", "word_count": 400, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c3998bd6-d46e-5dd3-bab3-f93828a6137a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 130, "real_page_number": null, "text": "118\nChapter 5. Recursion and Recurrence Relations\nThis continues until the first person in line is asked. Since the first person in line clearly knows that they are the first in line, they tell this to the\nsecond person. The second person now knows they are second in line, so they tell this information to the third person. The third person now\nknows they are third in line, so they tell this information to the fourth person. This continues all the way down the line, where each person just\nadds one to the previous person’s answer to get their own position. Eventually, the person in front of you will know what their position is. They\ntell you their position, and you can add one to this number to get your own position in line. Your initial question has now been answered!\nRecursion works in the same way, but this time you are a function call instead of a person waiting in line. The process of assuming the\nperson in front of you knows their own position is analogous to the recursive leap of faith. Asking the person in front of you for the answer to\nyour question is analogous to making a recursive call with a smaller input size. Lastly, the very first person in line is analogous to the base case,\nsince their position number is small enough that they can come up with an answer to your initial question trivially. If you were to implement this\nprocess as a recursive function, it would look something like this:\n1\nint32_t get_line_position(int32_t person) {\n2\n// if first person, return one (base case)\n3\nif (person == 1) {\n4\nreturn 1;\n5\n} // if\n6\n// otherwise, return one plus the position of the person before you\n7\nreturn 1 + get_line_position(person - 1);\n8\n} // get_line_position()\nAs another example, let’s apply recursion to solve a math problem. Suppose you wanted to implement a function that calculates the factorial of a\nnumber — that is, given a non-negative integer 𝑛, the function returns 𝑛!, or 𝑛×(𝑛−1)×(𝑛−2)×…×2×1. By definition, 1. We can0! =\ndefine a factorial in terms of itself as follows:\n𝑛!=\n{\n1,\nif 𝑛=0\n𝑛×(𝑛−1)!,\nif 𝑛>0\nLet’s convert this definition into a function that can calculate a factorial when passed in an integer 𝑛. First, we need to consider the base case: if\n0, we immediately know that the function should return 1. Thus, we can start off the function with the following code:𝑛=\n1\nint32_t factorial(int32_t n) {\n2\nif (n == 0) {\n3\nreturn 1;\n// base case\n4\n} // if\n5\n...\n6\n} // factorial()\n𝑛≠0,Now, what if we are not given the base case? To calculate the value of for we would have to calculate 𝑛×(𝑛−1)!. We know the value𝑛!\nof 𝑛, but what is the value of (𝑛−1)!? We do not know, but we can use recursion and pass into our own factorial function to get its value.(𝑛−1)\nSimilar to the people waiting in line, each recursive call is going to calculate for smaller values of 𝑛. Eventually, 𝑛will reach 0, and we(𝑛−1)!\nwould be able to return using our base case. This information will allow us to solve for 1!, 2!, 3!, …, all the way back up to 𝑛!. The completed1\ncode is shown below:\n1\nint32_t factorial(int32_t n) {\n2\nif (n == 0) {\n3\nreturn 1;\n// base case\n4\n} // if\n5\nreturn n factorial(n - 1);*\n6\n} // factorial()\nWhat happens when this code runs? For example, suppose we pass in the number 10 as the value of 𝑛. Line 5 would run, and the function\n10 factorial(9).would return However, this requires a recursive call, so the factorial function is run again with input 9. The𝑛=*\n9 factorial(8),recursive call with input 9 would return which triggers a recursive call with input 8. This process continues, where𝑛=*\nfactorial(8) factorial(7), factorial(6), factorial(5),calls which itself calls which itself calls and so on. The recursive\ncalls stop when the input reaches 0, as this would trigger the base case and immediately return without any additional recursive calls.\nfactorial(10),There is one important thing we have to note, however. When we first called we wanted to return the value\n10 factorial(9). factorial(9). factorial(9),This requires a recursive call to However, after calling we still have to*\n10, n. nmultiply the result we get by which is stored in the variable Because we still need to reference the variable after the recursive call to\nfactorial(9) completes, we have to save it somewhere in memory so that it can be retrieved after the necessary recursive calls are done.\nThis storage location is known as the program stack.", "word_count": 813, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bdf6693c-ea39-5243-9a3f-b15c62df4a12", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 131, "real_page_number": null, "text": "5.2 The Program Stack\n119\n5.2\nThe Program Stack\nSuppose we have two functions, A and B, and function A calls function B. Before function B can begin running after the function call, we have\nto store the state of function A so that we can return to it after function B is done. In other words, we have to remember all the local variables of\nfunction A so that, after function B runs to completion, we can return back to function A and continue where we left off.\nfunc_A() func_B() num 281For example, in the following code, calls on line 3. However, we have to remember that the value of is\nfunc_A() func_B(). func_B()and that we are currently on line 3 of before we start running That way, when is done, we can return back\nfunc_A() num), func_A().to line 3, restore the values of the local variables of (in this case, and continue the execution of\n1\nint32_t func_A() {\n2\nint32_t num = 281;\n3\nint32_t val = func_B();\n4\nint32_t res = num + val;\n5\nreturn res;\n6\n} // func_A()\nTo remember this information, we use stack frames, which are stored on the program stack in memory. Recall that the stack is a portion of\nmemory set aside for local variables and bookkeeping data (unlike the heap, which is reserved for dynamic memory). Information that is placed\non a stack is accessed in last-in first-out (LIFO) order: the most recently allocated block of information is also the next block to be removed.\nEach time a function call is made, the local variables that belong to the caller of the function gets stored on the program stack. Then, the\narguments of the function call are pushed onto the stack. Control is then transferred from the caller of the function to the function that has been\ncalled. The new function then pops the function arguments off the stack and begins running.\nWhen a function returns, the return value is pushed onto the program stack. The caller of the function that just returned then resumes\nexecution, popping the return value off the program stack and restoring its local variables.\nLet’s analyze the stack frames that are allocated for the following snippet of code:\n1\nint32_t factorial(int32_t n) {\n2\nif (n == 0) {\n3\nreturn 1;\n4\n} // if\n5\nreturn n factorial(n - 1);*\n6\n} // factorial()\n7\n8\nint main() {\n9\nint32_t res = factorial(3);\n10\nstd::cout << \"3! = \" << res << std::endl;\n11\nreturn 0;\n12\n} // main()\nWhat happens when this code runs? Let’s walk through the code one step at a time and analyze the contents of the stack frame.\n#\nStack Frame\nBehavior\nProgram Begins Running\nmain()A Stack Frame is Allocated for\n(1)\nmain()\nmain() main()When the program begins running, the function is called. Information for the function\nis stored in a block of memory on the stack (called the record; this stores the local variables ofactivation\nmain(), temporary objects, the return address, and other information that is needed by the function).\nres, res,Line 9 creates an instance of the integer which starts off uninitialized. To assign the value of a\n3.recursive call is made with an input of\nmain() main() factorial(3)The current state of is stored in the function’s stack frame before begins\n3running. The argument value is pushed onto the stack.\nfactorial(3)A Stack Frame is Allocated for\n(2)\nfactorial(3)\nfactorial() 3 n.The function pops the argument off the stack and sets it to the value of It then begins\nrunning.\nn 0, ifSince the value of is not the statement does not run. Line 5 makes a recursive call with an input size\n2 n - 1).of (the value of\nfactorial(3)The current state of the function call is stored in a stack frame before the recursive call to\nfactorial(2) 2,begins running. The value of the argument, is pushed onto the stack.\nfactorial(2)A Stack Frame is Allocated for\n(3)\nfactorial(2)\nfactorial() 2 n.The function pops the argument off the stack and sets it to the value of Because this is\nn = 2 factorial(2)a brand new function call, the value of is local to the function call. The function\nbegins running.\nn 0, ifSince the value of is not the statement does not run. Line 5 makes a recursive call with an input size\n1 n - 1).of (the value of\nfactorial(2)The current state of the function call is stored in a stack frame before the recursive call to\nfactorial(1) 1,begins running. The value of the argument, is pushed onto the stack.\nfactorial(1)A Stack Frame is Allocated for", "word_count": 799, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "49c12cc2-4b4b-5d67-b2ab-3ac0426261c9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 132, "real_page_number": null, "text": "120\nChapter 5. Recursion and Recurrence Relations\nThe table below continues from the previous page.\nfactorial(1)A Stack Frame is Allocated for\n(4)\nfactorial(1)\nfactorial() 1 n.The function pops the argument off the stack and sets it to the value of It then begins\nrunning.\nn 0, ifSince the value of is not the statement does not run. Line 5 makes a recursive call with an input size\n0 n - 1).of (the value of\nfactorial(1)The current state of the function call is stored in a stack frame before the recursive call to\nfactorial(0) 0,begins running. The value of the argument, is pushed onto the stack.\nfactorial(0)A Stack Frame is Allocated for\n(5)\nfactorial(0)\nfactorial() 0 n.The function pops the argument off the stack and sets it to the value of It then begins\nrunning.\nn 0,Since the value of is the base case (line 3) runs, and the return value is set to 1.\n1 factorial(0)The return value of is pushed onto the stack, and the function call exits.\nfactorial(0)The Stack Frame for is Deallocated\n(6)\nfactorial(1)\nfactorial(1), factorial(1)The most recent stack frame is so resumes execution on line 5. Local\nfactorial(0) nvariables for this function call are restored, and the return value of is multiplied by to get\n1 1 = 1.*\n1 factorial(1)The return value of is pushed onto the stack, and the function call exits.\nfactorial(1)The Stack Frame for is Deallocated\n(7)\nfactorial(2)\nfactorial(2), factorial(2)The most recent stack frame is so resumes execution on line 5. Local\nfactorial(1) nvariables for this function call are restored, and the return value of is multiplied by to get\n1 2 = 2.*\n2 factorial(2)The return value of is pushed onto the stack, and the function call exits.\nfactorial(2)The Stack Frame for is Deallocated\n(8)\nfactorial(3)\nfactorial(3), factorial(3)The most recent stack frame is so resumes execution on line 5. Local\nfactorial(2) nvariables for this function call are restored, and the return value of is multiplied by to get\n2 3 = 6.*\n6 factorial(3)The return value of is pushed onto the stack, and the function call exits.\nfactorial(3)The Stack Frame for is Deallocated\n(9)\nmain()\nmain(), main()Themostrecentstackframeis so resumesexecutiononline9. Localvariablesarerestored,\n6 res.and the return value of is set to\n\"3! = 6\".Line 10 prints out\n0, 0 main()Line 11 returns which is pushed onto the stack. Returning from indicates that the program\nmain()completed successfully (the return value of is known as the of the program).exit status\nmain()The Stack Frame for is Deallocated\nProgram Completes\nA visual representation of this process is shown on the next page. The numbers on the figure (above the stack frames) correspond with the\nnumbers in the first column of the above table.\nUnfortunately, the program stack is limited, so you could exhaust all the available stack space if you recurse too deeply. If you run out of\navailable stack space, you would end up with an error known as a stack overflow. One potential strategy for avoiding stack overflows is to use a\nprocess known as tail recursion, which is discussed in the following section.\n5.3\nTail Recursion\nIn the previous example, we allocated a stack frame each time we made a recursive call. This is because we needed the the values of local\nn).variables all the recursive calls were complete (e.g., multiplying the recursion result with the local variable Thus, stack frames wereafter\nneeded to store the values of local variables so that their values could be retrieved after the recursive calls were done. However, there are certain\nsituations where a stack frame does need to be allocated with each recursive call. Such cases happen when a function is tail recursive.not\nWhat is the difference between a tail recursive function and a non-tail recursive function? A tail recursive function is a recursivelinear\nfunction where function. Because the recursive call is the final step of a function, there is no needfinalthe recursive call is the instruction of the\nto remember the function’s local variables on a stack frame, as they will never be needed later. As a result, the compiler is able to optimize\nmemory and reuse stack frames when a tail recursive function call is made. For instance, if the factorial function we defined earlier were tail\nn).recursive, we could just use the stack frame for each factorial call (instead of having to allocate a separate stack frame for each value ofsame\nLet’s look at the factorial function we defined earlier:\n1\nint32_t factorial(int32_t n) {\n2\nif (n == 0) {\n3\nreturn 1;\n4\n} // if\n5\nreturn n factorial(n - 1);*\n6\n} // factorial()\nnHere, the result of the recursive call on line 5 must be multiplied with the value of after the recursive call completes, so this function is not tail\nnrecursive. Since we need to remember the value of before making the recursive call (so that we can use its value later), we cannot reuse the\nsame stack frame for each recursive call.", "word_count": 881, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b4a903fa-aff8-585d-b644-8ea4c0126228", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 133, "real_page_number": null, "text": "5.3 Tail Recursion\n121\nHowever, it turns out that we can optimize memory by converting this non-tail recursive implementation into a tail recursive one. In our\nnnon-tail recursive implementation, we needed to remember the value of before each recursive call so that we could multiply it with\nfactorial(n - 1) the recursive call completes. This is why each recursive call needed its own stack frame to keep track of the valueafter\nn. nof Thus, for our factorial function to be tail recursive, we would need a way to remember the value of and multiply it with the result of the\nrecursive call returns, so that we do not need to reference it later. That way, we can guarantee that the recursive call isbefore the recursive call\nthe last instruction of the function that called it, and that nothing else needs to be done once the recursive call returns.\nIn order to complete the multiplication before the recursive call returns, we can add something called an as aaccumulator argument\nparameter of each recursive call. This accumulator argument allows us to complete the multiplication of the recursive call,in the argument\nbefore the recursive call returns! The code for this is shown below:\n1\nint32_t factorial(int32_t int32_tn, res = 1) {\n2\nif (n == 0) {\n3\nreturn res;\n4\n} // if\n5\nreturn factorial(n - 1, n res);*\n6\n} // factorial()\nn factorial(n - 1)Since we are multiplying with the result of in an argument of the function, we no longer have to do any additional\nwork once the recursive call completes. Thus, our factorial function is now tail recursive, and the same stack frame can be reused for multiple\nrecursive calls.\nres = 1.Remark: On line 1, the function definition above sets This is known as a argument. By doing this, we are specifyingdefault\nfactorial() res factorial(3)), resthat, if the function is called without the argument (e.g., the value of for that function call is\ndefault set to a value of 1.\nIteration and recursion are two methods that can both be used to achieve repetition in a program. Although you will not have to know how to\nconvert between the two, any iterative function can in fact be converted to a recursive function. This is because iteration is simply a special case\nof tail recursion. Furthermore, any recursive function can be converted to an iterative function by simulating the program stack (e.g., you could\nstd::stack<>,keep track of a separate container, such as a that behaves just like the program stack would with recursive calls).", "word_count": 440, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "39f48c5f-1a6e-5b2d-a388-6f71b44a71c0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 134, "real_page_number": null, "text": "122\nChapter 5. Recursion and Recurrence Relations\n5.4\nStack Frames and Space Complexity\nAs shown in the previous section, if a recursive function is not tail recursive, each additional recursive call allocates a stack frame on the program\nstack. Because stack frames take up memory, you will need to take this memory into account when calculating the auxiliary space used by a\nrecursive function.\nIn general, the auxiliary space required by a non-tail recursive call is determined by its recursion depth, or the number of return statements\nthat must be executed until the base case is reached. The non-tail recursive implementation of the factorial function, for example, has a recursion\nfactorial(n) factorial(0).depth of 𝑛, since a call to has to execute 𝑛return statements before it reaches the base case of\nWhen assessing the auxiliary space of a recursive function, we look at the number of return statements until the base case, not the total\nnumber of recursive function calls that are made. This is because stack frames for each recursive call may not all exist in memory at the same\ntime. Consider the following example:\nExample 5.1 foo()Express the auxiliary space used by using big-O notation, in terms of the input size 𝑛.\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 1) {\n3\nreturn 1;\n4\n} // if\n5\nreturn foo(n - 1) + foo(n - 1);\n6\n} // foo()\nΘ(2𝑛) Θ(2𝑛)foo()Here, two recursive calls are made each time is invoked. Thus, it may be tempting to say that the function uses space, since\nrecursive calls are made. However, these recursive calls do not all exist on the stack frame at the exact same time. To visualize this, suppose you\nfoo(4). foo(4)call A stack frame for is allocated on the program stack:\nfoo(4) stack frame\nfoo(4) foo(3),The function call to then makes its first recursive call to and the stack frame now looks like:\nfoo(4) stack frame\nfoo(3) stack frame\nfoo(3) foo(2), foo(1).then makes its first recursive call to which makes its first recursive call to At this point, we end up at the base\ncase, and the stack frame looks like this:\nfoo(4) stack frame\nfoo(3) stack frame\nfoo(2) stack frame\nfoo(1) stack frame\nfoo(1) foo(2) foo(1)At this point, returns 1, and its stack frame is deallocated. then calls a second time, and a new stack frame is\nfoo(1),allocated. However, notice that this stack frame with the stack frame allocated for the first call to since that calldoes not coexist\n24 foo() foo(4)had to complete before the second call could be made. As a result, even though recursive calls to are made when is=16\ninvoked, at most 4 stack frames are actually needed at any point in time.\nGiven an initial input size of 𝑛, only 𝑛return statements are executed before the base case is reached, since 𝑛is decremented with each\nrecursive call. As a result, the auxiliary space required for stack frames is also Θ(𝑛).\nIn general, if the recursion depth of a non-tail recursive function is 𝑛, the auxiliary space it uses for stack frames must also be Θ(𝑛). Note\nthat the auxiliary space used by the function outside of stack frames must also be considered! For instance, if a recursive algorithm requires\nΘ(𝑛2) Θ(𝑛+𝑛2) Θ(𝑛2).stack frames, but also initializes a container that uses space, the overall auxiliary space of the algorithm isΘ(𝑛) =\nIn a tail recursive implementation, the stack frame is reused with each recursive call. Hence, the number of stack frames does not increase\nas the recursion depth increases, and the auxiliary space used by stack frames is Θ(1).\nExample 5.2 print_to_n()Express the auxiliary space used by using big-O notation, in terms of the input size 𝑛.\n1\nvoid print_to_n(int32_t n) {\n2\nif (n < 1) {\n3\nreturn;\n4\n} // if\n5\nstd::cout << n << \" \";\n6\nprint_to_n(n - 1);\n7\n} // print_to_n()\nIn this example, we have a recursive call at the very end. Since the input size 𝑛is decremented by 1 with each recursive call from 𝑛to 1, the\nreturn voidrecursion depth is 𝑛(note that the statement for a function is implicit if it is not specified). However, the recursive call to\nprint_to_n() is the very last thing that is done! As a result, the function is tail recursive, and the compiler reuses the same stack frame for\neach recursive call. Since the remaining memory usage of this function does not depend on input size, and the function is tail recursive, the\nauxiliary space used by the function is Θ(1).", "word_count": 775, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9902e37c-1241-5cac-be0c-6d535b164188", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 135, "real_page_number": null, "text": "5.5 Identifying Recurrence Relations\n123\nExample 5.3 bar()Express the auxiliary space used by using big-O notation, in terms of the input size 𝑛.\n1\nint32_t bar(int32_t n) {\n2\nif (n < 1) {\n3\nreturn;\n4\n} // if\n5\nreturn n + bar(n / 2);\n6\n} // bar()\nSince the input size of the recursive call is halved each time, the recursion depth is log(𝑛). This is because recursive calls are needed forlog(𝑛)\nthe input size to go from the initial value 𝑛to the base case of 1. Notice here that the recursive call is the last thing that is done, since itsnot\nvalue must still be added to 𝑛after the function returns. Thus the function is not tail recursive, and stack frames cannot be reused. The number\nof stack frames needed is equal to the recursion depth, so the auxiliary space required by the stack frames is Θ(log(𝑛)).\nTo summarize: when dealing with space complexity, it is important to analyze sources of memory usage for both the stack and the heap. A\nnon-tail recursive function that does not explicitly allocate memory may still allocate additional stack frames as the input size grows, as long as\nthe recursion depth also increases with the size of the input.\n5.5\nIdentifying Recurrence Relations\nIn the previous chapter, we discussed how the time complexity of an algorithm can be determined by counting steps. However, what if recursion\nis involved? If a function calls itself, how do we measure its time complexity?\nIn the following sections, we will discuss methods that can be used to find the time complexities of recursive algorithms. Previously, we\ndefined the runtime of an algorithm as a function of its input size 𝑛. We will still do this when dealing with recursive functions. However,𝑇(𝑛)\nbecause a recursive algorithm calls itself with a smaller input size, we will write as a recurrence relation, or an equation that defines the𝑇(𝑛)\nruntime of a problem in terms of the runtime of a recursive call on smaller input. As an example, let’s revisit the non-tail recursive factorial\nfactorial(n - 1)function defined at the beginning of section 5.3. In this function, we return 1 if and make a recursive call to𝑛==0\notherwise. This can be converted into the following recurrence relation:\n𝑇(𝑛)=\n{\nΘ(1),\nif 𝑛=0\n𝑇(𝑛−1)+Θ(1),\nif 𝑛>0\nfactorial(n), factorial(n - 1),Here, represents the runtime of represents the runtime of the recursive call to and𝑇(𝑛) 𝑇(𝑛−1)\nthe additional term represents the constant work we do outside the recursive call. Since the recurrence relation will be used to calculateΘ(1)\ntime complexities, having an exact number for this constant term does not matter.\nOn the next few pages, we will look at several recursive functions and define them as recurrence relations, where runtime is expressed\nas a function of input size. This procedure is similar to the one used for an iterative function: operations that follow one another have their\ncomplexities added, while operations that are nested within loops have their complexities multiplied. However, when a recursive call is reached,\nwe add a term that defines in terms of the input size of that recursive call.𝑇(𝑛)\nExample 5.4 foo() bar()Express the runtime of the function as a recurrence relation. You may assume that the function runs in\n𝑛≥1.time and thatΘ(log(𝑛))\n1\nvoid foo(int32_t n) {\n2\nif (n == 1) {\n3\nreturn;\n4\n} // if\n5\nfoo(n - 1);\n6\nint k = n n;*\n7\nfor (int i = 0; i < k; ++i) {\n8\nfor (int j = 0; j < n; ++j) {\n9\nbar(n);\n10\n} // for j\n11\n} // for i\n12\nfor (int i = 0; i < n; ++i) {\n13\nfoo(n / 2);\n14\n} // for i\n15\nbar(k);\n16\n} // foo()\nTo approach this problem, we will use the same approach as in the previous chapter. Walk through each line and determine its time complexity.\nHowever, if you reach a recursive call, express its contribution toward the runtime recursively in terms of the input size of that recursive call.\n• Lines 2-3: this is a constant operation, but it only runs if 1.𝑛=Θ(1)\n• Line 5: this is a recursive call with an input size of 𝑛−1, which we assign a recurrence term of 𝑇(𝑛−1).\n• Line 6: this is a constant calculation.Θ(1)\n𝑛2bar() for for k = n n).• Lines 7-9: runs in time, the inner loop runs 𝑛times, and the outer loop runs times (sinceΘ(log(𝑛)) *\nΘ(log(𝑛)×𝑛×𝑛2) Θ(𝑛3The total time complexity of this entire nested loop is thus log(𝑛)).=\n• Lines 12-13: line 13 makes a recursive call with an input size of 𝑛∕2, so this line gets assigned a recurrence term of 𝑇(𝑛∕2). However,\nforsince this recursive call is made 𝑛times in the loop defined on line 12, the overall contribution of the entire loop is 𝑛𝑇(𝑛∕2).\n𝑛2,bar()• Line 15: since has a time complexity of and is passed in an input size of the time complexity of this line isΘ(log(𝑛))\nΘ(log(𝑛2)), which is equivalent to using logarithm identities.Θ(2log(𝑛))", "word_count": 878, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "27e0b82f-5eb6-50f5-8d4b-cd658f3ae2d3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 136, "real_page_number": null, "text": "124\nChapter 5. Recursion and Recurrence Relations\nThere are two different cases that can happen. If the input size 𝑛is 1, only line 3 runs, so (i.e., constant time). However, if 1,𝑇(1) 𝑛>=Θ(1)\nfoo()we can express the runtime of using the following recurrence relation:\n𝑇(𝑛−1)+𝑛3𝑇(𝑛) log(𝑛)+𝑛𝑇(𝑛∕2)+2log(𝑛)+Θ(1)=\nThe full recurrence relation is therefore as follows:\n𝑇(𝑛)=\n{\nΘ(1),\nif 𝑛=1\n𝑇(𝑛−1)+𝑛3log(𝑛)+𝑛𝑇(𝑛∕2)+2log(𝑛)+Θ(1),\nif 𝑛>1\n5.6\nThe Iterative Substitution Method\nAfter we convert a recursive algorithm into a recurrence relation, we can use that recurrence relation to determine its time complexity. In this\nclass, we will be looking at two methods that can be used to accomplish this: the and the Theorem. In thisiterative substitution method Master\nsection, we will discuss the iterative substitution method (also known as the iteration method). To start, let’s look at the recursive function\ndefined below, which takes in a positive integer 𝑛as its input size:\n1\nint32_t foo(int32_t n) {\n2\nif (n == 1) {\n3\nreturn 1;\n4\n} // if\n5\nreturn foo(n - 1) + foo(n - 1) + 1;\n6\n} // foo()\nThe runtime of this function is constant when 1. Otherwise, it makes two recursive calls with input size and performs constant time𝑛= 𝑛−1\nmath operations. We can express the runtime of this function using the following recurrence relation:\n𝑇(𝑛)=\n{\nΘ(1),\nif 𝑛=1\n2𝑇(𝑛−1)+Θ(1),\nif 𝑛>1\nJust by looking at this recurrence relation alone, there is not much we can immediately conclude about its time complexity. If had been𝑇(𝑛)\n5𝑛2 Θ(𝑛2).equal to something like +6𝑛+7, we’d be able to easily tell that the runtime complexity is However, in this example, we have to\ndetermine the runtime complexity of 2𝑇(𝑛−1)+Θ(1). How do we go about doing that?\nWe do not know anything yet about the time complexity of the recursive call. However, we do know the time complexity of the𝑇(𝑛−1)\nbase case: performs a constant time operation. If we are somehow able to convert into a form that only contains the base𝑇(1) 2𝑇(𝑛−1)+Θ(1)\nequation.1case 𝑇(1), we could substitute with a constant value of 1 to get rid of all the recursive terms in our Such a form is known𝑇(1) 𝑇(𝑛)\nas a solution, which makes it easier to deduce a recurrence relation’s complexity class.closed form\nThis is the idea behind the iterative substitution method, which finds an explicit formula for a recursively defined sequence. After you\nhave a recurrence relation, the steps of the iterative substitution method are as follows:\nWrite out the recursive terms (𝑇(𝑛−1), 𝑇(𝑛−2), etc.), as their own recurrence relations and substitute their equations into the original1.\nformula at each step.𝑇(𝑛)\n𝑘th2. Look for a pattern that describes at the step (for any arbitrary 𝑘), and express it using a summation formula.𝑇(𝑛)\n3. Solve for 𝑘such that the base case is the only recursive term that is present on the right-hand side of the equation for 𝑇(𝑛). Determine the\nclosed form solution by replacing instances of the base case with its value (e.g., replacing with 1 if the base case is Θ(1)).𝑇(1) 𝑇(1)=\nLet’s walk through some examples:\n𝑛≥1.Example 5.5 foo()Express the time complexity of the function using big-O notation, in terms of the input size 𝑛. Assume\n1\nint32_t foo(int32_t n) {\n2\nif (n == 1) {\n3\nreturn 1;\n4\n} // if\n5\nreturn foo(n - 1) + foo(n - 1) + 1;\n6\n} // foo()\nYou may find the following identity helpful with your calculations:\n𝑛\n∑\n𝑖=𝑚\n𝑎(𝑟𝑚−𝑟𝑛+1)𝑎𝑟𝑖=\n1−𝑟\n1Since runsinconstanttime, wecanactually substitute itwithanyconstant,andourbig-Oresultwouldnotchange. However,replacinganyconstantwork𝑇(1)\nwith1intherecurrencerelationmakesthemathaloteasiertoworkwith.", "word_count": 651, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bb17b448-8aed-5a13-800e-bcbab766c0bb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 137, "real_page_number": null, "text": "5.6 The Iterative Substitution Method\n125\nTo solve this problem, first convert the function into a recurrence relation. This recurrence relation is:\n𝑇(𝑛)=\n{\nΘ(1),\nif 𝑛=1\n2𝑇(𝑛−1)+Θ(1),\nif 𝑛>1\nFrom now on, we will substitute with the constant 1. This simplifies our math without changing the result of our analysis.Θ(1)\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n2𝑇(𝑛−1)+1,\nif 𝑛>1\nNow, let’s follow the procedure specified above.\n(𝑇(𝑛−1), 𝑇(𝑛−2),Step 1: Write out the recursive terms etc.) as their own recurrence relations and substitute their equations into the original\n𝑇(𝑛) formula at each step.\nHere, the base case is 1, so we want to convert into a form that only contains the recursive term 𝑇(1). Assuming that 𝑛is not𝑇(1) 2𝑇(𝑛−1)=\nalready 1, we know from our recurrence relation that\n𝑇(𝑛) 2𝑇(𝑛−1)+1=\nUsing the definition of our recurrence relation, we can replace 𝑛with to get an expression for 𝑇(𝑛−1).𝑛−1\n𝑇(𝑛−1) 2𝑇((𝑛−1)−1)+1 2𝑇(𝑛−2)+1= =\nPlugging for into the first equation gives us2𝑇(𝑛−2)+1 𝑇(𝑛−1)\n𝑇(𝑛) 2𝑇(𝑛−1)+1 2×[2𝑇(𝑛−2)+1]= =\n⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟\n𝑇(𝑛−1)\n2×2×𝑇(𝑛−2)+2+1+1 =\nNow, we have written in terms of instead of 𝑇(𝑛−1). This process is then repeated to decrement the input size of the recursive𝑇(𝑛) 𝑇(𝑛−2)\ncall until the base case of is reached. Let’s do one more step. Using the definition of our recurrence relation, we can write as𝑇(1) 𝑇(𝑛−2)\n𝑇(𝑛−2) 2𝑇(𝑛−3)+1=\nPlugging this into our current recurrence relation, we get\n𝑇(𝑛) 2×2×𝑇(𝑛−2)+2+1 2×2×[2𝑇(𝑛−3)+1]= =\n⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟\n𝑇(𝑛−2)\n2×2×2×𝑇(𝑛−3)+4+2+1+2+1 =\nRepeating this substitution process for the first four steps will give you the following results:\nStep #\nRecurrence Equation\n1\n𝑇(𝑛) 2×𝑇(𝑛−1)+1=\n2\nRewrite as𝑇(𝑛−1) 2×𝑇(𝑛−2)+1\nSubstitute in for in the previous recurrence equation2×𝑇(𝑛−2)+1 𝑇(𝑛−1)\n𝑇(𝑛) 2×𝑇(𝑛−1)+1=\n→𝑇(𝑛) 2×2×𝑇(𝑛−2)+2+1=\n3\nRewrite as𝑇(𝑛−2) 2×𝑇(𝑛−3)+1\nSubstitute in for in the previous recurrence equation2×𝑇(𝑛−3)+1 𝑇(𝑛−2)\n𝑇(𝑛) 2×2×𝑇(𝑛−2)+2+1=\n→𝑇(𝑛) 2×2×2×𝑇(𝑛−3)+4+2+1=\n4\nRewrite as𝑇(𝑛−3) 2×𝑇(𝑛−4)+1\nSubstitute in for in the previous recurrence equation2×𝑇(𝑛−4)+1 𝑇(𝑛−3)\n𝑇(𝑛) 2×2×2×𝑇(𝑛−3)+4+2+1=\n→𝑇(𝑛) 2×2×2×2×𝑇(𝑛−4)+8+4+2+1=\nAfter substituting for a few iterations, we can move on to the next step.\n𝑘th𝑇(𝑛)Step 2: Look for a pattern that describes at the step, and express it using a summation formula.\nWith the first 4 steps, we have enough information to describe a pattern for 𝑇(𝑛).\n• At the first step, 𝑇(𝑛) 2×𝑇(𝑛−1)+1=\n• At the second step, 𝑇(𝑛) 2×2×𝑇(𝑛−2)+2+1=\n• At the third step, 𝑇(𝑛) 2×2×2×𝑇(𝑛−3)+4+2+1=\n• At the fourth step, 𝑇(𝑛) 2×2×2×2×𝑇(𝑛−4)+8+4+2+1=", "word_count": 423, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a2be1700-5d74-5cfc-ae61-f62ec90a8e3d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 138, "real_page_number": null, "text": "126\nChapter 5. Recursion and Recurrence Relations\n𝑘thWe can generalize this and derive the following formula for at the step:𝑇(𝑛)\n2𝑘𝑇(𝑛−𝑘)+𝑇(𝑛)=\n𝑘−1\n∑\n𝑖=0\n2𝑖\n2𝑘𝑇(𝑛−𝑘)+∑𝑘−1This equation states that the recurrence relation is identical to𝑇(𝑛) 2𝑇(𝑛−1)+1 𝑇(𝑛)= =\n𝑘th2𝑖at the step of substitution.𝑖=0\n𝑘such 𝑇(𝑛).Step 3: Solve for that the base case is the only recursive term that is present on the right-hand side of the equation for Determine\nthe closed form solution by replacing instances of the base case with the value of the base case.\n2𝑘𝑇(𝑛−𝑘)+∑𝑘−1We determined that can be rewritten as𝑇(𝑛) 2𝑇(𝑛−1)+1 𝑇(𝑛)= =\n𝑘th2𝑖at the step of substitution. Since we want the𝑖=0\nbase case to be the only recursive term present on the right-hand side of the equation, we want to select a 𝑘such that equals the base𝑇(𝑛−𝑘)\ncase of 𝑇(1). This happens when 1, or when 𝑛−1. Substituting for 𝑘in our equation gives us an expression for where𝑛−𝑘= 𝑘= 𝑛−1 𝑇(𝑛)\nis the only recursive term on the right-hand side.𝑇(1)\n2𝑘𝑇(𝑛−𝑘)+𝑇(𝑛)=\n𝑘−1\n∑\n𝑖=0\n2𝑖\n(substitute for 𝑘)𝑛−1\n2𝑛−1𝑇(𝑛−(𝑛−1))+=\n(𝑛−1)−1\n∑\n𝑖=0\n2𝑖\n2𝑛−1𝑇(1)+=\n𝑛−2\n∑\n𝑖=0\n2𝑖\nWe know from the base case that 1. Therefore, we can replace in the equation with 1, as follows:𝑇(1) 𝑇(1)=\n2𝑛−1𝑇(1)+𝑇(𝑛)=\n𝑛−2\n∑\n𝑖=0\n2𝑖\n2𝑛−1= ×1+\n𝑛−2\n∑\n𝑖=0\n2𝑖\n2𝑛−1= +\n𝑛−2\n∑\n𝑖=0\n2𝑖\nWe now have a closed form solution to the original recurrence relation without any recursive terms on the right-hand side! Using the provided\nsummation identity,\n𝑛\n∑\n𝑖=𝑚\n𝑎(𝑟𝑚−𝑟𝑛+1)𝑎𝑟𝑖=\n1−𝑟\nwhere 1, 2, 0, and 𝑛−2, we can show that𝑎= 𝑟= 𝑚= 𝑛=\n𝑛−2\n∑\n𝑖=0\n−2𝑛−1)1(202𝑖=\n1−2\n2𝑛−1= −1\nPlugging this back in our equation for gives us𝑇(𝑛)\n2𝑛−1𝑇(𝑛)= +\n𝑛−2\n∑\n𝑖=0\n2𝑖\n2𝑛−1+2𝑛−1= −1\n2×2𝑛−1= −1\n2𝑛−1=\n2𝑛−1, Θ(2𝑛).foo()Since the time complexity of the function is therefore𝑇(𝑛)=", "word_count": 345, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7443e5c6-88bd-53dc-8885-4319150cdaab", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 139, "real_page_number": null, "text": "5.6 The Iterative Substitution Method\n127\n𝑛≥0.Example 5.6 foo()Express the time complexity of the function using big-O notation, in terms of input size 𝑛. Assume\n1\nvoid foo(int32_t n) {\n2\nif (n == 0) {\n3\nreturn;\n4\n} // if\n5\nfor (int32_t i = 1; i < n; i *= 2) {\n6\nstd::cout << \"281\" << std::endl;\n7\n} // for i\n8\nfoo(n - 1);\n9\n} // foo()\nforJust by looking at this code, you may recognize that the time complexity is Θ(𝑛log(𝑛)), since the loop runs times, and a total oflog(𝑛) 𝑛\nrecursive calls are made. However, let’s do some substitution to prove that this is indeed the case.\nforFirst, let’s express this function as a recurrence relation. If 0, the base case executes, which takes constant time. Otherwise, the𝑛=\niloop and recursive call run. Since the value of is doubled with each iteration of the loop, the loop executes the print statement times.log(𝑛)\nThe recursive call can be denoted as 𝑇(𝑛−1), since its input size is 𝑛−1. Thus, the recurrence relation for this function is:\n𝑇(𝑛)=\n{\n1,\nif 𝑛=0\n𝑇(𝑛−1)+log(𝑛),\nif 𝑛>0\nNow, we will use the same procedure as above to compute the complexity of this recurrence relation. Note: Technically, it is more accurate to\n𝑇(𝑛) 𝑛> 𝑇(𝑛−1)+log(𝑛)+1,describe the runtime of for as since there is still some constant time work that is done (e.g., multiplication,0\nlog(𝑛)assignment, etc.). However, since the constant term is clearly a lower-order term that is dominated by the work, it is okay to ignore this\nconstant term altogether since it will not make a difference in the final complexity class.\n(𝑇(𝑛−1), 𝑇(𝑛−2),Step 1: Write out the recursive terms etc.) as their own recurrence relations and substitute their equations into the original\n𝑇(𝑛) formula at each step.\nStep #\nRecurrence Equation\n1\n𝑇(𝑛) 𝑇(𝑛−1)+log(𝑛)=\n2\nRewrite as𝑇(𝑛−1) 𝑇(𝑛−2)+log(𝑛−1)\nSubstitute in for in the previous recurrence equation𝑇(𝑛−2)+log(𝑛−1) 𝑇(𝑛−1)\n𝑇(𝑛) 𝑇(𝑛−1)+log(𝑛)=\n→𝑇(𝑛) 𝑇(𝑛−2)+log(𝑛−1)+log(𝑛)=\n3\nRewrite as𝑇(𝑛−2) 𝑇(𝑛−3)+log(𝑛−2)\nSubstitute in for in the previous recurrence equation𝑇(𝑛−3)+log(𝑛−2) 𝑇(𝑛−2)\n𝑇(𝑛) 𝑇(𝑛−2)+log(𝑛−1)+log(𝑛)=\n→𝑇(𝑛) 𝑇(𝑛−3)+log(𝑛−2)+log(𝑛−1)+log(𝑛)=\nNote: If you want to write a recurrence relation for T(n −1), you must replace ALL instances of with (n −1), even the termsn\noutside the recursive call! A common mistake is only substituting the 𝑛in the recursive term.\nCorrect: 𝑇(𝑛−1) 𝑇(𝑛−2)+log(𝑛−1)=\nIncorrect: 𝑇(𝑛−1) 𝑇(𝑛−2)+log(𝑛)=\nOnce you are able to recognize a pattern, you can continue to the next step.\n𝑘th𝑇(𝑛)Step 2: Look for a pattern that describes at the step, and express it using a summation formula.\nWith the first few steps, we have enough information to describe a pattern for 𝑇(𝑛).\n• At the first step, 𝑇(𝑛) 𝑇(𝑛−1)+log(𝑛)=\n• At the second step, 𝑇(𝑛) 𝑇(𝑛−2)+log(𝑛−1)+log(𝑛)=\n• At the third step, 𝑇(𝑛) 𝑇(𝑛−3)+log(𝑛−2)+log(𝑛−1)+log(𝑛)=\n• At the fourth step, 𝑇(𝑛) 𝑇(𝑛−4)+log(𝑛−3)+log(𝑛−2)+log(𝑛−1)+log(𝑛)=\n𝑘thWe can generalize this and derive the following formula for at the step:𝑇(𝑛)\n𝑇(𝑛) 𝑇(𝑛−𝑘)+=\n𝑘−1\n∑\n𝑖=0\nlog(𝑛−𝑖)\n𝑘such 𝑇(𝑛).Step 3: Solve for that the base case is the only recursive term that is present on the right-hand side of the equation for Determine\nthe closed form solution by replacing instances of the base case with the value of the base case.\n𝑇(𝑛−𝑘)+∑𝑘−1We determined that can be rewritten as𝑇(𝑛) 𝑇(𝑛−1)+log(𝑛) 𝑇(𝑛)= =\n𝑘that the step of substitution. Since we wantlog(𝑛−𝑖)𝑖=0\nthe base case to be the only recursive term present on the right-hand side of the equation, we want to select a 𝑘such that equals the𝑇(𝑛−𝑘)\nbase case of 𝑇(0). This happens when 0, or when 𝑛. Substituting 𝑛for 𝑘in our equation gives us an expression for where𝑛−𝑘= 𝑘= 𝑇(𝑛) 𝑇(0)\nis the only recursive term on the right-hand side.\n𝑇(𝑛) 𝑇(0)+=\n𝑛−1\n∑\n𝑖=0\nlog(𝑛−𝑖)", "word_count": 656, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "99397307-1b28-51d5-8b2d-f81df71d3ce9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 140, "real_page_number": null, "text": "128\nChapter 5. Recursion and Recurrence Relations\nSince we know that 1, we can plug this into the above equation.𝑇(0)=\n𝑇(𝑛)=1+\n𝑛−1\n∑\n𝑖=0\nlog(𝑛−𝑖)\nUsing log rules, we know that the sum of log terms is equal to the log of the product of the terms; that is, log(𝑎) log(𝑏) log(𝑧)+ +…+ =\nlog(𝑎×𝑏×…×𝑧).\n𝑇(𝑛) 1+log(𝑛×(𝑛−1)×(𝑛−2)×…×2×1)=\n1+log(𝑛!)=\nAfter dropping lower order terms, we can conclude that Θ(log(𝑛!)). We proved in chapter 4 that Θ(𝑛log(𝑛)), so the time𝑇(𝑛) log(𝑛!)= =\nfoo()complexity of the function is indeed Θ(𝑛log(𝑛)).\n𝑛≥0.Example 5.7 foo()Express the time complexity of the function using big-O notation, in terms of input size 𝑛. Assume\n1\nint64_t foo(int64_t n) {\n2\nint64_t s = 1;\n3\nif (n == 0) {\n4\nreturn s;\n5\n} // if\n6\nfor (int64_t i = 1; i <= n; ++i) {\n7\ns += foo(n - 1);\n8\n} // for i\n9\nreturn s;\n10\n} // foo()\nYou may find the following identity helpful with your calculations:\n𝑛\n∑\n𝑖=1\n𝑛!\n⌊𝑛!𝑒⌋−1, where 𝑒≈2.71828…=(𝑛−𝑖)!\nforFirst, convert this function to a recurrence relation. If 0, we have a base case that runs in constant time. If 0, we run the loop,𝑛= 𝑛>\nfoo(n - 1)which runs a recursive call to and performs some constant time work 𝑛times. The recurrence relation is therefore\n𝑇(𝑛)=\n{\n1,\nif 𝑛=0\n𝑛𝑇(𝑛−1)+𝑛,\nif 𝑛>0\n(𝑇(𝑛−1), 𝑇(𝑛−2),Step 1: Write out the recursive terms etc.) as their own recurrence relations and substitute their equations into the original\n𝑇(𝑛) formula at each step.\nStep #\nRecurrence Equation\n1\n𝑇(𝑛) 𝑛𝑇(𝑛−1)+𝑛=\n2\nRewrite as𝑇(𝑛−1) (𝑛−1)𝑇(𝑛−2)+(𝑛−1)\nSubstitute in for in the previous recurrence equation(𝑛−1)𝑇(𝑛−2)+(𝑛−1) 𝑇(𝑛−1)\n𝑇(𝑛) 𝑛𝑇(𝑛−1)+𝑛=\n→𝑇(𝑛) 𝑛[(𝑛−1)𝑇(𝑛−2)+(𝑛−1)]+𝑛=\n→𝑇(𝑛) 𝑛(𝑛−1)𝑇(𝑛−2)+𝑛(𝑛−1)+𝑛=\n3\nRewrite as𝑇(𝑛−2) (𝑛−2)𝑇(𝑛−3)+(𝑛−2)\nSubstitute in for in the previous recurrence equation(𝑛−2)𝑇(𝑛−3)+(𝑛−2) 𝑇(𝑛−2)\n𝑇(𝑛) 𝑛(𝑛−1)𝑇(𝑛−2)+𝑛(𝑛−1)+𝑛=\n→𝑇(𝑛) 𝑛(𝑛−1)[(𝑛−2)𝑇(𝑛−3)+(𝑛−2)]+𝑛(𝑛−1)+𝑛=\n→𝑇(𝑛) 𝑛(𝑛−1)(𝑛−2)𝑇(𝑛−3)+𝑛(𝑛−1)(𝑛−2)+𝑛(𝑛−1)+𝑛=\n𝑘th𝑇(𝑛)Step 2: Look for a pattern that describes at the step, and express it using a summation formula.\nWith the first few steps, we have enough information to describe a pattern for 𝑇(𝑛).\n• At the first step, 𝑇(𝑛) 𝑛𝑇(𝑛−1)+𝑛=\n• At the second step, 𝑇(𝑛) 𝑛(𝑛−1)𝑇(𝑛−2)+𝑛(𝑛−1)+𝑛=\n• At the third step, 𝑇(𝑛) 𝑛(𝑛−1)(𝑛−2)𝑇(𝑛−3)+𝑛(𝑛−1)(𝑛−2)+𝑛(𝑛−1)+𝑛=\n𝑘thWe can generalize this and derive the following formula for at the step:𝑇(𝑛)\n𝑇(𝑛)=\n𝑛!\n(𝑛−𝑘)!𝑇(𝑛−𝑘)+\n𝑘\n∑\n𝑖=1\n𝑛!\n(𝑛−𝑖)!", "word_count": 418, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "142238c0-f2bc-540f-b8c1-8d8f437e34e9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 141, "real_page_number": null, "text": "5.6 The Iterative Substitution Method\n129\n𝑘such 𝑇(𝑛).Step 3: Solve for that the base case is the only recursive term that is present on the right-hand side of the equation for Determine\nthe closed form solution by replacing instances of the base case with the value of the base case.\nSince we want the base case to be the only recursive term present on the right-hand side of the equation, we want to select a 𝑘such that 𝑇(𝑛−𝑘)\nequals the base case of 𝑇(0). This happens when 0, or 𝑛. Substituting 𝑛for 𝑘gives us an expression for where is the only𝑛−𝑘= 𝑘= 𝑇(𝑛) 𝑇(0)\nrecursive term on the right-hand side. We can then plug in into the equation and use the provided identity to simplify the equation.𝑇(0)=1\n𝑇(𝑛) =\n𝑛!\n(𝑛−𝑛)!𝑇(𝑛−𝑛)+\n𝑛\n∑\n𝑖=1\n𝑛!\n𝑛!𝑇(0)+=(𝑛−𝑖)!\n𝑛\n∑\n𝑖=1\n𝑛!\n𝑛!+=(𝑛−𝑖)!\n𝑛\n∑\n𝑖=1\n𝑛!\n⌊𝑛!(𝑒+1)⌋−1=(𝑛−𝑖)!\n⌊𝑛!(𝑒+1)⌋−1 foo()Since ≈3.71828𝑛!−1, the time complexity of the function is Θ(𝑛!).𝑇(𝑛)=\n𝑛≥1.Example 5.8 foo()Express the time complexity of the function using big-O notation, in terms of input size 𝑛. Assume\n1\nvoid foo(int32_t n) {\n2\nif (n == 1) {\n3\nreturn;\n4\n} // if\n5\nfoo(n / 2);\n6\nfor (int32_t i = 0; i < n; ++i) {\n7\nstd::cout << \"281\" << std::endl;\n8\n} // for i\n9\nfoo(n / 2);\n10\n} // foo()\nFirst, convert this function to a recurrence relation. If 1, we have a base case that runs in constant time. If 1, we make two recursive𝑛= 𝑛>\ncalls and print \"281\" 𝑛times. The recurrence relation is therefore\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n2𝑇(𝑛∕2)+𝑛,\nif 𝑛>1\n𝑇(𝑛)Step 1: Write out the recursive terms as recurrence relations and substitute their equations into the original formula at each step.\nStep #\nRecurrence Equation\n1\n𝑇(𝑛) 2𝑇(𝑛∕2)+𝑛=\n2\nRewrite as𝑇(𝑛∕2) 2𝑇(𝑛∕4)+𝑛∕2\nSubstitute in for in the previous recurrence equation2𝑇(𝑛∕4)+𝑛∕2 𝑇(𝑛∕2)\n𝑇(𝑛) 2𝑇(𝑛∕2)+𝑛=\n→𝑇(𝑛) 2(2𝑇(𝑛∕4)+𝑛∕2)+𝑛=\n→𝑇(𝑛) 4𝑇(𝑛∕4)+2𝑛=\n3\nRewrite as𝑇(𝑛∕4) 2𝑇(𝑛∕8)+𝑛∕4\nSubstitute in for in the previous recurrence equation2𝑇(𝑛∕8)+𝑛∕4 𝑇(𝑛∕4)\n𝑇(𝑛) 4𝑇(𝑛∕4)+2𝑛=\n→𝑇(𝑛) 4(2𝑇(𝑛∕8)+𝑛∕4)+2𝑛=\n→𝑇(𝑛) 8𝑇(𝑛∕8)+3𝑛=\n𝑘th𝑇(𝑛)Step 2: Look for a pattern that describes at the step, and express it using a summation formula.\nWith the first few steps, we have enough information to describe a pattern for 𝑇(𝑛).\n• At the first step, 𝑇(𝑛) 2𝑇(𝑛∕2)+𝑛=\n• At the second step, 𝑇(𝑛) 4𝑇(𝑛∕4)+2𝑛=\n• At the third step, 𝑇(𝑛) 8𝑇(𝑛∕8)+3𝑛=\n𝑘thWe can generalize this and derive the following formula for at the step:𝑇(𝑛)\n2𝑘𝑇𝑇(𝑛) =\n( 𝑛\n2𝑘\n)\n+𝑘𝑛\n𝑘such 𝑇(𝑛).Step 3: Solve for that the base case is the only recursive term that is present on the right-hand side of the equation for Determine\nthe closed form solution by replacing instances of the base case with the value of the base case.\n𝑇(𝑛∕2𝑘)Since we want the base case to be the only recursive term present on the right-hand side of the equation, we want to select a 𝑘such that\n2𝑘=equals the base case of 𝑇(1). This happens when 𝑛, or log2(𝑛). Substituting for 𝑘in our equation gives us an expression for𝑘= log2(𝑛)\nwhere is the only recursive term on the right-hand side. We can then plug in into the equation.𝑇(𝑛) 𝑇(1) 𝑇(1)=1\n2log2(𝑛)𝑇𝑇(𝑛) =\n(\n𝑛\n2log2(𝑛)\n)\n+𝑛log2(𝑛) 𝑛𝑇(1)+𝑛log2(𝑛) 𝑛+𝑛log2(𝑛)= =\nSince is the higher-order term, the time complexity of the function is Θ(𝑛log(𝑛)).𝑛log2(𝑛)\n𝑇(⌊𝑛∕2⌋)Remark: By convention, we only define 𝑇with integer arguments, so usually represents if 𝑛does not divide evenly.𝑇(𝑛∕2)\nHowever, if we only care about finding a big-O bound for a recurrence relation, this rounding should not make a difference. As a result, the\nfloor operation is usually implicitly assumed if division occurs in the recursive term.", "word_count": 656, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d95a3c14-e470-513e-846a-385c5f84f146", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 142, "real_page_number": null, "text": "130\nChapter 5. Recursion and Recurrence Relations\n5.7\nThe Master Theorem\n¸ 5.7.1\nApplying the Master Theorem\nThe Master Theorem provides a template that can be used to compute the time complexity of any recurrence relation that follows a given\nformat. The theorem is defined below:\nThe Master Theorem:\nIf the recurrence relation of an algorithm is of the form:\n𝑇(𝑛) 𝑎𝑇=\n(𝑛\n𝑏\n)\n+𝑓(𝑛)\n𝑎≥1, Θ(𝑛𝑐),where 1, and is a positive function that is then the following is true:𝑏> 𝑓(𝑛)\nΘ(𝑛log𝑏(𝑎)).𝑏𝑐,• If the complexity of is𝑎> 𝑇(𝑛)\n𝑏𝑐, Θ(𝑛𝑐log(𝑛)).• If the complexity of is𝑎= 𝑇(𝑛)\n𝑏𝑐, Θ(𝑛𝑐).• If the complexity of is𝑎< 𝑇(𝑛)\nMore formally,\n𝑇(𝑛)=\n⎧\n⎪\n⎨\n⎪⎩\nΘ(𝑛log𝑏(𝑎)),\n𝑏𝑐if 𝑎>\nΘ(𝑛𝑐log(𝑛)),\n𝑏𝑐if 𝑎=\nΘ(𝑛𝑐),\n𝑏𝑐if 𝑎<\nNotes:\n• There must exist a base case that is solvable in constant time.\n• The value 𝑎represents the number of times a recursive call is made, the value 𝑏represents the factor that the input is divided by with\neach recursive call, and the value 𝑐represents the highest power of the polynomial term 𝑓(𝑛).\n• The values of 𝑎and 𝑏cannot depend on 𝑛.\n• The Master Theorem can only be used if all these conditions are met.\nThe steps for using the Master Theorem are as follows:\n1. Determine the values of 𝑎, 𝑏, and 𝑐.\n2. Make sure that the Master Theorem can be used on the recurrence relation.\n• The coefficient of the recursive call, 𝑎, must be at least one.\n• The argument of the recursive call must be divided by some number, 𝑏, that is larger than one.\nΘ(𝑛𝑐).• The function must be an asymptotically positive function with complexity𝑓(𝑛)\n• There exists a base case that can be solved in constant time.\n𝑏𝑐to3. Compare the values of 𝑎and determine which case of the Master Theorem should be used.\nExample 5.9 The runtime of an algorithm can be expressed using the following recurrence relation. Can the Master Theorem be used, and\nif so, what is the time complexity of this algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n3𝑇(𝑛∕2)+5𝑛+13,\nif 𝑛>1\n1. Determine the values of a, b, and c.\nThe value of 𝑎is 3 because that is the coefficient of the recursive term. The value of 𝑏is 2 because that is the denominator of the recursive term.\nΘ(𝑛1).The value of 𝑐is 1 because 𝑓(𝑛) 5𝑛+13= =\n2. Make sure that the Master Theorem can be used on the recurrence relation.\n≥1, 1, and is part of a polynomial complexity class. The structure is correct, and there exists a base case that can be𝑎= 𝑏= > 5𝑛+133 2\nsolved in constant time. Thus, the Master Theorem can be used on the recurrence relation.\nbc3. Compare the values of and to determine which case of the Master Theorem should be used.a\nΘ(𝑛log𝑏(𝑎)) Θ(𝑛log2(3))≈Θ(𝑛1.58).𝑏𝑐is 𝑏𝑐,21The value of 𝑎is 3. The value of 2. Because the time complexity of the algorithm is𝑎>= =", "word_count": 514, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f77342e9-a782-56d6-b6ab-d734c61b436d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 143, "real_page_number": null, "text": "5.7 The Master Theorem\n131\nExample 5.10 The runtime of an algorithm can be expressed using the following recurrence relation. Can the Master Theorem be used,\nand if so, what is the time complexity of this algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n4𝑇(𝑛∕2)+5𝑛+13𝑛2,\nif 𝑛>1\n1. Determine the values of a, b, and c.\nThe value of 𝑎is 4 because that is the coefficient of the recursive term. The value of 𝑏is 2 because that is the denominator of the recursive term.\n5𝑛+13𝑛2 Θ(𝑛2).The value of 𝑐is 2 because 𝑓(𝑛)= =\n2. Make sure that the Master Theorem can be used on the recurrence relation.\n≥1, 5𝑛+13𝑛21, and is part of a polynomial complexity class. The structure is correct, and there exists a base case that can be𝑎= 𝑏= >4 2\nsolved in constant time. Thus, the Master Theorem can be used on the recurrence relation.\nbc3. Compare the values of and to determine which case of the Master Theorem should be used.a\nΘ(𝑛2log(𝑛)).𝑏𝑐is 𝑏𝑐, Θ(𝑛𝑐log(𝑛))22The value of 𝑎is 4. The value of 4. Because the time complexity of the algorithm is𝑎== =\nExample 5.11 The runtime of an algorithm can be expressed using the following recurrence relation. Can the Master Theorem be used,\nand if so, what is the time complexity of this algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n6𝑇(𝑛∕2)+5𝑛+2𝑛,\nif 𝑛>1\n5𝑛+2𝑛=Θ(2𝑛).Here, This is not a polynomial complexity class, so the Master Theorem cannot be used to compute the complexity of𝑓(𝑛)=\nthis recurrence relation.\nExample 5.12 The runtime of an algorithm can be expressed using the following recurrence relation. Can the Master Theorem be used,\nand if so, what is the time complexity of this algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n6𝑇(𝑛∕2)+3𝑇(𝑛∕5)+4𝑛+3,\nif 𝑛>1\nNo, this recurrence relation is not in the correct format. The Master Theorem cannot be used on a recurrence relation with two different recursive\ncalls with differing values of 𝑏.\nExample 5.13 The runtime of an algorithm can be expressed using the following recurrence relation. Can the Master Theorem be used,\nand if so, what is the time complexity of this algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n𝑇(𝑛∕4)+𝑛\n√\n𝑛,\nif 𝑛>1\n1. Determine the values of a, b, and c.\nThe value of 𝑎is 1 because that is the coefficient of the recursive term. The value of 𝑏is 4 because that is the denominator of the recursive term.\nThe value of 𝑐is 3/2 because 𝑓(𝑛) 𝑛=\n√\n𝑛(𝑛1∕2) Θ(𝑛3∕2).𝑛= =\n2. Make sure that the Master Theorem can be used on the recurrence relation.\n≥1, 𝑛3∕21, and is part of a polynomial complexity class. The format is correct, and there exists a base case that can be solved in𝑎= 𝑏= >1 4\nconstant time. Thus, the Master Theorem can be used on the recurrence relation.\nbc3. Compare the values of and to determine which case of the Master Theorem should be used.a\n𝑏𝑐is 𝑏𝑐, Θ(𝑛𝑐)43∕2 Θ(𝑛3∕2).The value of 𝑎is 1. The value of 8. Because the time complexity of the algorithm is𝑎<= =", "word_count": 540, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "245b0673-de8e-509b-b913-d760234056fe", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 144, "real_page_number": null, "text": "132\nChapter 5. Recursion and Recurrence Relations\nExample 5.14 The runtime of an algorithm can be expressed using the following recurrence relation. Can the Master Theorem be used,\nand if so, what is the time complexity of this algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n5𝑇(𝑛∕8)+15,\nif 𝑛>1\n1. Determine the values of a, b, and c.\nThe value of 𝑎is 5 because that is the coefficient of the recursive term. The value of 𝑏is 8 because that is the denominator of the recursive term.\n15𝑛0 Θ(𝑛0)The value of 𝑐is 0 because Θ(1).𝑓(𝑛)=15= = =\n2. Make sure that the Master Theorem can be used on the recurrence relation.\n≥1, 𝑛01, and is part of a polynomial complexity class. The structure is correct, and there exists a base case that can be solved in𝑎= 𝑏= >5 8\nconstant time. Thus, the Master Theorem can be used on the recurrence relation.\nbc3. Compare the values of and to determine which case of the Master Theorem should be used.a\nΘ(𝑛log𝑏(𝑎)) Θ(𝑛log8(5))≈Θ(𝑛0.774).𝑏𝑐is 𝑏𝑐,80The value of 𝑎is 5. The value of 1. Because the time complexity of the algorithm is𝑎>= =\nExample 5.15 The runtime of an algorithm can be expressed using the following recurrence relation. Can the Master Theorem be used,\nand if so, what is the time complexity of this algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n7𝑇(10𝑛∕11)+9𝑛,\nif 𝑛>1\n1. Determine the values of a, b, and c.\nThe value of 𝑎is 7 because that is the coefficient of the recursive term. The value of 𝑏is 11/10 because that is the denominator of the recursive\nΘ(𝑛1).term (since can be written as 𝑛∕(11∕10)). The value of 𝑐is 1 because10𝑛∕11 𝑓(𝑛) 9𝑛==\n2. Make sure that the Master Theorem can be used on the recurrence relation.\n≥1, 𝑛11, and is part of a polynomial complexity class. The structure is correct, and there exists a base case that can be solved𝑏=1.1>𝑎=7\nin constant time. Thus, the Master Theorem can be used on the recurrence relation.\nbc3. Compare the values of and to determine which case of the Master Theorem should be used.a\nΘ(𝑛log𝑏(𝑎)) Θ(𝑛log1.1(7))≈Θ(𝑛20.417)𝑏𝑐is 𝑏𝑐,1.11Thevalueof𝑎is7. Thevalueof 1.1. Because thetimecomplexityofthealgorithmis𝑎>= =\n.\nExample 5.16 The is a fast multiplication algorithm that can be used to multiply two 𝑛-digit numbers faster thanKaratsuba algorithm\nthe classical multiplication algorithm often taught in grade school. This algorithm is recursive and can be expressed using the following\nrecurrence relation. What is the time complexity of the Karatsuba algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n3𝑇(𝑛∕2)+Θ(𝑛),\nif 𝑛>1\n1. Determine the values of a, b, and c.\nThe value of 𝑎is 3 because that is the coefficient of the recursive term. The value of 𝑏is 2 because that is the denominator of the recursive term.\nΘ(𝑛1).The value of 𝑐is 1 because 𝑓(𝑛)=\n2. Make sure that the Master Theorem can be used on the recurrence relation.\n≥1, 𝑛11, and is part of a polynomial complexity class. The structure is correct, and there exists a base case that can be solved in𝑎= 𝑏= >3 2\nconstant time. Thus, the Master Theorem can be used on the recurrence relation.\nbc3. Compare the values of and to determine which case of the Master Theorem should be used.a\nΘ(𝑛log𝑏(𝑎)) Θ(𝑛log2(3))≈Θ(𝑛1.585).𝑏𝑐is 𝑏𝑐,21The value of 𝑎is 3. The value of 2. Because the time complexity of the algorithm is𝑎>= =", "word_count": 618, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1b1704fc-c9e3-5980-b051-3a0fcbf705db", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 145, "real_page_number": null, "text": "5.7 The Master Theorem\n133\nExample 5.17 In linear algebra, the is an algorithm that can be used to multiply two matrices of size 𝑛×𝑛faster thanStrassen algorithm\nthe standard matrix multiplication algorithm. This algorithm is recursive and can be expressed using the following recurrence relation.\nWhat is the time complexity of the Strassen algorithm?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n7𝑇(𝑛∕2)+Θ(𝑛2),\nif 𝑛>1\n1. Determine the values of a, b, and c.\nThe value of 𝑎is 7 because that is the coefficient of the recursive term. The value of 𝑏is 2 because that is the denominator of the recursive term.\nΘ(𝑛2).The value of 𝑐is 2 because 𝑓(𝑛)=\n2. Make sure that the Master Theorem can be used on the recurrence relation.\n≥1, 𝑛21, and is part of a polynomial complexity class. The structure is correct, and there exists a base case that can be solved in𝑎= 𝑏= >7 2\nconstant time. Thus, the Master Theorem can be used on the recurrence relation.\nbc3. Compare the values of and to determine which case of the Master Theorem should be used.a\nΘ(𝑛log𝑏(𝑎)) Θ(𝑛log2(7))≈Θ(𝑛2.807).𝑏𝑐is 𝑏𝑐,22 4. Because the time complexity of the algorithm isThe value of 𝑎is 7. The value of 𝑎>= =\n¸ 5.7.2\n(✽)Deriving the Master Theorem\nTo provide intuition into why the Master Theorem works, we can visualize the total work done by a recurrence relation using a recursion tree. A\nrecursion tree is a tree can be used to illustrate the work done by a recursive algorithm, where each branch represents a recursive call, and each\nnode represents the total amount of additional work that is done at that recursive call.\nRemark: Recursion trees gives us a way to visualize the total work that is done by a recursive algorthm. Each node of a recurrence tree\nrepresents a single recursive call, and it stores the work that is done at that recursive call. For example, if a recursive algorithm does 10 units\nof work and then makes two recursive calls, one that does 20 units of work, and one that does 30 units of work, we can visualize the total\nwork done using the following recurrence tree:\n10\n30\n20\nThe sum of all the nodes in the recursion tree represents the total work that is completed by the recursive algorithm. In the example above, a\ntotal of 10 + 20 + 30 = 60 units of work are done after the recursive algorithm runs to completion.\nGiven a recurrence relation, we can build a recursion tree by repeating the following steps until we reach the base case:\n1. At each level, each node is assigned the total amount of work that is done outside the recursive call(s).\n2. Each recursive call creates a branch to the next level of the tree.\nFor example, consider the recurrence relation 2𝑇(𝑛∕2)+𝑛. This recurrence relation calls itself twice with half the input size and does𝑇(𝑛)=\nan additional 𝑛work. With a recursion tree approach, we can visualize the first recursive call as follows:\n𝑛\n𝑇\n(\n𝑛\n2\n)\n𝑇\n(\n𝑛\n2\n)\nHow much work do these recursive calls to do? We know from our recurrence relation that 𝑛∕2, where a𝑇(𝑛∕2) 𝑇(𝑛∕2) 2𝑇(𝑛∕4)= +\nrecursive call with an input size of does work and makes two recursive calls, each with input size 𝑛∕4. We can use this to extend our𝑛∕2 𝑛∕2\ntree down another level.\n𝑛\n𝑛\n2\n𝑇\n(\n𝑛\n4\n)\n𝑇\n(\n𝑛\n4\n)\n𝑛\n2\n𝑇\n(\n𝑛\n4\n)\n𝑇\n(\n𝑛\n4\n)\nBy repeating this process to the base case, we can construct the full recursion tree.", "word_count": 625, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d5c4bcc5-f3c0-5efc-8d66-d9554012c538", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 146, "real_page_number": null, "text": "134\nChapter 5. Recursion and Recurrence Relations\nLet’s look at the following three recurrence relations, each of which falls into a Master Theorem category:\n1. 𝑇(𝑛) 4𝑇(𝑛∕2)+𝑛=\n𝑏𝑐)(condition where 𝑎>\n2𝑇(𝑛∕2)+𝑛22. 𝑇(𝑛)=\n𝑏𝑐)(condition where 𝑎<\n4𝑇(𝑛∕2)+𝑛23. 𝑇(𝑛)=\n𝑏𝑐)(condition where 𝑎=\nIn the first recurrence relation, the algorithm calls itself four times with an input size of and does an additional 𝑛work. We can represent the𝑛∕2\ntotal work done by this recurrence relation using the following tree:\n𝑛\n𝑛\n2\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n2\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n2\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n2\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n4\n⋮\n𝑛\n4\n⋮\nRecursion Tree for 𝑇(𝑛) 4𝑇(𝑛∕2)+𝑛=\n𝑇(𝑛)\n𝑛\n𝑇(𝑛∕2)\n𝑛4(\n2𝑛2)=\n𝑇(𝑛∕4)\n𝑛16(\n4𝑛4)=\n…\n…\nLevel\nWork\nHere, \"level\" represents the recursion depth, and \"work\" represents the total work that is done at each level.\nYou might notice here that the amount of work increases at each recursion level. We call this tree \"bottom-heavy\" or \"leaf-heavy,\" since the\namount of work spent on the recursive subproblems (the term of the recurrence relation) dwarfs the amount of work spent on splitting𝑎𝑇(𝑛∕𝑏)\nand recombining the results of the recursive subproblems (the additional work represented by in the recurrence relation). Because of this,𝑓(𝑛)\nasymptotically all of the work of this recurrence relation is done on the lowest level of the tree (i.e., the leaves)! From the perspective of the\nMaster Theorem, the value of 𝑎is so large that the amount of work attributed to the sheer number of recursive calls we are making dominates\nover the additional work we have to do at each recursive call. This dominance can be shown by the fact that the total work at each level𝑓(𝑛)\n𝑛increases even as the work attributed to decreases as the input size is cut in half at each level (i.e.,𝑓(𝑛) 𝑛< 4(\n𝑛< 16(2)\n…).<4)\nHow much work is done at the last level? We know that the last level represents calls to the base case, so every node at the last level must do\na constant amount of work. Thus, the total amount of work that is done at the lowest level can be determined using the number of nodes at that\n4𝑖nodes,level. Since we are making four recursive calls at each level, the last level of the tree has where 𝑖is the total number of levels in the\ntree. We also know that the number of levels in the tree is equal to the number of recursive calls we need to make to go from 𝑛to our base case:\nsince the input size is cut in half at each level (from 𝑛to to 𝑛∕4…), a total of levels are needed before the base case is reached.𝑛∕2 log2(𝑛)\nThus, the number of nodes at the last level is\n4𝑖=4log2(𝑛)\nsince 𝑖, or the number of levels in the tree, is approximately equal to log2(𝑛).\nThis can actually be generalized to any value of 𝑎and 𝑏. The number of nodes at the last level of a recursion tree for a recurrence relation of\nthe form can be approximated as𝑎𝑇(𝑛∕𝑏)+𝑓(𝑛)\n𝑎log𝑏(𝑛)\nWe can use log rules to manipulate this expression into something we are more familiar with. Using the following identity:\n𝑛log𝑛(𝑎)𝑎=\n𝑛log𝑛(𝑎)we can substitute 𝑎with in the original equation:\n𝑎log𝑏(𝑛) =\n(\n𝑛log𝑛(𝑎))log𝑏(𝑛)\n𝑛log𝑛(𝑎)log𝑏(𝑛)=\nUsing the change of base formula, we can simplify the exponent term:\nlog(𝑎)log𝑛(𝑎)log𝑏(𝑛)=\nlog(𝑛)×log(𝑛)\nlog(𝑎)=log(𝑏)\nlog𝑏(𝑎)=log(𝑏)\nTherefore, the number of nodes at the last level of the tree can be expressed as\n𝑛log𝑛(𝑎)log𝑏(𝑛) 𝑛log𝑏(𝑎)=\nSince each node at the bottom level of the tree is a base case that does a constant amount of work, the total amount of work attributable to the\nΘ(𝑛log𝑏(𝑎)).𝑛log𝑏(𝑎)bottom level is ×𝑐for some constant 𝑐, or Because the work in a bottom-heavy tree is asymptotically performed at the\nΘ(𝑛log𝑏(𝑎)).lowest level, the overall complexity of the recurrence relation must also be This matches the result of the Master Theorem.", "word_count": 722, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "290cb3d3-4860-5c00-9b16-b45035bd2f09", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 147, "real_page_number": null, "text": "5.7 The Master Theorem\n135\n𝑏𝑐by 2𝑇(𝑛∕2)+𝑛2.Now, let’s look at the case where drawing out the recursion tree of In this recurrence relation, the algorithm𝑎< 𝑇(𝑛)=\n𝑛2calls itself twice with an input size of and does an additional work.𝑛∕2\n𝑛2\n(\n𝑛\n2\n)2\n(\n𝑛\n4\n)2\n⋮\n⋮\n(\n𝑛\n4\n)2\n⋮\n⋮\n(\n𝑛\n2\n)2\n(\n𝑛\n4\n)2\n⋮\n⋮\n(\n𝑛\n4\n)2\n⋮\n⋮\n2𝑇(𝑛∕2)+𝑛2Recursion Tree for 𝑇(𝑛)=\n𝑇(𝑛)\n𝑛2\n𝑇(𝑛∕2)\n𝑛22(\n𝑛2)=4\n2\n𝑇(𝑛∕4)\n𝑛24(\n𝑛216)=\n4\n…\n…\nLevel\nWork\nHere, we can see that the amount of work decreases at each recursion level. We call this tree \"top-heavy\" or \"root-heavy,\" since the amount of\nwork spent on splitting and recombining the results of the recursive subproblems (the additional work represented by in the recurrence𝑓(𝑛)\nrelation) dwarfs the amount of work spent on the recursive subproblems (the term of the recurrence relation). Because of this,𝑎𝑇(𝑛∕𝑏)\nasymptotically all of the work that is done at the top (or root) of the tree! From the Master Theorem perspective, the contribution of is so𝑓(𝑛)\nlarge that it dominates over the contribution of the recursive calls. Because dominates the runtime of the recurrence relation, the time𝑓(𝑛)\nΘ(𝑛𝑐).complexity of the entire recurrence relation must also be Θ(𝑓(𝑛)), or This matches the result of the Master Theorem.\nThere exists a point where the work spent on the recursive subproblems balances out the additional work spent on splitting and recombining\n𝑏𝑐. 4𝑇(𝑛∕2)+𝑛2:the results of the recursive subproblems. This happens in the case where Let’s look at the example of𝑎= 𝑇(𝑛)=\n𝑛2\n(\n𝑛\n2\n)2\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n2\n)2\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n2\n)2\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n2\n)2\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n(\n𝑛\n4\n)2\n⋮\n4𝑇(𝑛∕2)+𝑛2Recursion Tree for 𝑇(𝑛)=\n𝑇(𝑛)\n𝑛2\n𝑇(𝑛∕2)\n𝑛24(\n𝑛2)=4\n𝑇(𝑛∕4)\n𝑛216(\n𝑛216)=\n…\n…\nLevel\nWork\nΘ(𝑛𝑐).Here, the amount of work done at every level of the tree is always The total number of levels in the tree is log2(𝑛), since 𝑛can be halved\nΘ(𝑛𝑐)a total of times before the base case is reached. Because each level does work, the overall time complexity of the entire recurrencelog2(𝑛)\nΘ(𝑛𝑐log2(𝑛)), Θ(𝑛𝑐log(𝑛)). 𝑏𝑐.relation is or just This is also the result of the Master Theorem when 𝑎=\nTo summarize, the total amount of work that a recurrence relation of the form does — where is a polynomial that is𝑎𝑇(𝑛∕𝑏)+𝑓(𝑛) 𝑓(𝑛)\nΘ(𝑛𝑐) 𝑏𝑐.— depends on the values of 𝑎and\n𝑏𝑐,If the work attributed to the number of recursive calls dominates over the additional work at each recursive call. As a result,• 𝑎> 𝑓(𝑛)\n𝑛log𝑏(𝑎)the work is concentrated at the bottom of the recursion tree — since there are nodes at this bottom level, and each node does a\nΘ(𝑛log𝑏(𝑎)).𝑏𝑐isconstant amount of work, the complexity of a recurrence where 𝑎>\n𝑏𝑐,• If the additional work done with each recursive call dominates over the work attributed to the number of recursive calls, and𝑎< 𝑓(𝑛)\nthe work is concentrated at the top of the recursion tree — since the top level does work, the overall complexity of a recurrence𝑓(𝑛)\n𝑏𝑐is Θ(𝑛𝑐).where 𝑎< Θ(𝑓(𝑛))=\n𝑏𝑐,• If the work attributed to the number of recursive calls is comparable to the additional work done with each recursive call.𝑎= 𝑓(𝑛)\nΘ(𝑛𝑐),Thus, the amount of work done at every level of the recursion tree is and since there are a total of levels in the tree, thelog(𝑛)\nΘ(𝑛𝑐log(𝑛)).complexity of the overall recurrence is\n¸ 5.7.3\n(✽)The Extended Master Theorem for Polylogarithmic Functions\nThere is actually an extension of the Master Theorem that can be used if is a polylogarithmic function, but you will need to know this𝑓(𝑛) not\nΘ(𝑛𝑐log𝑘(𝑛)),for the class. Given a recurrence relation of the form 𝑎𝑇(𝑛∕𝑏)+𝑓(𝑛), where is of the form the following are true:𝑇(𝑛) 𝑓(𝑛)=\nΘ(𝑛log𝑏(𝑎)).𝑏𝑐,• If then is𝑎> 𝑇(𝑛)\n𝑏𝑐,• If then𝑎=\n– If −1, then is𝑘> 𝑇(𝑛) Θ\n(\nlog𝑘+1(𝑛)𝑛log𝑏(𝑎)\n)\n.\nΘ(𝑛log𝑏(𝑎)log(log(𝑛))).– If −1, then is𝑘= 𝑇(𝑛)\nΘ(𝑛log𝑏(𝑎)).– If −1, then is𝑘< 𝑇(𝑛)\n𝑏𝑐,• If then𝑎<\nΘ(𝑛𝑐log𝑘(𝑛)).𝑘≥0,– If then is𝑇(𝑛)\nΘ(𝑛𝑐).– If 0, then is𝑘< 𝑇(𝑛)", "word_count": 817, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8b95c46d-22a1-567d-a98a-fc9992a9c0c7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 148, "real_page_number": null, "text": "136\nChapter 5. Recursion and Recurrence Relations\n5.8\nComplexities of Common Recurrence Relations\nIn this section, we will use the tools covered in the previous sections to determine the complexity classes of common recurrence relations. These\nrecurrence relations, along with their time complexities, are shown in the table below.\nRecurrence Type\nTime Complexity\n𝑇(𝑛) 𝑇(𝑛∕2)+1=\nΘ(log(𝑛))\n𝑇(𝑛) 𝑇(𝑛−1)+1=\nΘ(𝑛)\n𝑇(𝑛) 2𝑇(𝑛∕2)+1=\nΘ(𝑛)\n𝑇(𝑛) 𝑇(𝑛−1)+𝑛+1=\nΘ(𝑛2)\n𝑇(𝑛) 2𝑇(𝑛∕2)+𝑛+1=\nΘ(𝑛log(𝑛))\nExample 5.18 Show that a recurrence relation of the form has a time complexity of Θ(log(𝑛)).𝑇(𝑛) 𝑇(𝑛∕2)+1=\n𝑎≥1,This recurrence relation matches the form 𝑎𝑇(𝑛∕𝑏)+𝑓(𝑛), where 1, and is part of a polynomial complexity class. Thus,𝑇(𝑛) 𝑏> 𝑓(𝑛)=\n20,the Master Theorem can be used on this recurrence relation. The value of 𝑎is 1, the value of 𝑏is 2, and the value of 𝑐is 0. Since the1=\nΘ(𝑛𝑐log(𝑛)) Θ(𝑛0 Θ(log(𝑛)).complexity of this recurrence relation is log(𝑛))= =\nExample 5.19 Show that a recurrence relation of the form has a time complexity of Θ(𝑛).𝑇(𝑛) 𝑇(𝑛−1)+1=\nThis recurrence relation is not in a form that allows us to use the Master Theorem. As a result, we will try using iterative substitution.\n𝑇(𝑛)Step 1: Write out the recursive terms as recurrence relations and substitute their equations into the original formula at each step.\nStep #\nRecurrence Equation\n1\n𝑇(𝑛) 𝑇(𝑛−1)+1=\n2\nRewrite as𝑇(𝑛−1) 𝑇(𝑛−2)+1\nSubstitute in for in the previous recurrence equation𝑇(𝑛−2)+1 𝑇(𝑛−1)\n𝑇(𝑛) 𝑇(𝑛−1)+1=\n→𝑇(𝑛) 𝑇(𝑛−2)+1+1=\n→𝑇(𝑛) 𝑇(𝑛−2)+2𝑐=\n3\nRewrite as𝑇(𝑛−2) 𝑇(𝑛−3)+1\nSubstitute in for in the previous recurrence equation𝑇(𝑛−3)+1 𝑇(𝑛−2)\n𝑇(𝑛) 𝑇(𝑛−2)+2=\n→𝑇(𝑛) 𝑇(𝑛−3)+1+2=\n→𝑇(𝑛) 𝑇(𝑛−3)+3=\n𝑘th𝑇(𝑛)Step 2: Look for a pattern that describes at the step, and express it using a summation formula.\nWith the first few steps, we have enough information to describe a pattern for 𝑇(𝑛).\n• At the first step, 𝑇(𝑛) 𝑇(𝑛−1)+1=\n• At the second step, 𝑇(𝑛) 𝑇(𝑛−2)+2=\n• At the third step, 𝑇(𝑛) 𝑇(𝑛−3)+3=\n𝑘thWe can generalize this and derive the following formula for at the step:𝑇(𝑛)\n𝑇(𝑛) 𝑇(𝑛−𝑘)+𝑘=\n𝑘such 𝑇(𝑛).Step 3: Solve for that the base case is the only recursive term that is present on the right-hand side of the equation for Determine\nthe closed form solution by replacing instances of the base case with the value of the base case.\nSince we want the base case to be the only recursive term present on the right-hand side of the equation, we want to select a 𝑘such that 𝑇(𝑛−𝑘)\nequals the base case. We are not told what value of 𝑛the base case happens at, so let’s use an arbitrary constant 𝑐to represent the base case.\nequals the base case when 𝑐, or 𝑛−𝑐. Substituting 𝑛−𝑐for 𝑘in our equation gives us an expression for where𝑇(𝑛−𝑘) 𝑇(𝑐) 𝑛−𝑘= 𝑘= 𝑇(𝑛)\nis the only recursive term on the right-hand side. We can then plug in into the equation (since the base case does constant work).𝑇(𝑐) 𝑇(𝑐)=1\n𝑇(𝑛) 𝑇(𝑛−𝑘)+𝑘= 𝑇(𝑛−(𝑛−𝑐))+(𝑛−𝑐) 𝑇(𝑐)+(𝑛−𝑐) 1+𝑛−𝑐= = =\nAfter dropping lower order terms (1 and 𝑐), we get that the complexity of the recurrence relation is Θ(𝑛).\nExample 5.20 Show that a recurrence relation of the form has a time complexity of Θ(𝑛).𝑇(𝑛) 2𝑇(𝑛∕2)+1=\n𝑎≥1,This recurrence relation matches the form 𝑎𝑇(𝑛∕𝑏)+𝑓(𝑛), where 1, and is part of a polynomial complexity class. Thus,𝑇(𝑛) 𝑏> 𝑓(𝑛)=\n20,the Master Theorem can be used on this recurrence relation. The value of 𝑎is 2, the value of 𝑏is 2, and the value of 𝑐is 0. Since the>2\nΘ(𝑛log𝑏(𝑎)) Θ(𝑛log2(2))complexity of this recurrence relation is Θ(𝑛).= =", "word_count": 620, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "739c47f3-7a45-5bb8-b5c3-8a6fbc0c4aca", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 149, "real_page_number": null, "text": "5.8 Complexities of Common Recurrence Relations\n137\nΘ(𝑛2).Example 5.21 Show that a recurrence relation of the form has a time complexity of𝑇(𝑛) 𝑇(𝑛−1)+𝑛+1=\nThis recurrence relation is not in a form that allows us to use the Master Theorem. As a result, we will try using iterative substitution.\n𝑇(𝑛)Step 1: Write out the recursive terms as recurrence relations and substitute their equations into the original formula at each step.\nStep #\nRecurrence Equation\n1\n𝑇(𝑛) 𝑇(𝑛−1)+𝑛+1=\n2\nRewrite as𝑇(𝑛−1) 𝑇(𝑛−2)+(𝑛−1)+1\nSubstitute in for in the previous recurrence equation𝑇(𝑛−2)+(𝑛−1)+1 𝑇(𝑛−1)\n𝑇(𝑛) 𝑇(𝑛−1)+𝑛+1=\n→𝑇(𝑛) 𝑇(𝑛−2)+(𝑛−1)+1+𝑛+1=\n→𝑇(𝑛) 𝑇(𝑛−2)+(𝑛−1)+𝑛+2=\n3\nRewrite as𝑇(𝑛−2) 𝑇(𝑛−3)+(𝑛−2)+1\nSubstitute in for in the previous recurrence equation𝑇(𝑛−3)+(𝑛−2)+1 𝑇(𝑛−2)\n𝑇(𝑛) 𝑇(𝑛−2)+(𝑛−1)+𝑛+2=\n→𝑇(𝑛) 𝑇(𝑛−3)+(𝑛−2)+1+(𝑛−1)+𝑛+2=\n→𝑇(𝑛) 𝑇(𝑛−3)+(𝑛−2)+(𝑛−1)+𝑛+3=\n𝑘th𝑇(𝑛)Step 2: Look for a pattern that describes at the step, and express it using a summation formula.\nWith the first few steps, we have enough information to describe a pattern for 𝑇(𝑛).\n• At the first step, 𝑇(𝑛) 𝑇(𝑛−1)+𝑛+1=\n• At the second step, 𝑇(𝑛) 𝑇(𝑛−2)+(𝑛−1)+𝑛+2=\n• At the third step, 𝑇(𝑛) 𝑇(𝑛−3)+(𝑛−2)+(𝑛−1)+𝑛+3=\n𝑘thWe can generalize this and derive the following formula for at the step:𝑇(𝑛)\n𝑇(𝑛) 𝑇(𝑛−𝑘)+=\n𝑘−1\n∑\n𝑖=0\n(𝑛−𝑖)+𝑘\n𝑘such 𝑇(𝑛).Step 3: Solve for that the base case is the only recursive term that is present on the right-hand side of the equation for Determine\nthe closed form solution by replacing instances of the base case with the value of the base case.\nSince we want the base case to be the only recursive term present on the right-hand side of the equation, we want to select a 𝑘such that 𝑇(𝑛−𝑘)\nequals the base case. We are not told what value of 𝑛the base case happens at, so let’s use an arbitrary constant 𝑐to represent the base case.\nequals the base case when 𝑐, or 𝑛−𝑐. Substituting 𝑛−𝑐for 𝑘in our equation gives us an expression for where𝑇(𝑛−𝑘) 𝑇(𝑐) 𝑛−𝑘= 𝑘= 𝑇(𝑛)\nis the only recursive term on the right-hand side. We can then plug in into the equation.𝑇(𝑐) 𝑇(𝑐)=1\n𝑇(𝑛) 𝑇(𝑛−(𝑛−𝑐))+=\n(𝑛−𝑐)−1\n∑\n𝑖=0\n(𝑛−𝑖)+(𝑛−𝑐)\n𝑇(𝑐)+=\n(𝑛−𝑐)−1\n∑\n𝑖=0\n(𝑛−𝑖)+(𝑛−𝑐)\n1+(𝑛+(𝑛−1)+(𝑛−2)+…+(𝑐+1))+(𝑛−𝑐)=\nThe sum of all terms in an arithmetic sequence can be calculated using the equation 𝑛\n(𝑎1+𝑎𝑛\n2\n)\n, where 𝑛is the number of terms, is𝑎1\n𝑛ththe value of the first term, and 𝑎𝑛is the value of the term. Using this equation, we calculate the sum of all terms from to 𝑛as𝑐+1\n(𝑛−𝑐)\n(\n𝑐+1+𝑛\n2\n)\n𝑛2=\n𝑛+2\n−𝑐22\n−𝑐2\n2. This allows us to rewrite as𝑇(𝑛)\n𝑇(𝑛) 1+(𝑛+(𝑛−1)+(𝑛−2)+…+(𝑐+1))+(𝑛−𝑐)= = 1+\n(\n𝑛2\n𝑛+2\n−𝑐22\n−𝑐2\n2\n)\n+(𝑛−𝑐)\nΘ(𝑛2).After dropping constants and lower order terms, we can see that is𝑇(𝑛)\nExample 5.22 Show that a recurrence relation of the form has a time complexity of Θ(𝑛log(𝑛)).𝑇(𝑛) 2𝑇(𝑛∕2)+𝑛+1=\n𝑎≥1,This recurrence relation matches the form 𝑎𝑇(𝑛∕𝑏)+𝑓(𝑛), where 1, and is part of a polynomial complexity class. Thus,𝑇(𝑛) 𝑏> 𝑓(𝑛)=\n21,the Master Theorem can be used on this recurrence relation. The value of 𝑎is 2, the value of 𝑏is 2, and the value of 𝑐is 1. Since the2=\nΘ(𝑛𝑐log(𝑛)) Θ(𝑛1complexity of this recurrence relation is Θ(𝑛log(𝑛)).log(𝑛))= =", "word_count": 566, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "41678199-f4ac-54a7-92b2-a0ef4dfda320", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 150, "real_page_number": null, "text": "138\nChapter 5. Recursion and Recurrence Relations\n5.9\nAnalysis of Recursive Algorithms\nAt this point, we have covered several different methods that can be used to measure the time complexity of an algorithm. In the last chapter,\nwe focused on methods that can be used to determine the time complexity of an iterative algorithm. In this chapter, we expanded upon these\nmethods by introducing techniques that can be used when recursion is involved. Time complexity analysis will serve as an important foundation\nfor this class, as it allows us to compare different algorithms when trying to solve a problem. In this section, we will go through the algorithm\nanalysis process using the following algorithm design question.\nExample 5.23 You are given a two-dimensional matrix with 𝑚rows and 𝑛columns, where 𝑚≈𝑛, represented using an array of arrays.\nThis matrix is sorted along the rows and columns: for every element in the 2-D matrix, the value at index is less than the values at[𝑖][𝑗]\nand [𝑖][𝑗+1]. You are given a target value that you want to find in the 2-D matrix, and you have to return whether the value exists.[𝑖+1][𝑗]\nWhat are the time complexities of the following three recursive algorithms, and which one is asymptotically best for solving this problem?\nYou may find the following identity helpful with your calculations:\n𝑛\n∑\n𝑖=0\n𝑖2𝑖=2𝑛+1𝑛−2𝑛+1+2\n¸ Algorithm #1: Quad Partition\n(arr[m/2][n/2])The quad partition algorithm splits the 2-D matrix into four quadrants, compares the middle element with the target value,\nand eliminates the quadrant that cannot contain the target value (since the matrix is sorted). The algorithm is then recursively called on the other\nthree quadrants. An example is shown below.\nYou are asked to determine if 13 can be found in this 2-D matrix. The quad partition algorithm would\n1. Compare the target value (13) with the middle element (9).\n2. Eliminate the quadrant that cannot contain 13. In this case, because 13 is bigger than 9, we know that 13 must either be to the right of 9\nor below 9. It is impossible for 13 to be both to the left and above 9, so the top left quadrant is eliminated.\n3. Make three recursive calls, one for each of the three quadrants that were not eliminated.\n1\n4\n7\n1511\n2\n5\n8\n1912\n3\n6\n9\n16 22\n10 13 1714 24\n18 23 26 3021\nAttempt to find 13\n1\n4\n7\n1511\n2\n5\n8\n1912\n3\n6\n9\n16 22\n10 13 1714 24\n18 23 26 3021\nCompare 13 with\nmiddle element\n1\n4\n7\n1511\n2\n5\n8\n1912\n3\n6\n9\n16 22\n10 13 1714 24\n18 23 26 3021\n13 must be in one of\nthese quadrants\n¸ Algorithm #2: Binary Partition with Linear Search\nThe binary partition algorithm with linear search also splits the 2-D matrix into four quadrants. However, instead of just looking at the middle\nvalue, the algorithm looks through the entire middle column using a search to determine where the target value should be if it were in thatlinear\ncolumn. If the value is not in the middle column, the algorithm eliminates both the quadrant to the upper left and the quadrant to the lower right\nof where the target value should be. Two recursive calls are then made, one for each of the two quadrants that were not eliminated.\n1\n4\n7\n1511\n2\n5\n8\n1912\n3\n6\n9\n16 22\n10 13 1714 24\n18 23 26 3021\nAttempt to find 13\n1\n4\n7\n1511\n2\n5\n8\n1912\n3\n6\n9\n16 22\n10 13 1714 24\n18 23 26 3021\nFind 13’s position\nin middle column\n1\n4\n7\n1511\n2\n5\n8\n1912\n3\n6\n9\n16 22\n10 13 1714 24\n18 23 26 3021\n13 must be in one of\nthese quadrants\n¸ Algorithm #3: Binary Partition with Binary Search\nThe binary partition algorithm with binary search does the same thing as the algorithm with linear search. The only difference is that binary\nsearch is used to find where the target element should be in the middle column. For instance, using the example above, instead of checking 7, 8,\n9, , the search checks 9 first, compares 9 with 13, and then removes half of the search space (because the matrix is sorted, there is no need to…\ncheck elements above 9). Since binary search cuts the search space in half at every iteration, it takes time.Θ(log(𝑛))", "word_count": 782, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "29a3959a-873e-50f3-9000-553f5696c5b5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 151, "real_page_number": null, "text": "5.9 Analysis of Recursive Algorithms\n139\nWhich algorithm has the best time complexity? Since recursion is involved with all three algorithms, we will first convert them into recurrence\nrelations. We will ask four questions to help us develop a recurrence relation:\n1. Compared to the initial input size 𝑛, what input size is passed into the recursive call?\n2. How many recursive calls are made?\n3. What is the time complexity of the other operations outside the recursive calls?\n4. What is the base case?\nNote: In this problem, the input size 𝑛represents the dimension size of the matrix, and the total number of values in the matrix! This isnot\nbecause our algorithms recursively break up the problems by dimension size rather than the total number of elements.\n¸ Analysis of Algorithm #1: Quad Partition\nLet’s develop a recurrence relation for the quad partition algorithm:\n1. Compared to the initial input size n, what input size is passed into the recursive call?\nAfter splitting the input into four quadrants, the dimension of each quadrant is approximately 𝑛∕2. Since we recurse into these quadrants, the\ninput size of each recursive call is also 𝑛∕2.\n2. How many recursive calls are made?\nAfter splitting the matrix into four quadrants, we eliminate one based on the value of the middle element. Since we make a recursive call for\neach of the remaining quadrants, 3 recursive calls are made.\n3. What is the time complexity of the other operations outside the recursive calls?\nBefore making the recursive calls, the algorithm compares the middle value with the target value and decides which quadrant to eliminate. This\nis done in constant time.\n4. What is the base case?\nThe base case happens when the dimensions of the input array is 1x1 (when 𝑛is 1). If there is only one element in the array, it takes constant\ntime to determine whether this element is the target value.\nUsing this information, we can derive the following recurrence relation:\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n3𝑇(𝑛∕2)+1,\nif 𝑛>1\nThe Master Theorem can be used on this recurrence. Using 3, 2, and 0, we get𝑎= 𝑏= 𝑐=\n𝑏𝑐→Θ𝑎>\n(\n𝑛log𝑏(𝑎))\n=Θ\n(\n𝑛log2(3))\n≈Θ(𝑛1.585)\nΘ(𝑛log2(3)), Θ(𝑛1.585).Thus, the time complexity of the quad partition algorithm is or approximately\n¸ Analysis of Algorithm #2: Binary Partition with Linear Search\nWe will repeat this process for the binary partition with linear search:\n1. Compared to the initial input size n, what input size is passed into the recursive call?\nAfter splitting the input into four quadrants, the dimension of each quadrant is approximately 𝑛∕2. Since we recurse into these quadrants, the\ninput size of each recursive call is also 𝑛∕2.\n2. How many recursive calls are made?\nAfter splitting the matrix into four quadrants, we eliminate two quadrants instead of one. Thus, two recursive calls are made.\n3. What is the time complexity of the other operations outside the recursive calls?\nAlthough we do eliminate one recursive call, we had to do some extra work in order to do so. This work is in the form of a linear search down\nthe middle column. This takes linear time (Θ(𝑛) since 𝑚≈𝑛).\n4. What is the base case?\nThe base case happens when the dimensions of the input array is 1x1 (when 𝑛is 1). If there is only one element in the array, it takes constant\ntime to determine whether this element is the target value.\nUsing this information, we can derive the following recurrence relation:\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n2𝑇(𝑛∕2)+𝑛,\nif 𝑛>1\nThe Master Theorem can be used on this recurrence. Using 2, 2, and 1, we get𝑎= 𝑏= 𝑐=\n𝑏𝑐→Θ(𝑛𝑐log(𝑛))𝑎= Θ(𝑛log(𝑛))=\nThus, the time complexity of the binary partition with linear search algorithm is Θ(𝑛log(𝑛)).", "word_count": 637, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3396917b-e934-5f37-8974-8bdd059dcfc8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 152, "real_page_number": null, "text": "140\nChapter 5. Recursion and Recurrence Relations\n¸ Analysis of Algorithm #3: Binary Partition with Binary Search\nWe will repeat this process for the binary partition with binary search:\n1. Compared to the initial input size n, what input size is passed into the recursive call?\nAfter splitting the input into four quadrants, the dimension of each quadrant is approximately 𝑛∕2. Since we recurse into these quadrants, the\ninput size of each recursive call is also 𝑛∕2.\n2. How many recursive calls are made?\nLike with the binary partition with linear search algorithm, only 2 recursive calls are made.\n3. What is the time complexity of the other operations outside the recursive calls?\nAlthough we eliminate one recursive call, we had to do some extra work in order to do so. This work is in the form of a search down thebinary\nmiddle column. This takes logarithmic time, or log2(𝑛). (If you want more a more in-depth coverage of binary search, see chapter 15.)\n4. What is the base case?\nThe base case happens when the dimensions of the input array is 1x1 (when 𝑛is 1). If there is only one element in the array, it takes constant\ntime to determine whether this element is the target value.\nUsing this information, we can derive the following recurrence relation:\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n2𝑇(𝑛∕2)+log2(𝑛),\nif 𝑛>1\nΘ(𝑛𝑐).The Master Theorem cannot be used on this recurrence relation since log2(𝑛), which is not of the form As a result, we will need𝑓(𝑛)=\nto use iterative substitution. (Even though the base-2 of the logarithm does not affect the ultimate complexity class, we will explicitly keep it for\nthe math involved during the substitution process.)\nRemark: This is a case where the extended Master Theorem (section 5.7.3) comes in handy.\nΘ(𝑛log𝑏(𝑎)).𝑏𝑐,• If then is𝑎> 𝑇(𝑛)\n𝑏𝑐,• If then𝑎=\n– If −1, then is𝑘> 𝑇(𝑛) Θ\n(\nlog𝑘+1(𝑛)𝑛log𝑏(𝑎)\n)\n.\nΘ(𝑛log𝑏(𝑎)log(log(𝑛))).– If −1, then is𝑘= 𝑇(𝑛)\nΘ(𝑛log𝑏(𝑎)).– If −1, then is𝑘< 𝑇(𝑛)\n𝑏𝑐,• If then𝑎<\nΘ(𝑛𝑐log𝑘(𝑛)).𝑘≥0,– If then is𝑇(𝑛)\nΘ(𝑛𝑐).– If 0, then is𝑘< 𝑇(𝑛)\nlog1(𝑛)). 𝑏𝑐,Θ(𝑛0For the recurrence relation 2𝑇(𝑛∕2)+log2(𝑛), we can see that Since we can conclude that𝑇(𝑛) 𝑓(𝑛) log2(𝑛) 𝑎>= = =\nΘ(𝑛log𝑏(𝑎)) Θ(𝑛log2(2)) Θ(𝑛). However, since the extended Master Theorem is not required course material, we will still go𝑇(𝑛) = = =\nthrough the process of iterative substitution to arrive at the same result.\n𝑇(𝑛)Step 1: Write out the recursive terms as recurrence relations and substitute their equations into the original formula at each step.\nStep #\nRecurrence Equation\n1\n𝑇(𝑛) 2𝑇(𝑛∕2)+log2(𝑛)=\n2\nRewrite as𝑇(𝑛∕2) 2𝑇(𝑛∕4)+log2(𝑛∕2)\nSubstitute in for in the previous recurrence equation2𝑇(𝑛∕4)+log2(𝑛∕2) 𝑇(𝑛∕2)\n𝑇(𝑛) 2𝑇(𝑛∕2)+log2(𝑛)=\n→2(2𝑇(𝑛∕4)+log2(𝑛∕2))+log2(𝑛)\n→𝑇(𝑛) 4𝑇(𝑛∕4)+2log2(𝑛∕2)+log2(𝑛)=\n3\nRewrite as𝑇(𝑛∕4) 2𝑇(𝑛∕8)+log2(𝑛∕4)\nSubstitute in for in the previous recurrence equation2𝑇(𝑛∕8)+log2(𝑛∕4) 𝑇(𝑛∕4)\n𝑇(𝑛) 4𝑇(𝑛∕4)+2log2(𝑛∕2)+log2(𝑛)=\n→𝑇(𝑛) 4(2𝑇(𝑛∕8)+log2(𝑛∕4))+2log2(𝑛∕2)+log2(𝑛)=\n→𝑇(𝑛) 8𝑇(𝑛∕8)+4log2(𝑛∕4)+2log2(𝑛∕2)+log2(𝑛)=\n𝑘th𝑇(𝑛)Step 2: Look for a pattern that describes at the step, and express it using a summation formula.\nWith the first few steps, we have enough information to describe a pattern for 𝑇(𝑛).\n• At the first step, 𝑇(𝑛) 2𝑇(𝑛∕2)+log2(𝑛)=\n• At the second step, 𝑇(𝑛) 𝑇(𝑛) 4𝑇(𝑛∕4)+2log2(𝑛∕2)+log2(𝑛)= =\n• At the third step, 8𝑇(𝑛∕8)+4log2(𝑛∕4)+2log2(𝑛∕2)+log2(𝑛)\n𝑘thWe can generalize this and derive the following formula for at the step:𝑇(𝑛)\n2𝑘𝑇𝑇(𝑛)=\n( 𝑛\n2𝑘\n)\n+\n𝑘−1\n∑\n𝑖=0\n2𝑖log2\n( 𝑛\n2𝑖\n)", "word_count": 590, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b1af2a74-e9c6-5254-8510-0e9d36a9ebca", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 153, "real_page_number": null, "text": "5.9 Analysis of Recursive Algorithms\n141\n𝑘such 𝑇(𝑛).Step 3: Solve for that the base case is the only recursive term that is present on the right-hand side of the equation for Determine\nthe closed form solution by replacing instances of the base case with the value of the base case.\nSince we want the base case to be the only recursive term present on the right-hand side of the equation, we want to select a 𝑘such that\n𝑇(𝑛∕2𝑘) 𝑛∕2𝑘=equals the base case of 𝑇(1). This happens when 1, or when log2(𝑛). Substituting for 𝑘in our equation gives us an𝑘= log2(𝑛)\nexpression for where is the only recursive term on the right-hand side. We can then plug in into the equation.𝑇(𝑛) 𝑇(1) 𝑇(1)=1\n2log2(𝑛)𝑇(1)+𝑇(𝑛)=\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖log2\n( 𝑛\n2𝑖\n)\n𝑛𝑇(1)+=\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖log2\n( 𝑛\n2𝑖\n)\n𝑛+=\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖log2\n( 𝑛\n2𝑖\n)\nWe can simplify the summation term using log rules:\n𝑇(𝑛) 𝑛+=\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖log2\n( 𝑛\n2𝑖\n)\n𝑛+=\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖(log2(𝑛)−log2(2𝑖))\n𝑛+=\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖(log2(𝑛)−𝑖)\n𝑛+=\n⎛\n⎜\n⎜⎝\nlog2(𝑛)\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖\n⎞\n⎟\n⎟⎠\n−\n⎛\n⎜\n⎜⎝\nlog2(𝑛)−1\n∑\n𝑖=0\n𝑖2𝑖\n⎞\n⎟\n⎟⎠\nThe first summation is the sum of a finite geometric series, which can be calculated using the summation formula:\n𝑎1(1−𝑟𝑛)𝑆𝑛=\n1−𝑟\n20where 𝑛is the number of terms, is the first term, and 𝑟is the common ratio. Using log2(𝑛), 1, and 2, we get𝑎1 𝑛= 𝑎1 𝑟== =\nlog2(𝑛)\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖=log2(𝑛)\n((1−2log2(𝑛))\n1−2\n)\nlog2(𝑛)(𝑛−1) 𝑛log2(𝑛)−log2(𝑛)= =\nWe can use the following identity (provided in the example) to solve the second summation in our original expression:\n𝑛\n∑\n𝑖=0\n𝑖2𝑖=2𝑛+1𝑛−2𝑛+1+2\nFrom this identity, we can simplify the second summation term as follows:\nlog(𝑛)−1\n∑\n𝑖=0\n𝑖2𝑖=2log2(𝑛)(log2(𝑛)−1)−2log2(𝑛)+2\n𝑛(log(𝑛)−1)−𝑛+2=\n𝑛log(𝑛)−2𝑛+2=\nPutting everything together, we get:\n𝑇(𝑛) 𝑛+=\n⎛\n⎜\n⎜⎝\nlog2(𝑛)\nlog2(𝑛)−1\n∑\n𝑖=0\n2𝑖\n⎞\n⎟\n⎟⎠\n−\n⎛\n⎜\n⎜⎝\nlog2(𝑛)−1\n∑\n𝑖=0\n𝑖2𝑖\n⎞\n⎟\n⎟⎠\n𝑛+(𝑛log2(𝑛)−log2(𝑛))−(𝑛log2(𝑛)−2𝑛+2)=\n3𝑛−log2(𝑛)−2=\nBy dropping coefficients and lower order terms, we see that the binary partition with binary search algorithm has a time complexity of Θ(𝑛).", "word_count": 383, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c5dabf20-c4aa-5e8a-b6a5-727e2272fa20", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 154, "real_page_number": null, "text": "142\nChapter 5. Recursion and Recurrence Relations\nFrom our analysis, we determined the following:\nΘ(𝑛1.585)• The quad partition algorithm runs in approximately time.\n• The binary partition with linear search algorithm runs in time.Θ(𝑛log(𝑛))\n• The binary partition with binary search algorithm runs in time.Θ(𝑛)\nThus, the third algorithm, the binary partition with binary search, is the one that is asymptotically the fastest. From a performance perspective,\nthis algorithm should be chosen to solve the problem. This indeed turns out to be the right choice. Here is the runtime of each algorithm for 1\nmillion searches on a 100x100 matrix. The quad partition algorithm is the slowest, the binary partition with linear search algorithm is in the\nmiddle, and the binary partition with binary search algorithm is the fastest:\nAlgorithm\nComplexity\nRuntime\nQuad Partition\nΘ(𝑛1.585)\n17.33 seconds\nBinary Partition with Linear Search\nΘ(𝑛log(𝑛))\n10.93 seconds\nBinary Partition with Binary Search\nΘ(𝑛)\n6.56 seconds\nJust by looking at these three recursive algorithms, it may not seem apparent that the runtime saved by removing a single recursive call is worth\nan extra search down the middle column. However, from our analysis, we discovered that this extra work is worthwhile, as the removal of this\none recursive call dropped our complexity class from polynomial to linear. Even doing an inefficient linear search down the middle columnΘ(𝑛)\nis more efficient than making a third recursive call.\nThis is the power of big-O analysis; we can use it to analyze different approaches to a problem before we start implementation! However,\nexpectations do not always mirror reality — even if the correct algorithm is chosen, there may be factors that cause our runtimes to differ from\nperf,what we expect. Performance tools like which can identify the percentage of total time you are spending on an operation, can be used to\ndebug these issues. Furthermore, big-O only dictates how runtime scales, not actual runtime itself. An algorithm that takes 𝑛steps will be twice\nas fast as an algorithm that takes 2𝑛steps, even if both algorithms are Θ(𝑛). And lastly, finding an efficient algorithm is only half the battle; you\nmust also implement it optimally, using the correct choice of data structures. A suboptimal choice of data structure can worsen the performance\nof an otherwise efficient algorithm! We will begin exploring different data structures in the following chapters.\nRemark: EECS 281 is not a math class! All of the fancy math equations you see in this chapter, especially for iterative substitution problems,\nare provided for completeness. You do need to remember any summation formulas or identities for this class; everything you need shouldnot\nbe given to you, unless otherwise specified.\nChapter 5 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Which of the following statements on recursion is/are TRUE?\nI. A tail recursive function is a linear recursive function where the recursive call is the final instruction of the function.\nII. Since tail recursive functions do not need to store local variables on a stack frame, all tail recursive functions use auxiliary space.Θ(1)\nIII. The auxiliary space of a non-tail recursive function will always be the same as its time complexity, since all the recursive calls that\nare evaluated must be placed on the program stack.\nA) I only\nB) II only\nC) I and II only\nD) I and III only\nE) I, II, and III\nfoo(),2. What is the auxiliary space complexity of the function with respect to the input size 𝑛?\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 1) {\n3\nreturn 1;\n4\n} // if\n5\nreturn n + foo(n / 2);\n6\n} // foo()\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)", "word_count": 664, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "2ea8034f-46fd-59b9-9a9d-92a3569d5e7b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 155, "real_page_number": null, "text": "5.9 Analysis of Recursive Algorithms\n143\nfoo(),3. What is the auxiliary space complexity of the function with respect to the input size 𝑛?\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 1) {\n3\nreturn 1;\n4\n} // if\n5\nreturn foo(n / 2);\n6\n} // foo()\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\nfoo() bar():4. Consider the following two functions, and\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 1) {\n3\nreturn n;\n4\n} // if\n5\nreturn foo(n - 2) + foo(n - 1);\n6\n} // foo()\n7\n8\nint32_t bar(int32_t int32_t int32_tn, prev = 0, curr = 1) {\n9\nif (n == 0) {\n10\nreturn prev;\n11\n} // if\n12\nif (n == 1) {\n13\nreturn curr;\n14\n} // if\n15\nreturn bar(n - 1, curr, prev + curr);\n16\n} // bar()\nWhich of the following statements is/are TRUE regarding these two functions?\nfoo(n) bar(n) n.I. Both and would return the same value for all positive integer values of\nbar() foo()II. is tail recursive, while is not.\nfoo() bar()III. The auxiliary space used by is Θ(𝑛), while the auxiliary space used by is Θ(1).\nA) I only\nB) II only\nC) I and II only\nD) II and III only\nE) I, II, and III\nbar(n)5. Given the function below, calculate the recurrence relation. Assume that runs in time.log(𝑛)\n1\nvoid foo(int32_t n) {\n2\nif (n == 1) {\n3\nreturn;\n4\n} // if\n5\n6\nfoo(n / 7);\n7\nint32_t sq = n n;*\n8\n9\nfor (int32_t i = 0; i < sq; ++i) {\n10\nfor (int32_t j = 0; j < n; ++j) {\n11\nbar(n);\n12\n} // for j\n13\n} // for i\n14\n15\nfor (int32_t k = 0; k < n; ++k) {\n16\nfoo(n / 3);\n17\n} // for k\n18\n19\nbar(sq sq);*\n20\n} // foo()\nA) 𝑇(𝑛) 𝑇=\n(\n𝑛\n7\n)\n+𝑛2log(𝑛)+𝑛𝑇\n(\n𝑛\n3\n)\n+log(𝑛)\nB) 𝑇(𝑛) 𝑇=\n(\n𝑛\n7\n)\n+𝑛2log(𝑛)+𝑛𝑇\n(\n𝑛\n3\n)\n+2log(𝑛)\nC) 𝑇(𝑛) 𝑇=\n(\n𝑛\n7\n)\n+𝑛3log(𝑛)+𝑛𝑇\n(\n𝑛\n3\n)\n+log(𝑛)\nD) 𝑇(𝑛) 𝑇=\n(\n𝑛\n7\n)\n+𝑛3log(𝑛)+𝑛𝑇\n(\n𝑛\n3\n)\n+2log(𝑛)\nE) 𝑇(𝑛) 𝑇=\n(\n𝑛\n7\n)\n+𝑛3log(𝑛)+𝑛𝑇\n(\n𝑛\n3\n)\n+4log(𝑛)", "word_count": 415, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4cfc9e58-19f9-5c29-8b58-6d84187f9454", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 156, "real_page_number": null, "text": "144\nChapter 5. Recursion and Recurrence Relations\n𝑇(𝑛−2)+3𝑛.6. Consider the recurrence relation If we solve this recurrence using the substitution process, which of the following is a𝑇(𝑛)=\nvalid first step in the expansion?\n𝑇(𝑛−2)+3𝑛=𝑇(𝑛−4)+32𝑛=𝑇(𝑛−6)+33𝑛=A) 𝑇(𝑛)= …\n𝑇(𝑛−2)+3𝑛=𝑇(𝑛−4)+3𝑛+3𝑛=𝑇(𝑛−6)+3𝑛+3𝑛+3𝑛=B) 𝑇(𝑛)= …\n𝑇(𝑛−2)+3𝑛=𝑇(𝑛−4)+3𝑛+3𝑛−2 𝑇(𝑛−6)+3𝑛+3𝑛−2+3𝑛−4C) 𝑇(𝑛)= = =…\n𝑇(𝑛−2)+3𝑛=𝑇(𝑛−2)+𝑇(𝑛−4)+3𝑛=𝑇(𝑛−2)+𝑇(𝑛−4)+𝑇(𝑛−6)+3𝑛+…D) 𝑇(𝑛)=\nE) None of the above\n7. Which of the following recurrence relations can one solve by applying the Master Theorem?\nA) 𝑇(𝑛) 𝑛𝑇=\n(\n𝑛\n3\n)\n+Θ(𝑛2)\nB) 𝑇(𝑛) 24𝑇=\n(\n𝑛\n6\n)\n+32𝑇\n(\n𝑛\n8\n)\n+Θ(𝑛2)\nC) 𝑇(𝑛) 11𝑇=\n(\n𝑛\n13\n)\n+Θ(\n√\n𝑛)\n1D) 𝑇(𝑛)=\n2𝑇\n(\n𝑛\n3\n)\n+Θ(𝑛2)\n2𝑇(𝑛−1)+Θ(𝑛3)E) 𝑇(𝑛)=\nΘ(𝑛2)?8. Which of the following recurrence relations has the closed form expression Assume that for all the choices.𝑇(𝑛) 𝑇(1)= =1\nA) 𝑇(𝑛) 2𝑇=\n(\n𝑛\n4\n)\n+𝑛5\nB) 𝑇(𝑛) 𝑇=\n(\n𝑛\n2\n)\n+\n(\n𝑛\n2\n)\nC) 𝑇(𝑛) 2𝑇=\n(\n𝑛\n4\n)\n+\n√\n𝑛\nD) 𝑇(𝑛) 𝑇(𝑛−1)+𝑛+4=\nE) None of the above\n9. Solve the following recurrence.\n𝑇(𝑛)=\n{\n1,\nif 𝑛=0\n𝑇(𝑛−1)+2,\nif 𝑛>0\nA) 𝑇(𝑛) 𝑛=\nB) 𝑇(𝑛) 2𝑛+1=\nC) 𝑇(𝑛) 2𝑛−1=\nD) 𝑇(𝑛) 2𝑛=\nE) 𝑇(𝑛)=2\n10. What is the time complexity of the following recurrence relation?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n3𝑇(𝑛−1)+1,\nif 𝑛>1\nA) Θ(𝑛)\nΘ(𝑛2)B)\nΘ(𝑛3)C)\nΘ(2𝑛)D)\nΘ(3𝑛)E)\n11. What is the time complexity of the following recurrence relation?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n4𝑇\n(\n𝑛\n2\n)\n+16𝑛+𝑛2+1,\nif 𝑛>1\nA) Θ(𝑛)\nB) Θ(𝑛log(𝑛))\nΘ(𝑛2)C)\nΘ(𝑛2D) log(𝑛))\nΘ(𝑛4)E)", "word_count": 307, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a1b1d368-3e55-5610-b9d4-65351b65309d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 157, "real_page_number": null, "text": "5.9 Analysis of Recursive Algorithms\n145\n12. What is the time complexity of the following recurrence relation?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n5𝑇\n(\n𝑛\n25\n)\n+5\n√\n𝑛+1,\nif 𝑛>1\nA) Θ(\n√\n𝑛)\nB) Θ(\n√\n𝑛log(𝑛))\nC) Θ(𝑛)\nΘ(𝑛5D) log(𝑛))\nΘ(𝑛5)E)\n13. What is the time complexity of the following recurrence relation?\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n729𝑇\n(\n𝑛\n9\n)\n+3𝑛3√\n𝑛+81𝑛+1,\nif 𝑛>1\n3√A) Θ(\n𝑛log(𝑛))\n3√B) Θ(\n𝑛)\nC) Θ(𝑛)\nΘ(𝑛3√D)\n𝑛)\nΘ(𝑛3)E)\n14. Consider the following recurrence:\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n𝑎𝑇\n(\n𝑛\n𝑏\n)\n+19\n√\n𝑛+6𝑛,\nif 𝑛>1\nYou are told that the time complexity of is Θ(𝑛). Which of the following must be true?𝑇(𝑛)\nA) 𝑎=1\nB) 𝑎=𝑏\nC) 𝑎<𝑏\nD) 𝑎>𝑏\nE) None of the above\n15. Suppose you are given the following incomplete recurrence relation for a recursive function:\n𝑇(𝑛) 𝑇(?)+281𝑛=\nYou are told that this recurrence is directly solvable by the Master Theorem. Knowing this information, which of the following statements\nmust be true?\nA) 𝑇(𝑛)→Θ(1)\nB) 𝑇(𝑛)→Θ(𝑛)\nC) 𝑇(𝑛)→Θ(𝑛log(𝑛))\n→Θ(𝑛2)D) 𝑇(𝑛)\nE) The final complexity of cannot be determined from the information given𝑇(𝑛)", "word_count": 217, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7ade2c75-97bd-5ea0-9db3-00057a0f2bc9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 158, "real_page_number": null, "text": "146\nChapter 5. Recursion and Recurrence Relations\nfoo()16. Consider the following function below.\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 0) {\n3\nreturn 0;\n4\n} // if\n5\n6\nint32_t count = 0;\n7\nfor (int32_t i = 0; i < n; ++i) {\n8\nfor (int32_t j = 0; j < n; ++j) {\n9\n++count;\n10\n} // for j\n11\n} // for i\n12\n13\nfor (int32_t k = 0; k < 8; ++k) {\n14\ncount += foo(n / 2);\n15\n} // for k\n16\n17\nreturn count;\n18\n} // foo()\nfoo(),What is the time complexity of in terms of the input size 𝑛?\nA) Θ(𝑛)\nΘ(𝑛2)B)\nΘ(𝑛2C) log(𝑛)))\nΘ(𝑛3)D)\nΘ(𝑛3E) log(𝑛))\nfoo()17. Consider the following function below.\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 0) {\n3\nreturn 0;\n4\n} // if\n5\n6\nint32_t count = 0;\n7\nfor (int32_t i = 0; i < n; i *= 2) {\n8\n++count;\n9\n} // for i\n10\n11\ncount += foo(n - 1);\n12\nreturn count;\n13\n} // foo()\nfoo(),What is the time complexity of in terms of the input size 𝑛?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛)))\nΘ(𝑛2)D)\nΘ(𝑛2E) log(𝑛))\nfoo()18. Consider the following function below.\n1\nuint64_t foo(int32_t n) {\n2\nuint64_t count = 1;\n3\nif (n == 0) {\n4\nreturn count;\n5\n} // if\n6\n7\nfor (uint64_t i = 1; i <= n; ++i) {\n8\ncount += foo(n - 1);\n9\n} // for i\n10\n11\nreturn count;\n12\n} // foo()\nfoo(),What is the time complexity of in terms of the input size 𝑛?\nA) Θ(𝑛)\nΘ(𝑛2)B)\nΘ(2𝑛)C)\nD) Θ(𝑛!)\nΘ(𝑛𝑛)E)", "word_count": 304, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1962dbba-48be-57e8-a46d-e110fb949b06", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 159, "real_page_number": null, "text": "5.9 Analysis of Recursive Algorithms\n147\nfoo()19. Consider the following function below.\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 1) {\n3\nreturn 1;\n4\n} // if\n5\nreturn foo(n - 1) + foo(n - 1);\n6\n} // foo()\nfoo(),What is the time complexity of in terms of the input size 𝑛?\nA) Θ(𝑛)\nΘ(𝑛2)B)\nΘ(2𝑛)C)\nD) Θ(𝑛!)\nΘ(𝑛𝑛)E)\nfoo()20. Consider the following function below.\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 1) {\n3\nreturn 1;\n4\n} // if\n5\nreturn foo(n / 2) + foo(n / 2);\n6\n} // foo()\nfoo(),What is the time complexity of in terms of the input size 𝑛?\nA) Θ(𝑛)\nΘ(𝑛2)B)\nΘ(2𝑛)C)\nD) Θ(𝑛!)\nΘ(𝑛𝑛)E)\nfoo()21. Consider the following function below.\n1\nint32_t foo(int32_t n) {\n2\nif (n <= 1) {\n3\nreturn 1;\n4\n} // if\n5\n6\nint32_t count = 0;\n7\ncount += foo(n / 4);\n8\n9\nfor (int32_t i = 0; i < n; ++i) {\n10\nfor (int32_t j = 0; j < n; ++j) {\n11\n++count;\n12\n} // for j\n13\n} // for i\n14\n15\ncount += foo(n / 2);\n16\n17\nfor (int32_t k = 0; k < n; ++k) {\n18\n++count;\n19\n} // for k\n20\n21\nreturn count + foo(n / 2);\n22\n} // foo()\nfoo(),What is the time complexity of in terms of the input size 𝑛?\nA) Θ(𝑛)\nB) Θ(𝑛log(𝑛))\nΘ(𝑛2)C)\nΘ(𝑛2D) log(𝑛))\nΘ(𝑛3)E)", "word_count": 265, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6219b8e0-9267-59bd-93a2-4ca6b2a67b14", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 160, "real_page_number": null, "text": "148\nChapter 5. Recursion and Recurrence Relations\nsearch_for_life()22. Consider the following function below.\n1\nvoid search_for_life_helper(int32_t size_t size_tarr[], n, idx) {\n2\nif (idx < 0 || idx >= n) {\n3\nreturn;\n4\n} // if\n5\n6\nif (arr[idx] == 42) {\n7\nstd::cout << \"Found the answer to life\\n\";\n8\n} // if\n9\nelse {\n10\nstd::cout << \"Failure\\n\";\n11\n} // else\n12\n13\nsearch_for_life_helper(arr, n, idx + 1);\n14\n} // search_for_life_helper()\n15\n16\nvoid search_for_life(int32_t size_tarr[], n) {\n17\nfor (size_t i = 1; i < n; i *= 2) {\n18\nsearch_for_life_helper(arr, n, i);\n19\n} // for i\n20\n} // search_for_life()\nfoo(),What is the time complexity of in terms of the input size 𝑛?\nA) Θ(𝑛)\nB) Θ(𝑛log(𝑛))\nΘ(𝑛2)C)\nΘ(𝑛2D) log(𝑛))\nΘ(2𝑛)E)\n23. Consider the following functions below.\n1\nvoid foo(int32_t s) {\n2\nfor (int32_t i = s; i > 0; i /= 2) {\n3\nstd::cout << \"EECS 281\\n\";\n4\n} // for i\n5\n} // foo()\n6\n7\nvoid bar(int32_t k) {\n8\nif (k <= 1) {\n9\nreturn;\n10\n} // if\n11\n12\nfoo(k);\n13\nbar(k - 1);\n14\n} // bar()\n15\n16\nvoid baz(int32_t n) {\n17\nfor (int32_t i = 0; i < n; ++i) {\n18\nbar(n);\n19\n} // for i\n20\n21\nif (n > 281) {\n22\nfoo(n);\n23\n} // if\n24\nelse {\n25\nbar(n - 1);\n26\n} // else\n27\n} // baz()\nbaz(),What is the time complexity of in terms of the input size 𝑛?\nA) Θ(𝑛log(𝑛))\nΘ(𝑛2)B)\nΘ(𝑛2C) log(𝑛))\nΘ(𝑛3)D)\nΘ(𝑛3E) log(𝑛))", "word_count": 284, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7a67def9-3042-56ec-bf6a-f1874009e052", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 161, "real_page_number": null, "text": "5.9 Analysis of Recursive Algorithms\n149\nChapter 5 Exercise Solutions\n1. The correct answer is (A). Only statement I is true. Statement II is false because a tail recursive function may still use additional memory\nthat is not part of the stack frames. Statement III is false because not all recursive calls in a non-tail recursive function will need to be\nevaluated at once (and thus do not need to be stored on the program stack together).\n2. The correct answer is (B). This function is not tail recursive, so additional stack frames will be needed for the recursive call on line 5.\nHow many stack frames are needed? The number of stack frames needed is equal to the number of recursive calls we will encounter before\nreaching the base case; since we are halving 𝑛with each recursive call, this comes out to Θ(log(𝑛)). The function uses constant auxiliary\nspace outside the recursive call, so we can conclude that the auxiliary space usage of the entire function is also Θ(log(𝑛)).\n3. The correct answer is (A). This function is tail recursive, so the number of stack frames needed does not depend on the input size 𝑛.\n𝑛th bar()4. The correct answer is (E). Both functions can be used to calculate the Fibonacci number; the main difference is that is tail\nfoo()recursive (via an accumulator argument) and is not. This indicates that statements I and II are true. Statement II is also true because\nbar() foo().the tail recursiveness of allows it to use constant auxiliary space rather than the linear number of stack frames required by\n5. The correct answer is (E). On line 6, we perform a recursive call with input size 𝑛∕7, which adds a term to our recurrence. On𝑇(𝑛∕7)\n𝑛3bar(n) bar(n)lines 9-13, we perform a loop that executes a total of times; since we are given that runs in time, the totallog(𝑛)\n𝑛3contribution of this loop can be expressed as log(𝑛). On lines 15-17, we perform a loop that makes a recursive call with input size a𝑛∕3\n𝑛2×𝑛2 𝑛4,bar()total of 𝑛times, for a total contribution of 𝑛𝑇(𝑛∕3). Lastly, on line 19, we call with an input size of which contributes=\nlog(𝑛4) work. Adding all of these terms together, we get the recurrence relation matching option (E).4log(𝑛)=\n𝑇(𝑛−4)+3𝑛−2, 𝑇(𝑛−2)+3𝑛is 𝑇(𝑛−4)+3𝑛+3𝑛−2.6. The correct answer is (C). We know that so the substitution after When𝑇(𝑛−2)=\n𝑇(𝑛−4)+3𝑛−2 𝑇(𝑛−4)+3𝑛).substituting, make sure to substitute all instances of 𝑛in the expression (e.g., is and not𝑇(𝑛−2)\n7. The correct answer is (C). Option (A) does not work because the value of 𝑎cannot depend on the input size 𝑛. Option (B) does not work\nbecause it is not in the correct form (two different recursive calls that split the input into different sizes). Option (D) does not work because\n1. Option (E) does not work because it is also not in the correct form (the input size must be divided). Only option (C) works, where𝑎<\n11, 13, and 1∕2.𝑎= 𝑏= 𝑐=\n8. The correct answer is (D). For option (A), we can use the Master Theorem for 2, 4, and to conclude that the recurrence has a𝑎= 𝑏= 𝑐=5\nΘ(𝑛5).closed form that is For option (B), we can use the Master Theorem for 1, 2, and to conclude that the recurrence has a𝑎= 𝑏= 𝑐=1\nclosed form that is Θ(𝑛). For option (C), we can use the Master Theorem for 2, 4, and to conclude that the recurrence has a𝑎= 𝑐= 𝑐=1∕2\nclosed form that is Θ(\n√\nΘ(𝑛2)𝑛log(𝑛)). This leaves choice (D), which we can prove is using iterative substitution:\n𝑇(𝑛) 𝑇(𝑛−1)+𝑛+4 [𝑇(𝑛−2)+(𝑛−1)+4]+𝑛+4 [[𝑇(𝑛−3)+(𝑛−2)+4]+(𝑛−1)+4]+𝑛+4= = =\n=…\n𝑇(𝑛−𝑘)+𝑘𝑛+4𝑘−=\n𝑘−1\n∑\n𝑖=0\n𝑖\nThe base case happens when 1, so we can set to solve for the overall time complexity of the recurrence:𝑛−𝑘= 𝑘=𝑛−1\n𝑇(𝑛) 𝑇(1)+(𝑛−1)𝑛+4(𝑛−1)−=\n𝑛−2\n∑\n𝑖=0\n−𝑛+4𝑛−4−(𝑛−1)(𝑛−2)1+𝑛2𝑖=\n2\n1=\n32𝑛2+\nΘ(𝑛2)2𝑛−2=\n9. The correct answer is (B). We can solve this recurrence using substitution:\n𝑇(𝑛) 𝑇(𝑛−1)+2 𝑇(𝑛−2)+2+2 𝑇(𝑛−3)+2+2+2= = =\n=…\n𝑇(𝑛−𝑘)+2𝑘=\nThe base case happens when 0, so we can set 𝑛to solve the overall recurrence:𝑛−𝑘= 𝑘=\n𝑇(𝑛) 𝑇(0)+2𝑛=2𝑛+1=\n10. The correct answer is (E). We can solve this recurrence using iterative substitution:\n32𝑇(𝑛−2)+3+1 32[3𝑇(𝑛−3)+1]+3+1 33𝑇(𝑛−3)+9+3+1𝑇(𝑛) 3𝑇(𝑛−1)+1 3[3𝑇(𝑛−2)+1]+1= = = = =\n=…\n3𝑘𝑇(𝑛−𝑘)+=\n𝑘−1\n∑\n𝑖=0\n3𝑖\nThe base case happens when 0, so we can set 𝑛to solve for the overall time complexity of the recurrence:𝑛−𝑘= 𝑘=\n3𝑛𝑇(0)+𝑇(𝑛)=\n𝑛−1\n∑\n𝑖=0\n3𝑖=Θ(3𝑛)", "word_count": 788, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "32d4ea49-03d7-5b88-8986-8246f0c995a8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 162, "real_page_number": null, "text": "150\nChapter 5. Recursion and Recurrence Relations\n11. The correct answer is (D). We can use the Master Theorem for this recurrence. Here, 𝑎is 4, 𝑏is 2, and 𝑐is the exponent of the dominating\n𝑏𝑐, Θ(𝑛𝑐log(𝑛))𝑛2. Θ(𝑛2term, or 2 in the case of Since we can conclude that log(𝑛)).𝑎= 𝑇(𝑛)= =\n𝑏𝑐,12. The correct answer is (B). We can use the Master Theorem for this recurrence. Here, 𝑎is 5, 𝑏is 25, and 𝑐is 1/2. Since we can𝑎=\nΘ(𝑛𝑐log(𝑛))conclude that 𝑇(𝑛)= =Θ(\n√\n𝑛log(𝑛)).\n13. The correct answer is (E). We can use the Master Theorem for this recurrence. Here, 𝑎is 729, 𝑏is 9, and 𝑐is the exponent of the\n𝑏𝑐, Θ(𝑛log𝑏𝑎) Θ(𝑛log9729) Θ(𝑛3).dominating term, or 4/3. Since we can conclude that𝑎> 𝑇(𝑛)= = =\n14. The correct answer is (C). This is a problem we can solve using the Master Theorem. We know that 𝑐is equal to one since that is the\nΘ(𝑛𝑐log(𝑛))exponent of the dominating term of 6𝑛. Thus, if 𝑏, then the complexity of would have to be Θ(𝑛log(𝑛)), which we𝑎= 𝑇(𝑛) =\nΘ(𝑛𝑐)can remove as a possibility. If 𝑏, the time complexity would be Θ(𝑛), so this case would work. If 𝑏, the time complexity𝑎< 𝑎>=\nΘ(𝑛log𝑏𝑎),would be which can only be if 𝑎and 𝑏were equal, so this case would not work. In conclusion, for the given recurrence toΘ(𝑛)\nbe Θ(𝑛), then 𝑎must be less than 𝑏.\n15. The correct answer is (B). In this case, we know that 𝑎and 𝑐are both 1, since we are told the recurrence is directly solvable by the Master\nTheorem. This actually limits the Master Theorem conditions we can possibly encounter. Since is necessary for the Master Theorem𝑏>1\n𝑏𝑐condition,to be used, we know for certain that 𝑎cannot be greater than or equal to 𝑏. This leaves us with the which states that the𝑎<\nΘ(𝑛𝑐)complexity of the recurrence must be Θ(𝑛).=\n𝑛216. The correct answer is (D). If we convert this function into a recurrence relation, we would get (from the eight𝑇(𝑛) 8𝑇(𝑛∕2)= +\nrecursive calls with half the input size on line 14, plus the quadratic work done in the nested loop on line 7). We can use the Master Theorem\n𝑏𝑐, Θ(𝑛log28) Θ(𝑛3).on this recurrence, with 8, 2, and 2. Since the time complexity of this function is𝑎= 𝑏= 𝑐= 𝑎> =\n17. The correct answer is (C). If we convert this function into a recurrence relation, we would get (from the recursive𝑇(𝑛) 𝑇(𝑛−1)+log(𝑛)=\nwith input size on line 11, plus the logarithmic work done in the loop on line 7). This recurrence can be solved using substitution𝑛−1\n(note that is from chapter 4):Θ(log(𝑛!)) Θ(𝑛log(𝑛))\n𝑇(𝑛) 𝑇(𝑛−1)+log(𝑛) 𝑇(𝑛−2)+log(𝑛−1)+log(𝑛) 𝑇(𝑛−3)+log(𝑛−2)+log(𝑛−1)+log(𝑛)= = =\n=…\n1+log(1)+log(2)+…+log(𝑛−1)+log(𝑛) 1+log(𝑛!) Θ(𝑛log(𝑛))= = =\n18. The correct answer is (D). See example 5.7.\n19. The correct answer is (C). The recurrence relation for this function is 2𝑇(𝑛−1)+1, which can be solved using substitution as𝑇(𝑛) =\nΘ(2𝑛) (see problem 10 for a very similar process).\n20. The correct answer is (A). The recurrence relation for this function is 2𝑇(𝑛∕2)+1, which can be solved using the Master Theorem𝑇(𝑛)=\n𝑏𝑐, Θ(𝑛log𝑏𝑎) Θ(𝑛log22) Θ(𝑛1).for 2, 2, and 0. Since the time complexity of this function is𝑎= 𝑏= 𝑐= 𝑎> = =\n2𝑇(𝑛∕2)+𝑇(𝑛∕4)+𝑛221. The correct answer is (C). The recurrence relation for this function is +𝑛, which cannot be solved using𝑇(𝑛)=\nthe Master Theorem. One alternative method to solve this could be to use a recurrence tree (an optional concept detailed in this chapter).\nHowever, there is an interesting detail we can notice that allows us to avoid having to use recurrence trees: we know that the function would\ndo more work if the recursive call on line 7 had instead been a recursive call. In that situation, the recurrence would have become𝑛∕4 𝑛∕2\n2𝑇(𝑛∕2)+𝑇(𝑛∕2)+𝑛2 3𝑇(𝑛∕2)+𝑛2+𝑛, or +𝑛. This recurrence solvable using the Master Theorem, with 3, 2, and —𝑎= 𝑏= 𝑐=is 2\n𝑏𝑐, Θ(𝑛2).here, so the time complexity would be Therefore, we know that the time complexity of the original given function cannot𝑎<\nΘ(𝑛2),exceed since that would be the time complexity if the recursive call were replaced with a larger input size of 𝑛∕2. However, we𝑛∕4\nΘ(𝑛2) Θ(𝑛2)also know that the time complexity cannot be better than either due to the presence of the loop on line 9. Since the function’s\nΘ(𝑛2) Θ(𝑛2).foo()time complexity cannot be worse than but also cannot be better, the overall time complexity of must be\n22. The correct answer is (B). For any index value 𝑖, the recurrence relation of the helper function can be expressed as:\n𝑇(𝑖)=\n{\n1,\n𝑖≥𝑛if or𝑖<0\n𝑇(𝑖+1)+1,\notherwise\nThe helper function is initially called with an input size of 1. Using substitution, we see that the time complexity of this helper call is Θ(𝑛):\n𝑇(1) 𝑇(2)+1 (𝑇(3)+1)+1 (𝑇(4)+1)+1+1 𝑇(𝑛)+(𝑛−1) 1+(𝑛−1) 𝑛= = = =…= = =\nIn fact, we can see that the total work performed by the helper for a given index 𝑖is 𝑛+1−𝑖. Since the helper is invoked once for every\npower of two up to 𝑛, the total work of this function must be:\n𝑇(𝑛) 𝑛+1−1+𝑛+1−2+𝑛+1−4+…+𝑛+1−log2(𝑛) log2(𝑛)×(𝑛+1)−= =\nlog2(𝑛)\n∑\n𝑖=0\n2𝑖\nUsing the equation for the sum of a geometric series, we can conclude that:\n𝑇(𝑛) log2(𝑛)×(𝑛+1)−=\n(\n1−2log2(𝑛)\n1−2\n)\nΘ(𝑛log(𝑛)−𝑛) Θ(𝑛log(𝑛))= =\nfoo()23. The correct answer is (C). The time complexity of is Θ(log(𝑠)), since it performs a constant time operation in a loop that halves\nbar()the input from 𝑠to 0. As a result, the time complexity of can be expressed using the recurrence 𝑇(𝑘−1)+Θ(𝑘−1), which𝑇(𝑘)=\nbaz() bar()is (see question 17 for the same recurrence). The function invokes a total of 𝑛times in the loop on line 17, soΘ(𝑘log(𝑘))\nΘ(𝑛2the overall complexity of this loop is log(𝑛)). Note that the function calls on line 22 and 25 both contribute to lower𝑛×Θ(𝑛log(𝑛))=\nΘ(𝑛2baz()order terms, so the overall time complexity of is log(𝑛)).", "word_count": 1047, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "96e8efa1-ac9c-50f7-9564-29f7af89ecac", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 163, "real_page_number": null, "text": "Chapter 6\nFixed-Size Arrays and Array-Based Containers\n6.1\nIntroduction to Data Structures\nWhen designing a computational approach to solving a problem, there are two important considerations that need to be made: the andalgorithm\nthe that are used to implement the algorithm.data structures\nAn algorithm is a well-defined set of instructions that can be followed to solve a problem. In the previous two chapters, we introduced the\nconcept of time complexity and how it can be used to measure the efficiency of an algorithm. However, the performance of an algorithm not\nonly relies on its implementation, but also the data structures it is used on. A data structure is defined by a collection of data, the relationships\namong them, and the functions and operations that can be applied to these data values. Each data structure provides an interface that defines\nhow its data can be accessed and organizes this data in memory in a manner that efficiently supports this access.\nA container is a type of data structure that can be used to store other objects. A container manages the memory used by the objects it holds.\nThere are several types of containers that we will cover in this class; you do not need to know any of these details just yet, but this provides a\npreview of what is to come.\n• A is a container that allows its elements to be accessed sequentially. Examples of sequence containers includesequence container\nstd::array<>– (chapter 6)\nstd::vector<>– (chapter 7)\nstd::list<>– (chapter 8)\nstd::forward_list<>– (chapter 8)\nstd::deque<>– (chapter 9)\n• An is a container that stores its data in sorted order and allows elements to be quickly searched in time.Θ(log(𝑛))associative container\nExamples of associative containers include\nstd::map<>– (chapter 18)\nstd::multimap<>– (chapter 18)\nstd::set<>– (chapter 18)\nstd::multiset<>– (chapter 18)", "word_count": 308, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "10c6cb3c-1b3f-5a75-8236-8eb27dd7e069", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 164, "real_page_number": null, "text": "152\nChapter 6. Fixed-Size Arrays and Array-Based Containers\n• An is a container where elements have no defined order. Unordered associative containers are usuallyunordered associative container\nimplemented using hash tables, and they support fast lookup (i.e., average-case time). Examples includeΘ(1)\nstd::unordered_map<>– (chapter 17)\nstd::unordered_multimap<>– (chapter 17)\nstd::unordered_set<>– (chapter 17)\nstd::unordered_multiset<>– (chapter 17)\n• A is an interface that is adapted for specific purposes, and they usually restrict the functionality of a standard container.container adaptor\nExamples of container adapters include\nstd::queue<>– (chapter 9)\nstd::stack<>– (chapter 9)\nstd::priority_queue<>– (chapter 10)\nBoth data structures and algorithms play an important role in the design and development of any program, and throughout this course, you will\nlearn how to identify the best data structures and algorithms that can be used to approach a given problem. In this chapter, we will begin this\nprocess by discussing the most basic of sequential containers: the array.\n6.2\nArray Fundamentals\n¸ 6.2.1\nC-Style Arrays\nAn array is a fixed-size container of objects that are stored contiguously in memory. Each element of an array must have the same type. The\nfollowing code provides an example of how to declare and use a C-style array:\n1\nint32_t arr[] = {0, 1, 5, 3};\n2\narr[2] = 2;\n3\nint32_t* ptr = arr;\n4\nptr = &arr[1];\narrLine 1 initializes an array using an initializer list. In this case, is declared as an array of size 4 with data values 0, 1, 5, and 3.\nRemark: C++11 introduced the concept of initialization, which allows initialization to be done with one common syntax. Thisuniform\ninitialization uses curly braces and can be used to initialize anything from primitive types to larger objects. For example, the expression\nint32_t num{281} num 281.initializes the variable to\nThere are a few notable types of initialization, of which include default and value initialization. Atomic types in C++ are undefined\nwhen initialized. With curly braces, however, these types can be to have an initial value:default value initialized\nDefault Initialization:\nValue Initialization:\nint32_t x;\n// x is undefined\nint32_t x{};\n// x is 0\nint32_t* y;\n// y is undefined\nint32_t* y{};\n// y is nullptr\nint32_t z[5];\n// z has undefined data\nint32_t z[5]{};\n// z is init with 0's\nint32_t num = 281.57Curly braces cannot be used in narrowing initializations. For example, does a narrowing conversion from\ndouble int32_t num 281. numto by truncating the decimal, so ends up storing However, using braces to initialize (i.e., the expression\nint32_t num{281.57}) would cause a compiler error. The same applies for arrays.\n(operator[]) arr[i]Square brackets can be used to access or modify any element in the array in time. This is because does theΘ(1)\n*(arr + i)same thing as (pointer arithmetic and dereference), which can be done in constant time.\narr ptr,Line 3 of the above code creates a pointer to the array. Notice that, when we assign the value of to we end up assigning pointer\nsizeof()),to the first element of the array. This is an attribute of C-style arrays: with a few exceptions (such as an expression for an array\ndecays into a pointer to the first element of the array upon behaviors such as assignment or usage as a function argument.\nArrays do not provide bounds checking, so error checking is the responsibility of the programmer. This is because error checking adds\nadditional overhead to performance, which may not be desired if speed is important. As a result, the programmer must add in checks to ensure\nthat the program is not accessing memory it should not be accessing.\nIn the above example, we initialized a array, whose size is at compile time. If you wanted to insert more elements in thenon-dynamic fixed\narray, you would have to reallocate the data into a bigger array elsewhere in memory, since the original array has a fixed size of 4. We will cover\ndynamic arrays (which automatically resize to fit the data) later in this chapter.\n¸ 6.2.2\nThe STL Array (std::array)\nstd::array<> <array>The C++ standard template library (STL) provides the container in the library that can be declared as follows:\nstd::array<TYPE, SIZE> arr;\nFor example, the following declaration creates a C++ array of size 4 with the elements 0, 1, 5, and 3:\nstd::array<int32_t, 4> arr = {0, 1, 5, 3};\nOr, using curly brace initialization:\nstd::array<int32_t, 4> arr{0, 1, 5, 3};", "word_count": 745, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cac3fae9-4f1f-5aa9-90a6-66977372ef51", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 165, "real_page_number": null, "text": "6.2 Array Fundamentals\n153\nstd::array<>The STL is a wrapper over the standard C-style array, and it provides several additional features that are missing from the C\nstd::array<> .size())array. First, the gives you access to useful member functions (such as and iterators (which will be covered in\nchapter 11). The following table summarizes a few of these member functions (do not worry too much about these for now):\nFunction\nBehavior\n.size()\nReturns the size of the array\n.empty()\nReturns a Boolean specifying whether the array is empty\n.front()\nReturns the first element of the array\n.back()\nReturns the last element of the array\n.begin()\nReturns a random access iterator to the first element in the array\n.end()\nReturns a random access iterator to the position one past the last element in the array\n.cbegin()\nReturns a random access iterator to the first element in the arrayconstant\n.cend()\nReturns a random access iterator to the position one past the last element in the arrayconstant\n.rbegin()\nReturns a iterator to the last element in the arrayreverse\n.rend()\nReturns a iterator to the position one before the first element in the arrayreverse\n.crbegin()\nReturns a constant reverse iterator to the last element in the array\n.crend()\nReturns a constant reverse iterator to the position one before the first element in the array\nstd::array<>The following code illustrates an example of usage, contrasting how array size is determined between a C-style array and a\nstd::array<>. For C-style arrays, the size is determined by taking the size of all the elements in the array and dividing it by the size of a\nstd::array<> .size()single element. However, the provides a member variable that does this work for you.\n1\n// How to find size of standard C-array\n2\nint32_t c_arr[] = {0, 1, 5, 3};\n3\nsize_t sizeof(c_arr) sizeof(c_arr[0]);c_arr_size = /\n// stores 4\n4\n5\n// How to find size of C++ std::array<>\n6\nstd::array<int32_t, 4> cpp_arr = {0, 1, 5, 3};\n7\nsize_t cpp_arr_size = cpp_arr.size();\n// stores 4\nstd::array<>Additionally, unlike the standard C-style array, a can be treated like a fundamental type, supporting features such as direct\noperator=,assignment or copy using as well as value semantics (i.e., they can be passed into or returned from a function by value).\nstd::array<>In general, if you ever find the need to use a fixed-sized array in a C++ program, you should choose a over a C-style\narray due to its cleaner interface and additional features. However, we will not be delving too deeply into the usage of this data container here,\nstd::vector<>since a is often a better choice in most situations (which is another container type that will be covered in the next chapter).\n¸ 6.2.3\nCommon Oﬀ-By-One Errors\nWhen looping through arrays, it is important to include the correct bounds for what you want to do. Recall that bounds checking is not done\nautomatically and is the responsibility of the programmer! Since arrays in C++ use 0-indexing, the following loop would go off the end of the\nSIZE - 1, SIZE <=array. This is because the last element of the array has index where is the size of the array. To fix this issue, the in the loop\n<.definition should be replaced with a\n// INCORRECT\nfor (size_t i = 0; i <= SIZE; ++i) {\narr[i] = i;\n}\n// CORRECT\nfor (size_t i = 0; i < SIZE; ++i) {\narr[i] = i;\n}\ni) i + 1)If you are using both the element at the current index (index and the next index (index in the body of your loop, you will need to stop\nSIZE - 1. i + 1 = SIZE i SIZE - 1, SIZEthe loop before it reaches index This is because if reaches and is not a valid index!\n// INCORRECT\nfor (size_t i = 0; i < SIZE; ++i) {\narr[i] = arr[i + 1];\n}\n// CORRECT\nfor (size_t i = 0; i < SIZE - 1; ++i) {\narr[i] = arr[i + 1];\n}\ni - 1If you are accessing index in the body of your loop, you may need an additional iteration if you want to visit every element in the array.\n<= < i - 1This can be done using instead of when defining the bounds of the loop. In addition, you should start iterating at index 1, since\niwould not be a valid index if were 0.\n// INCORRECT\nfor (size_t i = 1; i < SIZE; ++i) {\narr[i - 1] = i;\n}\n// CORRECT\nfor (size_t i = 1; i <= SIZE; ++i) {\narr[i - 1] = i;\n}", "word_count": 789, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ad3909f9-a10e-5da3-8e16-c500356165f4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 166, "real_page_number": null, "text": "154\nChapter 6. Fixed-Size Arrays and Array-Based Containers\n6.3\nStoring Multidimensional Data in Arrays\nThe following initializes a fixed one-dimensional (1-D) array of size 12:\nint32_t arr_1D[12] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11};\nIn memory, the array is stored like this, since elements are contiguous in memory (the address numbers were arbitrary chosen, and they are\nseparated by 4 because that is the size of a 32-bit integer):\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0x100\n0x104\n0x108\n0x10c\n0x110\n0x114\n0x118\n0x11c\n0x120\n0x124\n0x128\n0x12c\nNow, suppose we want to represent two-dimensional data in array. To do so, we can declare a fixed-size 2-D array, using the following syntax:\nint32_t arr_2D[num_rows][num_cols];\nFor example, the following initializes a 2-D array with 3 rows and 4 columns:\nint32_t arr_2D[3][4] = { {0, 1, 2, 3}, {4, 5, 6, 7}, {8, 9, 10, 11} };\narr_2DThe first row of the matrix would have the elements 0, 1, 2, and 3; the second row would have the elements 4, 5, 6 and 7; the third and\nfinal row would have the elements 8, 9, 10, and 11.\n[] [] []To access an element in a 2-D array, use the operator twice, where the first contains the row index and the second contains the\narr_2D[1][2] 6).column index. For example, retrieves the element in row 1, column 2 of the matrix (which is\nHow is this 2-D array stored in memory? Because memory chips are 1-dimensional, the 2-D array is laid out in memory as if it were a 1-D\n(in row-major order). That is, the 2-D array is stored like this in memory:array\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0x100\n0x104\n0x108\n0x10c\n0x110\n0x114\n0x118\n0x11c\n0x120\n0x124\n0x128\n0x12c\nWhen you try to access an element of a 2-D array, the compiler actually converts the 2-D row and column indices into the corresponding index\nof a 1-D array. The conversion formula is:\n1-D_index = 2-D_row_index num_columns + 2-D_column_index×\narr_2D[1][2]For example, when is called, the compiler converts the 2-D indices into the index 1 4 + 2 = 6. It then accesses index 6 of×\nthe underlying 1-D array that is stored in memory to get the correct value.\nAdditionally, if you wanted to treat a one-dimensional array as a two-dimensional one, you can use the following equations to convert a 1-D\nindex into its corresponding 2-D ones:\n2-D_row_index = 1-D_index / num_columns\n2-D_column_index = 1-D_index % num_columns\nNote: the % symbol represents the modulo operation, which finds the remainder when one number is divided by another. For example, 11 % 5 =\n1, because dividing 11 by 5 yields a remainder of 1.\nExample 6.1 A 2-D array is initialized with 14 rows and 11 columns. Suppose that the element at row 7, column 9 is accessed. What is the\nindex of this element in the underlying 1-D array in memory?\nTo solve this, we will use the conversion formula from a 2-D index to a 1-D index:\n1-D_index = 2-D_row_index num_columns + 2-D_column_index = = = 86× 7×11+9 77+9\nExample 6.2 A 1-D array is initialized with a size of 126. This 1-D array stores two-dimensional data with 7 rows and 18 columns. What\nis the row and column index of the element at index 59 of the 1-D array?\nTo solve this, we will use the conversion formula from a 1-D index to a 2-D index:\n2-D_row_index = 1-D_index / num_columns = = 359∕18\n2-D_column_index = 1-D_index % num_columns = = 559 % 18\nThe element at index 59 corresponds with the element at row 3, column 5 of the 2-D array. Note that integer division is used to calculate the row\nindex, since the decimal is truncated.\nIn summary, regardless of the dimension of the data, all arrays look like 1-D arrays in memory. As a result, any 𝑛-dimensional array can be\nconverted to a one-dimensional array using arithmetic operations.", "word_count": 680, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ec12ea00-f18e-5cd4-83ad-c6f5df916a67", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 167, "real_page_number": null, "text": "6.4 Heap-Allocated Arrays\n155\n6.4\nHeap-Allocated Arrays\nIn the previous examples, we declared an array on the stack. Stack-allocated arrays are automatically deallocated once they go out of scope.\nHowever, there is also a limitation to stack-allocated arrays — in order to declare an array on the stack, its size must be a constant value that\nis known at compile time. This is an issue if you do not know what the size of an array should be until you actually receive the data during\nruntime. Consider the following code, which takes in a size value from the user and attempts to initialize an array with that size:\n1\nsize_t size;\n2\nstd::cin >> size;\n3\nint32_t arr[size];\nsizeThe problem with this code is that is a variable whose value is determined at runtime, not at compile time. This is not allowed per the\nstandard.1C++ Although some compilers support this behavior, you should not be declaring an array on the stack if you do not know its\nsize,expected size at compile time, as doing so could cause issues (for example, if the user inputs an absurdly large value for your program\nwould exceed the amount of stack space available and immediately crash).\nInstead, it is preferable to declare variable-size arrays on the heap. The heap is a portion of your computer’s memory that is not managed\nautomatically (compared to a stack), so you are responsible for deallocating any memory that you allocate on the heap. To allocate memory on\nnew; delete.the heap, you should use the keyword to deallocate memory, you should use the keyword The following code takes in a size\nvalue at runtime and initializes an array with that size on the heap:\n1\nsize_t size;\n2\nstd::cin >> size;\n3\nint32_t* new int32_t[size];arr =\n4\n/* do stuff with array */\n5\ndelete[] arr;\nnew,After you are done with an array that is declared on the heap using you must deallocate it yourself. Failure to do so would lead to a\nmemory leak, where memory that is no longer accessible still remains in the heap. To delete an array that is allocated on the heap, you must\ndelete delete[],follow the keyword with square brackets (i.e., as shown on line 5 of the code above).\nYou can declare a 2-D array with variable size in a similar fashion. Since the 2-D array is dynamically allocated on the heap during runtime\nrather than during compile time, we will need to first allocate the outer dimension (rows) and then the inner dimension (columns):\n1\nsize_t rows, cols;\n2\nstd::cout << \"Enter the number of rows: \" << std::endl;\n3\nstd::cin >> rows;\n4\nstd::cout << \"Enter the number of columns: \" << std::endl;\n5\nstd::cin >> cols;\n6\nint32_t** new int32_t*[rows];arr =\n7\nfor (size_t r = 0; r < rows; ++r) {\n8\nnew int32_t[cols];arr[r] =\n9\n} // for r\n10\nint32_t val = 0;\n11\nfor (size_t r = 0; r < rows; ++r) {\n12\nfor (size_t c = 0; c < cols; ++c) {\n13\narr[r][c] = val++;\n14\n}\n// for c\n15\n} // for r\n16\n/* do stuff with the array */\n17\nfor (size_t r = 0; r < rows; ++r) {\n18\ndelete[] arr[r];\n19\n} // for r\n20\ndelete[] arr;\nforLine 6 of the code above initializes an array of pointers to other arrays, and each row is individually initialized in the loop on lines 7-8.\nThis initialization is shown on the top of the next page.\nLines 11-13 initialize the individual elements in the 2-D array itself. When looping through a multidimensional array, it is faster to loop\nthrough the outermost dimension (rows) in the outermost loop. This is due to something known as caching (which is covered in EECS 370 and\nnot this class): essentially, it is much more efficient to sequentially access items that are closer together in memory than items that are farther\napart. This idea is shown in the figure below.\nLoop rows, then columns\nFaster since memory accesses are contiguous\nLoop columns, then rows\nSlower since memory accesses are not contiguous\n1VariablelengthstackarrayswerepermittedbackinC99,buttheywerediscontinuedinlaterversionsofthelanguage. Ifyouwanttospecifyanarrayonthestack\nint arr[size]), constexprusingavariable(e.g., youwouldhave toexplicitlyinitializethevariableand declareitusingthe keyword,whichspecifies\nthatispossibletoevaluatethevalueofthevariableatcompiletime.", "word_count": 765, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cb9c5a72-7636-5986-8267-0e2f7ea1bdeb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 168, "real_page_number": null, "text": "156\nChapter 6. Fixed-Size Arrays and Array-Based Containers\nThe array stores\nintpointers to arrays\narr is a pointer to\nan array on the heap\nint*\n*arr\nnew int*[rows];=\nint*\nnew int[cols];arr[0] =\nint*\nnew int[cols];arr[1] =\nint*\nnew int[cols];arr[2] =\n⋮\nfor colsThe loop initializes each row array with elements\nnew deleteSince we called to initialize this 2-D array, we will have to deallocate it after we are done using it. The operations must be called\nnew. (arr) (arr[0],in the opposite order in which we called Since we initialized the array of row pointers before the individual row arrays\narr[1], arr arr.…), we have to iterate through and delete all of the row arrays before we can delete This is shown on lines 17-20.\nTo review, we have discussed two different ways to initialize an array. One way is to declare the array on the stack. Stack-allocated arrays\nare automatically deallocated when they go out of scope, and accessing an element only requires one access to memory regardless of dimension\n(since the compiler does math to obtain the correct memory location of an object). However, arrays on the stack are limited in size and require\nthe programmer to know the dimensions of the array during compilation. As a result, stack-allocated arrays are not very versatile.\nTo address the issue of variable-sized input, a dynamically allocated pointer-based array can be initialized instead. There are a few caveats\nto watch for when an array is declared on the heap. With a heap-allocated array, an element would require more than one memory access for\nlarger dimensions: in the 2-D array above, we would first need to find the pointer to the correct row array, and then the address of the element we\nwant within the row array itself. Furthermore, arrays that are allocated on the heap must be deallocated by the programmer before the program\nends. Nonetheless, dynamically allocated arrays are more flexible and should be used if the array size cannot be determined until runtime.\nWorking with heap-allocated arrays can be a messy process at times. Luckily, in your projects, you will not be using C-style arrays. Instead,\nstd::vector<>,you will be using the C++ which abstracts away these details and provides a cleaner interface for storing data in an\narray-like format. A vector gives you all the functionality of a dynamically allocated array, but it handles all the dynamic memory allocation and\ndeallocation for you. In addition, it can also grow in size based on the amount of data it is required to store, unlike a fixed-size array. Vectors\nwill be discussed in more detail in the next chapter.\nnew delete.Remark: In general, when dynamic memory is allocated, whoever called is also responsible for calling When working with\ndelete new,dynamic memory, make sure to add in a whenever you call before you do any other work! This ensures that you don’t forget\nto deallocate memory, which can lead to memory leaks that can be difficult to track down.\nnullptr:In addition, after deleting a pointer to dynamic memory, it is good practice to set it to\n1\ndelete ptr;\n2\nptr = nullptr;\ndelete nullptrThis acts as a safeguard against unintentional uses of a deleted value, and it also protects you from double deletes (i.e.,\ndelete ptris safe and does nothing, but calling twice could crash your program).\nnew delete, <memory>Note: To avoid all the hassle of and you can use smart pointers defined in the library to manage your memory\n(std::unique_ptr<>, std::shared_ptr<>, std::weak_ptr<>).for you Smart pointers are essentially wrappers over normal\npointers that provide enhanced functionality and automatic memory management. However, smart pointers are allowed in this classnot\n(although they are still good to know, especially for upper-level classes - see chapter 27, but only after the class is done).\n6.5\nCopying with Pointers\nSuppose you are given two arrays, and you want to copy the data from one array (the source array) to the other (the destination array). Assume\nthat the destination array is large enough to fit all the data in the source array. You are given three pointers:\n1\nint32_t* first;\n// pointer to the first element of the source array\n2\nint32_t* last;\n// pointer one past the last element of the source array\n3\nint32_t* dest;\n// pointer to the first position of the destination array\nTo copy over elements from the first array into the second, you can use the following process:\nfirst dest *dest = *first).1. Dereference the value of and assign it to the value of (i.e.,\nfirst first++).2. Increment to move the next element of the source array (i.e.,\ndest dest++).3. Increment to move to the next position of the destination array (i.e.,\nfirst last.4. Repeat for all the elements in the array, stopping the loop when reaches", "word_count": 817, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f0979f07-f758-58ed-8596-7fc078905921", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 169, "real_page_number": null, "text": "6.6 Sequential and Random Access\n157\nwhile forThis process can be converted into either of the following loops (both a loop and a loop are shown):\n1\nwhile (first != last) {\n2\n*dest++ = *first++;\n3\n}\n1\nfor (; first != last; ++first, ++dest) {\n2\n*dest = *first;\n3\n}\nwhile (++For the loop implementation, because the postfix operator is used after the variable name), the incrementation is done theafter\nwhile first == last, firstdereference and assignment. Essentially, the loop runs until each time copying the value pointed to by into\ndest. first last,the position pointed to by The loop terminates once reaches which points one past the end of the source array. Note that\nfirst != last first < last, first != lastis used instead of since only is guaranteed to work on all types of containers.\nThis method can be extended across different containers using iterators, which will be covered in chapter 11. Using pointers and iterators to\ncopy elements between containers is preferable to using indexing, since they are more generic and allow the same code to be used for multiple\ncontainer types, even ones that do not support indexing.\nmemcpy(),In addition, there exists a function that can be used to copy a block of memory from one location to another. This function is\nand there is a good chance you will have to use it in a later class, even if you do not use it during this class. To use this function, you have to\n<cstring>include the library.\nvoid* memcpy(void* const void* size_tdest, source, num);\nnum source dest.Copies bytes from the object pointed to by to the object pointed to by The underlying type of the objects pointed to by\nsource dest source destand are irrelevant, as the data is copied over as raw bytes. If the objects pointed to by and overlap, or if\nnullptr, void*either point to an invalid address or the behavior of this function is undefined. (A is a generic pointer that has no type\nassociated with it, and can be used to reference an object of any data type.)\nmemcpy()An example of is shown below:\n1\nint32_t src[] = {1, 2, 3, 4, 5};\n2\nint32_t dst[5];\n3\nsizeof(src));std::memcpy(dst, src,\n4\nfor (size_t i = 0; i < 5; ++i) {\n5\nstd::cout << dst[i] << \" \";\n// prints 1 2 3 4 5\n6\n} // for i\nsizeof() sizeof(src)The function returns the number of bytes that a data type or object occupies in memory. Here, calculates the total\nsrc memcpy() dstsize of the array so that can copy over the correct number of bytes to the array.\n6.6\nSequential and Random Access\nAs mentioned in the first section of this chapter, a container is an object that stores a collection of data types. Different containers can have\ndifferent properties. In this section, we will discuss two different access methods for iteration that may be supported by a container of data:\nrandom access and sequential access. With random access, you are able to access any arbitrary element in a sequence of data as efficiently as\nany other element at any point in time, regardless of the size of the sequence. On the other hand, with sequential access, the elements in a\nsequence of data are visited in a predetermined order, where each element is accessed directly after its predecessor. An array is an example of a\noperator[].container that supports random access, since any item in an array can be directly accessed in time usingΘ(1)\nRandom Access:\n5thUser wants to access element;\ncan directly access it in constant time.\nOn the other hand, a container does support efficient random access if its elements must be accessed in a predeteremined sequence. Suchnot\ncontainers only support efficient sequential access. For instance, if a user requests to access an element in a container that only supports\nsequential access, the entire container must be read from the beginning to the element that is desired; there is no way to calculate the memory\nlocation of a random element directly. As such, there is no guarantee that an element in such a container can accessed as easily as any other. An\n𝑛thexample of a sequential container is a linked list (covered in chapter 8). Without any supporting data structures, accessing an arbitrary\nelement of a linked list would require you to iterate through the first 𝑛elements in the container beforehand.\nSequential Access:\n5thUser wants to access element;\nmust iterate through all elements before it.", "word_count": 774, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "96e46552-31d5-589d-857f-1ad342471d94", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 170, "real_page_number": null, "text": "158\nChapter 6. Fixed-Size Arrays and Array-Based Containers\n6.7\nData Storage and Retrieval\nSuppose you wanted to create a container to store a data type. How would you store the data? You have two options: you could store the values\nof the data elements themselves in the container, or you could store to your data values in your container.pointers\nStoring by Value:\n0\n1\n2\n3\n4\n5\nStoring by Pointer:\n0x5620 0x573c 0x5818 0x59f4 0x5ad0 0x5bec\n0\n1\n2\n3\n4\n5\nContainers that store data by value have full ownership over the memory used to store their data, and they are responsible for managing their\nown memory. If you want to insert or delete elements from a container that stores data by value, you should ask the container to do it for you via\ninsert()some sort of internal operation (such as calling an member function). Containers that store values are the most common, but they\ncan be expensive to copy if the objects they store are large.\nOn the other hand, you could store pointers to your data in the container instead. If you do this, the container no longer has direct ownership\nover the data, and it is not responsible for allocating or deallocating the memory for the data values themselves. Instead, the container is only\nresponsible for the to the data. This can be dangerous, since the container no longer provides direct protection: the data in the containerpointers\ncan be modified elsewhere without permission from the container itself. A container of pointers, for example, can easily be invalidated if\nsomething else deletes what its pointers are pointing to. That being said, containers of pointers can be useful in certain cases, such as with\n(char*)containers of C-strings and shared data (e.g., having one master container that stores the data and smaller containers that access this\ndata in different ways using pointers; this ensures that your data only exists in memory once).\nThere are also several considerations that need to made when getting values out of a container. Suppose you wanted to retrieve an element\noperator[]).from a container (e.g., by calling How should the element be returned to the user? There are three ways that this can be done.\nFirst, the container could return a of the element that the user wants. For example, if the user requests the element at the fifth index of acopy\ncontainer, the container could return a separate copy of this element to the user. However, this method is inefficient: if the objects in a container\nwere large, accessing elements in the container would be slow since a copy would have to be made every time the user requests an element.\nAn alternative option would be to return a to the element that is requested. Since an address to an object is returned, no copies arepointer\nneeded. However, this option is unsafe, as it gives the user the ability to do things that they shouldn’t be allowed to do! With a pointer to the\ncontents of a container, a user could modify anything they wanted, iterate off the end of the container and/or access memory they shouldn’t be\nable to access, or even delete the contents of the data entirely! As a result, returning pointers to private data generally isn’t ideal.\nThethirdoptionwouldbetoreturna totheelementthatwasrequested. Thisoptionistypicallythebestchoice. Becausereferencesreference\ndon’t require copies, returning by reference is faster than returning by value. In addition, references are safer than pointers because they cannot\nbe modified — a user cannot use a reference to maliciously access other memory without permission. Furthermore, if you want to prevent the\nconstuser from modifying the contents of the data you give them, you can return a reference to the requested element instead of a normal,\nnon-const reference.\nWhat to Store in a Container\nValue\nPointer\nExample\nT data;\nT* data;\nData Ownership\nContainer\nContainer or other object\nDrawbacks\nLarge objects can take time to copy\nUnsafe, can be modified elsewhere or invalidated\nUsage\nMost common\nchar*,Used for shared data\nWhat to Retrieve from a Container\nValue\nPointer, Const Pointer\nReference, Const Reference\nE.g.,\nT get_elt(size_t idx);\nT* get_elt(size_t idx);\nT& get_elt(size_t idx);\nNotes\nCostly for copying\nUnsafe, can be used to\nUsually a good choice; safer than\nlarge objects\naccess prohibited memory\npointers and faster than copies", "word_count": 742, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "75bfd19c-84b9-5eb3-8791-4eb6f0a6c3c1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 171, "real_page_number": null, "text": "6.8 Implementing an Array-Based Container\n159\n6.8\nImplementing an Array-Based Container\n¸ 6.8.1\nA Blueprint of the Custom Array-Based Container\nIn this section, we will combine the concepts of the previous sections and begin implementing an array-based container that can be used to\nmanage a dynamically allocated array. This container will not only have the functionality of a standard C-style array — it will also have features\nthat abstract away memory allocation work from the user. Recall that standard C-style arrays have fixed size; for instance, if you wanted to add a\nfifth element to an array that can only store at most four elements, you would have to reallocate a new, larger array in memory to be able to add\nthat element. It is also cumbersome to copy or assign standard C-style arrays, since doing so would require deep copies of all the elements to be\nmade. Our goal is to implement a container that can handle this work for the user, so that users of the container will not have to worry about\nmemory allocation and deallocation when trying to insert an element or copy an array.\nclass. ArraySinceourcontainerisacustomobject, wewilldefineitusingaC++ Hereisablueprintforthiscustom objectthatsupports\nlimited functionality. We will be adding to this as we continue through this section.\n1\ntemplate <typename T>\n2\nclass Array {\n3\nT* data;\n// Array data\n4\nsize_t length;\n// Array size\n5\npublic:\n6\nArray(size_t len = 0) : length{len} {\n7\nnew nullptr);data = (len ? T[len] :\n8\n} // Array()\n9\n10\nsize_t constsize() {\n11\nreturn length;\n12\n} // size()\n13\n};\nArray Array.Let’s take a look at what this object currently does. There are currently two private member variables that are managed by the\ndata Array lengthThe variable on line 3 points to a heap-allocated array that contains the container’s data. The variable on line 4 stores\nArray. Array Arraythe number of elements in the On line 6, we have the constructor; the user can initialize the to have any size they want\nlen. length 5by passing in a value for For example, if the following code were run, the constructor would initialize the value of to and set\ndata (data = new T[len]).to a dynamically allocated array of size 5\nArray<int32_t> a{5};\n// initialize Array called 'a' with length 5\nArray len, (len = 0).If the user creates an without specifying then the default argument value is used\nArray<int32_t> b;\n// len not specified, so b has a length 0\nlen data nullptr (data = nullptr), lengthIf has a value of 0, then is set to on line 7 and the value of is also set to 0 on line 6.\nlen new T[len];The assignment expression on line 7 (using the ternary operator) ensures that if is a non-zero value, it is set to otherwise, it\nnullptr.is set to\nsize() Array.Lastly, we have the method on line 9, which returns the length of the Since this member function does not modify any\nArray const. 5:member variables of the class, it is declared as For example, the following code would print out\nArray<int32_t> a{5};\n// initialize Array called 'a' with length 5\nstd::cout << a.size() << std::endl;\n// prints 5\nArrayThese features are what we have so far with our implementation. However, there is a problem! Suppose we ran the following code:\nArray<int32_t> a{5};\n// initialize Array called 'a' with length 5\nArray<int32_t> b = a;\n// initialize 'b' with contents of 'a'\nb, a dataTo initialize all the member variables of are copied over one by one. However, since the member variable is an array of pointers, the\na bpointers are copied over and not the data objects themselves! Thus, both and end up sharing the same data.\na\n0x5ee0 0x5ee4 0x5ee8 0x5eec 0x5ef0\nb\n0x5ee0 0x5ee4 0x5ee8 0x5eec 0x5ef0\n0\n1\n2\n3\n4\na bWhen the contents of is modified, the contents of would also change, since both containers point to the same memory. In addition, if\na b! b athe memory of is deallocated, so would the memory of This is not intended behavior. If you try to access an element of after is\nb adeallocated, you would be accessing invalid memory; on the other hand, if you try to deallocate after is deallocated, you would end up\nwith a double delete that could crash your program! To prevent these errors from happening, we will need to implement a custom assignment\noperator that copies the data objects themselves rather than just the pointers.", "word_count": 790, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b255079d-c187-5e12-ad35-abc034c08e89", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 172, "real_page_number": null, "text": "160\nChapter 6. Fixed-Size Arrays and Array-Based Containers\n¸ 6.8.2\nThe Big Three\nIn general, if you are designing a class that manages dynamic memory as its data, you should also define the following \"Big Three\" as member\nfunctions of your class to handle dynamic memory:\n~Array())1. Destructor (e.g.,\nArray(const Array& other))2. Copy constructor (e.g.,\nArray& operator=(const Array& rhs))3. Overloaded assignment operator (e.g.,\nRemark: You may instead see this rule of the \"Big Three\" as the rule of the \"Big Five.\" There are two additional methods that are often\nincluded, but you do not need to implement them for this course:\nArray(Array&& other))4. Move constructor (e.g.,\nArray& operator=(Array&& rhs))5. Overloaded move assignment operator (e.g.,\nAs a brief introduction, the move constructor and assignment operator can be used to transfer ownership of data from one object to another in\ncertain situations (rather than make a copy of the data object, assign the copy to the new object, and discard the original object). The double\n(&&)ampersand indicates that an object is an reference, which is a temporary, unnamed object that is created during the execution of arvalue\nC++ program. A useful mnemonic is to think of the \"r\" in \"rvalue\" as representing \"right-hand side,\" since rvalues can never go on the\nleft-hand side of an assignment — this is because rvalues are temporary and often cannot last beyond the statement they are created in.\nint32_t a = 10;\n// '10' is an rvalue\nint32_t b = 20;\n// '20' is an rvalue\nint32_t c = a + b;\n// 'a + b' is an rvalue\nint32_t d = a;\n// 'a' NOT an rvalue, is a named user-defined variable\n&&other)The move constructor and move assignment operator accept an rvalue as an argument (e.g, and thus can only be invoked if an\nrvalue is present. This is because it is harmless to transfer an rvalue’s ownership since they are temporary objects that would otherwise be\ndestructed. If an instance of an object is ever assigned or copy constructed from an rvalue, and a move constructor and move assignment\noperator are defined, then that custom object will claim ownership of the rvalue’s data during construction (instead of making a copy of the\nrvalue and assigning the custom object to the copy). This can speed things up if an object is large and can take time to copy.\nIf no rvalue is present, then a move constructor or move assignment operator will NOT be automatically invoked. This is because\nnon-rvalues are not temporary and may be used later on, so it is not safe to transfer ownership of their data. If you want to move such a\nstd::move()variable, youmustexplicitlycallthe functiontoshowthatyouintendtotransfertheownershipofthatvariable. Forexample,\nconsider the following code:\nArray<int32_t> a{5};\nArray<int32_t> b = std::move(a);\nArray a Array b.Here, the object essentially transfers ownership of its data to the object This speeds up the assignment process since a\na aseparate copy of does not need to be made. However, after the move is complete, ends up in an unspecified state and should no longer be\nused until it is assigned a new value. You do not need to know about move semantics for 281, but we will go over it more in section 6.9.\nArrayLet’s build upon our implementation of the class to include these methods.\n1\ntemplate <typename T>\n2\nclass Array {\n3\nT* data;\n// Array data\n4\nsize_t length;\n// Array size\n5\npublic:\n6\n// Constructor\n7\nArray(size_t len = 0) : length{len} {\n8\nnew nullptr);data = (len ? T[len] :\n9\n} // Array()\n10\n11\n// Destructor\n12\n~Array();\n13\n14\n// Copy Constructor\n15\nArray(const Array& other);\n16\n17\n// Overloaded Assignment Operator\n18\noperator=(constArray& Array& rhs);\n19\n20\n// Move Constructor\n21\ndelete;Array(Array&& other) =\n22\n23\n// Overloaded Move Assignment Operator\n24\noperator=(Array&& delete;Array& rhs) =\n25\n26\nsize_t constsize() {\n27\nreturn length;\n28\n} // size()\n29\n};\nWe will not be implementing the move constructor and the overloaded move assignment operator for now, so we have disabled them by setting\ndelete deletetheir member functions equal to the keyword on lines 21 and 24. In general, setting any member function to suppresses the\nfunction and disallows it from being invoked.", "word_count": 739, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b61abfbb-d159-59b3-8d11-41c56e5fa333", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 173, "real_page_number": null, "text": "6.8 Implementing an Array-Based Container\n161\n¸ 6.8.3\nImplementing the Destructor\nArrayLet’s start by implementing the destructor, which deallocates all dynamic memory owned by the container. This is done by calling\ndelete[] data Ton the heap-allocated array. Since both lines of code take 1 step (assuming the type does not have a destructor on its own),\nArraythe overall time complexity of the destructor is Θ(1).\n1\n~Array() {\n2\ndelete[] data;\n// delete dynamic memory owned by container\n3\nnullptr;data =\n// safeguard against usage/double delete\n4\n} // ~Array()\n¸ 6.8.4\nImplementing the Copy Constructor\nThe copy constructor is also fairly straightforward. A copy constructor has no return value, has the same name as the class it is defined in, and\naccepts a to the object it wants to copy from (the reference is important: without it, a copy of the object would have to be made beforereference\nArray datathe copy constructor can even run, which is not allowed). In the case of our object, we want all the elements in the array to be\ndatacopied over, and not just the pointer. To do this, the body of the copy constructor must explicitly copy over each data element one by one:\n1\nArray(const Array& other) {\n2\nnewdata = T[other.length];\n3\nlength = other.length;\n4\n// make copies of all the elements in the data array\n5\nfor (size_t i = 0; i < length; ++i) {\n6\ndata[i] = other.data[i];\n7\n} // for i\n8\n} // Array()\ndata lengthWe can simplify this code by initializing and in a member-initializer list instead of the function body:\n1\nArray(const Array& other)\n2\ndata{new: T[other.length]}, length{other.length} {\n3\n// make copies of all the elements in the data array\n4\nfor (size_t i = 0; i < length; ++i) {\n5\ndata[i] = other.data[i];\n6\n} // for i\n7\n} // Array()\nArraySince every element in the must be visited when running the copy constructor, the time complexity of the copy constructor is Θ(𝑛),\nArray.where 𝑛represents the number of elements in the\n¸ 6.8.5\nImplementing the Overloaded Assignment Operator and the Copy-Swap Method\nArrayThe overloaded assignment operator is where things get a little bit interesting. For the class, the goal of an overloaded assignment\nArray Array.operator would be copy all the data elements from the source to the destination The following implementation does just that:\n1\noperator=(constArray& Array& rhs) {\n2\n// delete current data\n3\ndelete[] data;\n4\n// set length and data pointer\n5\nlength = rhs.length;\n6\nnewdata = T[length];\n7\n// copy contents of rhs over\n8\nfor (size_t i = 0; i < length; ++i) {\n9\ndata[i] = rhs.data[i];\n10\n} // for i\n11\nreturn *this;\n12\n} // operator=()\nArray a = a).However, this implementation would fail if someone tried to assign an object to itself (e.g. To fix this, we must add a check to\nrhs Arraymake sure is not the current that we want to assign to (by checking whether the memory addresses are the same on line 3 below):\n1\noperator=(constArray& Array& rhs) {\n2\n// don't do anything if self-assigning\n3\nif (this == &rhs) {\n4\nreturn *this;\n5\n} // if\n6\n// delete current data\n7\ndelete[] data;\n8\n// set length and data pointer\n9\nlength = rhs.length;\n10\nnewdata = T[length];\n11\n// copy contents of rhs over\n12\nfor (size_t i = 0; i < length; ++i) {\n13\ndata[i] = rhs.data[i];\n14\n} // for i\n15\nreturn *this;\n16\n} // operator=()", "word_count": 614, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c0421ff6-a867-5418-8947-cb0ae6d7f78b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 174, "real_page_number": null, "text": "162\nChapter 6. Fixed-Size Arrays and Array-Based Containers\nThis version of the overloaded assignment operator is better and works for most cases. However, it is not the most ideal way to do an assignment.\nthis == &rhsFirst, we need to do an additional check to see if every time we try to use the operator, despite the fact that self-assignment\nnearly never happens (if the programmer knows what they are doing). Second, if the program somehow fails to allocate memory on line 10, the\nprogram would throw an exception and terminate the operation; since we deallocated the array on line 7, all the original data is lost and the\nArray length Arraydata would point to deleted memory (the member variable would be messed up as well). If this happens, our would be\ncorrupted! We do not want this — if the assignment somehow fails, we still want to retain the state of our original data.\nA better method for implementing the assignment operator is known as the copy-swap method. The copy-swap method creates a temporary\nArray Array Array.of the new and swaps the contents of the old with the contents of the new The old contents are then automaticallycopy\ndeallocated when they go out of scope. This fixes the two problems with the initial approach: no self-assignment check is necessary (which\nmakes our code cleaner), and the original data is retained until an assignment is successfully made. If assignment fails and an exception is\nthrown, the failed operation does not have any side effects on the existing data (this safety is known as a guarantee). Tostrong exception\nswap Array Arrayimplement copy-swap, we will first implement a function in our class that allows us swap the contents of two objects:\n1\nvoid swap_array(Array& other) {\n2\nstd::swap(data, other.data);\n3\nstd::swap(length, other.length);\n4\n} // swap_array()\nWe will then use this method in our overloaded assignment operator function:\n1\noperator=(constArray& Array& rhs) {\n2\n// create temporary Array that stores the contents of the new Array\n3\n// a deep copy is made since this uses the copy constructor\n4\nArray temp{rhs};\n5\n// swap the original data with temp's data\n6\nswap_array(temp);\n7\n// the old data is now stored in temp, and the\n8\n// updated data is now stored in the current Array (this)\n9\nreturn *this;\n10\n} // operator=()\n11\n// temp goes out of scope, so the old data is cleaned up by destructor\narr1 = arr2A visualization of the copy-swap process is shown below, depicting what happens when is executed:\n4\ndata\nlength\narr1\n1\n2\n3\n4\n5\ndata\nlength\narr2\n5\n6\n7\n8\n9\noperator=(), arr2 temp.On line 4 of the function body of we make a deep copy of and assign it to a local variable with the name\n4\ndata\nlength\narr1\n1\n2\n3\n4\n5\ndata\nlength\narr2\n5\n6\n7\n8\n9\n5\ndata\nlength\ntemp\n5\n6\n7\n8\n9\narr1 Array operator= tempOn line 6, the contents of (the current that is being invoked on) and are swapped.\n5\ndata\nlength\narr1\n5\n6\n7\n8\n9\n5\ndata\nlength\narr2\n5\n6\n7\n8\n9\n4\ndata\nlength\ntemp\n1\n2\n3\n4\narr1 arr2,At this point, contains the contents of which is what we wanted from the assignment. If someone tried to self-assign an object\n(arr1 = arr1), Arrayto itself the copy-swap approach would just swap the with a copy of its own data, so there is no need to check this\ncondition every time an assignment is made. In addition, if the program had failed to allocate new memory, it would have thrown an exception\nArray operator=on line 4, before the original could be modified on line 6. This ensures that our implementation of does not corrupt our\ndata if there was not enough memory to make a copy.", "word_count": 660, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a9ae592b-df54-5046-a2bd-274fd93d0c64", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 175, "real_page_number": null, "text": "6.8 Implementing an Array-Based Container\n163\n*this, Array arr1).On line 9, we return which is a reference to the that we just assigned to (in this case, After line 10, the function goes\ntemp arr1)out of scope, and all local variables are automatically deallocated. In this case, (which contains the old data of is automatically\nArraycleaned up by the destructor.\n5\ndata\nlength\narr1\n5\n6\n7\n8\n9\n5\ndata\nlength\narr2\n5\n6\n7\n8\n9\n4\ndata\nlength\ntemp\n1\n2\n3\n4\nrhs.Since the assignment operator does a deep copy, its time complexity is also Θ(𝑛), where 𝑛is the size of\n¸ 6.8.6\nImplementing the Subscript Operator\noperator[] ArrayNowthatwehaveimplementedtheBigThree,wewillimplement sothatour supportsindexing. Ascoveredpreviously,it\noperator[]isgenerallyidealtoreturnreferencestoelementsthatarerequested,sincetheyarefasterthancopiesbutsaferthanpointers. Thus,\nwill be implemented to return a reference to the requested data element.\noperator[]The implementation of below is fairly straightforward. It checks to see if the provided index is valid, and then indexes\nArray data std::runtime_errorthe correct position of the object’s underlying array. If the index is not valid, the operation throws a\nexception (since we do not want to return a reference to something the user can unintentionally modify).\n1\noperator[](size_tT& idx) {\n2\nif (idx < length) {\n3\nreturn data[idx];\n4\n} // if\n5\nthrow std::runtime_error{\"Invalid index provided to operator[]\"};\n6\n} // operator[]()\nEven with this code, we are not done; there is still an issue we need to address. However, this issue does not arise from the implementation,\n\"0 1 2 3 4\".since it handles array indexing correctly. For instance, the output for the following code would be\n1\nArray<int> a{5};\n// init Array of size 5\n2\nfor (size_t i = 0; i < a.size(); ++i) {\n3\na[i] = i;\n4\nstd::cout << a[i] << \" \";\n5\n} // for i\nThe issue arises when we try to run something like this:\n6\nconst Array<int> b = a;\n7\nfor (size_t i = 0; i < b.size(); ++i) {\n8\nstd::cout << b[i] << \" \";\n9\n} // for i\nIf you try to compile this, you would get the following error:\nerror:\npassing 'const Array' as 'this' argument discards qualifiers [-fpermissive]\nconst non-const b constWhy did this happen? Recall from chapter 1 that a object call a member function! Since is defined as acannot\nArray, operator[] non-const,it is prohibited from calling since its definition is which is why the indexing on line 8 fails. For this to\nconst operator[]:work, we also need to implement a version of\n1\nconst operator[](size_t constT& idx) {\n2\nif (idx < length) {\n3\nreturn data[idx];\n4\n} // if\n5\nthrow std::runtime_error{\"Invalid index provided to operator[]\"};\n6\n} // operator[]()\noperator[] const non-const Array non-constBoth versions of need to be defined for indexing to be supported on and objects. The\noperator[] non-const Array, const const Array.definition of would be invoked for a while the definition would be invoked for a", "word_count": 538, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9208ab0b-5d3f-5111-9c2c-7246718fd584", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 176, "real_page_number": null, "text": "164\nChapter 6. Fixed-Size Arrays and Array-Based Containers\nArrayRemark: The previously implemented member functions of are assumed to be implemented the class definition. For example,within\noperator[] Arraywe wrote as if it were implemented within the definition:\n1\ntemplate <typename T>\n2\nclass Array {\n3\nT* data;\n4\nsize_t length;\n5\npublic:\n6\n...\n7\nconst operator[](size_t constT& idx) {\n8\nif (idx < length) {\n9\nreturn data[idx];\n10\n} // if\n11\nthrow std::runtime_error{\"Invalid index provided to operator[]\"};\n12\n} // operator[]()\n13\n...\n14\n};\n(::)If you want to define a member function the class definition, you will need to use the scope resolution operator to identify theoutside\nclass that the member function belongs to (with template declarations as needed). The scope resolution operator goes before the function\nname, but after the return type. Examples using the member functions we have implemented so far are shown below:\n1\ntemplate <typename T>\n2\nclass Array {\n3\nT* data;\n4\nsize_t length;\n5\npublic:\n6\n// define member function headers within the class, but\n7\n// implement them outside the class using scope resolution (::)\n8\nArray(size_t len = 0);\n9\n~Array();\n10\nArray(const Array& other);\n11\nvoid swap_array(Array& other);\n12\noperator=(constArray& Array& rhs);\n13\noperator[](size_tT& idx);\n14\nconst operator[](size_t const;T& idx)\n15\nsize_t const;size()\n16\n};\n17\n18\ntemplate <typename T>\n19\nArray<T>::Array(size_t len) : length{len} {\n20\nnew nullptr);data = (len ? T[len] :\n21\n} // Array()\n22\n23\ntemplate <typename T>\n24\nArray<T>::~Array() {\n25\ndelete[] data;\n26\nnullptr;data =\n27\n} // ~Array()\n28\n29\ntemplate <typename T>\n30\nArray<T>::Array(const Array& other)\n31\ndata{new: T[other.length]}, length{other.length} {\n32\nfor (size_t i = 0; i < length; ++i) {\n33\ndata[i] = other.data[i];\n34\n} // for i\n35\n} // Array()\n36\n37\ntemplate <typename T>\n38\nvoid Array<T>::swap_array(Array& other) {\n39\nstd::swap(data, other.data);\n40\nstd::swap(length, other.length);\n41\n} // swap_array()\n42\n43\ntemplate <typename T>\n44\nArray<T>::operator=(constArray<T>& Array& rhs) {\n45\nArray temp{rhs};\n46\nswap_array(temp);\n47\nreturn *this;\n48\n} // operator=()\n49\n50\ntemplate <typename T>\n51\nArray<T>::operator[](size_tT& idx) {\n52\nif (idx < length) {\n53\nreturn data[idx];\n54\n} // if\n55\nthrow std::runtime_error{\"Invalid index provided to operator[]\"};\n56\n} // operator[]()", "word_count": 388, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "79ce9574-7a28-5d95-85bb-5e596258b447", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 177, "real_page_number": null, "text": "6.8 Implementing an Array-Based Container\n165\n58\ntemplate <typename T>\n59\nconst Array<T>::operator[](size_t constT& idx) {\n60\nif (idx < length) {\n61\nreturn data[idx];\n62\n} // if\n63\nthrow std::runtime_error{\"Invalid index provided to operator[]\"};\n64\n} // operator[]()\n65\n66\ntemplate <typename T>\n67\nsize_t constArray<T>::size() {\n68\nreturn length;\n69\n} // size()\n¸ 6.8.7\nImplementing Insert\nArrayWhat if you wanted to implement a member function that could insert an element anywhere into the container? This process may seem\nsimple at first, but you will quickly realize it is complicated for two primary reasons:\n1. Elements in an array-based container must be stored contiguously in memory.\ndata2. Once you allocate a array, its size cannot change; if you want more space, you must allocate a new array and move the data over.\nArray. 2 1 3:Consider the following Suppose you wanted to insert the number between and\n4\ndata\nlength\narr\n1\n3\n4\n5\n2\n2 1Because elements in an array-based container must be contiguous in memory, the element must end up the element anddirectly after\n3the element after the insertion. For this to happen, every element after the insertion point must be shifted one to the right todirectly before\n2.create space for the\n4\ndata\nlength\narr\n1\n3\n4\n5\n2\n4\ndata\nlength\narr\n1\n3\n4\n5\n2\n5\ndata\nlength\narr\n1\n2\n3\n4\n5\nWe could attempt to write an insert function as follows. This function takes in two arguments: the index of insertion and the value to insert. The\ninsert(1, 2) 2function then shifts all the elements after the insertion point and inserts the value at the specified index (e.g., would insert at\n1 Array). true falseindex of the The function returns if the element was successfully inserted and if it was not.\n1\ntemplate <typename T>\n2\nbool insert(size_t idx, T val) {\n3\nif (idx < length) {\n4\nfor (size_t i = length - 1; i > idx; --i) {\n5\ndata[i] = data[i - 1];\n6\n} // for i\n7\ndata[idx] = val;\n8\nreturn true;\n9\n} // if\n10\nreturn false;\n11\n} // insert()\nidxIf we analyze this function’s complexity, we can conclude the following (assuming that is valid and the array has 𝑛elements):\n• The time complexity is Θ(1): this occurs when an element is inserted at the very end of the array (since no shifting is needed).best-case\n• The time complexity is Θ(𝑛): this occurs when an element is inserted at the very beginning of the array (since all elementsworst-case\nafter it must be shifted).\n• The time complexity is Θ(𝑛): if we consider all possible cases, the average number of elements we have to shift is (and𝑛∕2average-case\n= after dropping coefficients).Θ(𝑛∕2) Θ(𝑛)\nHowever, this insertion method does not work in all cases because of the other issue: once an array is allocated, its size is fixed! For instance, if\ndata 2is initialized to a heap-allocated array of size 4, that array cannot expand in size. As a result, we would not even be able to insert using", "word_count": 539, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b9c54e51-279a-581a-bf1c-0f230e634364", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 178, "real_page_number": null, "text": "166\nChapter 6. Fixed-Size Arrays and Array-Based Containers\ndatathe implementation above. If we wanted our underlying array to grow with the size of the data, we would have to allocate a larger array\nelsewhere and move our current data there.\n2 1 3, 2Let’s consider a simpler insertion example. Instead of inserting between and let’s suppose we wanted to insert at the end of the\nArray<int32_t> arr{4};).array. The array is initialized to have a size of 4 (e.g.,\n4\ndata\nlength\narr\n1\n3\n4\n5\n2\ndata data = new int32_t[4];),Since the underlying array was initialized to hold 4 elements (e.g., its size is fixed at 4 and cannot be\n5thchanged. If we want to store more than 4 elements, we would have to allocate an entirely new, larger array so that the element can be added\nin. One way to do this is to allocate an entirely new array that can hold 5 elements, copy over the original 4 elements, deallocate the old array,\n5thand add in the element, as shown:\n4\ndata length\narr\n1\n3\n4\n5\nallocate new memory\n4\ndata length\narr\n1\n3\n4\n5\n1\n3\n4\n5\ncopy elements over\n4\ndata length\narr\n1\n3\n4\n5\n1\n3\n4\n5\nreset and deallocate\n5\ndata length\narr\n1\n3\n4\n5\n2\n2insert into array\nThis approach, however, is not the most performant. This is because the copying process takes time for 𝑛elements, and you would need toΘ(𝑛)\nmake a copy every time an element is inserted beyond the array’s capacity! For instance, if you had an array with 1,000 elements, adding the\n1,001st 1,002ndand elements would require two array allocations and 2,001 copies (1,000 on the first allocation; 1,001 on the second).\nTo reduce the number of copies needed while still ensuring space is available for additional elements, we could plan in advance and allocate\nmore memory than needed. Instead of incrementing the capacity by 1 every time, we could prepare for future insertions by the capacitydoubling\nof the array whenever reallocation is done. For example, instead of increasing our array capacity from 4 to 5, we could increase it from 4 to 8:\n4\n4\ndata\nsize\ncap.\narr\n1\n3\n4\n5\nallocate new memory for\n8 elements instead of 5\n4\n8\ndata\nsize\ncap.\narr\n1\n3\n4\n5\n1\n3\n4\n5\n2\ncopy elements over, deallocate\ndataand reset pointer\n6th, 7th, 8th 9thIf we do this, adding the and elements would not require the array to be copied. When the element is added, the array is\n10th 16threallocated and its elements are copied over, but the new array capacity would double to 16 — this would allow us to insert the to\nelements in constant time without any additional copying overhead.\nIf we double on reallocation, we would allocate more space than we need, and the size and capacity of the underlying data array may be\ndifferent. Thus, wewillneedtokeeptrackofboththesizeandcapacityvalues. Thecapacitywouldtelluswhenwewouldhavetogrowourarray\nand reallocate, while the size would tell us the index of the last valid element (e.g., if the array had a capacity of 8 but only 5 valid elements, the\nArrayuser of the array should not be accessing indices 5 to 7). Using this implementation, we can redefine our member variables as follows:\n1\ntemplate <typename T>\n2\nclass Array {\n3\nT* data;\n// pointer to Array data\n4\nsize_t size;\n// number of valid elements in 'data' array\n5\nsize_t capacity;\n// capacity of underlying 'data' array\n6\n...\n7\n};\nsize capacityHere, the membervariablerepresentsthenumberofvalidelementsthatareinthearray, whilethe membervariablerepresents\nsize 5the maximum number of elements that the underlying array can hold. In the previous example, the of the array would be and the\n8.capacity of the array would be", "word_count": 683, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "951d75ac-3785-5ba2-94ed-6147fe785e28", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 179, "real_page_number": null, "text": "6.9 Lvalues, Rvalues, and Move Semantics\n167\nCould we plan further in advance by tripling or quadrupling the array capacity with every new allocation? We could, but this would be wasteful\nwhen it comes to memory. Although tripling or quadrupling capacity could reduce the number of copies we have to make, there is no guarantee\nthat all this memory will be used, and the improvement in performance is asymptotically insignificant.\nArrayWe have gotten quite far with our custom-implemented class in this section, implementing a container that abstracts away the work\nof memory allocation from the user and supports several features that are not present with just a standard C-style array. However, there is still\nmore we can do. For instance, we can implement a member function that can delete elements from our container, or a member function that can\ndatamanually change the capacity of the underlying array.\nArrayHowever, we will not be implementing any more member functions in our custom class. First, several of these features can get\nquite complicated. Insertion and deletion, for instance, both require elements in the array to be shifted. More importantly, however, doing so\nwould not be necessary — most of the work has already been done for us, so we do not need to spend time reinventing the wheel! C++ already\nprovides us with an array-based container that manages dynamic memory and performs all of the functionalities we have implemented in this\nstd::vector<>,section (and much more!). This container is known as the which we will cover in the next chapter.\n6.9\nLvalues, Rvalues, and Move Semantics (✽)\nIn this section, we will be briefly discussing the concept of move semantics, which allows resources to be transferred between objects instead\nof copied. Move semantics can be used to optimize the performance of your code, as it allows you to avoid making unnecessary copies of\ntemporary objects that you know are not going to exist for much longer.\nRemark: The material in this section is beyond the scope of this class, and you will need to know it for projects or exams. That beingnot\nsaid, you are still allowed to take advantage of move semantics in your projects or lab assignments if you know how they work. If you don’t\nknow how move semantics work and don’t have the time or commitment to read this section, that’s perfectly fine as well! The runtime\nbenchmarks that you are required to meet for coding assignments in this class are based off of instructor solutions that do not take advantage\nof these optimizations.\n¸ 6.9.1\n(✽)Lvalues and Rvalues\nTo begin, we must first introduce the concepts of lvalues and rvalues. Lvalues, historically known as \"left-hand\" values since they can go on the\nleft-hand side of an assignment, are named objects with an identifiable location in memory (because of this, lvalues are also known as locator\nvalues). Rvalues, historically known as \"right-hand\" values since they can only go on the right-hand side of an assignment, are temporary,\nunnamed objects that cannot be assigned to (i.e., they cannot go on the left-hand side of an assignment). For example, consider the following:\nint32_t x = 281;\nx,There are two components to this assignment. On the left-hand side, we have the integer variable which is named and has an identifiable\nx 281,address in memory. Thus, the variable is an lvalue. On the right-hand side, we have the number which is a temporary numeric value that\n281has no identifiable location in memory until it is assigned. Thus, is an rvalue.\nLvalues can be assigned from rvalues or other lvalues, but rvalues cannot be assigned to at all. For instance, the following is okay since both\nx yand are lvalues, and thus can be assigned to:\nint32_t x = 281;\nint32_t y = x;\n// OK, x and y are both lvalues\n281However, the following is not okay, since is an rvalue, and thus cannot be assigned to:\nint32_t x = 281;\n281 = x;\n// NOT OK, 281 is an rvalue\nIf you return a value from a function that returns by value (instead of by reference), then that value will be returned as an rvalue. In the following\nget_favorite_class() x:code, returns an rvalue, which is then assigned to the lvalue\n1\nint32_t get_favorite_class() {\n2\nstatic int32_t fav_class = 281;\n3\nreturn fav_class;\n4\n} // get_favorite_class()\n5\n6\nint main() {\n7\nint32_t x = get_favorite_class();\n// x is lvalue, get_favorite_class() is rvalue\n8\n} // main()\nget_favorite_class()Because is an rvalue here, the following would not work:\nget_favorite_class() = 370;\n// not OK, get_favorite_class() is an rvalue\nget_favorite_class()However, this would work if the returned by reference instead of by value. If a function returns by reference, it\nwould return an lvalue reference, which can be assigned to:\n1\nint32_t& get_favorite_class() {\n2\nstatic int32_t fav_class = 281;\n3\nreturn fav_class;\n4\n} // get_favorite_class()\n5\n6\nint32_t main() {\n7\nget_favorite_class() = 370;\n// OK, get_favorite_class() is an lvalue reference\n8\n} // get_favorite_class()", "word_count": 846, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5f6c558f-f1c9-5456-851b-a56b9fe72d74", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 180, "real_page_number": null, "text": "168\nChapter 6. Fixed-Size Arrays and Array-Based Containers\nYou cannot construct an lvalue reference from an rvalue; if you declare an lvalue reference, you must assign it with another lvalue:\nint32_t& x = 281;\n// not OK, since 281 is an rvalue\nconst:There is an exception, however: an lvalue reference be assigned to an rvalue if the lvalue reference is declared ascan\nconst int32_t& x = 281;\n// OK, since x is a const lvalue reference\nThis distinction is important. For instance, consider the following two functions:\n1\nvoid foo(std::string& s) {\n2\n/* ... do stuff ... */\n3\n} // foo()\n1\nvoid bar(const std::string& s) {\n2\n/* ... do stuff ... */\n3\n} // bar()\nfoo() non-const bar()Because the function takes in a lvalue reference, it can only be called on other lvalues. However, the function takes\nconstin a lvalue reference, which allows it to be called on both lvalues and rvalues:\n1\nstd::string str1 = \"EECS\";\n2\nstd::string str2 = \"281\";\n3\n4\nfoo(str1);\n// OK, str1 is lvalue\n5\nfoo(str2);\n// OK, str2 is lvalue\n6\nfoo(str1 + str2);\n// NOT OK, str1 + str2 is rvalue\n7\nfoo(\"EECS 281\");\n// NOT OK, \"EECS 281\" is rvalue\n8\n9\nbar(str1);\n// OK, str1 is lvalue\n10\nbar(str2);\n// OK, str2 is lvalue\n11\nbar(str1 + str2);\n// OK, str1 + str2 is rvalue, but lvalue reference parameter is const\n12\nbar(\"EECS 281\");\n// OK, \"EECS 281\" is rvalue, but lvalue reference parameter is const\nSo, why do we care so much about lvalues and rvalues, and how can they help us optimizeour code? The reason is that we can operate on lvalues\nand rvalues with different levels of care. If we are given an lvalue, we have to be careful with how we work with its data, since it is entirely\npossible that something else will need to use it after we are done. However, if we are given an rvalue, we know that the object we are working\non is temporary, and that we can abuse it however we want without worrying about consequences down the road. This gives us the ability to\nmake optimizations that we could not have done with lvalues, such as stealing an object’s resources and distributing them somewhere else.\n(&&),This leads us to the concept of rvalue references. An rvalue reference is denoted with a double ampersand and can only take on\ntemporary rvalue objects. For example, consider the following function:\n1\nvoid baz(std::string&& s) {\n2\n/* ... do stuff ... */\n3\n} // baz()\nUnlike the previous two functions, this function only accepts rvalues (since the string is passed in with a double ampersand).\n1\nstd::string str1 = \"EECS\";\n2\nstd::string str2 = \"281\";\n3\n4\nbaz(str1);\n// NOT OK, str1 is lvalue\n5\nbaz(str2);\n// NOT OK, str2 is lvalue\n6\nbaz(str1 + str2);\n// OK, str1 + str2 is rvalue\n7\nbaz(\"EECS 281\");\n// OK, \"EECS 281\" is rvalue\nRvalue references allow us to perform different optimizations depending on whether an object is temporary or not. If we have a function that can\nperform special optimizations on rvalues, we can simply write an overloaded version of that function that takes in an rvalue reference, as shown:\n1\nvoid qux(const std::string& s) {\n2\n// runs if input is an lvalue\n3\n// even though rvalues can be accepted as const lvalue references\n4\n// rvalues will always be sent to the && version if one exists\n5\n...\n6\n} // qux()\n7\n8\nvoid qux(std::string&& s) {\n9\n// runs if input is a temporary rvalue\n10\n// can do rvalue optimizations (e.g., safely steal data from s)\n11\n...\n12\n} // qux()\n13\n14\nint main() {\n15\nstd::string str1 = \"EECS\";\n16\nstd::string str2 = \"281\";\n17\n18\nqux(str1);\n// first version runs (const& s)\n19\nqux(str2);\n// first version runs (const& s)\n20\nqux(str1 + str2);\n// second version runs (&&s)\n21\nqux(\"EECS 281\");\n// second version runs (&&s)\n22\n} // main()", "word_count": 682, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "63206d09-b7d0-5694-941b-eb783818f27a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 181, "real_page_number": null, "text": "6.9 Lvalues, Rvalues, and Move Semantics\n169\n¸ 6.9.2\n(✽)Implementing the Move Constructor\nOne practical use case of rvalue references comes up with move semantics, which allows us to avoid unnecessary copies of temporary rvalue\nStringobjects. As an example, consider the following implementation of a class:\n1\nclass String {\n2\nchar* data;\n// pointer to char array on heap\n3\nsize_t length;\n// length of String\n4\npublic:\n5\n// constructor from char*\n6\nString(const char* str) {\n7\nlength = strlen(str);\n8\nnew char[lengthdata = + 1];\n9\nstd::cout << \"Char Ctor: Heap allocation made\" << std::endl;\n10\nfor (size_t i = 0; i < length; ++i) {\n11\ndata[i] = str[i];\n12\n} // for i\n13\ndata[length] = '\\0';\n14\n} // String()\n15\n16\n// copy constructor from another String\n17\nString(const String& other)\n18\ndata{new char[other.length: + 1]}, length{other.length} {\n19\nstd::cout << \"Copy Ctor: Heap allocation made\" << std::endl;\n20\nfor (size_t i = 0; i < length; ++i) {\n21\ndata[i] = other.data[i];\n22\n} // for i\n23\ndata[length] = '\\0';\n24\nstd::cout << \"Copy Ctor: String successfully copied\" << std::endl;\n25\n} // String()\n26\n/* ... other member functions ...*/\n27\n};\nStudent StringNow, suppose we have a object that stores a object internally, as shown below:\n29\nclass Student {\n30\nString name;\n31\npublic:\n32\n// constructor\n33\nStudent(const String& name_in)\n34\n: name{name_in} {}\n35\n/* ... other member functions ... */\n36\n};\nStudentLet’s instantiate a object:\n38\nint main() {\n39\nStudent s{\"Alice\"};\n40\n} // main()\nmain(),If we try to run the following would get printed:\nChar Ctor: Heap allocation made\nCopy Ctor: Heap allocation made\nCopy Ctor: String successfully copied\nStudent StringWhat happened here? Even though we created a single object that stores a single object, we somehow ended up making two\nStudentheap allocations and a copy operation. When the object was constructedThis is because we ended up copying a temporary rvalue!\n\"Alice\" Stringon line 39, the string was first converted into a temporary object (an rvalue), which resulted in the first heap allocation (line\nname Student8). Then, this rvalue was passed into the constructor and assigned into the member variable of the object. This invokes the\nString copy constructor, which makes a of the temporary rvalue that we just created, allocates new memory for this copy (line 18), andcopy\nthen copies the contents of the rvalue to this new copy. The rvalue is then destructed after this is all done.\nObviously, this is inefficient: we ended up making two heap allocations and a copy operation for no reason, when we could have just\nStringconstructed a object using a single heap allocation. This is where move semantics come in; instead of doing this excess work, we\nString name Studentcan the data from the first we constructed to the variable of the object. By doing so, only a single heaptransfer\nStringallocation is needed: when the is first constructed on line 39.\nFor this to work, we will need to overload the standard copy constructor to accept rvalue references. A constructor that accepts an rvalue of\nthe same type is known as a move constructor, and it is invoked when an object is initialized to an rvalue. Unlike the copy constructor, the\nString Stringmove constructor does not need to allocate new memory and copy the over. Instead, since we know the passed into the move\nconstructor is an rvalue, we can safely take ownership of its data without the need to make a new copy, as the rvalue will get destroyed afterward.", "word_count": 616, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3f2be22f-863d-5e92-a72c-3f3bb445fa68", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 182, "real_page_number": null, "text": "170\nChapter 6. Fixed-Size Arrays and Array-Based Containers\nThe move constructor is implemented below:\n1\nclass String {\n2\nchar* data;\n3\nsize_t length;\n4\npublic:\n5\n/* ... previously defined members not shown to save space ... */\n6\n// move constructor\n7\nnoexceptString(String&& other)\n8\n: data{other.data}, length{other.length} {\n9\nnullptr;other.data =\n10\nother.length = 0;\n11\nstd::cout << \"Move Ctor: Data transferred from rvalue\" << std::endl;\n12\n} // String()\n13\n/* ... other member functions ...*/\n14\n};\ndataAfter the move constructor \"steals\" the data from the rvalue that was passed in (via a shallow copy on line 8), it sets the rvalue’s pointer\nnullptr.to This step is important, since the rvalue’s data will get destructed after the move constructor completes! If you don’t set\nnullptr,the rvalue’s data pointer to the data you stole will also end up getting deleted when the rvalue gets cleaned up!\nnoexcept.Remark: Moveconstructors(andmoveassignmentoperators,whichwewilldiscusslater)shouldbedeclaredas Thisessentially\nnoexceptindicates that the move constructor (and the move assignment operator) should not throw any exceptions. The keyword was\ncovered back in chapter 1 (in the exceptions section).\nStudentWe will also add a member function to the class to handle rvalue references for the move constructor:\n1\nclass Student {\n2\nString name;\n3\npublic:\n4\n// constructor (for lvalues)\n5\nStudent(const String& name_in)\n6\n: name{name_in} {}\n7\n// constructor (for rvalues)\n8\nStudent(String&& name_in)\n9\n: name{name_in} {}\n10\n/* ... other member functions ... */\n11\n};\nIf we run the code again and check the output, this is what we get:\nChar Ctor: Heap allocation made\nCopy Ctor: Heap allocation made\nCopy Ctor: String successfully copied\nname_inNothing changed! Why did this happen? Shouldn’t line 9 invoke the move constructor, since is passed in as an rvalue reference?\nStringWell, it turns out there is a catch! It is true that the constructor on lines 8-9 only runs if the that is passed is an rvalue reference.\nString (name_in)However, once the is passed into this constructor, it gains a name and an identifiable location in memory (i.e., you can\ntake its address). Thus, the rvalue reference in the function parameter For the movebehaves like an lvalue reference inside the function!\nname_in Stringconstructor to be invoked, you would have to explicitly cast back into an rvalue in the constructor. This can be done using\nstd::move()the function, which is added to line 9 below.\n1\nclass Student {\n2\nString name;\n3\npublic:\n4\n// constructor (for lvalues)\n5\nStudent(const String& name_in)\n6\n: name{name_in} {}\n7\n// constructor (for rvalues)\n8\nStudent(String&& name_in)\n9\n: name{std::move(name_in)} {}\n10\n/* ... other member functions ... */\n11\n};\nStringAfter running this modified code, we get the following output, which is what we want. The object is only allocated once, and its data\nname Studentis to the member of the object instead of copied.transferred\nChar Ctor: Heap allocation made\nMove Ctor: Data transferred from rvalue\nstd::move()Remark: The function casts its argument to an rvalue; it does not actually physically move anything in memory. When\nstd::move()you call on an object, you are essentially telling the compiler that it can treat that object as an rvalue, and that its data\ncan be safely stolen without repercussions (the actual \"stealing\" is done in the move constructor or move assignment operator). If you call\nstd::move() on an object to transfer its data, it ends up in an unspecified state, and you should use it until it is assigned a new value.not", "word_count": 613, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8eccea9f-6731-5465-8ee8-67fb228f8bc8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 183, "real_page_number": null, "text": "6.9 Lvalues, Rvalues, and Move Semantics\n171\n¸ 6.9.3\n(✽)Implementing the Overloaded Move Assignment Operator\nThe rules for transferring ownership of data also apply to the assignment operator. The standard overloaded assignment operator implementation\n(operator=) a = b, bhas to make a copy of the object it is passed in before assigning it (i.e., if you want to assign you have to make a copy of\na).andthenassignitto However,ifyouareassigninganobjectfromarvalue,youcantakeadvantageofrvalueoptimizationsbyimplementating\nthe overloaded move assignment operator. The implementation of this operator is very similar to the implementation of the move constructor:\nsimply overload the operator to take in an rvalue reference, and then steal all the data associated with this rvalue. An implementation using the\nStringabove object is shown below:\n1\nclass String {\n2\nchar* data;\n3\nsize_t length;\n4\npublic:\n5\n/* ... previously defined members not shown to save space ... */\n6\n// move assignment operator\n7\noperator=(String&& noexceptString& rhs) {\n8\n// don't do anything if self-assigning\n9\nif (this == &rhs) {\n10\nreturn *this;\n11\n} // if\n12\n// delete original data\n13\ndelete[] data;\n14\n// steal length and data pointer from rvalue\n15\nlength = rhs.length;\n16\ndata = rhs.data;\n17\n// clear rvalue's data so the data you stole is\n18\n// not deleted when the rvalue gets destructed\n19\nrhs.length = 0;\n20\nnullptr;rhs.data =\n21\n} // operator=()\n22\n/* ... other member functions ...*/\n23\n};\nstd::swap()Much like the standard assignment operator, we can get rid of the self-assignment check by using to swap the contents of an\nobject with the contents of the rvalue it is being assigned.\n1\nclass String {\n2\nchar* data;\n3\nsize_t length;\n4\npublic:\n5\n/* ... previously defined members not shown to save space ... */\n6\n// move assignment operator\n7\noperator=(String&& noexceptString& rhs) {\n8\nstd::swap(this->length, rhs.length);\n9\nstd::swap(this->data, rhs.data);\n10\nreturn *this;\n11\n} // operator=()\n12\n/* ... other member functions ...*/\n13\n};\nstd::swap()Remark: Before C++11 introduced move semantics, the function was essentially implemented like this:\n1\ntemplate <typename T>\n2\nvoid swap(T& a, T& b) {\n3\nT temp = a;\n4\na = b;\n5\nb = temp;\n6\n} // swap()\nstd::swap()However, now that we have covered move semantics, we can build a better function that does not perform unnecessary\ncopies. Instead of making copies of the data we want to swap, we can use move construction and move assignment to transfer ownership of\nstd::swap()data instead. This is essentially how is implemented in C++11 and later:\n1\ntemplate <typename T>\n2\nvoid swap(T& a, T& b) {\n3\nT temp = std::move(a);\n4\na = std::move(b);\n5\nb = std::move(temp);\n6\n} // swap()", "word_count": 484, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "65166718-506e-5c12-bef9-41d229163ae3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 184, "real_page_number": null, "text": "172\nChapter 6. Fixed-Size Arrays and Array-Based Containers\n¸ 6.9.4\n(✽)Return Value Optimization\nstd::move()You should avoid using to return a local object from a function, even if it may seem like an optimization. This is because it\nsuppresses something known as return value optimization (RVO). Consider the following function:\n1\ngen_big_string(size_tstd::string n) {\n2\nstd::string big_string;\n3\n/* ... generate big string ... */\n4\nreturn big_string;\n5\n} // gen_big_string()\n6\n7\nint main() {\n8\nstd::string str = gen_big_string(281);\n9\n} // main()\nstd::move() big_string big_stringIt may be tempting to call on when it is returned, since we do not want the to be copied upon\nreturn. However, compilers are smart enough to detect situations like this! If a function returns a local object by value, and the object’s type\nmatches the return type of the function, the compiler may use RVO to build the object Thatdirectly in its intended destination in memory.\nbig_string str,is, instead of building a temporary copy of and then copying (or moving) it to the memory address of RVO allows the\nbig_string str.contents of to be constructed at the memory address of If RVO occurs, no copying or moving needs to be done!directly\nBecause the copy/move operation is elided (i.e., omitted) by the compiler under the specified conditions, the RVO procedure is formally known\nas a optimization.copy elision\nstd::move()However, consider what happens if we attempt to call on a local object before we return it from a function:\n1\ngen_big_string(size_tstd::string n) {\n2\nstd::string big_string;\n3\n/* ... generate big string ... */\n4\nreturn std::move(big_string);\n5\n} // gen_big_string()\n6\n7\nint main() {\n8\nstd::string str = gen_big_string(281);\n9\n} // main()\nstd::move()This may seem innocuous, but it may actually be detrimental to the performance of your code! If you call on a local object\nbefore you return it from a function, you are no longer returning a local object of the same type as the return type of the function. Instead, you\nstd::string&&),are returning an rvalue reference to that object (in this case, which violates the conditions required for RVO. As a result,\nbig_stringthe compiler is unable to take advantage of copy elision and cannot construct directly in its intended destination. Instead, it\nbig_string gen_big_string()has to build a separate instance of the object in the stack frame of and then transfer its data into the\nstr main(). std::move()memory address of in the stack frame of This is why you should not call when returning a local object that is\neligible for RVO: by doing so, you would prevent your compiler from performing copy elision optimizations!\n¸ 6.9.5\n(✽)Perfect Forwarding and Forwarding References\nLastly, we will briefly discuss a concept related to move semantics known as perfect forwarding. With perfect forwarding, a templated function\n&&),accepts a special type of reference known as a forwarding reference (denoted using a double ampersand which can be used in conjunction\nstd::forward<>with to preserve the original value category of its argument (i.e., whether it is an lvalue or rvalue). An example of perfect\nfoo() bar().forwarding is shown below, where the templated function takes in an argument and passes it into another function\n1\ntemplate <typename T>\n2\nvoid foo(T&& arg) {\n3\nbar(std::forward<T>(arg));\n4\n} // foo\narg foo() bar() argIf gets passed into as an lvalue reference, it also gets passed to the method as an lvalue reference. If gets passed into\nfoo() bar()as an rvalue reference, it also gets passed to the method as an rvalue reference.\nstd::forward<> std::forward<>At this point, you might be wondering: what is the purpose of the method on line 3? The use of\nis needed here because, as we mentioned previously, all function parameters behave like lvalues, even if they were initially passed in as an rvalue\narg foo()(since the function argument itself is a named object). Hence, the variable on line 2 will always be an lvalue even if was called\nstd::forward<> argon an rvalue. What does here is that it performs a cast on based on the value category of the argumentconditional\nfoo(). foo() std::forward<>that was passed into If was called on an lvalue reference, then does not need to do anything (since the\narg bar() foo()function parameter is already an lvalue), and is thereby called on the same lvalue reference. However, if was called on an\nstd::forward<> arg bar().rvalue reference, casts into an rvalue before passing it into This ensures that an rvalue that is passed into\nfoo() bar().will also be passed as an rvalue into\nstd::forward<> foo()While we will not go into too much detail here, is able to detect whether was invoked on an lvalue or an rvalue\nusing a process known as when deducing templates. When the compiler deduces templates involving references:reference collapsing\nT& & T&• An argument of type resolves to (lvalue reference).\nT& && T&• An argument of type resolves to (lvalue reference).\nT&& & T&• An argument of type resolves to (lvalue reference).\nT&& && T&&• An argument of type resolves to (rvalue reference).", "word_count": 870, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cc0d2037-c510-5ebc-b873-5b64fccd649d", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 185, "real_page_number": null, "text": "6.9 Lvalues, Rvalues, and Move Semantics\n173\nWhenever you have a forwarding reference in the following format:\n1\ntemplate <typename T>\n2\nvoid foo(T&& arg);\nfoo()the provided reference collapsing rules make it possible to determine whether was called on an lvalue or an rvalue. If an lvalue reference\nfoo(T&&) foo(Thing&&&), foo(Thing&)Thing foo(),of type were passed to then gets deduced as which resolves to (and thus the\nfoo() Thingcompiler knows that must have been called on an lvalue reference). On the other hand, if an rvalue reference of type were\nfoo(T&&) foo(Thing&&&&), foo(Thing&&)foo(),passed to then gets deduced as which resolves to (and thus the compiler knows\nfoo()that must have been called on an rvalue reference).\nPerfectforwardingandforwardingreferencesarequitevaluable: theycanbeusedtoavoidexcessivecopying,andtheyalsoallowyoutowrite\nmethods that support many different input value types without having to implement multiple overloads (e.g., one version that accepts lvalues, an-\notherversionthatacceptsrvalues,etc.). Oneofthebestusecasesofperfectforwardingoccurswhenanobjectneedstobemovedthroughmultiple\nfunctioncallsbetweenitspointofcreationanditsdestination. Thisstrategyisemployedbymanystandardlibraryfunctionstosimplifytheprocess\nstd::vector<>::emplace_back(Args&&... args),of object creation (one notable example we will cover in the next chapter is\nwhich takes in a set of constructor arguments and forwards it to the constructor of the object to be created, which is then directly constructed at\nthe back of the vector).\nChapter 6 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Which of the following statements is/are TRUE about arrays in C++?\nI. The size of an array may be changed during runtime.\nII. Elements in an array are contiguous in memory.\na1 a2, a1 = a2 a2 a1.III. Given two arrays of the same type and the expression performs an element-by-element copy from into\nA) I only\nB) II only\nC) I and II only\nD) II and III only\nE) I, II, and III\n2. Suppose you had a 2-D array with 7 rows and 9 columns. You are told that an element 𝑘is located at row index 5 and column index 6. Zero\nindexing is used. If this 2-D array were instead represented as a 1-D array, what would the index of 𝑘be?\nA) 11\nB) 30\nC) 42\nD) 51\nE) 60\n3. Consider the following snippet of code:\n1\nint main () {\n2\nconstexpr size_t sz = 5;\n3\nint32_t arr[sz];\n4\nint32_t eecs = 281;\n5\n6\nfor (int& i : arr) {\n7\ni = eecs++;\n8\n} // for i\n9\n10\nfor (int32_t j = 0; j < sz; ++j) {\n11\narr[j] = arr[j + 1];\n12\n} // for j\n13\n14\nfor (int32_t k = 1; k <= sz; ++k) {\n15\narr[k - 1] = k;\n16\n} // for k\n17\n} // main()\nThe above code has a bug. On which line is this bug located?\nA) Line 3\nB) Line 6\nC) Line 7\nD) Line 10\nE) Line 14", "word_count": 568, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "100c26ce-107e-5ebc-9e9e-5925547be54d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 186, "real_page_number": null, "text": "174\nChapter 6. Fixed-Size Arrays and Array-Based Containers\n4. Which of the following statements is FALSE?\nA) If a container stores pointers to dynamic memory, only the copy constructor needs to be overloaded to support deep copies\nB) A drawback of storing data by value in a container is that large objects may be expensive to copy\nC) If pointers to data are stored in a container, data ownership may not be exclusive to that container\nD) It is typically safer to return an element in a container by reference than it is to return a pointer to the element\nE) None of the above\n5. What is the worst-case time complexity of copying an array of 𝑛elements to another array, if you use the most efficient algorithm?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n6. The copy-swap method is a technique that is primarily designed for implementing/overloading which one of the following?\nA) Constructor\nB) Copy Constructor\nC) Destructor\nD) Assignment Operator\nE) None of the above\n7. Which of the following statements is TRUE regarding the storage of data in a container?\nA) A container that stores primitive types by value has the drawback of taking a long time to copy, and is thus not often used\nB) Returning a pointer to data stored inside a container could be dangerous, as you have no control over what someone could do with\nthat pointer\nC) Storing data by pointer in a container has the unique drawback in that its data cannot be initialized nor modified\nD) If you need to store data that is shared by multiple parts of your program, then this shared data must be stored in a container by value\nE) None of the above\n8. What is the time complexity of inserting an element at the of an array, assuming that the array has enough capacity to support the newback\nelement without reallocation?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n9. What is the time complexity of inserting an element at the of an array, assuming that the array has enough capacity to support the newfront\nelement without reallocation?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n10. Which of the following statements is TRUE regarding sequential and random access?\nI. Sequential access processes elements by starting at the beginning of a container, whereas random access directly finds an element\nwithout sequentially accessing all previous elements.\nII. There exist container interfaces that do not support random access at all.\nIII. Random access is possible in arrays because their elements are homogeneous in size and stored contiguously in memory.\nA) I only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n11. Templated container classes often include some type of \"get element\" operation that can be used to access an element in the container.\nget_element()Which function header prototype is typically the best choice for a method for a templated container that can hold any\nTYPE,type assuming that you do NOT want the container’s data to be modified?\nconst TYPE& get_element(int);A)\nTYPE& get_element(int);B)\nTYPE get_element(int);C)\nconst TYPE* get_element(int);D)\nE) None of the above", "word_count": 543, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3ee9fccf-9ab9-536e-b5a9-79cf661911ad", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 187, "real_page_number": null, "text": "6.9 Lvalues, Rvalues, and Move Semantics\n175\n12. Which of the following statements is FALSE?\nconst non-const operator[]A) Having both a and implementation of can help the compiler optimize code for speed\nB) If only a non-const version of is implemented for type T, then would fail on a object of typeoperator[] operator[] const T\noperator[] constC) If does not modify the object it is called on, then the version will always be run\noperator[]D) should return a reference to the element that is accessed\nE) None of the above\nArray13. Consider the following code, which provides a partial definition of an object.\n1\nstruct Array {\n2\ndouble* data;\n3\nsize_t length;\n4\nArray(size_t len = 0) : length{len} {\n5\nnew double[len] nullptr;data = len ? :\n6\n} // Array()\n7\n8\ndouble& operator[](size_t idx) {\n9\nif (idx < length) {\n10\nreturn data[idx];\n11\n} // if\n12\nthrow runtime_error(\"invalid index\");\n13\n} // operator[]\n14\n15\nconst double& operator[](size_t constidx) {\n16\nif (idx < length) {\n17\nreturn data[idx];\n18\n} // if\n19\nthrow runtime_error(\"invalid index\");\n20\n} // operator[] const\n21\n};\n22\n23\n// special operator<< for Array\n24\noperator<<(std::ostream& conststd::ostream& os, Array& a) {\n25\nfor (size_t i = 0; i < a.length; ++i) {\n26\nos << a[i] << \" \";\n27\n} // for i\n28\nreturn os;\n29\n} // operator<<()\n30\n31\nint main() {\n32\nArray a{3};\n33\na[0] = 5;\n34\na[1] = 4;\n35\na[2] = 3;\n36\nfor (size_t i = 0; i < std::min(a[0], a[a.length - 1]); ++i) {\n37\nstd::cout << a << \" \";\n38\n} // for i\n39\n} // main()\nmain(), const operator[]While running how many times is the version of called?\nA) 9\nB) 11\nC) 15\nD) 17\nE) 20\nArray14. Consider the following implementation of an class’s copy constructor:\n1\ntemplate <typename T>\n2\nclass Array {\n3\nT* data;\n4\nsize_t length;\n5\npublic:\n6\nArray(const Array arr)\n7\ndata{new: T[arr.length]}, length{arr.length} {\n8\nfor (size_t i = 0; i < length; ++i) {\n9\ndata[i] = arr.data[i];\n10\n} // for i\n11\n} // Array()\n12\n};\nDoes the above copy constructor work as expected? If not, which line(s) of the above code is/are implemented incorrectly?\nA) The copy constructor as written is correctly implemented and works without issue\nB) No, there is an issue on line 6\nC) No, there is an issue on line 7\nD) No, there is an issue on line 8\nE) More than one of (B), (C), and (D)", "word_count": 447, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3cbb2288-30ef-5a59-8ab9-6fd78a9a7ae7", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 188, "real_page_number": null, "text": "176\nChapter 6. Fixed-Size Arrays and Array-Based Containers\nChapter 6 Exercise Solutions\n1. The correct answer is (B). Only statement II is true, since elements in an array are stored contiguously in memory. Statement I is false\noperator=because arrays are fixed-size and cannot be changed at runtime. Statement III is false because (without an explicit overload)\nwill only perform a shallow copy of the array that does not copy each of the individual elements over.\n2. The correct answer is (D). Using the formula for calculating the index, we get row num_columns + column = 5 9 + 6 = 45 + 6 = 51.× ×\nj sz - 1, arr[sz], j3. The correct answer is (D). When has a value of the program attempts to access which is invalid. To fix this, must\nsz - 1 sz.be less than rather than just\nThe correct answer is (A). Statement (A) is false, since the copy constructor is not the only thing that needs to be overloaded for the4.\ncontainer to manage its dynamic memory properly. You will also need to implement the other members of the big three (destructor and\noverloaded assignment operator).\n5. The correct answer is (C). To copy an array of 𝑛elements, you cannot do better than since you have to visit each element at leastΘ(𝑛)\nonce to copy it. There is no need to do any work beyond either; as the number of elements you have to copy grows, the number ofΘ(𝑛)\noperations you need will also grow linearly.\n6. The correct answer is (D). Copy-swap is a technique devised to implement the overloaded assignment operator by leveraging existing\nimplementations of the copy constructor and destructor. With copy swap, the assignment of one entity to another creates a copy of the\noriginal entity, swaps this object so that it becomes the object being assigned to, and deallocates the previous object.\nThe correct answer is (B). Statement (B) is true, since the user may be able to do anything with the data via the pointer without any7.\nknowledge of thecontainer that holds it. Statement (A) is false, since primitive types are easier to copy if they are stored by value (since they\nare more lightweight than copying a pointer, which is what passing by reference does behind the scenes — see section 1.2.2). Statement (C)\nis false, since data can be initialized and modified in a container of pointers. Statement (D) is false, since shared data is better stored as\npointers or references, so that all the parts of the program can access the same data (as opposed to storing by value, which requires a copy).\n8. The correct answer is (A). Assuming no reallocation is necessary, the time complexity of inserting an element to the back of an array is\nconstant (since it can be placed at the very back of the array without having to touch any of the other elements).\n9. The correct answer is (C). Assuming no reallocation is necessary, the time complexity of inserting an element to the front of an array is\nlinear, since you would have to shift all existing elements over by one to open up a spot at the beginning for the new element.\n10. The correct answer is (E). All of the statements are true. Statement I provides the definition of sequential vs. random access. Statement II\nis true because certain containers (such as lists) do not support random access at all. Statement III is true because these two conditions\nallow arrays to identify the memory position of any of its elements using simple pointer arithmetic.\n11. The correct answer is (A). To return an element from a container that should not be modified, it is best to return a const reference. The\nconst specified indicates the return value should not be modified, and the reference avoids making a copy of elements you want to return in\nthe container.\nconst operator[] const.12. The correct answer is (C). The version of is run if the container it is called on is specified as Even if\noperator[] non-const operator[]a call to does not modify a value in the container, the version of would still be called if the\nconst.container itself is not\nconst operator[] Array const.13. The correct answer is (A). The version of is called if the it is invoked on is This happens in the\noperator<< operator[]overloaded method, which calls on the array 3 times. Since the overloaded operator is invoked 3 times on\nstd::min(a[0], a[a.length - 1]) const operator[]line 37 (since is 3), the version of is called a total of 3 3 = 9 times.×\n14. The correct answer is (B). The issue here is that the parameter of the copy constructor is passed in by value instead of by reference (i.e.,\nArray Array&).instead of Passing the array by value involves making a copy, which invokes the copy constructor we are trying to\nimplement, causing an error.", "word_count": 839, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "1cc4003d-f69e-567b-a006-4a1b4576b52b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 189, "real_page_number": null, "text": "Chapter 7\nVectors: Dynamically Resizable Arrays in C++\n7.1\nThe STL Vector Container\nIf you wanted a dynamic array-based container that can grow with the size of your input, you do not have to implement it from scratch! This\nis thanks to the C++ standard template library (STL), which provides efficient implementations of containers and algorithms for you. In\nstd::vector<>this chapter, we will discuss the C++ container, which is an array-based container that manages its elements using a\ndynamically-sized heap-allocated C-style array. You can store elements in a vector much like how you can store elements in an array, but the\nvector can manage its own memory and will resize itself automatically if additional space is needed (contrary to an array, which is fixed in size).\n#include <vector>To use a vector in your program, you must at the top of your code file. The following line declares a vector of\n{1, 2, 3, 4}:integers with initial contents\nstd::vector<int32_t> vec = {1, 2, 3, 4};\n{1, 2, 3, 4}.When the vector is initialized, it creates a C-style array on the heap (i.e., dynamic memory) with the contents What exactly\nstd::vector<>does a vector look like under the hood? This is implementation specific, but the GCC libstdc++ implementation of stores\nthree pointers on the program stack: (1) a pointer that points to the beginning of the heap-allocated array, (2) a pointer that points one past the\nlast element in the heap-allocated array, and (3) a pointer that points one past the last element in the heap-allocated array. Thevalid allocated\nlast valid element and the last allocated element need be the same.not\nvec\nbegin\n0x100\nend_size\n0x110\nend_cap\n0x110\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c", "word_count": 292, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d5771856-1884-54c6-a155-223df702a96b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 190, "real_page_number": null, "text": "178\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nstd::vector<>,Note that this is only one implementation of the and the exact implementation details of a STL vector may be platform\ndependent as long as the public interface of the container conforms with the C++ standard. The underlying structure of a vector can differ\nacross different standard library implementations (e.g., an implementation could keep track of one pointer and two integers for size and capacity\ninstead of three pointers, and that would still be valid). The purpose of this chapter is to explore the general idea of how a vector works internally,\nand not to delve into the details of a single implementation, so you do not need to specifically memorize this vector implementation.\nRegardless of how a vector is implemented, there are two important values that the vector must be able to determine at all times. The first is\nits size, which represents the number of valid elements that the underlying array holds. The second is its capacity, which represents theactually\nnumber of elements the underlying array hold. These two values do not need to be the same. For instance, suppose we removed theis able to\n4,last element, from the back of the previously defined vector:\nvec\nbegin\n0x100\nend_size\n0x10c\nend_cap\n0x110\nstack\nheap\n1\n2\n3\nundef\n0x100\n0x104\n0x108\n0x10c\nSince the vector’s underlying array has a fixed size, it can still hold at most 4 elements. However, only 3 elements in the vector are still valid\nafter the last element was removed. Thus, the size of the vector is 3, while its capacity is 4.\nThe vector is able to calculate its size by finding the distance between the pointer to the first element and the pointer one past the last valid\nend_size - beginelement (i.e., using the variable names above). It is also able to calculate its capacity by finding the distance between\nbegin).1end_cap -the first element and the pointer one past the last allocated element (i.e.,\nThe following methods can be used to initialize a vector:\ntemplate <typename T>\nstd::vector<T>();\nT;Default constructor for vector that holds elements of type creates an empty vector with no elements; size and capacity are initially set to 0.\n// initializes the vector v1 with zero size and zero capacity\nstd::vector<int32_t> v1;\ntemplate <typename T>\nstd::vector<T>(std::initializer_list<T> init);\nInitializes the vector with the contents of the initializer list.\n// initializes v2 and v3 to have contents {1, 2, 3}, with size and capacity 3\nstd::vector<int32_t> v2{1, 2, 3};\nstd::vector<int32_t> v3 = {1, 2, 3};\ntemplate <typename T>\nstd::vector<T>(const std::vector<T>& other);\notherCopy constructor, copies the contents of into the constructed vector.\n// initializes v4 with the contents of v3, or {1, 2, 3}\nstd::vector<int32_t> v4{v3};\ntemplate <typename T>\nstd::vector<T>(size_t sz);\nsz sz.Creates a vector of elements, where each element is value initialized; size and capacity both initially equal to\n// initializes v5 with size and capacity 5, each element initialized to 0\nstd::vector<int32_t> v5(5);\ntemplate <typename T>\nstd::vector<T>(size_t sz, T& val);\nsz val; sz.Creates a vector of elements, where each element is initialized to a value of size and capacity both equal to\n// initializes v6 with size and capacity 5, each element initialized to 1\nstd::vector<int32_t> v6(5, 1);\n1Pointer 0x100arithmeticisperformedrelativetothebasetypeofthepointer. Sinceintegerstakeup4byteseach,incrementing by1actuallyreturnstheaddress\n0x100 0x104.+1 4=×", "word_count": 583, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9aa41aa2-0468-571e-821f-32634a96526f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 191, "real_page_number": null, "text": "7.2 Inserting and Removing Elements\n179\ntemplate <typename typenameT, InputIterator>\nstd::vector<T>(InputIterator begin_iter, InputIterator end_iter);\n[begin_iter, end_iter)Creates a vector with all elements in the iterator range — inclusive begin but exclusive end. Both\nbegin_iter end_iterand are input iterators.\n// initializes v7 with first three elements of v6\n// .begin() returns an iterator to the first element of the vector\nstd::vector<int32_t> v7{v6.begin(), v6.begin() + 3};\n7.2\nInserting and Removing Elements\n¸ 7.2.1\nPush Back\nstd::vector::push_back() std::vector::pop_back().Twomethodsforinsertingandremovingelementsfromavectorare and\n.push_back(val) valWhen is called, is added to the back of the vector, and the size of the vector increases by one.\ntemplate <typename T>\nvoid std::vector<T>::push_back(const T& val);\nvalAppends a new value initialized to to the back of the vector, after the current last element (if there is available capacity). If the new\nvector size surpasses the current capacity, the underlying array is reallocated before the new element is appended. After the object is\nsuccessfully appended, the size of the vector increases by one.\nLet’s consider the state of the previous vector, as reproduced below.\nvec\nbegin\n0x100\nend_size\n0x10c\nend_cap\n0x110\nstack\nheap\n1\n2\n3\nundef\n0x100\n0x104\n0x108\n0x10c\nvec.push_back(4),If we call we push the value 4 to the back of the vector’s underlying array, after the current last element (which is 3).\nvec\nbegin\n0x100\nend_size\n0x110\nend_cap\n0x110\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\nvec.push_back(5)?What happens if we try to push back another element by calling If we add this element, the size of the vector would\nbe 5. However, the underlying heap-allocated array that stores the vector’s data has a fixed size of 4, so it cannot hold a fifth element! Therefore,\nwe will need a new array that can support a size of 5. When this happens, the vector allocates a new array in memory that is double the capacity\nof the original array, copies over the data, frees up the memory of the old array, and resets its internal pointers to point to the new array.\nvec\nbegin\n0x100\nend_size\n0x110\nend_cap\n0x110\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\nundef undef undef undef undef undef undef undef\n0x200\n0x204\n0x208\n0x20c\n0x210\n0x214\n0x218\n0x21c\nAllocate a new array\nwith capacity 8", "word_count": 391, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "500255bc-a974-5cea-928b-137d86ac737b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 192, "real_page_number": null, "text": "180\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nvec\nbegin\n0x100\nend_size\n0x110\nend_cap\n0x110\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\n1\n2\n3\n4\nundef undef undef undef\n0x200\n0x204\n0x208\n0x20c\n0x210\n0x214\n0x218\n0x21c\nCopy the contents from\nold to new array\nvec\nbegin\n0x200\nend_size\n0x210\nend_cap\n0x220\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\n1\n2\n3\n4\nundef undef undef undef\n0x200\n0x204\n0x208\n0x20c\n0x210\n0x214\n0x218\n0x21c\nDeallocate old array\nand reset pointers\nvec\nbegin\n0x200\nend_size\n0x214\nend_cap\n0x220\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\n1\n2\n3\n4\n5\nundef undef undef\n0x200\n0x204\n0x208\n0x20c\n0x210\n0x214\n0x218\n0x21c\nPush back\nnew value\n¸ 7.2.2\nPop Back\n.pop_back()To remove the last element in a vector, the method can be used.\ntemplate <typename T>\nvoid std::vector<T>::pop_back();\nRemoves the last element in the vector, reducing its size by one.\nvec.pop_back()If we call on our vector above, the 5 would be removed.\nvec\nbegin\n0x200\nend_size\n0x214\nend_cap\n0x220\nstack\nheap\n1\n2\n3\n4\n5\nundef undef undef\n0x200\n0x204\n0x208\n0x20c\n0x210\n0x214\n0x218\n0x21c\nvec\nbegin\n0x200\nend_size\n0x210\nend_cap\n0x220\nstack\nheap\n1\n2\n3\n4\nundef undef undef undef\n0x200\n0x204\n0x208\n0x20c\n0x210\n0x214\n0x218\n0x21c", "word_count": 224, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d001eab8-2e14-5ce0-b349-78a0d8273b3e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 193, "real_page_number": null, "text": "7.2 Inserting and Removing Elements\n181\n.pop_back()Removing an element using does not trigger a reallocation. Here, the size of the vector is 4 after the last element is removed,\nbut the capacity is still 8. Reallocation can only happen if an insertion occurs when the vector is at capacity, or the programmer explicitly\nshrinks or expands the capacity of the underlying array. We will discuss several reasons why this is the case in the next section.\n.pop_back()In addition, calling on an empty vector causes undefined behavior. The programmer is expected to know whether the\n.pop_back()vector is empty or not; this removes the need for to conduct an extra (and potentially unnecessary) \"empty\" check every time it\nis called (effectively sacrificing safety for performance).\n.pop_back() .push_back()A call to takes time. A call to takes time if reallocation occurs and time otherwise.Θ(𝑛)Θ(1) Θ(1)\nHowever, because the capacity of the underlying array is doubled upon reallocation, each push makes subsequent pushes less expensive; itΘ(𝑛)\n.push_back()turns out that this allows the time complexity of to be treated as a operation if we average out all the work of reallocationΘ(1)\nacross all the individual pushes. This idea is known as amortization, and we will cover it in more detail in chapter 12.\n¸ 7.2.3\nEmplace Back\nstd::vector::emplace_back() std::vector::push_back().Ifyouhaveavectoroflargeobjects, canbeusedasanalternativeto\ntemplate <typename typename...T, Args>\nT& std::vector<T>::emplace_back(Args&&... args);\nAppends a new element at the back of the vector, right after the current last element (if there is available capacity). If the new vector size\nsurpasses the current capacity, the underlying array is reallocated before the new element is emplaced. The new element is constructed\n(args), .emplace_back()using arguments for its constructor which are passed into the function call. After the object isin place\nsuccessfully emplaced, the size of the vector increases by one. Since C++17, a reference to the emplaced element is returned.\nStudentTo illustrate how this works, consider the following object defined below:\n1\nstruct Student {\n2\nint32_t id;\n3\nstd::string name;\n4\ndouble gpa;\n5\nStudent(int32_t doubleid_in, std::string name_in, gpa_in) :\n6\nid{id_in}, name{name_in}, gpa{gpa_in} {}\n7\n};\n8\n9\nstd::vector<Student> students;\nStudent Student .push_back()Whenyoupushbacka object,youmustfirstcreateatemporaryinstanceofthe youwanttopushback(as\n.push_back() Studentcan only accept an argument of the same type as the vector). Then, when is invoked, the temporary is copied to\nthe back of the vector.\n1\nStudent s{12345678, \"Bob\", 3.5};\n2\nstudents.push_back(s);\n// or on one line: students.push_back(Student{12345678, \"Bob\", 3.5});\nstudents\nPointer to\ndata array\n12345678\n\"Bob\"\n3.5\nStudentA object\nis constructed here\nand copied over here\n.emplace_back() StudentWhat makes different is that it accepts a list of rather than a object itself. It then forwards thearguments\n.emplace_back()arguments to a constructor that can construct the object in the destination memory address of the vector itself. Here,\nStudenttakes the three arguments, sends it to the constructor, and builds the object in the right spot. There is no need to create a temporary\nvariable and copy it over!\n1\n// pass into emplace_back() what you would pass into the object's constructor\n2\nstudents.emplace_back(12345678, \"Bob\", 3.5);\nstudents\nPointer to\ndata array\n12345678\n\"Bob\"\n3.5\nStudentThe object is\nconstructed at the correct\nmemory location; no need\nto copy a temporary object", "word_count": 575, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6587df96-1678-5b36-a520-f25095d3f521", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 194, "real_page_number": null, "text": "182\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\n¸ 7.2.4\nInserting and Erasing From Any Position\nAppending an element to the back of a vector is easy, but inserting elements anywhere else becomes slightly trickier. Because vectors require\ntheir elements to be stored contiguously in memory, if you attempt to insert an element to the vector, you must shift all the elements after the\ninsertion point rightward to make room for the new element. The same idea applies with deletion; when you delete an element, you must shift\nall elements after the deletion point leftward to ensure there are no gaps in your data. Recall the illustration from the previous chapter:\n4\ndata\nsize\nvec\n1\n3\n4\n5\n2\n4\ndata\nsize\nvec\n1\n3\n4\n5\n2\n5\ndata\nsize\nvec\n1\n2\n3\n4\n5\n.push_back(), .emplace_back(), .pop_back()Because and all deal with adding or removing elements at the end of the vector,\nthere is no need to shift any elements (since there is no data after the insertion or deletion point). Thus, these operations all take time,Θ(1)\nassuming no reallocation is done.\n.insert()If you want to insert elements elsewhere in a vector, you should use the function. There are several ways this can be done:\ntemplate <typename T>\nconstiterator std::vector<T>::insert(const_iterator position, T& val);\nval positionInserts directlybeforetheelementpointedtobytheiterator andreturnsaniteratorthenewlyinsertedelement. Reallocation\nis done if the new size after insertion exceeds capacity.\n1\nstd::vector<int32_t> vec = {0, 1, 2, 4, 5};\n2\n// insert 3 at index 3\n3\nvec.insert(vec.begin() + 3, 3);\n4\n// vec now {0, 1, 2, 3, 4, 5}\ntemplate <typename T>\nsize_t constiterator std::vector<T>::insert(const_iterator position, n, T& val);\nn val positionInserts copies of directly before the element pointed to by the iterator and returns an iterator the first new element added.\nReallocation is done if the new size after insertion exceeds capacity.\n1\nstd::vector<int32_t> vec = {0, 1, 2, 4, 5};\n2\n// insert 4 copies of 3 at index 3\n3\nvec.insert(vec.begin() + 3, 4, 3);\n4\n// vec now {0, 1, 2, 3, 3, 3, 3, 4, 5}\ntemplate <typename typenameT, InputIterator>\niterator std::vector<T>::insert(const_iterator position, InputIterator first, InputIterator last);\n[first, last) positionInserts all elements in the iterator range directly before and returns an iterator the first new element added.\nReallocation is done if the new size after insertion exceeds capacity.\n1\nstd::vector<int32_t> vec1 = {0, 1, 5};\n2\nstd::vector<int32_t> vec2 = {2, 3, 4};\n3\n// insert vec2 at index 2 of vec 1\n4\nvec1.insert(vec1.begin() + 2, vec2.begin(), vec2.end());\n5\n// vec1 now {0, 1, 2, 3, 4, 5}\ntemplate <typename T>\niterator std::vector<T>::insert(const_iterator position, std::initializer_list<T> init);\npositionInserts the elements in the initializer list into the vector directly before the iterator and returns an iterator to the first new element\nadded. Reallocation is done if the new size after insertion exceeds capacity.\n1\nstd::vector<int32_t> vec = {0, 1, 2};\n2\n// insert {3, 4, 5} at the back\n3\nvec.insert(vec.end(), {3, 4, 5});\n4\n// vec1 now {0, 1, 2, 3, 4, 5}\n.insert()The time complexity of is linear on the number of elements inserted plus the number of the elements after the point of insertion\n.emplace_back(),(since each element after this point needs to be shifted over). Similar to which directly constructs an element at the back\n.emplace().of a vector, it is also possible to directly construct an element in the middle of a vector using", "word_count": 592, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7eaf7763-81ee-595f-bea0-b817816284b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 195, "real_page_number": null, "text": "7.2 Inserting and Removing Elements\n183\ntemplate <typename typename...T, Args>\niterator std::vector<T>::emplace(const_iterator pos, Args&&... args);\npos.Insertsanewelementdirectlybeforetheelementatposition Thenewelementisconstructedinplaceusingargumentsforitsconstructor\n(args). An iterator pointing to the emplaced object is returned.\n1\nstd::vector<Student> students;\n2\nstudents.emplace_back(12345677, \"Alice\", 3.4);\n3\nstudents.emplace_back(12345679, \"Cathy\", 3.6);\n4\nstudents.emplace(students.begin() + 1, 12345678, \"Bob\", 3.5);\n5\n// students now stores the students in this order: {Alice, Bob, Cathy}\n.erase()To remove elements from the vector at any position, you should use the member function:\ntemplate <typename T>\niterator std::vector<T>::erase(const_iterator position);\npositionErases the element pointed to by the iterator and returns an iterator pointing to the new location of the element that followed the\nerased element.\ntemplate <typename T>\niterator std::vector<T>::erase(const_iterator first, const_iterator last);\n[first, last)Erases all elements in the iterator range and returns an iterator pointing to the new location of the element that followed\nthe last element erased.\n1\nstd::vector<int32_t> vec = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n2\n3\n// erase everything from index 6 to the end\n4\nvec.erase(vec.begin() + 6, vec.end());\n5\n// vec now {0, 1, 2, 3, 4, 5}\n6\n7\n// erase element at index 3\n8\nvec.erase(vec.begin() + 3);\n9\n// vec now {0, 1, 2, 4, 5}\n.erase()The time complexity of is linear on the number of elements erased plus the number of elements after the erasure point (since\nelements have to be shifted to ensure there are no gaps in the data).\n5\ndata\nlength\nvec\n1\n2\n3\n4\n5\n4\ndata\nlength\nvec\n1\n3\n4\n5\n4\ndata\nlength\nvec\n1\n3\n4\n5\n.insert() .erase()Both and utilizeiterators, whicharegeneralizedpointersthatcanbeusedtotraversethecontentsofacontainer. More\nstd::vector<>detail on iterators will be provided in chapter 11, but the following methods can be used to retrieve iterators for a container:\nFunction\nBehavior\n.begin()\nReturns a random access iterator to the first element in the vector\n.end()\nReturns a random access iterator to the position one past the last element in the vector\n.cbegin()\nReturns a random access iterator to the first element in the vectorconstant\n.cend()\nReturns a random access iterator to the position one past the last element in the vectorconstant\n.rbegin()\nReturns a iterator to the last element in the vectorreverse\n.rend()\nReturns a iterator to the position one before the first element in the vectorreverse\n.crbegin()\nReturns a constant reverse iterator to the last element in the vector\n.crend()\nReturns a constant reverse iterator to the position one before the first element in the vector", "word_count": 453, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3738e9b6-0174-5374-85b4-8ae1ea9be20b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 196, "real_page_number": null, "text": "184\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nstd::vector<>:Listed below are some additional operations that are supported by a\nFunction\nBehavior\noperator[]\nReturns a reference to an element in the vector at a given index\n.front()\nReturns a reference to the first element in the vector (undefined behavior if empty)\n.back()\nReturns a reference to the last element in the vector (undefined behavior if empty)\n.empty()\nReturns whether the vector is empty (true or false)\n.clear()\nErases all elements in the vector and leaves it with a size of 0 (but keeps capacity unchanged)\n1\nstd::vector<int32_t> vec = {183, 203, 280, 281};\n2\n3\nstd::cout << vec.front() << '\\n';\n// prints 183\n4\nstd::cout << vec.back() << '\\n';\n// prints 281\n5\nstd::cout << vec[1] << '\\n';\n// prints 203\n6\n7\nvec.clear();\n8\nstd::cout << vec.empty() << '\\n';\n// prints 1 (true)\n7.3\nResize and Reserve\n¸ 7.3.1\nThe Costs of Reallocation\nA vector stores and manages its data in an array that is dynamically allocated on the heap. The size of a vector represents the number of valid\nelements that the vector holds, while the capacity of a vector represents the number of elements the vector’s underlying array is toactually able\nhold. The capacity of the vector’s underlying array may be larger than the size of the vector itself, since additional space may be allocated in\nadvance whenever reallocation occurs. To get the size or capacity of the vector, you can use the following member functions:\nFunction\nBehavior\n.size()\nReturns the size of the vector\n.capacity()\nReturns the capacity of the vector’s underlying data array\nThe size and capacity of an empty vector both start at 0. Once the first element is pushed back, space is allocated for an underlying array and the\ncapacity becomes 1. After another element is pushed back, the data is reallocated to another array and its capacity doubles to 2. The doubling\nprocess continues whenever reallocation occurs: adding the third element would double the capacity to 4, adding the fifth element would double\n.push_back().the capacity to 8, and so on and so forth. This is the behavior that happens if every element were added using\nIf you instead initialize the vector with a non-zero number of elements (either using a range constructor or initializer list), the initial size and\ncapacity are set to the number of elements present at initialization. For instance, if you create the following:\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n.push_back())the vector would start off with a size and capacity of 5. Inserting another element into this vector (using would increase size\nto 6, but double capacity to 10.\nIt is important to understand the difference between size and capacity for many reasons. Even though a vector’s size may be more important\nwhen implementing an algorithm or program, failure to consider its capacity could make your code less efficient. Several issues with capacity\nand reallocation are summarized below.\n1. Reallocation invalidates all existing pointers and iterators to elements in the vector.\nThe data in a vector is stored in a fixed-size array on the heap. If the amount of data you have exceeds the capacity of the underlying array, the\nvector has to move the data to a larger array memory. Thus, the memory address that a pointer or iterator pointed to in the old arrayelsewhere in\nwould no longer be valid. Consider the following code, which initializes a vector of size (and capacity) 4 and creates a pointer to the last element:\nstd::vector<int32_t> vec = {1, 2, 3, 4};\nint* ptr = &vec[3];\nvec\nbegin\n0x100\nend_size\n0x110\nend_cap\n0x110\nptr\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\n0x10c", "word_count": 621, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "65757895-fac3-5a20-8a36-231f453e0d4b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 197, "real_page_number": null, "text": "7.3 Resize and Reserve\n185\nNow, suppose we push back a fifth element to the back of the vector. Since the vector’s underlying array only has a capacity of 4, a new larger\narray must be allocated to hold the fifth element. This triggers a reallocation.\nvec.push_back(5);\nvec\nbegin\n0x200\nend_size\n0x214\nend_cap\n0x220\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\n1\n2\n3\n4\n5\nundef undef undef\n0x200\n0x204\n0x208\n0x20c\n0x210\n0x214\n0x218\n0x21c\nPush back\nnew value\n0x10c\nptr\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\nptr 0x10c, 4Here, the value of is the address location since that was where the element was originally located. However, because of\n4 0x10c!reallocation, the moved in memory — it is no longer at address Thus, pointers or iterators that were declared before the reallocation\ninvalid.2are now\n2. Reallocation takes time.\nIt takes time to move data to a larger array, since all the elements have to be shifted over from the old array to the the new array. If you push\nback 1,000,000 elements into a vector that doubles capacity during every reallocation, the reallocation process happens 21 times!\n3. If you know the intended size of your data, letting the vector set the capacity for you can waste memory.\nIf you do not tell the vector the size of your data, its default behavior is to double the capacity of the underlying array with every reallocation.\n.push_back()This may end up being more than you need! For example, if you wanted to store 1,025 elements in a vector, and you only call\nto insert these elements, the vector would double capacity from 1 to 2 to 4 to 8 to 16 to 32 to 64 to 128 to 256 to 512 to 1,024. When you insert\n1,025ththe element, the vector sees that the underlying array can only hold 1,024 elements, so it doubles capacity to 2,048. The vector doesn’t\nknow that element 1,025 was the last one! As a result, even though you only need space for 1,025 elements, you are actually reserving memory\nfor an array of size 2,048 under the hood. All of this remaining memory is unused and wasted.\n¸ 7.3.2\nReserving a Vector\nFrom the three issues above, we can see that reallocation should be avoided whenever possible. If you know the intended size of the vector\nbeforehand, you can prevent these three issues from happening by presetting the capacity of the vector’s underlying array upon initialization. For\ninstance, if you know you only need space for 1,025 elements, you could tell the vector to allocate that exact amount of space at the beginning.\n.reserve()This can be done using the member function:\ntemplate <typename T>\nvoid std::vector<T>::reserve(size_t n);\nn. nRequests that the vector capacity is at least If is larger than the current vector capacity, this operation causes the vector to reallocate its\nn.data to an array with capacity Otherwise, no reallocation occurs. This function does affect the size of the vector.not\n.reserve()It is important to note that the function does actually change the size of the vector itself! You are creating new elementsnot not\nwhen you reserve a vector; you are just allocating more space for future elements. For example, the following code reserves a vector of size 10:\nstd::vector<int32_t> vec;\nvec.reserve(10);\nvec\nbegin\n0x270\nend_size\n0x270\nend_cap\n0x298\nstack\nheap\nundef undef undef undef undef undef undef undef undef undef\n0x270\n0x274\n0x278\n0x27c\n0x280\n0x284\n0x288\n0x28c\n0x290\n0x294\nThe size of the vector is still 0, since it holds no elements, but the capacity is 10 since the underlying array can hold at most 10 elements. Calling\n.push_back() (0x270).would add the new element at the first open location\n2Pointeranditeratorinvalidationisapeskybugthatishardtodetect. Youmustbecarefulwhenworkingwithpointersoriteratorsifyoumodifythecontentsofthe\nboost::container::stable_vector<>underlyingcontaineraftercreatingthem. Also,youdonotneedtoknowthis,buttheboostlibraryprovidesa\ncontainerthatensuresthatreferencesanditeratorstoanelementwillalwaysremainvalid,aslongastheelementisnoterased.", "word_count": 705, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e52844a3-7962-5de9-a023-0264fe3e95b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 198, "real_page_number": null, "text": "186\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nIf you reserve a vector with a capacity that is larger than the current capacity, reallocation occurs. For example, the following code creates a\nvector that starts at a capacity of 4, but is reserved to a vector of capacity 7.\nstd::vector<int32_t> vec = {1, 2, 3, 4};\nvec\nbegin\n0x300\nend_size\n0x310\nend_cap\n0x310\nstack\nheap\n1\n2\n3\n4\n0x300\n0x304\n0x308\n0x30c\nvec.reserve(7);\nvec\nbegin\n0x370\nend_size\n0x380\nend_cap\n0x38c\nstack\nheap\n1\n2\n3\n4\nundef undef undef\n0x370\n0x374\n0x378\n0x37c\n0x380\n0x384\n0x388\n1\n2\n3\n4\n0x300\n0x304\n0x308\n0x30c\n.push_back()Once again, the size of the vector hasn’t changed, but the capacity went up to 7. If you a new element, it will get added to the\n(0x380).first open position\n¸ 7.3.3\nResizing a Vector\n.resize(),An alternative would be to use which sets the of the vector.size\ntemplate <typename T>\nvoid std::vector<T>::resize(size_t n);\nn n n,Resizes the vector so that it contains elements. If is smaller than the current size, the vector is forced down to a size of and any\nnadditional elements are destroyed (but capacity remains unchanged). If is greater than the current size, new elements are value-initialized\nn. nand added to the end of the array until the vector size becomes If is greater than the current capacity, reallocation occurs, and the\nrule).3ncapacity also becomes (see footnote for an exception to this\ntemplate <typename T>\nvoid std::vector<T>::resize(size_t n, T& val);\nn val nSame as above, but if is greater than the current size, copies of are added to the end of the vector until the vector size becomes\n(instead of being value-initialized).\n.resize() .resize()Anexampleusing isshownbelow. Acallto actuallyinstantiatesdataelementsinthevector,insteadofjustallocating\nmore space for the underlying array:\nstd::vector<int32_t> vec = {1, 2, 3, 4};\nvec\nbegin\n0x400\nend_size\n0x410\nend_cap\n0x410\nstack\nheap\n1\n2\n3\n4\n0x400\n0x404\n0x408\n0x40c\n3This std::vector<> nrule is not entirely true in all cases. For the GCC implementation, if is larger than the current size, it turns out that capacity only\nn n n n.becomes if > [2×(previous size)]. If < [2×(previous size)], capacity is actually set to [2×(previous size)] instead of That being said, don’t go and\nmemorizetheserules,sincetheyareimplementation-specificandnotexplicitlystatedinthestandard.", "word_count": 413, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f5fe4d72-f078-56f0-b4cc-8402d3479f9f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 199, "real_page_number": null, "text": "7.3 Resize and Reserve\n187\nIn the example below, the vector is resized to a size of 9 rather than reserved. As a result, the last five elements are value-initialized to 0 (the\ndefault behavior for integers), and both size and capacity become 9.\nvec.resize(9);\nvec\nbegin\n0x470\nend_size\n0x490\nend_cap\n0x490\nstack\nheap\n1\n2\n3\n4\n0\n0\n0\n0\n0\n0x470\n0x474\n0x478\n0x47c\n0x480\n0x484\n0x488\n0x48c\n0x490\n1\n2\n3\n4\n0x400\n0x404\n0x408\n0x40c\nvec.push_back(5)), 0x480If we attempt to push back a new element here (e.g., it will not be added to address location since an element\n.resize()).already exists there (created by Instead, this element will be added after the ninth element. Since the capacity is only 9, a\nreallocation must be done to create room for an ninth element. The capacity thus doubles to 18, as shown below.\nvec.push_back(5);\nvec\nbegin\n0x520\nend_size\n0x540\nend_cap\n0x568\nstack\nheap\n1\n2\n3\n4\n0\n0\n0\n0x470\n0x474\n0x478\n0x47c\n0x480\n0x484\n0x488\n1\n2\n3\n4\n0\n0\n0\n5\nundef undef undef undef undef undef undef undef undef undef\n0x520\n0x524\n0x528\n0x52c\n0x530\n0x534\n0x538\n0x53c\n0x540\n0x544\n0x548\n0x54c\n0x550\n0x554\n0x558\n0x55c\n0x560\n0x564\n.resize() .reserve()Both and can be used if you know the size of your vector in advance. Not only can this save memory by ensuring\nyou are only allocating memory you intend to use, it can also speed up your program by reducing the number of reallocations you have to make.\n.reserve()The process of adding new elements differs depending on which approach you use. If you use to preset the capacity of the\n.push_back()vector, you aren’t creating any elements. As a result, you should use to add an element to the back of the vector. On the other\n.resize() .push_back()hand, if you use to preset the size of the vector, you are elements in the vector. In this case, using maycreating\nresult in unintended behavior, since it would add an element the values that were created (and may even trigger a reallocation, as shownafter\nabove). To add values to a vector that has been resized upon initialization, you will have to modify the data values that were created using\noperator[]. An example is shown below:\n1\nstd::vector<int32_t> v1;\n2\nv1.reserve(5);\n// reserve capacity to 5\n3\nv1.push_back(1);\n// add elements to back\n4\nv1.push_back(2);\n// using .push_back()\n5\nv1.push_back(3);\n6\nv1.push_back(4);\n7\nv1.push_back(5);\n1\nstd::vector<int32_t> v2;\n2\nv2.resize(5);\n// resize size to 5\n3\nv2[0] = 1;\n// add elements by\n4\nv2[1] = 2;\n// modifying the values\n5\nv2[2] = 3;\n// that were created\n6\nv2[3] = 4;\n7\nv2[4] = 5;\n.resize() .reserve()Remark: For a real life analogy of how and work, imagine a portable refrigerator filled with water bottles for\na sporting event. You have to bring the refrigerator to an event, but you are only allowed to bring a single refrigerator. You also need to\nensure that you have enough water bottles on hand to satisfy all demand.\nIn this scenario, the refrigerator represents the underlying heap-allocated array that stores the vector’s data. The water bottles represent\nthe data values that are stored. And you, the person carrying the refrigerator around, represents the vector itself — you are responsible for\nmanaging the refrigerator and the water bottles inside it. The fact that you can only carry one refrigerator is analogous to the fact that a\nvector only manages one data array.\n.resize(), vec.resize(50),When you call you are changing the number of valid elements in the vector. For example, if you call\nyou are setting the number of elements in the vector to 50. This is analogous to adding or removing water bottles from the refrigerator until\nonly 50 water bottles are remaining.\n.reserve(),When you call you are changing the capacity of the underlying data array, or how many elements the array is able to hold.\nIf you reserve a vector to have a capacity of 50, the vector’s underlying array will be allocated to have a capacity of 50. This is analogous to\n.reserve()purchasing a refrigerator that can fit a total of 50 water bottles. A call to does not change a vector’s size, much like how", "word_count": 719, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "932e35d0-e45c-54da-b3f1-c25777eab590", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 200, "real_page_number": null, "text": "188\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nbuying a bigger fridge does not change the number of water bottles you have.\nIf you have more water bottles than you have space in your refrigerator, you will need to buy a larger fridge to be able to hold all your\nwater bottles. This is equivalent to allocating a new, larger array in memory. Then, you’ll have to move all of your water bottles from the old\nfridge to the new fridge, and \"discard\" the old fridge (i.e., not carry it around anymore using our example). This is similar to the process of\ncopying elements over and deallocating the old array.\nExample 7.1 b true false?Consider the following code. What does line 3 do? Is the value of on line 4 or\n1\nstd::vector<int32_t> vec;\n2\nvec.reserve(10);\n3\nvec[0] = 5;\n4\nbool b = vec.empty();\n.reserve()Line 3 actually results in undefined behavior. When is called, it does not actually change the contents of the vector itself, just the\ncapacity of the underlying data array. As a result, the vector is still empty, and the indexing on line 3 ends up accessing uninitialized memory\nb true,(as there is no element at index 0 yet). Similarly, line 4 assigns to since the vector is indeed empty (its size is 0).\nvec\nbegin\n0x570\nend_size\n0x570\nend_cap\n0x598\nstack\nheap\nundef\nundef\nundef\nundef\nundef\nundef\nundef\nundef\nundef\nundef\n0x570\n0x574\n0x578\n0x57c\n0x580\n0x584\n0x588\n0x58c\n0x590\n0x594\nExample 7.2 Consider the following code. What does line 4 print?\n1\nstd::vector<int32_t> vec;\n2\nvec.resize(10);\n3\nvec.push_back(1);\n4\nstd::cout << vec.size() << '\\n';\n.resize()When is called, it changes the size of the array. Thus, line 2 sets the size of the array to 10. Line 3 pushes back an element after\n.resize(),the 10 elements that were created by so the size becomes 11 (which is printed out on line 4).\n¸ 7.3.4\n(✽)Accessing a Vector’s Underlying Data Array\noperator[],When you use you end up retrieving an element from the vector by (the reasoning for this is explained in section 6.7).reference\n0x570However, what if you wanted to return a to the underlying array that stores the vector’s data (e.g., using the addresses in examplepointer\nvec, &vec.7.1)? If you are given a vector one idea would be to retrieve the address by calling However, this does give you a pointer tonot\nstd::vector<>the data stored on the heap. Rather, this gives you a pointer to the vector object that lives on the (remember that thestack\nobject itself, which is composed of three pointers, lives on the stack — this is not the same as the vector’s data, which lives on the heap).\n.data()To retrieve a pointer to the vector’s underlying data array, you will have to call the member function instead. Given a vector\nvec, vec.data() vec.calling would return a pointer to the the underlying data array of\ntemplate <typename T>\nT* std::vector<T>::data();\nReturns a pointer to the vector’s underlying data array.\n&vec[0],Remark: An alternative would be to use which also returns a pointer to the underlying data array. However, this variant may\n.data()yield undefined behavior if the vector is empty. This is why is preferred, since it is always safe to use regardless of whether or not\nthe vector is empty.", "word_count": 566, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c3108b36-e9de-5762-a80d-3cbfb6941fee", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 201, "real_page_number": null, "text": "7.4 Storing Multidimensional Data in Vectors\n189\n7.4\nStoring Multidimensional Data in Vectors\nSimilar to regular C-style arrays, you can use a vector to store multidimensional data. To create a vector that stores two-dimensional data, simply\ndeclare a vector that stores other vectors (i.e., a vector of vectors). Here, the outer vector represents one dimension (e.g., rows) while the inner\nvectors represent another dimension (e.g., columns).\nstd::vector<std::vector<int32_t>> vec_2d;\nRemark: For these notes, we will refer to a vector of vectors as a \"2-D vector\" for succinctness (and a vector of vector of vectors as a \"3-D\nvector\", etc.). However, it should be emphasized that a multidimensional vector is merely a one-dimensional vector that stores other vectors.\nFor example, even though we will refer to the above vector as a two-dimensional vector, it is actually represented as a one-dimensional vector\nstd::vector<int32_t> int32_t.of behind the scenes, rather than a two-dimensional vector of\nWe can extend this definition to multiple dimensions. To declare a three-dimensional vector, simply create a vector object that stores 2-D vectors\n(where each layer represents a dimension).\nstd::vector<std::vector<std::vector<int32_t>>> vec_3d;\nHow do we fill the contents of a multidimensional vector? For instance, suppose we wanted to create a 5 5 vector with the following data:×\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n.push_back()A naïve approach would be to use to insert every element into the 2-D vector. This approach is shown below:\n1\nstd::vector<std::vector<int32_t>> vec_2d;\n2\nwhile (vec_2d.size() < 5) {\n3\nstd::vector<int32_t> temp;\n4\nwhile (temp.size() < 5) {\n5\ntemp.push_back(1);\n6\n} // while\n7\nvec_2d.push_back(temp);\n8\n} // while\nHowever, there is an issue with this approach. If we were to look at the vector’s underlying memory, we would see something like this (each\nvector object stores three pointers that each point to a position in its underlying data array):\nundef\nundef\nundef\n0x600\n0x614\n0x620\n0x640\n0x654\n0x660\n0x680\n0x694\n0x6a0\n0x6c0\n0x6d4\n0x6e0\n0x700\n0x714\n0x720\n1\n1\n1\n1\n1\nundef\nundef\nundef\n0x600\n0x604\n0x608\n0x60c\n0x610\n0x614\n0x618\n0x61c\n1\n1\n1\n1\n1\nundef\nundef\nundef\n0x640\n0x644\n0x648\n0x64c\n0x650\n0x654\n0x658\n0x65c\n1\n1\n1\n1\n1\nundef\nundef\nundef\n0x680\n0x684\n0x688\n0x68c\n0x690\n0x694\n0x698\n0x69c\n1\n1\n1\n1\n1\nundef\nundef\nundef\n0x6c0\n0x6c4\n0x6c8\n0x6cc\n0x6d0\n0x6d4\n0x6d8\n0x6dc\n1\n1\n1\n1\n1\nundef\nundef\nundef\n0x700\n0x704\n0x708\n0x70c\n0x710\n0x714\n0x718\n0x71c\nvec_2d[0]\nvec_2d[1]\nvec_2d[2]\nvec_2d[3]\nvec_2d[4]\nvec_2d[5]\nvec_2d[6]\nvec_2d[7]\nStack\nHeap\nbegin\n0x800\nend_size\n0x878\nend_cap\n0x8c0\nvec_2d\nThis ended up happening because we did not specify the size of the vector at the beginning! Thus, the vector automatically doubled with each\nreallocation, and each dimension ended up having a capacity of 8 instead of 5. To resolve this issue, we must explicitly tell the vector that its\n.resize() .reserve().size will be 5 using either or\n1\nvector<vector<int>> vec_2d;\n2\nvec_2d.reserve(5);\n3\nwhile (vec_2d.size() < 5) {\n4\nvector<int> temp(5, 1);\n// resizes vector with 5 elements, each init to 1\n5\nvec_2d.push_back(temp);\n6\n} // while", "word_count": 529, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "166f3058-392d-5668-9032-9af963ce533d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 202, "real_page_number": null, "text": "190\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nHowever, this is still a better way to do this. It turns out that a multidimensional vector can be declared on a single line using the vector fill\nsz, val:constructor. Recall that a vector declared using the following syntax will have a size of where each element is initialized to\nvec(size_tstd::vector<T> sz, T& val);\nvector<int> temp(5, 1),This is exactly what we did on line 4 of the previous code. When we ran the line we created a vector of size 5,\nwhere each element was initialized to 1. The great thing about the fill constructor is that it also sets the to the specified size! Becausecapacity\nof this, the fill constructor will allow you to declare a vector whose underlying array has exactly the right capacity (e.g., 5 instead of 8).\nvalThe in the above fill constructor syntax dictates what our vector is filled with. In the case of a 2-D vector, we want to create a vector of\nvalvectors. As a result, we can declare a 2-D vector by setting equal to the constructor of our inner vector, as shown:\nstd::vector<std::vector<T>> vec_2d(m, std::vector<T>(n, data));\nm n dataHere, represents the size of the outer vector, represents the size of the inner vector, and represents the value each element is initialized\nm,to. In other words, we are essentially creating an outer vector of size where each element of this outer vector is initialized to a separate vector\nn data.of size whose elements are all initialized to For instance, we can create the previous 5 5 vector by running the following line of code.×\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(5,vec_2d(5, 1));\nwhileThis single line of code does the exact same thing as the previous loop. This is the preferred way of declaring a 2-D vector, since it is\nclean, efficient, and ensures that memory is not wasted due to excess capacity. The result of constructing the 2-D vector with the correct size is\nshown at the top of the next page; unlike the vector in the previous diagram, no memory is wasted.\nTo initialize vectors of multiple dimensions, simply increase the number of nested vectors. For example, if you want to initialize a 3-D\nvector with dimensions with all elements initialized to 1, you can construct it using the following line of code:2×5×10\nstd::vector<std::vector<std::vector<int32_t>>> std::vector<std::vector<int32_t>>(5, std::vector<int32_t>(10,vec_3d(2, 1)));\noperator[] vec[2][3]To access an element in a multidimensional vector, you can use multiple times (e.g., for the element at row index\nvec[2] vec.2, column index 3 of a 2-D vector). This works because is also a vector object itself, and thus can be indexed just like\nOuter vector initialized to size 5\nEach index of the outer vector is\nfilled with an inner vector of size 5,\nwhich is itself initialized with all 1’s\nstd::vector<std::vector<int32_t>> vec_2d( 5 ,\nstd::vector<int32_t>(5, 1) );\n0x720\n0x734\n0x734\n0x740\n0x754\n0x754\n0x760\n0x774\n0x774\n0x780\n0x794\n0x794\n0x7a0\n0x7b4\n0x7b4\n1\n1\n1\n1\n1\n0x720\n0x724\n0x728\n0x72c\n0x730\n1\n1\n1\n1\n1\n0x740\n0x744\n0x748\n0x74c\n0x750\n1\n1\n1\n1\n1\n0x760\n0x764\n0x768\n0x76c\n0x770\n1\n1\n1\n1\n1\n0x780\n0x784\n0x788\n0x78c\n0x790\n1\n1\n1\n1\n1\n0x7a0\n0x7a4\n0x7a8\n0x7ac\n0x7b0\nvec_2d[0]\nvec_2d[1]\nvec_2d[2]\nvec_2d[3]\nvec_2d[4]\nStack\nHeap\nbegin\n0x900\nend_size\n0x978\nend_cap\n0x978\nvec_2d", "word_count": 566, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4c7e9694-174e-5d24-ba35-1474151a0e7e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 203, "real_page_number": null, "text": "7.5 Vector Performance and Memory Overhead\n191\n7.5\nVector Performance and Memory Overhead\nWith a 2-D vector, a common convention is to treat the row dimension as the outer vector and the column dimension as the inner vectors\n(i.e., each index of the outer vector stores a of data). However, the order in which you define your dimensions could potentially affect therow\nperformance of data access. Recall that it is faster to sequentially access memory addresses that are closer in memory than ones that are farther\napart. Thus, the dimension that you plan on iterating through first should be given the outermost vector in your vector declaration, if possible.\nLoop rows, then columns\nFaster since memory accesses are contiguous\nLoop columns, then rows\nSlower since memory accesses are not contiguous\nBecausetherowdimensionistypicallysetastheouterdimension, itisfastertoloopthrougha2-Dvectorwithrowsinanouterloopandcolumns\nin an inner loop (under the assumption that rows are the outer dimension). However, different types of problems may require different orders of\niteration. For instance, if a problem requires you to visit a 2-D vector column-wise from left to right, it may be preferable to store columns as the\nouter vector and rows as the inner vector. This allows you to take advantage of faster sequential access for memory in close proximity. If you do\noperator[] vec_2d[row][col] vec_2d[col][row]).this, you will have to make sure to flip the indices when using (i.e., becomes\nStoring columns as outer dimension\nspeeds up column-wise data access\nHowever, this approach would\nslow down row-wise data access\nIt turns out that the order of dimensions not only affects performance; it may also affect memory usage! This may seem a bit counterintuitive;\nfor instance, if your data has dimensions of 2 5 10, you would need to store 100 data values in your 3-D vector regardless of whether the× ×\noutermost dimension is 2 or 5 or 10. Since the number of values you need to store is constant, wouldn’t the memory usage be constant as well,\nregardless of how you declare your vectors?\nIt turns out that this is not exactly true. Even though the underlying data itself will always take up 400 bytes of memory (assuming the vector\nis properly resized or reserved), each vector object also stores 3 pointers of bookkeeping data on the stack! The ordering of the dimensions\nactually changes the total number of pointers needed for our multidimensional vector. Since each pointer takes up 8 bytes of memory on a 64-bit\nmachine, differences in memory usage can quickly accumulate among different dimension orderings.\nLet’s look at our 2 5 10 vector again. On the previous page, we declared this vector with 2 as the size of the outermost dimension, 5 as× ×\nthe size of the middle dimension, and 10 as the size of the innermost dimension:\nstd::vector<std::vector<std::vector<int32_t>>> std::vector<std::vector<int32_t>>(5, std::vector<int32_t>(10,vec_3d(2, 1)));\nHow much bookkeeping memory does this use? There is only 1 outer vector, which uses 3 pointers. This outermost vector stores 2 vectors of\nsize 5 (since 2 is the outermost dimension), which use up 2 3, or 6 pointers. These 2 vectors of size 5 each store a vector of size 10 at each×\nindex. Thus, there are 2 5 vectors of size 10, which use up a total of 2 5 3, or 30 pointers. The total number of pointers needed to declare× × ×\nthis 2 5 10 vector is therefore 3 + 6 + 30, or 39. The below diagram illustrates the number of pointers needed at each level.× ×\n3\n3\n3\nDimension of size 2\n3\n3\n3\n3\n3\nDimension\nof size 5\ndata\n⋮\nThis represents a data vector of size 10 (and there are 10 of them)\ndata\n⋮\ndata\n⋮\ndata\n⋮\ndata\n⋮\n3\n3\n3\n3\n3\nDimension\nof size 10\ndata\n⋮\ndata\n⋮\ndata\n⋮\ndata\n⋮\ndata\n⋮\nNumber of pointers\nat this level: 3\nNumber of pointers\nat this level: 6\nNumber of pointers\nat this level: 30\nNumber of pointers\nat this level: 0\nTotal number of\npointers needed: 39\nvec_3d\nstack\nheap\n3 begin, end_size, end_cap).In this diagram, each boxed represents the three pointers of a vector object (i.e., and", "word_count": 726, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e413b68b-b950-543d-872f-2bbc15dfbbcd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 204, "real_page_number": null, "text": "192\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nThe total number of bookkeeping pointers needed for the 3-D vector can be obtained by summing up the number of pointers at each level. The\nvectors at the lowest layer of the diagram have size 10 (they were not fully drawn out for space reasons), but note that there are no pointers at\nthis level. This is because the vectors at the bottom layer are filled with integers, and not other vector objects.\nLet’s see what happens when we flip the dimensions around and declare a 10 5 2 vector instead.× ×\nstd::vector<std::vector<std::vector<int32_t>>> std::vector<std::vector<int32_t>>(5, std::vector<int32_t>(2,vec_3d(10, 1)));\nNow, our outermost vector stores 10 vectors of size 5, rather than 2 vectors of size 5 as before. This ends up using 10 3, or 30 pointers. These×\n10 vectors of size 5 each store a vector of size 2 at each index, which end up using 10×5×3, or 150 pointers. Thus, the total number of pointers\nneeded for this 3-D vector is 3 + 30 + 150, or 183. This is over 4x the number of pointers we needed before we flipped the dimensions!\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n15\n15\n15\n15\n15\n15\n15\n15\n15\n15\ndata\nNumber of pointers\nat this level: 3\nNumber of pointers\nat this level: 30\nNumber of pointers\nat this level: 150\nNumber of pointers\nat this level: 0\nTotal number of\npointers needed: 183\nvec_3d\nstack\nheap\n15In the above diagram, each boxed represents a vector of 5 vector objects (each vector uses 3 pointers, so a vector of 5 vectors uses 15\npointers in total). The arrows were omitted due to space constraints, but each of the 10 vectors in the first heap layer points to a vector of size 5,\nand each vector within the vectors of size 5 points to a data vector of size 2. Like before, the data layer stores integers and not pointers, so there\nare 0 pointers at this final level.\nIn general, if you have a three-dimensional vector with dimensions 𝑎, 𝑏, and 𝑐that is declared as follows, where 𝑎is the outermost dimension\nand 𝑐is the innermost dimension:\nstd::vector<std::vector<std::vector<T>>> vec_3d(a, std::vector<std::vector<T>>(b, std::vector<T>(c, VAL)));\nthe number of bookkeeping pointers needed is 3+3𝑎+3𝑎𝑏. Therefore, when defining a multidimensional vector, the dimensions should be\nordered from smallest to largest (i.e., 𝑐) to minimize additional memory overhead. This is true for dimensions above 3 as well (which𝑎<𝑏<\ncan be proven using a similar process).\n7.6\nSummary of Vector Complexities\nIn this section, we will summarize the time complexities of several vector operations.\nOperation\nAverage-Case Time\nWorst-Case Time\nFinding an element\nΘ(𝑛)\nΘ(𝑛)\nThe worst-case of finding an element in a vector of size 𝑛occurs when the element is the last one in the vector, or if the element cannot be found\nat all. This requires the search to look at all 𝑛elements in the vector. In the average case, the element is somewhere in the middle of the vector,\nwhich would require the algorithm to look at approximately elements. This is still since we can drop the coefficient term of 1/2.Θ(𝑛)n/2\nstd::find() <algorithm>In the STL, this can be done using in the library, which will be covered in chapter 11.\nOperation\nAverage-Case Time\nWorst-Case Time\nAccessing first element\nΘ(1)\nΘ(1)\nThe vector keeps track of a pointer to the first element in its underlying array. Thus, the first element can always be accessed in constant time. In\nstd::vector::front().the STL, this can be done using\nOperation\nAverage-Case Time\nWorst-Case Time\nAccessing last element\nΘ(1)\nΘ(1)\nThe vector keeps track of a pointer one past the last element in its underlying array. Thus, the last element can always be accessed in constant\nstd::vector::back().time. In the STL, this can be done using", "word_count": 647, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ccce71e6-bf0f-518b-afc7-893f22fe75f2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 205, "real_page_number": null, "text": "7.6 Summary of Vector Complexities\n193\nOperation\nAverage-Case Time\nWorst-Case Time\nAccessing arbitrary element\nΘ(1)\nΘ(1)\nSince elements in a vector are contiguous in memory, pointer arithmetic can be used to access an element at any index of the vector. Since\narithmetic takes constant time, the complexity of accessing an arbitrary element in a vector also takes constant time. In the STL, this can be\nstd::vector::operator[] std::vector::at()done using or (the latter throws an exception if the given index is out of bounds).\nOperation\nAverage-Case Time\nWorst-Case Time\nInserting element at front\nΘ(𝑛)\nΘ(𝑛)\nSince elements in a vector are contiguous in memory, you must shift all elements after the insertion point before you can insert an element. If\nyou attempt to insert an element at the front of a vector of size 𝑛, you must shift all 𝑛elements after it to the right, which takes time.Θ(𝑛)\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n2\n3\n4\n5\nstd::vector::insert() std::vector::emplace()In the STL, this can be done using or with the begin iterator.\nOperation\nAverage-Case Time\nWorst-Case Time\nInserting element at back\nΘ(1)\nΘ(𝑛)\nOnaverage, youcaninsertanelementatthebackofavectorinconstanttime, sincenoelementsneedtobeshifted. However, theworst-casetime\ncomplexityis becausethere’sachancethatyouwillrunoutofcapacity;ifthishappens,thevectorwillneedtoreallocateanewarrayandcopyΘ(𝑛)\nstd::vector::push_back() std::vector::emplace_back().theexisting𝑛elementsover. IntheSTL,thiscanbedoneusing or\nOperation\nAverage-Case Time\nWorst-Case Time\nInserting element at arbitrary index\nΘ(𝑛)\nΘ(𝑛)\nOn average, an insertion will be done in the middle of the vector. Because of this, roughly half of the elements in the vector (i.e., elements)n/2\nmust be shifted to make room for the new element while maintaining the contiguity of elements. This is still considered as since theΘ(𝑛)\ncoefficient term of is dropped. The worst-case happens if the index is at the front, which requires all 𝑛elements to be shifted. In the STL, this1/2\nstd::vector::insert() std::vector::emplace()can be done using or with an iterator pointing to the position of insertion.\nOperation\nAverage-Case Time\nWorst-Case Time\nErasing element at front\nΘ(𝑛)\nΘ(𝑛)\nMuch like insertion, erasing an element will require all elements after the erasure point to the shifted to maintain the contiguity of elements. If\nyou attempt to erase an element at the front of a vector of size 𝑛, you must shift all 𝑛elements after it to the left, which takes time. This isΘ(𝑛)\nneeded to ensure that the data starts at index 0 and that there are no gaps in the data.\n1\n2\n3\n4\n5\n2\n3\n4\n5\n2\n3\n4\n5\nstd::vector::erase()In the STL, this can be done using with the begin iterator.\nOperation\nAverage-Case Time\nWorst-Case Time\nErasing element at back\nΘ(1)\nΘ(1)\nIf you erase an element at the back of the vector, you do not need to shift anything — the remaining elements are still in their correct positions.\nTheworst-caseofaneraseatthebackisalsoΘ(1). Thisisbecause,unlikeinsertion,erasinganelementwillnevertriggerareallocationonitsown\nstd::vector::pop_back().(so the vector will never allocate a new array and copy elements over). In the STL, this can be done using\nOperation\nAverage-Case Time\nWorst-Case Time\nErasing element at arbitrary index\nΘ(𝑛)\nΘ(𝑛)\nOn average, an erasure will be done in the middle of the vector. This requires roughly half of the elements in the vector to be shifted to remove\nthe gap in the data, which is after dropping the coefficient of 1/2. The worst case happens if the index is at the front, which requires allΘ(𝑛) 𝑛\nstd::vector::erase()elements to be shifted. In the STL, this is done using with an iterator (or iterator range) to the value(s) to erase.\nOperation\nAverage-Case Time\nWorst-Case Time\nChecking if vector is empty\nΘ(1)\nΘ(1)\nstd::vector::empty().This takes constant time by using the vector’s pointers to check if the size is 0. In the STL, this is done using", "word_count": 692, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2b7ffde9-0df8-5044-a982-4b297639f350", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 206, "real_page_number": null, "text": "194\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nChapter 7 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Which of the following statements is/are TRUE?\nstd::vector<>I. The actual elements in a are stored on the heap.\nstd::vector<>II. Inserting an element to the back of a always takes time.Θ(1)\n.reserve() std::vector<>III. Calling on a never changes the vector’s size.\nA) I only\nB) II only\nC) I and II only\nD) I and III only\nE) I, II, and III\nstd::vector<>,2. What is the worst-case time complexity of inserting an item at position of a assuming reallocation is needed, andany\nassuming reallocation is not needed?\nA) if reallocation is needed, if reallocation not neededΘ(1) Θ(1)\nB) if reallocation is needed, if reallocation not neededΘ(𝑛)Θ(1)\nC) if reallocation is needed, if reallocation not neededΘ(𝑛) Θ(1)\nD) if reallocation is needed, if reallocation not neededΘ(𝑛) Θ(𝑛)\nE) None of the above\nstd::vector<>.3. Suppose you are given a program that stores its primary data in a The program executes correctly for small test datasets,\nbut it runs out of memory while reading large datasets. What is a possible solution to overcome this problem?\n.resize() .push_back().I. Use with the exact number of elements, then use\n.resize() operator[].II. Use with the exact number of elements, then use\n.reserve() .push_back().III. Use with the exact number of elements, then use\n.reserve() operator[].IV. Use with the exact number of elements, then use\nA) I only\nB) II only\nC) I and IV only\nD) II and III only\nE) I, II, III, and IV\n4. Suppose you wanted to instantiate a vector of vectors of integers. The outer vector holds five smaller vectors of integers, each of which\nindividually holds three integer values all initialized to 4.\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\nWhich one of the following expressions correctly initializes this vector of vectors?\nstd::vector<std::vector<int32_t>> vec(std::vector<int32_t>(5, 4), 3);A)\nstd::vector<std::vector<int32_t>> vec(std::vector<int32_t>(3, 4), 5);B)\nstd::vector<std::vector<int32_t>> vec(4, std::vector<int32_t>(5, 3));C)\nstd::vector<std::vector<int32_t>> vec(5, std::vector<int32_t>(4, 3));D)\nstd::vector<std::vector<int32_t>> vec(5, std::vector<int32_t>(3, 4));E)\n5. If the addition of a new element causes a vector to go over capacity, it has to reallocate. Which of the following is/are TRUE regarding the\ndownsides of reallocation?\nI. Reallocation can be expensive, especially if the elements to be copied are large.\nII. Reallocation invalidates all existing pointers and iterators to elements in the old underlying array.\nIII. If you know the intended size of your container but do not reserve capacity beforehand, reallocation could result in wasted memory\nthat is allocated but not used.\nA) I only\nB) III only\nC) I and II only\nD) I and III only\nE) I, II, and III", "word_count": 506, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "a4852570-db18-547a-9df4-6fd9898b3969", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 207, "real_page_number": null, "text": "7.6 Summary of Vector Complexities\n195\n6. Suppose you have a container that doubles in capacity and copies all elements to a new location whenever an element is added while the\ncontainer is at full capacity. You initialize the container’s size to 12 and its capacity to 16. Which of the following operations would NOT\n.push_back(),result in a reallocation? Assume that all additional elements are inserted using and that no additional modifications to\nsize or capacity are made.\n13thA) Inserting the element into this container\n25thB) Inserting the element into this container\n33rdC) Inserting the element into this container\n49thD) Inserting the element into this container\nE) More than one of the above\n7. How many times does the vector allocate space in this code? Assume that, upon initialization, the vector’s capacity is equal to its size.\n1\nint main () {\n2\nstd::vector<int32_t> vec(2);\n3\nvec.reserve(4);\n4\nfor (int32_t i = 0; i < 7; ++i) {\n5\nvec.push_back(i);\n6\n} // for\n7\n} // main()\nA) 2\nB) 3\nC) 4\nD) 5\nE) 6\n8. Consider the following snippet of code:\n1\nint main () {\n2\nstd::vector<int32_t> vec;\n3\nvec.reserve(10);\n4\nfor (int32_t i = 0; i < 10; ++i) {\n5\nvec[i] = 281 + i;\n6\n} // for i\n7\nfor (int32_t j = 0; j < vec.size(); ++j) {\n8\nstd::cout << vec[j] << \" \";\n9\n} // for j\n10\nstd::cout << '\\n';\n11\n} // main()\nRunning the above code results in undefined behavior. Which line directly causes this behavior?\nA) Line 4\nB) Line 5\nC) Line 7\nD) Line 8\nE) None of the above\n9. Consider the following snippet of code:\n1\nint main () {\n2\nstd::vector<int32_t> vec;\n3\nfor (int32_t i = 1; i < 10; ++i) {\n4\nvec.push_back(i);\n5\n} // for i\n6\nvec.resize(5);\n7\nvec.resize(8, 100);\n8\nvec.resize(9);\n9\nvec.reserve(10);\n10\nvec[9] = 10;\n11\nfor (size_t i = 0; i < vec.size(); ++i) {\n12\nstd::cout << vec[i] << ' ';\n13\n} // for i\n14\nstd::cout << '\\n';\n15\n} // main()\nRunning the above code results in undefined behavior. Which line directly causes this behavior?\nA) Line 3\nB) Line 6\nC) Line 7\nD) Line 10\nE) None of the above", "word_count": 392, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "05aaa9c6-c97a-59df-89ae-bc7f43514c5b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 208, "real_page_number": null, "text": "196\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\n10. Consider the following snippet of code:\n1\nint main () {\n2\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n3\nvec.reserve(6);\n4\nint32_t* ptr = &vec[1];\n5\nvec.push_back(6);\n6\nvec.push_back(7);\n7\nstd::cout << *ptr + vec[3] << std::endl;\n8\n} // main()\nWhat does this code print?\n4A)\n5B)\n6C)\n7D)\nE) Impossible to determine, since this is undefined behavior\n11. Consider the following snippet of code:\n1\nint main () {\n2\nstd::vector<int32_t> v1;\n3\nstd::vector<int32_t> v2;\n4\nfor (int32_t i = 281; i < 381; i += 10) {\n5\nv1.push_back(i);\n6\n} // for i\n7\nv1.resize(6);\n8\nv2.resize(6);\n9\nfor (int32_t j = 482; j < 494; ++j) {\n10\nv2.push_back(j);\n11\n} // for j\n12\nv1.resize(v2.size(), v2.size());\n13\nfor (int32_t k = 1; k < val; k += 4) {\n14\nstd::cout << v1[k] + v2[k] << ' ';\n15\n} // for k\n16\n} // main()\nWhat does this code print?\n281 321 484 488 492A)\n281 321 502 506 510B)\n291 331 497 501 505C)\n291 331 503 507 511D)\n291 331 504 508 512E)\n12. Consider the following snippet of code:\n1\nint main () {\n2\nstd::vector<int32_t> v1{2, 5, 1, 3, 4};\n3\nstd::vector<int32_t> v2;\n4\nfor (size_t i = 0; i < v1.size(); ++i) {\n5\nv2.resize(v1[i]);\n6\nv2.back() = i;\n7\n} // for i\n8\nfor (auto num : v2) {\n9\nstd::cout << num << ' ';\n10\n} // for num\n11\n} // main()\nWhat does this code print?\n2 0 3 4A)\n2 0 3 4 1B)\n2 5 1 3C)\n2 5 1 3 4D)\nSegmentation faultE)\n.push_back(),13. Assume that you have a vector that, when its size is equal to its capacity and a new element is appended using all of the\nelements are copied to a new block of dynamic memory with a new capacity that is twice the previous capacity. When the capacity is 0 and\na new element is added, the new container’s capacity becomes 1. Suppose you push back 𝑛elements into a vector with initial size 0. What\nis the total number of times that an elements is copied from an old block of dynamic memory to a new one?\nA) 𝑛−1\n2×∑⌊log2(𝑛−1)⌋B)\n𝑖=0\n𝑖\n∑⌊log2(𝑛−1)⌋C)\n𝑖=0\n2\n2×∑⌊log2(𝑛−1)⌋D)\n𝑖=0\n𝑖2\n∑⌊log2(𝑛−1)⌋E)\n𝑖=0\n2𝑖", "word_count": 418, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7212b25c-38ad-5379-a78a-497500ac3e76", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 209, "real_page_number": null, "text": "7.6 Summary of Vector Complexities\n197\n14. You are reading in a text file with 1,000,000 words. Because you know in advance how many words you are going to read in, you reserve a\nvectorofstringswithacapacityof1,000,000. However,youmakeamistakewhenreadingthefileandaccidentallyinsertanadditionalempty\nstring into the vector after pushing in all 1,000,000 words. Which of the following statements is FALSE? Assume that the implementation\nof this vector is the same as the vector detailed in the previous question.\nA) Adding in this empty string would cause the vector’s memory usage to be roughly twice the memory it would have used had the\nempty string not been pushed in\nB) Adding in this empty string would require your program to copy or move 1,000,000 strings that you wouldn’t have needed to copy or\nmove otherwise\nC) Adding in this empty string would cause the vector’s size to double to 2,000,000 at the end of the program, assuming that no\nadditional elements are pushed in\nD) If you had initialized any iterators or pointers to the vector’s data prior to pushing in the empty string, all these iterators/pointers\nwould have been invalidated after pushing in the empty string\nE) None of the above\nstd::vector<>?15. Which one of the following statements is FALSE regarding the\nA) Inserting new elements into a vector can invalidate pointers and iterators pointing to existing elements in the vector\n.resize()B) Calling on a vector can invalidate pointers and iterators pointing to existing elements in the vector\n.reserve()C) Calling on a vector can invalidate pointers and iterators pointing to existing elements in the vector\nv i v, &v[i] == &v[0] + i trueD) If is a vector and is a valid index in then will always evaluate to\nE) None of the above\n16. Consider the following snippet of code:\n1\nclass Thing {\n2\nint32_t* x;\n3\nThing(int32_t x{new int32_t{x_in}}x_in) : {}\n4\ndelete~Thing() { x; }\n5\n};\n6\n7\nint main() {\n8\nint32_t arr[5];\n9\nstd::vector<int32_t> v1;\n10\nstd::vector<Thing> v2;\n11\nstd::vector<int32_t*> v3;\n12\nstd::vector<Thing*> v4;\n13\nfor (int32_t i = 0; i < 5; ++i) {\n14\narr[i] = i;\n15\nv1.push_back(i);\n16\nv2.push_back(Thing{i});\n17\nv3.push_back(&arr[i]);\n18\nv4.push_back(new Thing{i});\n19\n} // for i\n20\n} // main()\nDoes this code leak memory, and if so, what is the cause of the leak?\nx Thing v2A) Yes, the integers pointed to by in the objects of are leaked\nv3B) Yes, the integers pointed to by the elements of are leaked\nThing v4C) Yes, the objects pointed to by the elements of are leaked\nD) More than one of the above\nE) No memory is leaked after the vector destructors are run\nstd::vector<>.17. You want to store a four-dimensional data matrix in the form of a You know that your data has dimensions of\n203×281×183×280. If you want to minimize the amount of memory needed to create the 4-D vector (i.e., a vector of vector of vector of\nvectors), what should the dimension of the outermost vector be?\nA) 203\nB) 281\nC) 183\nD) 280\nE) It does not matter, since the memory required to create the vector will always be the same\n18. Suppose you have a vector containing 𝑛elements. What is the overall time complexity of appending 𝑛more elements, if the vector always\nreallocates its underlying array with each new element?\nA) Θ(1)\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛2)D)\nΘ(𝑛3)E)", "word_count": 593, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "77905281-bd55-5802-ad90-5f8650f8b7c5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 210, "real_page_number": null, "text": "198\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nnums. i nums19. You are given a vector of integers Implement a function that returns the smallest index of such that the last digit of\nnums[i] i nums[i] i -1.matches the last digit of its index (i.e., mod 10 == mod 10). If no such index exists, return For example,\ngiven the following vector:\n32\n46\n10\n53\n24\n67\n98\n0\n1\n2\n3\n4\n5\n6\n3,you would return since that is the first index whose last digit matches the last digit of its value (e.g., 53 at index 3).\nint32_t smallest_equal_index(const std::vector<int32_t>& nums);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of the vector. Note that you canΘ(𝑛) Θ(1)\noperator% 53 % 10 = 3).use to calculate a modulus (e.g.,\n20. You are given an 𝑛×𝑛square matrix of integers in the form of a vector of vectors. Implement a function that returns the sum of the values\non the matrix diagonals (each value is only counted once, even if it is on two diagonals). For example, given the following matrix:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n25,you would return since that is the sum of the diagonals (1 + 3 + 5 + 7 + 9 = 25).\n1\n2\n3\n4\n5\n6\n7\n8\n9\nint32_t matrix_diagonal_sum(const std::vector<std::vector<int32_t>>& matrix);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of a matrix dimension.Θ(𝑛) Θ(1)\n21. In linear algebra, a Toeplitz matrix is a matrix where each descending diagonal from top-left to bottom-right contains the same value. For\nexample, the following matrix is a Toeplitz matrix because each top-left to bottom-right diagonal consists of the same elements.\n1\n2\n3\n4\n0\n1\n2\n3\n2\n8\n1\n2\n1\n2\n3\n4\n0\n1\n2\n3\n2\n8\n1\n2\nGiven a 𝑚×𝑛matrix in the form of a vector of vectors, implement a function that returns whether the matrix is a Toeplitz matrix or not. If a\ntrue; false.matrix is Toeplitz, return otherwise, return\nbool is_toeplitz(const std::vector<std::vector<int32_t>>& matrix);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑚and 𝑛are the dimensions of the matrix.Θ(𝑚𝑛) Θ(1)", "word_count": 386, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5d80e842-abc6-5ef7-b93c-0e7b3c51407a", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 211, "real_page_number": null, "text": "7.6 Summary of Vector Complexities\n199\nChapter 7 Exercise Solutions\n1. The correct answer is (D). Only statements I and III are true. Statement I is true because the data in a vector are stored in an underlying\n.reserve()array that is allocated on the heap. Statement III is true because only affects the vector’s capacity, and not its size. Statement\nII is false because you may have to reallocate if the new element causes the vector to go over capacity, which would take linear time.\n2. The correct answer is (D). The worst-case time complexity of insertion assuming reallocation is Θ(𝑛), since you would have to move\nall the existing data to a new underlying array. Without reallocation, however, the worst-case time complexity could still be if theΘ(𝑛)\ninsertion takes place at the front of the vector, which would require all subsequent elements to be shifted over by one position.\n.resize()3. The correct answer is (D). When you call on a vector, you actually change the number of elements it contains. On the\n.reserve(),other hand, if you call you only reserve space for more elements, but you do not create them. Hence, you cannot use\n.push_back() on a vector that has been resized, since you would be adding elements on top of the elements you already created using\n.resize() (i.e., if you want 20 elements in your vector and nothing more, resizing the vector would initialize 20 elements in the vector,\n.push_back()and using would add a 21st element instead of modifying the 1st element). Meanwhile, after reserving space for a vector\nto hold more elements, you still must add the elements themselves. For instance, if you reserve a vector to hold 20 elements, it now has the\ncapacity to hold 20 elements, but it does not actually have any elements yet (similar to how an empty 5-gallon bucket hold 5 gallons ofcan\noperator[]water, butitdoesnotactuallyholdanywater). Thus, youcannotuse onavectorthatonlyhasspacereserved, astheelements\nyou want to create do not exist yet. The only ways to ensure that a vector has the number of elements you want it to have are to either resize\noperator[] .push_back().the vector and modify each element using or reserve space for the vector and add the elements using\n4. The correct answer is (E). When initializing a vector using a fill constructor, the first argument is the intended size of the vector, and the\nsecond argument is the value you want initialized for each of the elements in the vector. Only choice (E) works here: you are initializing a\nvector of size 5 (first argument) where each element is initialized to a vector of integers (second argument). This inner vector is given a size\nof 3 (first argument), where each element is initialized to 4 (second argument).\n5. The correct answer is (E). All of the statements are true. Statement I is true because elements must be copied from one array to another\nduring reallocation, which can be an expensive process. Statement II is true because all pointer or iterator references to the old array would\nbe invalidated when the elements are copied to a new, larger array. Statement III is true because you may end up with a larger capacity\nthan you need, which would waste memory (e.g., you only need to store 65 elements, but your vector ends up with a capacity of 128 since\ncapacity is doubled with each reallocation, leaving 63 slots unused).\n6. The correct answer is (C). The vector reallocates with double the capacity when you try to insert an element that exceeds its capacity.\nCurrently, the capacity is 16, so a reallocation is triggered when you insert the 17th element, which doubles the capacity to 32. The next\nreallocation happens when you try to insert the 33rd element, which doubles the capacity to 64. The next reallocation happens when you try\nto insert the 65th element, which doubles the capacity to 128 (and so on).\n7. The correct answer is (C). The vector allocates space four times. The first allocation occurs on line 2, where the vector’s underlying array\nis initially set to a size of 2 (with the values initialized to zero). The second allocation occurs on line 3, since the requested capacity of 4 is\nlarger than the existing capacity of 2. This is what the vector looks like after the second allocation:\n0\n0\nundef\nundef\nNext, the loop on line 4 inserts 7 elements to the back of the vector. Pushing back the 3rd of these elements forces a reallocation since there\nwould be 5 elements after the insertion, but only an existing capacity of 4. This reallocation doubles the vector’s capacity to 8. Following\nthis, the insertion of the 7th element in this loop increases the size of the vector to 9, which forces another reallocation of the vector from a\ncapacity of 8 to a capacity of 16.\n.reserve()8. Thecorrectansweris(B).The callonline2reservesthevector’scapacityto10,butitdoesnotactuallycreateanyelements.\nThus, trying to index into the values on line 5 causes undefined behavior, since you cannot index a vector with a position that is larger than\nits size (which is still zero).\n9. The correct answer is (D). On line 10, the vector only has a size of 9 (from the resize on line 8). Since vectors are zero-indexed, there is no\noperator[]value at index 9, so the use of on line 10 results in undefined behavior.\nptr10. The correct answer is (E). It is impossible to determine what line 7 prints, since is invalidated when the vector’s underlying array is\nforced to reallocate on line 6 (since a 7th element is added when the capacity is 6).\n11. The correct answer is (D). The loop on line 4 pushes back all numbers from 281 to 381 (exclusive), incrementing by 10 each time. After\nv1the loop runs to completion, the contents of are:\nv1\n281 291 301 311 321 331 341 351 361 371\nv1 v2 v1On lines 7 and 8, both and are resized to a size of 6. This removes all but the first six elements in and initializes six elements in\nv2 with a default value of zero.\nv1\n281 291 301 311 321 331\nv2\n0\n0\n0\n0\n0\n0", "word_count": 1081, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "30f075d4-d923-5211-a6b9-4fd88cec5b7a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 212, "real_page_number": null, "text": "200\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nv2.The loop on line 9 pushes back all values from 482 to 494 (exclusive) to the back of\nv1\n281 291 301 311 321 331\nv2\n0\n0\n0\n0\n0\n0\n482 483 484 485 486 487 488 489 490 491 492 493\nv1 v2, v2Line 12 resizes to the same size as with values initialized to the size of (which is 18).\nv1\n281 291 301 311 321 331\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\nv2\n0\n0\n0\n0\n0\n0\n482 483 484 485 486 487 488 489 490 491 492 493\nThe loop on line 13 then prints out the sum of the two vectors at indices 1, 5, 9, 13, and 17.\nv1\n291 331281 301 311 321\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\n18\nv2\n0\n0\n0\n0\n0\n0\n485 489 493482 483 484 486 487 488 490 491 492\n291\n331\n503\n507\n511\ni v2 v1[i] i.12. The correct answer is (A). For each index in the loop on line 4, is resized to a value of and has its last element assigned to\nv2 v1[0]On the first iteration, is resized to a size of or 2, and the last element is assigned to a value of 0.\nv2\n0\n0\nv2 v1[1]On the next iteration, is resized to a size of or 5, and the last element is assigned a value of 1.\nv2\n0\n0\n0\n0\n1\nv2 v1[2]On the next iteration, is resized to a size of or 1, and the last element is assigned with a value of 2.\nv2\n2\nv2 v1[3]On the next iteration, is resized to a size of or 3, and the last element is assigned with a value of 3.\nv2\n2\n0\n3\nv2 v1[4]On the next and final iteration, is resized to a size of or 4, and the last element is assigned with a value of 4.\nv2\n2\n0\n3\n4\nThe contents of this vector are then printed out.\n13. The correct answer is (E). When the 2nd element is inserted, the vector reallocates and 1 element is copied over. When the 3rd element is\ninserted, the vector reallocates and 2 elements are copied over. When the 5th element is inserted, the vector reallocates and 4 elements are\ncopied over. When the 9th element is inserted, the vector reallocates and 8 elements are copied over. Since reallocations arelog2(𝑛−1)\ndone if 𝑛elements are inserted without explicitly reserving capacity (since that is the number of times the capacity can double before you\n2⌊log2(𝑛−1)⌋,can support 𝑛elements), the total number of times an element is copied can be expressed using the summation 1 + 2 + 4 + +…\nwhich matches option (E).\n14. The correct answer is (C). Options (A), (B), and (D) are all true, since these are consequences of vector reallocation, which would happen\nif you pushed in a 1,000,001st string into a vector whose capacity is 1,000,000. Option (C) is incorrect because the size of the vector is not\ndoubled with reallocation (as with capacity), so the size of the vector would end up being 1,000,001 instead of 2,000,000.\n15. The correct answer is (E). All of the statements are true. Options (A), (B), and (C) could all result in a reallocation if the new requested\nsize or capacity exceeds the existing capacity, which would result in pointer and iterator invalidation. Option (D) is true because elements\nin a vector are stored contiguously in memory, allowing us to use pointer arithmetic to identify the position of elements in the vector.\n16. The correct answer is (C). Even though vectors store their values on the heap, they are responsible for cleaning up all memory that they\nv1 v2 v3own. Thus, the values in and are automatically cleaned up their respective vector is destructed. The pointers in refer to the values\narr, arr v4,in which are automatically cleaned up as well when goes out of scope. This is not the case for the values in however, since\nThing new,they are explicitly allocated on the heap by the programmer. Since the programmer created these entities using they are also\ndelete; Thing v4responsible for cleaning them up using this is not done, so the objects in are leaked.\n17. Thecorrectansweris(C).Ifyouhaveafour-dimensionalvectorthatisdeclaredintheorder𝑎, 𝑏, 𝑐, and𝑑, thetotalnumberofbookkeeping\npointers needed can be estimated using 3+3𝑎+3𝑎𝑏+3𝑎𝑏𝑐. Thus, to reduce the total number of pointers you need to keep track of, you\nshould declare the outermost vector (in this case, the one with dimension 𝑎) using the size corresponding to the smallest dimension, which\nin this case is 183.", "word_count": 824, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f72f1a25-da20-53bf-8e16-1971738c12a0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 213, "real_page_number": null, "text": "7.6 Summary of Vector Complexities\n201\n18. The correct answer is (D). If the vector always reallocates with each new element added, the fist new element added would force the\nexisting 𝑛elements to be copied; the second new element added would force the existing elements to be copied; and so on. From this,𝑛+1\nyou can see that each new insertion would take time for the old elements that need to be copied during reallocation. Since we areΘ(𝑛)\nΘ(𝑛2).appending 𝑛new elements, this process is done a total of 𝑛times, for an overall time complexity of 𝑛×Θ(𝑛)=\n19. This problem can be solved using an iteration of the vector, checking each value to see if its last digit matches its index. Once we encounter\nsuch a value, we can return its index immediately. If we finish iterating the entire vector without encountering a value whose last digit\n-1.matches its index, we would return Since we only need to iterate over the vector of 𝑛elements once, and we perform a constant\noperation at each element (checking its last digit with its index), this solution takes time.Θ(𝑛)\n1\nint32_t smallest_equal_index(const std::vector<int32_t>& nums) {\n2\nfor (size_t i = 0; i < nums.size(); ++i) {\n3\nif (i % 10 == nums[i] % 10) {\n4\nreturn i;\n5\n} // if\n6\n} // for i\n7\n8\nreturn -1;\n9\n} // smallest_equal_index()\nforOne strategy for solving this problem is to use a nested loop, one that iterates over the rows and one that iterates over the columns,20.\nΘ(𝑛2)and add the values where the row and column index identify a diagonal. However, that would take time, which exceeds the time\ncomplexity specified by the problem. Instead, we can implement a linear-time solution by only iterating over the rows and using arithmetic\nto determine which two values in that row are on a diagonal. For each row index 𝑖, the value on a diagonal must be located at column 𝑖and\n𝑛−𝑖−1. One thing to consider is the possibility of duplicating the center value if the dimension of the matrix is odd, so we will also need\nto subtract the center value in this scenario. One potential implementation is shown below:\n1\nint32_t matrix_diagonal_sum(const std::vector<std::vector<int32_t>>& matrix) {\n2\nint32_t sum = 0;\n3\nfor (size_t i = 0; i < matrix.size(); ++i) {\n4\n// value on top-left to bottom-right diagonal\n5\nsum += matrix[i][i];\n6\n// value on top-right to bottom-left diagonal\n7\nsum += matrix[i][n - i - 1];\n8\n} // for i\n9\n10\n// if dimension is odd, subtract center value so it is not double-counted\n11\nif (matrix.size() % 2 == 1) {\n12\nreturn sum - matrix[n / 2][n / 2];\n13\n} // if\n14\n15\nreturn sum;\n16\n} // matrix_diagonal_sum()\nfor21. One strategy for solving this problem is to use a nested loop, one that iterates over the rows and one that iterates over the columns, and\nmatrix[i][j] matrix[i - 1][j - 1] i jcheck if equals for all and from 1 to 𝑛−1. One implementation is shown below:\n1\nbool is_toeplitz(const std::vector<std::vector<int32_t>>& matrix) {\n2\nfor (size_t i = 1; i < matrix.size(); ++i) {\n3\nfor (size_t j = 1; j < matrix[i].size(); ++j) {\n4\nif (matrix[i][j] != matrix[i - 1][j - 1]) {\n5\nreturn false;\n6\n} // if\n7\n} // for j\n8\n} // for i\n9\n10\nreturn true;\n11\n} // is_toeplitz()", "word_count": 588, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f14debfb-17c2-5b8b-a760-c31121f142b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 214, "real_page_number": null, "text": "202\nChapter 7. Vectors: Dynamically Resizable Arrays in C++\nThis page has been intentionally left blank.", "word_count": 16, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "07137cdb-5a8b-5f64-8f41-a9e408f2c0ae", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 215, "real_page_number": null, "text": "Chapter 8\nLinked Lists\n8.1\nSingly- and Doubly-Linked Lists\nIn the previous chapter, we introduced the vector data structure, which stores its data contiguously in memory. The contiguity of elements\nmakes it possible to access any element in a vector in constant time using pointer arithmetic. Vectors also tend to provide fast access to its data\ndue to a phenomenon known as caching, which makes it faster to sequentially access elements that are closer together in memory. However, the\nneed to keep elements contiguous in memory causes inefficiencies for certain operations. For instance, inserting or erasing elements from\nanywhere other than the back of the vector would require elements to be shifted.\nIn this chapter, we will discuss the concept of a linked list, which does not require its elements to be stored contiguously in memory. A\nlinked list is made up of a sequence of nodes. Each node not only stores a data element of the list, but also pointers that can be used to determine\nthe relative ordering of nodes within the entire list.\nnullptr.In a singly-linked list, each node in the list stores a pointer to the next node in the sequence. The last node in the list points to A\ndepiction of a singly-linked list is shown below:\n0x1d10\nhead\n0x1d10\n1\n0x3588\nval\nnext\n0x3588\n2\n0x9630\nval\nnext\n0x9630\n3\n0x4fc8\nval\nnext\n0x4fc8\n4\n0x0\nval\nnext\n0x0\nnullptr", "word_count": 237, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8c62d1d1-96a8-5b49-96b5-69e4a7c680f4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 216, "real_page_number": null, "text": "204\nChapter 8. Linked Lists\nnextIn a doubly-linked list, each node in the list stores a pointer to both the previous and next nodes in the sequence. The last node’s pointer\nprev nullptr.and the first node’s pointer both point to Doubly-linked lists make it easier to traverse the list in both directions, but they\nrequire more memory due to the extra pointer. A depiction of a doubly-linked list is shown below:\n0x1d10\nhead\n0x1d10\n1\n0x3588\n0x0\nval\nnext\nprev\n0x3588\n2\n0x9630\n0x1d10\nval\nnext\nprev\n0x9630\n3\n0x4fc8\n0x3588\nval\nnext\nprev\n0x4fc8\n4\n0x0\n0x9630\nval\nnext\nprev\n0x0\nnullptr\nBecause the nodes in a linked list are not contiguous in memory, pointer arithmetic cannot be used to determine the memory address of any\n𝑛thnode in the list. If we wanted to access the element in the list, for some arbitrary value 𝑛, we would have to start at the head and iterate\nthrough 𝑛elements of the list (which takes time).Θ(𝑛)\nIn what ways is a linked list more efficient than a vector? As mentioned in the previous chapter, vectors store their data in a heap-allocated\narray, and reallocation is necessary if the size of the data exceeds the array’s capacity. Because excess capacity is allocated ahead of time in a\nvector, memory may be wasted. Furthermore, reallocation takes time and invalidates pointers and iterators to a vector’s data. Another issue with\nvectors is that insertions and removals require elements after the modification point to be shifted, which could take up to time.Θ(𝑛)\nA linked list does not face these issues. Memory is allocated as needed, so you do not need to worry about allocated memory going unused.\nIn addition, iterators and pointers to an element in a list are never invalidated until the element is destroyed; this is because, unlike in a vector, an\nelement in a linked list will never have to be reallocated in memory. Adding or removing elements at the beginning or middle of a container is\nalso asymptotically more efficient in a list, since none of the elements afterward need to be shifted.\nThat being said, linked lists also have their downsides. Each node in a linked list not only needs to store its data value, but also pointers\nto the next element (and previous element if doubly-linked). Hence, linked lists often require much more memory than a vector to store the\nexact same data (assuming the vector is properly resized or reserved to the correct size). Linked lists are often slower than vectors as well; to\naccess an arbitrary element, you will have to traverse through all the elements before it. Additionally, lists cannot take advantage of memory\ncontiguity and caching as easily as a vector. Data access in a vector is often faster than data access in a list — so much faster that vectors tend to\noutperform lists in many situations, even in cases where lists asymptotically have an advantage!\n8.2\nNode Insertion\nArrayMuch like the container class that we began implementing in chapter 6, we will start by discussing a linked list’s implementation as a\ncontainer class. Each node in a linked list stores a value, as well as a pointer that points to the next node (and a pointer to the previous node if it\nhead tailis doubly-linked). The list also maintains a pointer that points to the first element in the list, as well as an optional pointer that\npoints to the last element in the list. This is shown in the class definition below:\n1\ntemplate <typename T>\n2\nclass LinkedList {\n3\nstruct Node {\n4\nT data;\n5\nNode* prev;\n6\nNode* next;\n7\nprev{nullptr}, next{nullptr}Node() : {}\n8\nprev{nullptr}, next{nullptr}Node(T x) : data{x}, {}\n9\n};\n10\n11\nNode* head;\n12\nNode* tail;\n13\npublic:\n14\nLinkedList() {\n15\nnullptr;head = tail =\n16\n} // LinkedList()\n17\n18\n~LinkedList() {\n19\nNode* temp;\n20\nwhile nullptr)(head != {\n21\ntemp = head->next;\n22\ndelete head;\n23\nhead = temp;\n24\n} // while\n25\n} // ~LinkedList()\n26\n...\n27\n};", "word_count": 691, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bead3378-c2f4-51fe-8dd1-e25e097c92e6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 217, "real_page_number": null, "text": "8.2 Node Insertion\n205\nGiven a linked list, how would you insert a new node into the list? It turns out that this implementation is slightly different depending on where\nthe node is inserted. There are four cases that you have to consider:\n1. The insertion is done on an empty list.\n2. The insertion happens at the front of the list.\n3. The insertion happens at the back of the list.\n4. The insertion happens anywhere in the middle of the list (not beginning or end).\n1. The insertion is done on an empty list.\nhead tail nullptr. val),If a list is empty, its (and if doubly-linked) will point to To insert an element into an empty list (with initial value\nhead tailsimply allocate a new node and set and to point to that new element.\n1\nif nullptr)(head == {\n2\nnewNode* new_node = Node{val};\n3\nhead = new_node;\n4\ntail = new_node;\n5\n} // if\n-> nullptr! ->Before you use the operator to access the data of a node, you should always check to make sure that the node is not Using on\nnullptra will cause a segmentation fault.\n2. The insertion happens at the front of the list.\nIf the insertion happens at the front of the list, the following steps should be completed:\n1. Allocate a new node.\nnext2. Set the pointer of this node to the head of the linked list.\nprev nullptr prev head3. If doubly-linked, set the pointer of the new node to and the pointer of to the new node.\nhead4. Set to point to the new node.\nval:The following code inserts a node at the very front of a doubly-linked list, initialized to a value of\n1\n// allocate a new node, initialized to val\n2\nnewNode* new_node = Node{val};\n3\n// set the next of this node to the current head\n4\nnew_node->next = head;\n5\n// if list is not empty, set prev of current head to new node\n6\n// otherwise, set tail to the new node\n7\nif nullptr)(head != {\n8\nhead->prev = new_node;\n9\n} // if\n10\nelse {\n11\ntail = new_node;\n12\n} // else\n13\n// set head to the new node\n14\nhead = new_node;\n3. The insertion happens at the back of the list.\nIf the insertion happens at the very back of the list, the following steps should be completed:\n1. Allocate a new node.\nprev tail2. If doubly-linked, set the pointer to the last node in the list (you can retrieve this last element by either using the pointer if\ntailthere is one, or iterating to the end of the list if no pointer exists).\ntail3. It there is a pointer, set it to the new node.\nval:The following code inserts a node at the very back of a doubly-linked list, initialized to a value of\n1\n// allocate a new node, initialized to val\n2\nnewNode* new_node = Node{val};\n3\n// set prev to tail\n4\nnew_node->prev = tail;\n5\n// if list is not empty, set next of tail to new node\n6\n// otherwise, set head to the new node\n7\nif nullptr)(tail != {\n8\ntail->next = new_node;\n9\n} // if\n10\nelse {\n11\nhead = new_node;\n12\n} // else\n13\n// set tail to the new node\n14\ntail = new_node;", "word_count": 582, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b5c04b6d-9e98-5c12-8f52-78b6f3b7c284", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 218, "real_page_number": null, "text": "206\nChapter 8. Linked Lists\n4. The insertion happens in the middle of the list.\nIf the insertion happens in the middle of the list, you will have to allocate the new node and update the connections of the two nodes adjacent to\nprev_node)the insertion point (or just the node before it if singly-linked). The following code takes a pointer to a node in the list (named and\nvalinserts a node initialized to directly after it:\n1\n// allocate a new node\n2\nnewNode* new_node = Node{val};\n3\n// update prev and next of the new node\n4\nnew_node->prev = prev_node;\n5\nnew_node->next = prev_node->next;\n6\n// update next of prev_node\n7\nprev_node->next = new_node;\n8\n// update prev of the node directly after insertion point\n9\nif nullptr)(new_node->next != {\n10\nnew_node->next->prev = new_node;\n11\n} // if\n12\nelse {\n13\ntail = new_node; // this runs if prev_node is the last node\n14\n} // else\nprevThe insertion process for a doubly-linked list is summarized below. The process for a singly-linked list is similar, just without the pointers.\nInsert at beginning:\nnext new_node head.1. Set of to\nprev new_node nullptr.2. Set of to\nprev head new_node.3. Set of to\nhead new_node.4. Update to point to\nInsert at end:\nprev new_node tail.1. Set of to\nnext new_node nullptr.2. Set of to\nnext tail new_node.3. Set of to\ntail new_node.4. Update to point to\nprev_node:Insert in middle, given\nprev new_node prev_node.1. Set of to\nnext new_node prev_node->next.2. Set of to\nnext prev_node new_node.3. Set of to\nprev new_node->next new_node.4. Set of to\n2\n1\n3\nnew_node\nhead\n4\n1\n3\n2\nnew_node\ntail\n4\n3\n4\n1\n2\nnew_node\nprev_node\nExample 8.1 Node:Consider the following definition of a\n1\nstruct Node {\n2\nint32_t data;\n3\nNode* prev;\n4\nNode* next;\n5\nprev{nullptr}, next{nullptr}Node() : {}\n6\nNode(int prev{nullptr}, next{nullptr}x) : data{x}, {}\n7\n};\nvalWrite a function that inserts a value into a doubly-linked list. In other words, you have to insert a given value in a position tosorted\nhead tailkeep the list sorted. The list class keeps track of a and pointer, which point to the first and last elements in the list respectively.\nhead tail nullptr. Node prev next nullptr.If the list is empty, both and point to The constructor sets and to\nSince this question involves inserting elements into a list, we have to take into account the four conditions covered previously. In addition, we\nneed to take duplicates into account. For the sake of consistency, we will insert any duplicate value directly after the duplicates that are already\nin the list. By doing this, we will only need to worry about the equality case when adding to the middle or end, and not the beginning.", "word_count": 486, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f286ca66-7503-54e1-8dbc-789b699b1f63", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 219, "real_page_number": null, "text": "8.2 Node Insertion\n207\n1. The list is empty.\nhead tail head tailIf the list is empty, we must create a new node and set both and to the new node. This happens when both the and\nnullptr.pointers are The code for this first condition is shown below:\n1\n// Case #1: the list is empty\n2\nif nullptr)(head == {\n3\n// set both head and tail to the new node\n4\nnewNode* new_node = Node{val};\n5\nhead = new_node;\n6\ntail = new_node;\n7\n} // if\nval2. is smaller than all the other elements in the list.\nval valIf is the smallest element in the entire list, it would have to be inserted at the very beginning. Since the list is sorted, we can check if\nval. val,is smaller than all the other elements by just comparing the first element in the list with If the first element in the list is larger than\nwe can follow the procedure for inserting the node at the beginning:\n1\n// Case #2: insert at beginning of list\n2\nif (val < head->data) {\n3\nnewNode* new_node = Node{val};\n4\nnew_node->next = head;\n5\nhead->prev = new_node;\n6\nhead = new_node;\n7\n} // if\nval3. is larger than all the other elements in the list.\nval valIf is the largest element in the entire list, it would have to be inserted at the very end. Since the list is sorted, we can check if is larger\nval. val,than all the other elements by just comparing the last element in the list with If the last element in the list is smaller than we can\nfollow the procedure for inserting the node at the end:\n1\n// Case #3: insert at end of list\n2\nif (val >= tail->data) {\n3\nnewNode* new_node = Node{val};\n4\nnew_node->prev = tail;\n5\ntail->next = new_node;\n6\ntail = new_node;\n7\n} // if\ntail tailNote that this process was simple because we had access to the last element through a pointer. If the list did not have a pointer, we\nvalwould need to iterate through the entire list to reach the point of insertion if were larger than all other values in the list.\nval4. is neither the smallest nor largest value in the list.\nvalIf is neither the smallest nor largest value in the list, it should be inserted somewhere in the middle. As a result, we will need to find the\nval valposition should be inserted at; because the list is sorted, should be inserted after the largest value that less than or equal to it.\nwhileTo find the correct position of insertion, a good technique is to use a loop to iterate through the elements of the list until the largest\nvalvalue that is less than or equal to is found. In our case, we want to find the node directly before the position of insertion, which stores the\nvallargest value that is less than or equal to (this allows us to follow the template of inserting in the middle of the list, which we covered\nnext val,earlier). This can be done by iterating through the list until we reach the first node whose is larger than as shown:\n1\nNode* prev_node = head;\n2\nwhile (prev_node->next && prev_node->next->data < val) {\n3\nprev_node = prev_node->next;\n4\n} // while\nprev_nodeAt the end of this loop, should be the node directly before the position of the new node. We can now use the insertion procedure\nval prev_node nullptr whileto add to the correct position. There is no need to check if is here, since the loop guarantees that\nprev_node prev_node prev_node->next prev_node->next nullptr.is valid since is only assigned to if is not\n1\n// Case #4: insert in middle of list\n2\nnewNode *new_node = Node{val};\n3\nnew_node->prev = prev_node;\n4\nnew_node->next = prev_node->next;\n5\nprev_node->next = new_node;\n6\nnew_node->next->prev = new_node;", "word_count": 678, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e89281e9-c99e-5bba-ad46-caac72c0bc3f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 220, "real_page_number": null, "text": "208\nChapter 8. Linked Lists\nPutting this all together, we get the following solution (written as a member function of the list class). Note that it is also possible to combine\ntailthe third and fourth cases (which would have to be done if there is no pointer), but this will require you to iterate through the entire list if\nyou want to insert an element at the end.\n1\nvoid insert(int32_t val) {\n2\nnewNode* new_node = Node{val};\n3\nif nullptr)(head == {\n4\nhead = new_node;\n5\ntail = new_node;\n6\nreturn;\n7\n} // if\n8\nif (val < head->data) {\n9\nnew_node->next = head;\n10\nhead->prev = new_node;\n11\nhead = new_node;\n12\nreturn;\n13\n} // if\n14\nif (val >= tail->data) {\n15\nnew_node->prev = tail;\n16\ntail->next = new_node;\n17\ntail = new_node;\n18\nreturn;\n19\n} // if\n20\nNode* prev_node = head;\n21\nwhile (prev_node->next && prev_node->next->data < val) {\n22\nprev_node = prev_node->next;\n23\n} // while\n24\nnew_node->prev = prev_node;\n25\nnew_node->next = prev_node->next;\n26\nprev_node->next = new_node;\n27\nnew_node->next->prev = new_node;\n28\n} // insert()\n8.3\nNode Deletion\nWhat is the time complexity of deleting a node in a linked list, given its index? Since linked lists do not offer random access, this process is Θ(𝑛)\non average for both singly- and doubly-linked lists: to delete an element at a specified index, you would have to iterate through the list from the\nbeginning until you find the node that you want to delete.\nWhat if you were given a to the node you wanted to delete, instead of its index? Does the time complexity of deleting that elementpointer\nchange? For a doubly-linked list, the time complexity would become Θ(1), since we can just update the nodes that are adjacent to the deleted\nnode (the following code assumes deletion occurs in the middle; additional checks should be made if the node is at the beginning or end):\n1\nnode_to_delete->next->prev = node_to_delete->prev;\n2\nnode_to_delete->prev->next = node_to_delete->next;\n3\ndelete node_to_delete;\nnode_to_delete\n2\n1\nHowever, the time complexity of deleting an element given a pointer to a singly-linked list is still Θ(𝑛). This is because we need to update the\nnext next.previous node’s so that it points to the deleted node’s We do not have direct access to the previous node in a singly-linked list, so\nwe would have to traverse through the list to find it.\nIs it somehow possible for a singly-linked list to support deletion in certain situations? It is possible if the pointer passed in is a pointerΘ(1)\ndeleteNode(Node* prev)).to the node directly the node to be deleted (i.e., However, this approach is rather counter-intuitive and,before\ndepending on the list implementation, may not support deleting the head node of a list.\nWhat if the pointer passed into the deletion function point to the node to be deleted? If we are given nothing else, is deletion stillΘ(1)must\npossible for a singly-linked list? The following shows one potential approach:\n1. Overwrite the data in the node to be deleted with the data of the next node’s data.\n2. Delete the next node.\n1\nNode* next_node = node_to_delete->next;\n2\nnode_to_delete->data = next_node->data;\n3\nnode_to_delete->next = next_node->next;\n4\ndelete next_node;", "word_count": 547, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5ff7c807-0281-5a6d-a792-32b85fe6a079", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 221, "real_page_number": null, "text": "8.3 Node Deletion\n209\n1\nnode_to_delete\n2\n3\n4\nnullptr\n1\nnode_to_delete\n3\n3\n4\nnullptr\nnextOverwrite with data of\n1\nnode_to_delete\n3\n3\n4\nnullptr\nAfter this deletion, the contents of the list are exactly what we wanted. Since we only swapped a few pointers around and changed a value, this\nwas all done in constant time. However, this approach does not always work! There are three reasons why:\nnext1. If the node we wanted to delete were the last node, trying to overwrite and delete would fail.\n2. This implementation assumes that the data can be copied, which cannot be guaranteed.\nnode_to_delete->next next.3. This method is not safe, since we end up deleting rather than Even though we swapped the data so\nthat the list still valid, the memory addresses are no longer the same. If we had any pointers, iterators, or any data that depended onlooks\nnode_to_delete->next,the address of they would be invalidated.\nAs a result, the above approach is not perfect, even if it can delete a given node of a singly-linked list in constant time. In general, if you\ndo not have a reference to the node directly the one you want to delete, the worst-case time complexity of deleting an element from abefore\nsingly-linked list cannot be better than Θ(𝑛).\nThe deletion process is very similar to the insertion process. Make sure you consider all possible edge cases and update the surrounding\nnodes after the deletion to ensure that the entire list remains valid.\nExample 8.2 Consider the following definition of a node:\n1\nstruct Node {\n2\nint32_t data;\n3\nNode* next;\n4\nnext{nullptr}Node() : {}\n5\nNode(int next{nullptr}x) : data{x}, {}\n6\n};\nWrite a function that deletes a node from a singly-linked list, given the index of this node. 0-indexing is used. The node constructor sets\nnext nullptr headto by default. The list class only supports a pointer, which points to the first element in the list.\nTo solve this problem, we want to first find the node that we want to delete. This requires us to iterate through the list. However, since this is a\ndeletion problem on a singly-linked list, we also need to find the node directly before the one we want to delete to implement a solution.Θ(1)\nWe will also need to consider several edge cases:\n1. If the linked list is empty, we shouldn’t delete anything at all.\nhead2. If the index we want to delete is 0, we should update the pointer before deleting.\n3. If the index we get is larger than the largest index possible, we should not delete anything at all.\ntail tail4. If there were a pointer, deleting the last element in the list would require an update to the pointer (this does not apply here).\nThe solution to this problem is shown below. It checks the cases and iterates up to the node directly before the one that needs to be deleted. It\nthen deletes the correct node and resets the pointers so that the entire list remains valid after the deletion.\n1\nvoid delete_at_index(size_t index) {\n2\nif nullptr)(head == {\n// check for empty list\n3\nreturn;\n4\n} // if\n5\nif (index == 0) {\n// check if index is 0\n6\nNode* temp = head;\n7\nhead = temp->next;\n8\ndelete temp;\n9\nreturn;\n10\n} // if\n11\n// find the node before one to delete by looping through list - make sure not to iterate off end\n12\nNode* prev_node = head;\n13\nfor (size_t nullptri = 0; prev_node != && i < index - 1; ++i) {\n14\nprev_node = prev_node->next;\n15\n} // for i\n16\n// make sure the index is not larger than the largest index possible\n17\nif nullptr nullptr)(prev_node == || prev_node->next == {\n18\nreturn;\n19\n} // if\n20\n// delete the node and update the pointers\n21\nNode* node_to_delete = prev_node->next;\n22\nprev_node->next = node_to_delete->next;\n23\ndelete node_to_delete;\n24\n} // delete_at_index()", "word_count": 683, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "55147467-b084-59f5-87c5-8c5ecae98524", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 222, "real_page_number": null, "text": "210\nChapter 8. Linked Lists\n8.4\nReversing a Linked List\n¸ 8.4.1\nA Naïve Solution For Reversing a Linked List\nIn this section, we will discuss a common interview problem: reversing a linked list. To start off, we will introduce a naïve approach for solving\nthis problem: simply iterate through the linked list and copy the elements to the front of a new list. An illustration of this process is shown below:\nhead\n1\n2\n3\n4\nnullptr\nnew_head\nnullptr\n1Begin iterating through the original list, and copy to the beginning of the new list:\nhead\n1\n2\n3\n4\nnullptr\nnew_head\n1\nnullptr\n2Iterate to and copy it to the beginning of the new list:\nhead\n1\n2\n3\n4\nnullptr\nnew_head\n2\n1\nnullptr\n3Iterate to and copy it to the beginning of the new list:\nhead\n1\n2\n3\n4\nnullptr\nnew_head\n3\n2\n1\nnullptr\n4Iterate to and copy it to the beginning of the new list:\nhead\n1\n2\n3\n4\nnullptr\nnew_head\n4\n3\n2\n1\nnullptr\nnullptr, headOnce you iterate to a deallocate the old list and update the pointer to point to the new list:\nhead\n1\n2\n3\n4\nnullptr\n4\n3\n2\n1\nnullptr\nThe code for this solution is shown below (here, the deallocation occurs during the traversal, but the idea is the same):\n1\nvoid reverse_list(Node*& head) {\n2\nnullptr;Node* new_head =\n3\nwhile nullptr)(head != {\n4\nnewNode* new_node = Node{head->data};\n5\nnew_node->next = new_head;\n6\nnew_head = new_node;\n7\nNode* old_node = head;\n8\nhead = head->next;\n9\ndelete old_node;\n10\n} // while\n11\nhead = new_head;\n12\n} // reverse_list()", "word_count": 284, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ccc0b074-769f-55a5-8ab6-ef854887dbd7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 223, "real_page_number": null, "text": "8.4 Reversing a Linked List\n211\nThis solution takes time and auxiliary space, since it iterates through all 𝑛elements of the list and makes an additional copy of the listΘ(𝑛) Θ(𝑛)\nto write the reversed elements to. Although this solution works, it is the most efficient solution. Not only are we making a separate copy ofnot\nthe list (which takes up additional memory), we are also wasting time constructing new nodes (and if the data we store in each node were large,\ntrying to duplicate each element could be costly).\n¸ 8.4.2\nAn Optimized Iterative Approach\nA better solution does not involve making a duplicate copy of the list at all. Instead, we only need to make one pass through the list and modify\npointers along the way. The algorithm is as follows:\ncurr, prev, next. curr head1. Initialize three pointers: and should be set to upon initialization.\ncurr nullptr,2. While is not repeat the following:\nnext curr->next.a. Set to\ncurr’s next curr->next prev.b. Reverse pointer by setting to\nprev curr curr next.c. Move all pointers one position forward by setting to and to\n(curr nullptr), head prev.3. After the loop ends hits set to\nAn illustration of this algorithm is shown below:\nhead\n1\ncurr\n2\nnext\n3\n4\nnullptr\ncurr->next prev prev nullptr):Set to (in this case, is\nhead\n1\ncurr\n2\nnext\n3\n4\nnullptr\nMove each pointer forward by one:\nhead\n1\nprev\n2\ncurr\n3\nnext\n4\nnullptr\ncurr->next prev:Set to\nhead\n1\nprev\n2\ncurr\n3\nnext\n4\nnullptr\nMove each pointer forward by one:\nhead\n1\n2\nprev\n3\ncurr\n4\nnext\nnullptr\ncurr->next prev:Set to\nhead\n1\n2\nprev\n3\ncurr\n4\nnext\nnullptr\nMove each pointer forward by one:\nhead\n1\n2\n3\nprev\n4\ncurr\nnullptr\nnext\ncurr->next prev:Set to\nhead\n1\n2\n3\nprev\n4\ncurr\nnullptr\nnext", "word_count": 321, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bd4166f2-8bd5-516d-816d-0715b0f8f4cd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 224, "real_page_number": null, "text": "212\nChapter 8. Linked Lists\nMove each pointer forward by one:\nhead\n1\n2\n3\n4\nprev\nnullptr\ncurr\ncurr == nullptr, head prev:Once set to\nhead\n1\n2\n3\n4\nprev\nnullptr\nThe list has been successfully reversed. The benefit of this approach over the first method is that an additional copy of the list is not needed; the\nreversing is done in-place! In addition, we do not have to deallocate an entire list after we are done, since we are modifying the original list itself\nrather than making an copy. The following code implements the above approach and reverses a singly-linked list using auxiliary space:Θ(1)\n1\nvoid reverse_list(Node*& head) {\n2\n// initialize curr, prev, and next\n3\nNode* curr = head;\n4\nnullptr;Node* prev =\n5\nnullptr;Node* next =\n6\n// loop until curr is nullptr\n7\nwhile nullptr)(curr != {\n8\n// update next to curr->next\n9\nnext = curr->next;\n10\n// reverse curr's next pointer\n11\ncurr->next = prev;\n12\n// move all pointers forward by one, next gets updated during next iteration\n13\n// of while loop (this is done because we want to guarantee that the updated\n14\n// curr is not nullptr before we try to assign curr->next to next)\n15\nprev = curr;\n16\ncurr = next;\n17\n} // while\n18\n// after the loop terminates, set head to prev\n19\nhead = prev;\n20\n} // reverse_list()\nThe time complexity of this function is because the entire list is traversed. The auxiliary space used by this function is since theΘ(𝑛) Θ(1)\nadditional space needed to complete the reversal does not depend on the size of the list 𝑛.\nThe above approach can be used to reverse a doubly-linked list using auxiliary space as well. The only difference is that two directionalΘ(1)\npointers have to be adjusted at each step rather than just one.\n¸ 8.4.3\nAn Optimized Recursive Solution\nA linked list can also be reversed recursively. The recursive algorithm is as follows:\ncurr1. Initialize a pointer that points to the head of the list.\n2. Check for the base cases:\ncurr nullptr,a. If is return.\ncurr->next nullptr,b. If is it must be the last node in the list, so make this node the new head and return.\ncurr->next.3. Make a recursive call on\ncurr->next->next curr.4. Set to\ncurr->next nullptr.5. Set to\nLet’s look at how this works visually. Suppose we are given the following linked list, which we want to reverse recursively:\nhead\n1\n2\n3\n4\nnullptr\ncurrFirst, we initialize to point to the head of the list:\nhead\n1\ncurr\n2\n3\n4\nnullptr\ncurr curr->next nullptr,We then check for the base cases. Since neither nor are the base case does not run. As a result, we make a\ncurr->next.recursive call on\nhead\n1\ncurr\n2\n3\n4\nnullptr", "word_count": 489, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2ff8ab4d-3219-53d0-9651-b5553a0cb7cc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 225, "real_page_number": null, "text": "8.4 Reversing a Linked List\n213\ncurr curr->next nullptr, curr->next.We check the base cases again. Neither nor are so we make a recursive call on\nhead\n1\n2\ncurr\n3\n4\nnullptr\ncurr curr->next nullptr, curr->next.Again, neither nor are so we make another recursive call on\nhead\n1\n2\n3\ncurr\n4\nnullptr\ncurr->next nullptr, 4At this point, is so the base case runs. We know that must be the last element in the list, so we make this node the\n3.new head and return. The recursion unrolls, and we end up back at node\nhead\n1\n2\n3\ncurr\n4\nnullptr\n3 curr->next->nextTherecursivecallthatwasmadeatnode isnowcomplete,sowecanmoveontosteps4and5ofthealgorithm. Weset\ncurr, curr->next nullptr.to and to\nhead\n1\n2\n3\ncurr\n4\nnullptr\n2. 2 curr->next->nextTherecursionunrolls,andweendupbackatnode Sincetherecursivecallthatwasmadeatnode iscomplete,wewillset\ncurr, curr->next nullptr.to and to\nhead\n1\n2\n3\ncurr\n4\nnullptr\n1. 1 curr->next->nextTherecursionunrolls,andweendupbackatnode Sincetherecursivecallthatwasmadeatnode iscomplete,wewillset\ncurr, curr->next nullptr.to and to The linked list has now been successfully reversed.\nhead\n1\n2\n3\ncurr\n4\nnullptr\nThe code for recursively reversing a linked list is shown below:\n1\nvoid helper(Node*& head, Node*& curr) {\n2\nif nullptr)(curr == {\n3\nreturn;\n4\n} // if\n5\nif nullptr)(curr->next == {\n6\nhead = curr;\n7\nreturn;\n8\n} // if\n9\nhelper(head, curr->next);\n10\ncurr->next->next = curr;\n11\nnullptr;curr->next =\n12\n} // helper()\n13\n14\nvoid reverse_list(Node *&head) {\n15\nNode* curr = head;\n16\nhelper(head, curr);\n17\n} // reverse_list()", "word_count": 314, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d0113d6f-0c65-5708-880c-436c9f0f7c92", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 226, "real_page_number": null, "text": "214\nChapter 8. Linked Lists\n*&head).Remark: In all of these functions, a was passed in (e.g., Passing in the pointer by reference allows us topointer by reference\nheadmodify where the pointer is pointing to inside the function (in these examples, we were able to change where was pointing to). Without\nheadthe pointer by reference, the function would take in the pointer by (e.g., the variable in the function would be a local of thevalue copy\nhead head headactual pointer, and reassigning in the function would reassign the function’s local copy, and not the original of the linked\nlist). Consider the following two functions:\n1\nvoid foo(Node* head) {\n2\nnullptr;head =\n3\n} // foo()\n4\n5\nvoid bar(Node*& head) {\n6\nnullptr;head =\n7\n} // bar()\n8\n9\nint main() {\n10\nnewNode* head = Node{281};\n11\nfoo(head);\n12\nstd::cout << head << std::endl;\n// prints address 0xd20c20 (arbitrary)\n13\nbar(head);\n14\nstd::cout << head << std::endl;\n// prints address 0x0\n15\n} // main()\n0x0 (nullptr). foo()Notice that line 12 did not print out address This is because the function took in a pointer by value, so the function\nhead nullptr. headmade a local copy of and set that local copy to Thus, the original variable created on line 10 did not change after it\nfoo(). bar() head nullptr.waspassedinto Ontheotherhand, the functiontookinapointerbyreference, soitsettheoriginal valueto\n0x0 head bar(). f()This is why was printed after was passed into In summary, if you pass a pointer into a function by reference, any\nf() f().changes to the pointer in will also be reflected in the function that invoked\n8.5\nTechniques for Solving Linked List Problems\nOne common technique that can be used to solve linked list problems involves using two pointers to iterate through a linked list, with one either\nat a fixed distance from the other, or one that moves faster than the other. Let’s look at a few examples that can be solved using this approach.\n𝑘thExample 8.3 You are given an integer 𝑘and the head pointer of a singly-linked list. Write a function to find the to last element in\nthe list. You may assume that the list is not empty and that 𝑘is a valid number that makes sense for the problem. Return the value of this\ndata).element (i.e., the value stored in The definition of a node is shown below:\n1\nstruct Node {\n2\nint32_t data;\n3\nNode* next;\n4\nNode(int32_t next{nullptr}x) : data{x}, {}\n5\n};\n𝑘thThe trick to notice here is that the to last element is 𝑘from the end of the list. Thus, we can use two pointers that are a distance of 𝑘nodes\n𝑘thapart to find the to last element. In our algorithm, we can start from the beginning of the list and increment both pointers until the front\npointer reaches the end of the list. Since the back pointer is 𝑘nodes behind the front pointer, once the front pointer reaches the end, the back\n𝑘thpointer must point to the to last element — the element we want!\n3rdThe following illustrates the process of finding the to last element in a list:\nhead\nnullptr\nInitialize two pointers that point to the head of the list:\nhead\nnullptr\nMove one pointer ahead by 𝑘positions (in this case 3 positions, since 3):𝑘=\nhead\nnullptr\nNow, iterate both pointers forward at the same time, at the same speed. This ensures that the pointers are always 𝑘nodes apart. Stop iterating as\nnullptrsoon as the first pointer reaches the at the end of the list:\nhead\nnullptr", "word_count": 630, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dc9d8876-cb11-5af2-b2dd-03af6063d53c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 227, "real_page_number": null, "text": "8.5 Techniques for Solving Linked List Problems\n215\n𝑘thReturn the value pointed to by the second pointer, which must be pointing to the to last node:\nhead\nnullptr\nThe above algorithm can be implemented as follows:\n1\nint32_t int32_tkth_to_last(Node* head, k) {\n2\n// initializes two pointers\n3\nNode* fast = head;\n4\nNode* slow = head;\n5\n// move the fast pointer forward by k positions\n6\n// per instructions, k is always valid (else you need to check for nullptr)\n7\nfor (int32_t i = 0; i < k; ++i) {\n8\nfast = fast->next;\n9\n} // for i\n10\n// move both fast and slow forward until fast reaches the end\n11\nwhile nullptr)(fast != {\n12\nfast = fast->next;\n13\nslow = slow->next;\n14\n} // while\n15\n// once fast reaches the end, slow must point to the kth to last element\n16\nreturn slow->data;\n17\n} // kth_to_last()\nExample 8.4 You are given the head of a singly-linked list. Write a function that returns the value of the middle node. If there are two\nmiddle nodes, return the value of the second middle node. A node is defined as follows:\n1\nstruct Node {\n2\nint32_t data;\n3\nNode* next;\n4\nNode(int32_t next{nullptr}x) : data{x}, {}\n5\n};\nSimilar to the previous problem, we can iterate both pointers through the list in such a way that one pointer points to the node we want once\nfastthe other reaches the end. This can be done by having one pointer move twice as fast as the other. We first initialize two pointers, and\nslow, fast slowthat both start iterating at the beginning. However, with each iteration, we would increment by two and by one. When\nfast slowreaches the end, must point to the middle node.\nhead\nnullptr\nhead\nnullptr\nhead\nnullptr\nfast nullptr, slow:Incrementing the pointer here would cause it to point to so return the value pointed to by\nhead\nnullptr\nThis solution is implemented in the code below:\n1\nint32_t find_middle_node(Node* head) {\n2\n// initialize two pointers\n3\nNode* fast = head;\n4\nNode* slow = head;\n5\n// move fast forward 2 steps for each step slow moves forward\n6\n// continue iterating until fast reaches the end; we only need\n7\n// to check validity of fast since slow will always be valid\n8\n// if fast is valid (since fast hits the end of the list first)\n9\nwhile nullptr nullptr)(fast != && fast->next != {\n10\nslow = slow->next;\n11\nfast = fast->next->next;\n12\n} // while\n13\n// once fast reaches the end, slow must hold the value we want\n14\nreturn slow->data;\n15\n} // find_middle_node()", "word_count": 459, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "69786df0-507c-52d6-b3eb-a45bc5b37d81", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 228, "real_page_number": null, "text": "216\nChapter 8. Linked Lists\nExample 8.5 You are given the head of a singly-linked list. Write a function to determine if the list contains a cycle (or loop), where there\nexists a node that points to a previous node in the list. A node is defined as follows:\n1\nstruct Node {\n2\nint32_t data;\n3\nNode* next;\n4\nNode(int32_t next{nullptr}x) : data{x}, {}\n5\n};\nThe following is an example of a linked list that has a cycle:\nThis is a slightly more challenging question, but it can be solved using the same approach as the previous two problems. How can we move two\npointers through the list in a way that can help us determine whether there exists a cycle?\nLet’s start off with a similar problem that can lead us to the solution: suppose we are given a linked list, and we want to know whether there\nexists (such as in the illustration shown on the previous page). How can we use two pointers to determine if a cycle of lengtha cycle of length 6\n6 exists? Well, if we had two pointers that were a distance of 6 nodes apart, then eventually the two pointers would always point to the same\nnode if we increment them at the exact same rate. This is because, if a cycle of length 6 exists, the node that is 6 nodes away from one pointer\nmust be itself, since everything loops around.\nNow, what if we were instead asked if the list had a cycle of length 7? We could solve this problem in a similar manner: instead of iterating\ntwo pointers 6 nodes apart, we would iterate two pointers 7 nodes apart. If a cycle of length 7 exists, then two pointers that are 7 nodes apart in\nthe cycle must point to the exact same node.\nEven though keeping two pointers a constant distance apart does not solve the problem (since we do not know the length of the cycle we\nwant to find), we can use this idea to construct a solution that can find the existence of any cycle in the list, regardless of its length. The trick to\nnotice here is that, even though a constant distance does not work, we can cover all possible cycle lengths if we iterate one pointer at twice the\nspeed of the other! Why is this the case? Suppose that there exists a loop in the list. With each iteration, the distance between the fast and slow\npointer increases by 1. Thus, regardless of what the length of the cycle is, the distance between the pointers will eventually reach the length of\nthat cycle. Because of this, if the fast and slow pointer ever meet and point to the same node, then there must exist a cycle.\nWhat if the distance between the two pointers already exceeds the length of the cycle when the slow pointer enters the cycle? For instance,\nsuppose a cycle of length 20 exists in our list, but that cycle is more than 20 nodes away from the head. As a result, when the two pointers are\n20 nodes apart, the slow pointer has not made it into the cycle yet. Would our algorithm fail in this case, since the two pointers would not be\npointing to the same node, even though a cycle of length 20 exists and the two pointers are 20 nodes apart?\nIt turns out that this case is still handled by our algorithm. Even if the distance between the two pointers already exceeds the size of the loop\nwhen the slow pointer enters the cycle, the nature of the loop ensures that multiples of the cycle size would also allow us to detect the cycle. For\ninstance, if there existed a cycle of length 20, and the fast and slow pointers were already more than 20 nodes apart when the slow pointer\nentered the cycle, both pointers would eventually point to the same node again when they are 40, 60, 80, nodes apart. Think about it: in a…\ncycle of size 20, the node 40 steps way is also the exact same node! No matter how far the cycle is from the head of the list, eventually both the\nfast and slow pointers will be in the cycle, and the first multiple of the cycle length will allow us to detect the cycle’s existence.\nThis algorithm is officially known as Algorithm, and it is implemented below:Floyd’s Cycle-Finding\n1\nbool has_cycle(Node* head) {\n2\n// use two pointers, where one moves faster than the other;\n3\n// if the fast pointer catches up to the slow pointer.\n4\n// then there exists a cycle\n5\nNode* fast = head;\n6\nNode* slow = head;\n7\nwhile (fast && fast->next) {\n8\nslow = slow->next;\n9\nfast = fast->next->next;\n10\nif (fast == slow) {\n11\nreturn true;\n12\n} // if\n13\n} // while\n14\nreturn false;\n15\n} // has_cycle()", "word_count": 836, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e4313667-d88b-56bc-9452-e2c5041ecb6a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 229, "real_page_number": null, "text": "8.5 Techniques for Solving Linked List Problems\n217\nIn the following examples, we will cover some additional interview-style list problems that you may encounter.\nExample 8.6 p.You are given the head of a singly-linked list and an integer Write a function that partitions the list in a way such that all\np p.nodes less than come before nodes that are greater than or equal to The relative order of nodes should be preserved in each of the two\npartitions. This partitioned list should be returned by your function. A node is defined as follows:\n1\nstruct Node {\n2\nint32_t data;\n3\nNode* next;\n4\nNode(int32_t next{nullptr}x) : data{x}, {}\n5\n};\np = 3:For example, given the following list and a partition value of\nhead\n2\n8\n1\n3\n7\n0\nnullptr\n3 2, 1, 0)you should return a list with the values rearranged so that all values less than (in this case, and come before all values greater than\n3 8, 3, 7). 2 1, 1or equal to (in this case, and The relative order of these elements is maintained (so still comes before and still comes\n0; 3).before same for the values greater than or equal to\nhead\n2\n1\n0\n8\n3\n7\nnullptr\nTo approach this problem, a key insight is to notice that the list you want to return is actually a combination of two lists that are joined together:\np, p.one with values less than and one with values greater than or equal to In our example, these two lists are as follows:\nhead\n2\n1\n0\nnullptr\nhead\n8\n3\n7\nnullptr\nIf we can recreate these two smaller lists, then we simply have to join them to get our solution. This can be done by iterating over the original\np, p.list and moving each node to one of two sublists, one that stores values less than and one that stores values greater than or equal to This is\n≥p.before_head p, after_headshown below, where is a pointer to the sublist with values and is a pointer to the sublist with values<\nbefore_ptr after_ptr,We will also keep track of two pointers, and to indicate the position of insertion for each of the two sublists.\nbefore_head\nafter_head\nhead\n2\n8\n1\n3\n7\n0\nnullptr\nhead head x, before_headWe iterate over the original list using its pointer. If the value pointed to by is less than we move it to the\nafter_head 2 p = 3,list; otherwise, we move it to the list. In the example, the first value of is less than the value of so it is moved to the\nbefore_head list, as shown:\nbefore_head\nbefore_ptr\n2\nafter_head\nhead\n8\n1\n3\n7\n0\nnullptr", "word_count": 468, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e409df54-09ba-5ce2-9cc7-d3af07e97158", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 230, "real_page_number": null, "text": "218\nChapter 8. Linked Lists\n8, p, 8 after_headThe next value, is greater than so is moved to the list:\nbefore_head\nbefore_ptr\n2\nafter_head\nafter_ptr\n8\nhead\n1\n3\n7\n0\nnullptr\n1, p, 1 before_head before_ptrThe next value, is less than so is moved to the list. We also increment the pointer to indicate where the\nnext value in the list should be added (which facilitates insertion of additional elements).\nbefore_head\nbefore_ptr\n2\n1\nafter_head\nafter_ptr\n8\nhead\n3\n7\n0\nnullptr\n3, p, after_head p). after_ptrThe next value, is equal to so it is moved to the list (per the rules for elements equal to the given The\npointer is also incremented to facilitate insertion of additional elements.\nbefore_head\nbefore_ptr\n2\n1\nafter_head\nafter_ptr\n8\n3\nhead\n7\n0\nnullptr\n7, p, after_head after_ptrThe next value, is greater than so it is moved to the list, and is incremented.\nbefore_head\nbefore_p\n2\n1\nafter_head\nafter_ptr\n8\n3\n7\nhead\n0\nnullptr\n0, p, before_head before_ptrThe next value, is less than so it is moved to the list, and is incremented.\nbefore_head\nbefore_ptr\n2\n1\n0\nafter_head\nafter_ptr\n8\n3\n7\nhead\nnullptr", "word_count": 199, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "32506022-7efe-5a53-9d7b-3d15181f614e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 231, "real_page_number": null, "text": "8.5 Techniques for Solving Linked List Problems\n219\nhead nullptr,The value of is now so we have successfully completed our traversal of the original list. The two smaller lists are then\nbefore_head->next = after_head)combined (i.e., to obtain our solution list. An implementation of this solution is provided below:\n1\nint32_tNode* partition_list(Node* head, p) {\n2\n// this creates a dummy node at the beginning of the\n3\n// before_head and after_head sublists to make implementation easier\n4\n// (also why before_head.next and after_head.next are used for return value)\n5\nNode before_head{0}, *before_ptr = &before_head;\n6\nNode after_head{0}, *after_ptr = &after_head;\n7\nwhile nullptr)(head != {\n8\nif (head->val < p) {\n9\nbefore_ptr->next = head;\n10\nbefore_ptr = before_ptr->next;\n11\n} // if\n12\nelse {\n13\nafter_ptr->next = head;\n14\nafter_ptr = after_ptr->next;\n15\n} // else\n16\nhead = head->next;\n17\n} // while\n18\n19\n// combine the two lists at before_head and after_head\n20\nnullptr;after_ptr->next =\n21\nbefore_ptr->next = after_head.next;\n22\nreturn before_head.next;\n23\n} // partition_list()\nThe time complexity of this solution is Θ(𝑛), where 𝑛is the number of nodes in the original list. This is because our implementation iterates\nover all the nodes of this list. The auxiliary space is because we only reorganized the nodes in the original list instead of duplicating them;Θ(1)\nthus, the additional memory allocated by this solution is a constant that does not depend on the size of the original list.\nExample 8.7 head1 head2.You are given the head of two singly-linked lists, and Write a function that returns the node at which these\nnullptr.two lists intersect. If the two lists do not intersect at all, return You may assume that there are no cycles in the list structure you\nare given. A node is defined as follows:\n1\nstruct Node {\n2\nint32_t data;\n3\nNode* next;\n4\nNode(int32_t next{nullptr}x) : data{x}, {}\n5\n};\n3:For example, given the following head pointers, you would return the node with the value\nhead1\n1\n8\n3\n7\n6\n1\n8\n2\nhead2\nFor this problem, we will go over two solutions: a relatively straightforward solution, and one that requires a bit more ingenuity. To start, let’s\nconsider the case where both lists have the same number of nodes before the point of intersection, as shown:\nhead1\nhead2\nhead1 head2.To find the node of intersection here, we can simply initialize two pointers, one that points to and one that points to We then\nincrement each pointer in tandem; if the two pointers ever end up pointing to the same node, then that node must be the point of intersection.\n➀Increment\n➁Increment\n➂Increment\n➃Intersection\nThis approach does work if the two lists have a different number of nodes before the point of intersection, as with our initial example.not\nHowever, we can use this idea to come up with a solution; instead of incrementing both pointers immediately, we first allow the pointer in the\nlonger chain to catch up with the pointer in the shorter chain. Then, we begin incrementing the two pointers in tandem, similar to before.\n➀Only increment pointer #2\n➁Caught up, increment both\n➂Increment\n➃Intersection", "word_count": 537, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "402a5b89-099a-5209-b730-58cb96fe5d61", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 232, "real_page_number": null, "text": "220\nChapter 8. Linked Lists\nTo determine when the pointer on the longer end has caught up with the pointer on the shorter end, we would need to know the difference in\nhead1 head2.length between the linked list at and the one at This requires us to do some preprocessing beforehand to compute the lengths of\nthe two lists. Once we have the lengths, we find the difference and increment the pointer on the longer end by this difference to line it up with\nthe pointer on the shorter end. The code for this is shown below:\n1\n// helper function that finds length of list\n2\nint32_t get_list_length(Node* head) {\n3\nint32_t length = 0;\n4\nwhile nullptr)(head != {\n5\n++length;\n6\nhead = head->next;\n7\n} // while\n8\nreturn length;\n9\n} // get_list_length()\n10\n11\nNode* get_list_intersection(Node* head1, Node* head2) {\n12\nint32_t len1 = get_list_length(head1);\n13\nint32_t len2 = get_list_length(head2);\n14\n15\n// increment pointer in longer list to catch up with pointer in shorter list\n16\nif (len1 < len2) {\n17\nint32_t diff = len2 - len1;\n18\nfor (int32_t i = 0; i < diff; ++i) {\n19\nhead2 = head2->next;\n20\n} // for i\n21\n} // if\n22\nelse if (len2 < len1) {\n23\nint32_t diff = len1 - len2;\n24\nfor (int32_t j = 0; j < diff; ++j) {\n25\nhead1 = head1->next;\n26\n} // for j\n27\n} // else if\n28\n29\nwhile nullptr nullptr(head1 != && head2 != && head1 != head2) {\n30\nhead1 = head1->next;\n31\nhead2 = head2->next;\n32\nif (head1 == head2) {\n33\nreturn head1;\n34\n} // if\n35\n} // while\n36\n37\n// ternary handles the case where head1 and head2 initially point to the same node\n38\nreturn nullptr;head1 == head2 ? head1 :\n39\n} // get_list_intersection()\nHowever, it is possible to write a solution using two pointers that does not need to precompute the lengths at all! This solution requires a key\ninsight: we do not need to know the list lengths to align our two pointers if we simply allow each pointer to iterate over lists. In otherboth\nwords, we are going to increment both pointers in tandem, regardless of how far away they are. However, once a pointer reaches the end, we\nrestart it at the beginning of the opposite list (i.e., once the pointer of the shorter list reaches the end, reset it to the head of the longer one, and\nvice versa). By doing this, both pointers are guaranteed to meet at the same node if an intersection exists.\n➀Increment\n➁Increment\n➂Increment\n➃Increment\n➄Increment\n➅Increment (reset to head of opposite list)\n➆Increment (reset to head of opposite list)\n➇Increment\n➈Intersection", "word_count": 469, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6d2696b8-f718-54dd-839e-88b346714977", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 233, "real_page_number": null, "text": "8.5 Techniques for Solving Linked List Problems\n221\nWhy does this work? Recall from earlier that the two pointers are guaranteed to meet at the intersection point if they traverse the same number of\nnodes before this point of intersection. By forcing each pointer to restart back at the head of the opposite list once it reaches the end of its initial\ntraversal, we ensure that both pointers traverse the same number of nodes before the point of intersection, regardless of the lengths of the lists.\n𝑥\n⏞⏞⏞⏞⏞\n⏞⏞⏞⏞\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n𝑦\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n𝑧\n𝑥nodes 𝑧nodes 𝑦nodesPointer starting at shorter head traverses in the shorter list, to the end, and then in the longer list.\n𝑦nodes 𝑧nodes 𝑥nodesPointer starting at longer head traverses in the longer list, to the end, and then in the shorter list.\n𝑥+𝑦+𝑧nodes,After traversing both pointers are guaranteed to meet at the intersection node, if there is one!\nAn implementation of this solution is shown below. If both pointers make it past the second iteration without meeting at the exact same node,\nnullptrthen there is no intersection and is returned.\n1\nNode* get_list_intersection(Node* head1, Node* head2) {\n2\nNode *p1 = head1, *p2 = head2;\n3\nif nullptr nullptr)(p1 == || p2 == {\n4\nreturn nullptr;\n5\n} // if\n6\n7\nwhile nullptr nullptr(p1 != && p2 != && p1 != p2) {\n8\np1 = p1->next;\n9\np2 = p2->next;\n10\n11\n// return if both pointers meet at the same node\n12\nif (p1 == p2) {\n13\nreturn p1;\n14\n} // if\n15\n16\n// if p1 reaches the end, restart it at the head of head2\n17\nif nullptr)(p1 == {\n18\np1 = head2;\n19\n} // if\n20\n21\n// if p2 reaches the end, restart it at the head of head1\n22\nif nullptr)(p2 == {\n23\np2 = head1;\n24\n} // if\n25\n} // while()\n26\n27\nreturn p1;\n28\n} // get_list_intersection()\nThe following is a concise version of the same solution. The ternary operators on lines 4 and 5 reset the pointers once they reach the end, and\nwhile nullptr):the condition of the loop on line 3 handles the case when there is no intersection (since both would eventually point to\n1\nNode* get_list_intersection(Node* head1, Node* head2) {\n2\nNode *p1 = head1, *p2 = head2;\n3\nwhile (p1 != p2) {\n4\np1 = p1 ? p1->next : head2;\n5\np2 = p2 ? p2->next : head1;\n6\n} // while()\n7\nreturn p1;\n8\n} // get_list_intersection()\nThe time complexity of this solution is Θ(𝑛), where 𝑛is the total number of nodes in the combined lists, since this is the number of nodes that\neach pointer may have to traverse if there is an intersection. If there is no intersection, the pointers may need to visit more than 𝑛nodes, but\nnullptrthe total number of nodes visited cannot be greater than 2𝑛(since the algorithm terminates once both pointers hit a on the second\ntraversal), so the time complexity in this scenario would still be Θ(𝑛). The auxiliary space is since the amount of additional memoryΘ(1)\nallocated by the function is a constant that does not depend on the size of the lists that are passed in.", "word_count": 556, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "23cca7d4-1784-5ae4-bf4f-09180a5d350c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 234, "real_page_number": null, "text": "222\nChapter 8. Linked Lists\nExample 8.8 You are given two non-empty singly-linked lists that represent two non-negative integers. The digits in the linked list are\nstored in reverse order, such that the least-significant digit is stored at the head, and the most-significant digit is stored at the tail. Each node\nonly stores a single digit, and the two lists do not contain any leading zeros. Implement a function that adds the two numbers represented by\nthe two given lists and returns the sum as a linked list in the same format. A node is defined as follows:\n1\nstruct Node {\n2\nint8_t digit;\n3\nNode* next;\n4\nNode(int8_t next{nullptr}x) : digit{x}, {}\n5\n};\nFor example, the following two lists represent the numbers 281 and 370:\nhead1\n1\n8\n2\nnullptr\nnullptr\n3\n7\n0\nhead2\nIf given these two lists, your function should return the sum of these two numbers, 651, as a list with the same reverse-digit format:\nresult\n1\n5\n6\nnullptr\nThis problem may seem complicated at first, but the algorithm for solving this problem is the same as the one you learned in grade school: add\nthe digits from right to left, and carry any additional significant digits for values over 9 to the next column.\n82 1\n3 07\n+\n82 1\n3 07\n+\n1\n82 1\n3 07\n+\n1\n5\n1\n82 1\n3 07\n+\n1\n5\n1\n6\nBecause the digits in the list are stored in reverse order, we can follow this process by iterating over both of the lists from front to back and\nadding the digits together. Much like our initial algorithm, we will also keep a carry variable that keeps track of any value we need to carry. A\ndepiction of this process is shown below:\n1 + 0 = 1, so we append a node with value 1 to the back of the result list.\ncarry\n0\nhead1\n1\n8\n2\nnullptr\nnullptr\n3\n7\n0\nhead2\nresult\n1\nnullptr\n8 + 7 = 15, so we append a node with value 5 to the back of the result list and set the carry value to 1.\ncarry\n1\nhead1\n1\n8\n2\nnullptr\nnullptr\n3\n7\n0\nhead2\nresult\n1\n5\nnullptr\n1 + 2 + 3 = 6, so we append a node with value 6 to the back of the result list.\ncarry\n0\nhead1\n1\n8\n2\nnullptr\nnullptr\n3\n7\n0\nhead2\nresult\n1\n5\n6\nnullptr", "word_count": 429, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "21978f6b-99d4-5441-8697-b667ec2bc949", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 235, "real_page_number": null, "text": "8.6 The STL List Container\n223\nA non-trivial portion of this problem’s difficulty comes from identifying the edge cases that may be encountered. In our example, we used two\nthree-digit numbers that also summed up to a three-digit number. This made list construction a fairly straightforward process. However, this is\nnot guaranteed, and your algorithm will need to be able to handle additional edge cases. A few of these edge cases are highlighted below.\nOne list is longer than the other (make sure you do not iterate offthe end).\nhead1\n1\n8\n2\n1\nnullptr\nnullptr\n3\n7\n0\nhead2\nThere is an additional carry value at the end (which should be appended as an additional node at the end of the list).\nhead1\n1\nnullptr\nnullptr\n9\n9\n9\nhead2\nAn implementation of this solution is provided below.\n1\nNode* add_two_lists(Node* head1, Node* head2) {\n2\n// same as previous example, the dummy node makes implementation easier\n3\n// pre_head.next is the head of the actual list we want to return\n4\nNode pre_head{0}, *result = &pre_head;\n5\nNode *iter1 = head1, *iter2 = head2;\n6\nint8_t carry = 0;\n7\n// continue iterating as long as one list still has nodes to visit\n8\nwhile nullptr nullptr)(iter1 != || iter2 != {\n9\nint8_t sum = (iter1 ? iter1->val : 0) + (iter2 ? iter2->val : 0) + carry;\n10\n// the value of the next digit is the remainder of sum when divided by 10\n11\nnewresult->next = Node{sum % 10};\n12\nresult = result->next;\n13\n// value of carry is the number of times sum evenly divides into 10 (here, either 0 or 1)\n14\ncarry = sum / 10;\n15\n// iterate as long as there are more nodes to visit\n16\nif nullptr)(iter1 != {\n17\niter1 = iter1->next;\n18\n} // if\n19\nif nullptr)(iter2 != {\n20\niter2 = iter2->next;\n21\n} // if\n22\n} // while\n23\n// if carry left over, append it as the final node\n24\nif (carry != 0) {\n25\nnewresult->next = Node{carry};\n26\n} // if\n27\nreturn pre_head.next;\n28\n} // add_two_lists()\nAssuming that the two lists have lengths of 𝑚and 𝑛, the time complexity of this algorithm is Θ(max(𝑚,𝑛)), since the algorithm iterates over both\nlists (and the total number of iterations is dependent on whichever list is longer). The auxiliary space used by this algorithm is also Θ(max(𝑚,𝑛)),\nsince the length of the return list (i.e., the number of digits in the sum) is at least max(𝑚,𝑛) and at most max(𝑚,𝑛)+1.\nRemark: What if we are given the same problem, but with the list direction reversed? That is, what if the most significant digits were stored\nnear the head of the list rather than near the tail? If such a list were singly-linked, we would not be able to iterate over the list as easily as\nwhen the digits were stored in reverse order, like in the previous example. However, there are several approaches that can be used to address\nthis limitation. One approach is to reverse the linked list (using the algorithm covered in section 8.4) and then repeat the above process once\nstd::stack<>the digits are in reverse order. Another potential approach is to use a last-in, first-out container like a to help you access\nthe digits in the correct order — we will cover stacks in greater detail in the next chapter.\n8.6\nThe STL List Container (✽)\nstd::vector<>,Much like the the C++ standard template library provides its own implementations of linked lists. A brief overview of\nthese containers is provided below. You likely will not ever use any of these containers for EECS 281, but these are included for your reference\nsince a few upper-level classes do use them.\n¸ 8.6.1\nstd::list\nstd::list<>, #include <list>.First, the STL implementation of a doubly-linked list is defined as a and it can be used if you A few\nstd::list<> operations are shown in the table on the next page. Note that this table is not comprehensive — for full coverage on list\n<list>operations, you are encouraged to read the STL documentation.", "word_count": 700, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "88958bbc-d577-525c-845e-374fcfb47898", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 236, "real_page_number": null, "text": "224\nChapter 8. Linked Lists\nstd::list<>:The following methods can be used to initialize a\ntemplate <typename T>\nstd::list<T>();\nT.Default constructor for list that holds elements of type Creates an empty list without any elements; size is initially 0.\ntemplate <typename T>\nstd::list<T>(const std::list<T>& other);\notherCopy constructor, copies the contents of into the constructed list.\ntemplate <typename T>\nstd::list<T>(size_t sz);\nsz sz.Creates a list of elements, where each element is value initialized; size equal to\ntemplate <typename T>\nstd::list<T>(size_t sz, T& val);\nsz val; sz.Creates a list of elements, where each element is initialized to a value of size equal to\ntemplate <typename typenameT, InputIterator>\nstd::list<T>(InputIterator begin_iter, InputIterator end_iter);\n[begin_iter,end_iter) begin_iterCreatesalistwithallelementsintheiteratorrange —inclusivebeginbutexclusiveend. Both\nend_iterand are input iterators.\ntemplate <typename T>\nstd::list<T>(std::initializer_list<T> init);\nInitializes the list with the contents of the initializer list.\nstd::list<>.The following methods can be used to insert and remove elements from the front or back of a\ntemplate <typename T>\nvoid std::list<T>::push_back(const T& val);\nvalAppends an element to the back of the list.\ntemplate <typename typename...T, Args>\nT& std::list<T>::emplace_back(Args&&... args);\nConstructs a new element in place at the back of the list using the constructor arguments that are passed in. Returns reference since C++17.\ntemplate <typename T>\nvoid std::list<T>::pop_back();\nRemoves last element in list; references/iterators to the erased element are invalidated. Causes undefined behavior if used on an empty list.\ntemplate <typename T>\nvoid std::list<T>::push_front(const T& val);\nvalAppends an element to the front of the list.\ntemplate <typename typename...T, Args>\nT& std::list<T>::emplace_front(Args&&... args);\nConstructs a new element in place at the front of the list using the constructor arguments that are passed in. Returns reference since C++17.\ntemplate <typename T>\nvoid std::list<T>::pop_front();\nRemoves first element in list; references/iterators to the erased element are invalidated. Causes undefined behavior if used on an empty list.\ntemplate <typename T>\nvoid std::list<T>::resize(size_t constsz, T& val);\nsz val.Resizes the container to hold elements, each initialized to The second argument is optional, and if omitted, new elements are\nsz. sz, szvalue-initialized and added to the list until its size is If current size is greater than the container is reduced to the first elements.", "word_count": 381, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bb2dda26-ae05-5c91-bc90-7e1acceee54c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 237, "real_page_number": null, "text": "8.6 The STL List Container\n225\n.insert() .erase()The and member functions of a list behave similarly to those of a vector:\ntemplate <typename T>\nconstiterator std::list<T>::insert(const_iterator pos, T& val);\nval posInserts directly before the element pointed to by the iterator and returns an iterator the newly inserted element.\ntemplate <typename T>\nsize_t constiterator std::list<T>::insert(const_iterator pos, n, T& val);\nn val posInserts copies of directly before the element pointed to by the iterator and returns an iterator the first element added.\ntemplate <typename typenameT, InputIterator>\niterator std::list<T>::insert(const_iterator pos, InputIterator first, InputIterator last);\n[first, last) posInserts all elements in the iterator range directly before and returns an iterator the first new element added.\ntemplate <typename T>\niterator std::list<T>::insert(const_iterator pos, std::initializer_list<T> init);\nposInserts the elements in the initializer list directly before the iterator and returns an iterator to the first new element added.\ntemplate <typename typename...T, Args>\niterator std::list<T>::emplace(const_iterator pos, Args&&... args);\npos.Insertsanewelementdirectlybeforetheelementatposition Thenewelementisconstructedinplaceusingargumentsforitsconstructor\n(args). An iterator pointing to the emplaced object is returned.\ntemplate <typename T>\niterator std::list<T>::erase(iterator pos);\nposErases the element pointed to by the iterator and returns an iterator to the element following the one that was erased.\ntemplate <typename T>\niterator std::list<T>::erase(iterator first, iterator last);\n[first, last)Erases all elements in the range and returns an iterator to the element following the last element erased.\nstd::list<> .splice()The container also provides a method that can be used to transfer nodes around, either to another list or to a\ndifferent position in the same list. Because of how a list stores its data, this method simply repoints the internal pointers of the lists, and no\niterators or references are invalidated. Thus, splicing a single item in a list takes constant time.\ntemplate <typename T>\nvoid std::list<T>::splice(const_iterator pos, list& other);\nother .splice() *this).Transfers all elements from into the list on which is called (i.e., The elements are inserted before the element\npos. other other *this .splice()pointed to by The list becomes empty after this operation. If is the same as (the list that is\ncalled on), this method produces undefined behavior.\ntemplate <typename T>\nvoid std::list<T>::splice(const_iterator pos, list& other, const_iterator it);\nit other .splice() *this).Transfers the element pointed to by from into the list on which is called (i.e., The element is inserted\npos.before the element pointed to by\ntemplate <typename T>\nvoid std::list<T>::splice(const_iterator pos, list& other, const_iterator first, const_iterator last);\n[first,last) other .splice() *this).Transferstheelementsintherange from intothelistonwhich iscalled(i.e., Theelements\npos. pos [first, last).are inserted before the element pointed to by Behavior is undefined if is an iterator in the range\nstd::list<> .sort()Some additional member functions are summarized below. Note that lists have their own method (unlike most other\nstd::sort()containers); this is because the algorithm library’s generic function cannot be used on a list (for reasons we will discuss later).\nFunction\nBehavior\n.front()\nReturns a reference to the first element in the list\n.back()\nReturns a reference to the last element in the list\n.empty()\nReturns whether the list is empty\n.size()\nReturns the number of elements in the list\n.clear()\nClears the contents of the list\n.begin()\nReturns a bidirectional iterator to the first element in the list\n.end()\nReturns a bidirectional iterator to the position one past the last element in the list\n.cbegin()\nReturns a bidirectional iterator to the first element in the listconstant\n.cend()\nReturns a bidirectional iterator to the position one past the last element in the listconstant\n.rbegin()\nReturns a iterator to the last element in the listreverse\n.rend()\nReturns a iterator to the position one before the first element in the listreverse\n.crbegin()\nReturns a constant reverse iterator to the last element in the list\n.crend()\nReturns a constant reverse iterator to the position one before the first element in the list\n.sort()\nSorts the contents of the list in ascending order (or using an optional comparator that is passed in)", "word_count": 692, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "49db2f70-c7a2-5c91-bd39-018976f97fcc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 238, "real_page_number": null, "text": "226\nChapter 8. Linked Lists\nExamples of list operations are shown in the code below:\n1\n// initializes the list lst1 with zero size\n2\nstd::list<int32_t> lst1;\n3\n// initializes lst2 to have contents {1, 2, 3}\n4\nstd::list<int32_t> lst2 = {1, 2, 3};\n5\n// push 0 to the front of lst2\n6\nlst2.push_front(0);\n7\n// push 4 to the back of lst2\n8\nlst2.push_back(4);\n9\n// initializes lst3 with the contents of lst2, or {0, 1, 2, 3, 4}\n10\nstd::list<int32_t> lst3{lst2};\n11\n// pops 0 off the front of lst3\n12\nlst3.pop_front();\n13\n// pops 4 off the back of lst3\n14\nlst3.pop_back();\n15\n// resizes lst3 to size 5\n16\nlst3.resize(5);\n// contents of lst3 are now {1, 2, 3, 0, 0}\n17\n// sorts lst3\n18\nlst3.sort();\n// contents of lst3 are now {0, 0, 1, 2, 3}\n¸ 8.6.2\n(✽)std::forward_list\nstd::forward_list<>.The STL also provides an implementation for a singly-linked list, defined as a A forward list can be used if\n#include <forward_list>. std::list<>you This container is typically preferable to a in cases where bidirectional iteration is not\nneeded, as forward lists do not keep track of a previous pointer (thereby saving memory). However, forward lists are also more limited in terms\n.push_back() .pop_back()of functionality; they do not support or operations, nor do they support reverse iterators. They also do not\n.size()support a member function for efficiency purposes.\nForward lists have several practical uses and are optimized for containers that are typically empty or have very small sizes. A few member\nstd::forward_list<>functions of the container are summarized below. To explore this container’s full functionality, you are encouraged\n<forward_list>to read the STL documentation for the container class.\ntemplate <typename T>\nstd::forward_list<T>();\nT.Default constructor for forward list that holds elements of type Creates an empty list without any elements.\ntemplate <typename T>\nstd::forward_list<T>(const std::forward_list<T>& other);\notherCopy constructor, copies the contents of into the constructed forward list.\ntemplate <typename T>\nstd::forward_list<T>(size_t sz);\nszCreates a list of elements, where each element is value initialized.\ntemplate <typename T>\nstd::forward_list<T>(size_t sz, T& val);\nsz val.Creates a list of elements, where each element is initialized to a value of\ntemplate <typename typenameT, InputIterator>\nstd::forward_list<T>(InputIterator begin_iter, InputIterator end_iter);\n[begin_iter,end_iter) begin_iterCreatesalistwithallelementsintheiteratorrange —inclusivebeginbutexclusiveend. Both\nend_iterand are input iterators.\ntemplate <typename T>\nstd::forward_list<T>(std::initializer_list<T> init);\nInitializes the forward list with the contents of the initializer list.\ntemplate <typename T>\nvoid std::forward_list<T>::clear();\nClears out the contents of the forward list.\ntemplate <typename T>\nbool std::forward_list<T>::empty();\nReturns whether the forward list is empty.\ntemplate <typename T>\nconstiterator std::forward_list<T>::insert_after(iterator pos, T& val);\nval posInserts after the element pointed to by and returns an iterator to the inserted element.\ntemplate <typename T>\nconstiterator std::forward_list<T>::insert_after(const_iterator pos, T& val);\nval posInserts after the element pointed to by and returns an iterator to the inserted element.\ntemplate <typename T>\nsize_t constiterator std::forward_list<T>::insert_after(const_iterator pos, n, T& val);\nn val posInserts copies of after the element pointed to by and returns an iterator to the last element inserted.\ntemplate <typename typenameT, InputIterator>\niterator std::forward_list<T>::insert_after(const_iterator pos, InputIterator first, InputIterator last);\n[first, last) posInserts all elements in the iterator range after the element at and returns an iterator to the last element inserted.", "word_count": 562, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b8556ba7-e50e-59f4-bcea-73d730832d60", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 239, "real_page_number": null, "text": "8.7 Summary of List Complexities\n227\ntemplate <typename T>\niterator std::forward_list<T>::insert_after(const_iterator pos, std::initializer_list<T> init);\npos posInserts all elements in the initializer list after the element at and returns an iterator to the last element inserted, or if the given\ninitializer list is empty.\ntemplate <typename typename...T, Args>\niterator std::forward_list<T>::emplace_after(const_iterator pos, Args&&... args);\nposConstructs a new element in place after the element at using the constructor arguments that are passed in.\ntemplate <typename T>\niterator std::forward_list<T>::erase_after(const_iterator pos);\nposErases the element after the one pointed to by the iterator and returns an iterator to the element following the one that was erased.\ntemplate <typename T>\niterator std::forward_list<T>::erase_after(const_iterator first, const_iterator last);\nfirst, last,Erases all elements after the one pointed to by the iterator up until the element pointed to by and returns an iterator to the\nelement following the last element erased.\ntemplate <typename T>\nvoid std::forward_list<T>::push_front(const T& val);\nvalPrepends to the beginning of the forward list.\ntemplate <typename typename...T, Args>\nT& std::forward_list<T>::emplace_front(Args&&... args);\nConstructs a new element in place at the front of the forward list, using the given constructor arguments. Returns reference since C++17.\ntemplate <typename T>\nvoid std::forward_list<T>::pop_front();\nRemoves the first element of the container (undefined behavior if list is empty).\n8.7\nSummary of List Complexities\nhead tailIn this section, we will summarize of time complexities of several list operations. Note that a pointer is required, but a pointer is\noptional depending on implementation. The complexities of both implementation variations are provided.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nFinding an element\nSingly-linked\nΘ(𝑛)\nΘ(𝑛)\nDoubly-linked\nΘ(𝑛)\nΘ(𝑛)\nRegardless of whether the list is singly- or doubly-linked, the worst-case of finding an element in a list of size 𝑛occurs when the element is the\nlast one in the list, or if the element cannot be found at all. This requires the search to look at all 𝑛elements in the list. In the average case, the\nelement is somewhere in the middle of the list, which would require the algorithm to look at approximately elements. This is still𝑛∕2 Θ(𝑛)\nsince we can drop the coefficient term of 1∕2.\nstd::find() <algorithm>In the STL, this can be done using in the library, which will be covered in chapter 11.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nAccessing first element\nSingly-linked\nΘ(1)\nΘ(1)\nDoubly-linked\nΘ(1)\nΘ(1)\nheadSince the pointer points to the first element of the list, the first element of a list can be accessed in constant time regardless of whether the\n.front()list is singly- or doubly-linked. In the STL, this can be done using the member of both lists and forward lists.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nAccessing last element\nSingly-linked\nif no tail pointerΘ(𝑛)\nif no tail pointerΘ(𝑛)\nif tail pointerΘ(1)\nif tail pointerΘ(1)\nDoubly-linked\nif no tail pointerΘ(𝑛)\nif no tail pointerΘ(𝑛)\nif tail pointerΘ(1)\nif tail pointerΘ(1)\ntailThe efficiency of accessing the last element depends on whether a pointer exists. If it exists, you can just refer to it to retrieve the last\nelement. Otherwise, you will have to iterate through all 𝑛elements to get to the last element. For the STL list (doubly-linked), this can be done\n.back() .back(),using the member method. On the other hand, STL forward lists (singly-linked) do not support so you would need to\n.end()iterate until the iterator if you wanted to find the last element.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nAccessing arbitrary element\nSingly-linked\nΘ(𝑛)\nΘ(𝑛)\nDoubly-linked\nΘ(𝑛)\nΘ(𝑛)\nUnlike a vector, elements in a list are not contiguous in memory. As a result, you cannot use arithmetic to access arbitrary elements in constant\ntime. You would have to iterate through all the nodes of the list in the worst-case, which is for a list size of 𝑛. In the average-case, theΘ(𝑛)\nelement you are trying to access is in the middle, which would still require you to iterate through elements — this is also a operation.𝑛∕2 Θ(𝑛)", "word_count": 670, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "232a9ab9-3ddb-54fe-9bc3-e0b91bb0770b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 240, "real_page_number": null, "text": "228\nChapter 8. Linked Lists\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nInserting element at front\nSingly-linked\nΘ(1)\nΘ(1)\nDoubly-linked\nΘ(1)\nΘ(1)\nTo insert an element at the front of the list, all you have to do is change some pointers around. Since you can access the front of the list directly\nheadusing the pointer, this is a constant time operation regardless of the type of list you are trying to insert into. For STL lists and forward\n.push_front() .emplace_front()lists, this can be done using the and members.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nInserting element at back\nSingly-linked\nif no tail pointerΘ(𝑛)\nif no tail pointerΘ(𝑛)\nif tail pointerΘ(1)\nif tail pointerΘ(1)\nDoubly-linked\nif no tail pointerΘ(𝑛)\nif no tail pointerΘ(𝑛)\nif tail pointerΘ(1)\nif tail pointerΘ(1)\ntailIf you have access to a pointer, this operation can be done in constant time, since you just need to modify a few pointers to insert\nthe node. However, if there is no tail pointer, you will have to traverse the list before you can insert the element. Note that this complexity\nassumes you are not given the node to insert after. If you are passed in the actual node to insert after, then this insertion becomes a constant\ntail .push_back() .emplace_back().time operation (much like with the pointer). For the STL list, this can be done using or For\n.push_back() .emplace_back()singly-linked STL forward lists, and are not supported, and insertion at the back can be done by using\n.insert_after() .emplace_after()or on the last element in the list.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nInserting element at arbitrary index\nSingly-linked\nΘ(𝑛)\nΘ(𝑛)\nDoubly-linked\nΘ(𝑛)\nΘ(𝑛)\nGiven just an index of insertion, you would have to iterate through the list first to get the position you want to insert at. This is why the time\ncomplexities here are Θ(𝑛). Again, if you were given a pointer to the node to insert after rather than an index, insertion would be a constant time\n.insert() .emplace() .insert_after()operation. In the STL, insertion can be done using and for a doubly-linked list, or and\n.emplace_after() for a singly-linked forward list (these STL methods take constant time, but you must pass in an iterator to the insertion\nposition, which could take linear time to obtain).\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nErasing element at front\nSingly-linked\nΘ(1)\nΘ(1)\nDoubly-linked\nΘ(1)\nΘ(1)\nheadSince you have access to the first element through the pointer, you can erase the first element my shifting a few pointers around. This\n.pop_front()takes constant time, regardless of the type of list. In the STL, this can be done using the method of both lists and forward lists.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nErasing element at back\nSingly-linked\nΘ(𝑛)\nΘ(𝑛)\nDoubly-linked\nif no tail pointerΘ(𝑛)\nif no tail pointerΘ(𝑛)\nif tail pointerΘ(1)\nif tail pointerΘ(1)\nRegardless of whether you are given a pointer to the last node of the list, the complexity of deleting the last element in a singly-linked list is\nnextΘ(𝑛). This is because you will need to change the value of the node directly before the last node, which you do not have access to without\ntailiterating through the list. For a doubly-linked list, erasing at the back is if you have a pointer, because you can access the nodeΘ(1)\nprevbefore the last node using the last node’s pointer (which isn’t possible in a singly-linked list). For the STL’s doubly-linked list, you can\n.pop_back(). .pop_back()erase the last element using For the STL’s singly-linked forward lists, the operation is not supported, so you\n.erase_after()will have to find the element directly before the one at the back and pass its iterator to the member function.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nErasing element at arbitrary index\nSingly-linked\nΘ(𝑛)\nΘ(𝑛)\nDoubly-linked\nΘ(𝑛)\nΘ(𝑛)\nIf you are given the index of the node to delete, you must first iterate to the correct position of the list (since you can’t use constant-time pointer\narithmetic to get to the node you want to delete). This is why erasing a node given only its index is a linear time operation.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nErasing element given its pointer\nSingly-linked\nΘ(𝑛)\nΘ(𝑛)\nDoubly-linked\nΘ(1)\nΘ(1)\nnext prevTo delete a node, you must update the pointer of the node directly before the one you want to delete, and (if doubly-linked) the\npointer of the node directly after the one you want to delete. With a doubly-linked list, you can access these two nodes in constant time, so\nprevthe entire deletion process is also constant time (you just need to modify some pointers). However, singly-linked lists do not support a\npointer, so you cannot access the node directly before the one you want to delete in constant time. As a result, you must iterate through the list to\naccess this previous node, which is why erasing from a singly-linked list is still even if you are given a pointer to the node you want toΘ(𝑛)\n.erase() .erase_after()delete. In the STL, erasure can be done using for a doubly-linked list, or for a singly-linked forward list.\nOperation\nType\nAverage-Case Time\nWorst-Case Time\nChecking if list is empty\nSingly-linked\nΘ(1)\nΘ(1)\nDoubly-linked\nΘ(1)\nΘ(1)\nhead nullptr.This can be done in constant time by just checking if is For the STL’s list and forward list containers, you can use the\n.empty() method to determine if the list is empty.", "word_count": 927, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9ed747e6-7d9f-5add-a9e8-12497aa77076", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 241, "real_page_number": null, "text": "8.7 Summary of List Complexities\n229\nChapter 8 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n47th1. The largest Fibonacci number that can be represented as a 32-bit integer is the Fibonacci number. Suppose you store the first 47\nint32_tFibonacci numbers as values in both a doubly-linked list and an array. What is the difference in the number of bytes that these\ntwo data structures take up in memory? Assume that we are using a 64-bit system, where pointers take up 8 bytes.\n1\n// Linked list of 47 Fibonacci Numbers\n2\nstruct Node {\n3\nNode* next;\n4\nNode* prev;\n5\nint32_t value;\n6\n};\n7\n8\n// Array of 47 Fibonacci Numbers\n9\nint32_t arr[47];\nA) 376 bytes (47 8)×\nB) 752 bytes (47 16)×\nC) 1,128 bytes (47 24)×\nD) 1,504 bytes (47 32)×\nE) None of the above\n2. Which of the following statements is FALSE?\nA) Traversing a doubly-linked list from beginning to end takes timeΘ(𝑛)\nB) Searching for a particular element in a singly-linked list can take timeΘ(𝑛)\nC) Inserting into a doubly-linked list takes time if you are given an iterator to the insertion pointΘ(1)\nD) The last element in a singly-linked list with a tail pointer can be removed in timeΘ(1)\nE) None of the above\n3. What is the worst-case time complexity of appending an element to the back of an array vs. a singly-linked list with a head pointer but NO\ntail pointer? Assume the there is enough capacity in the array to store another element (so no reallocation is involved).\nA) Both the array and linked list are Θ(1)\nB) The array is but the linked list is Θ(𝑛)Θ(1)\nC) The array is but the linked list isΘ(𝑛) Θ(1)\nD) Both the array and linked list are Θ(𝑛)\nE) None of the above\n4. What is the worst-case time complexity of inserting an element after another element with a given value (i.e., the value of the element to\ninsert after is known, but not the position) array vs. a singly-linked list with a head pointer but NO tail pointer? Assume the there is enough\ncapacity in the array to store another element (so no reallocation is involved).\nA) Both the array and linked list are Θ(1)\nB) The array is but the linked list is Θ(𝑛)Θ(1)\nC) The array is but the linked list isΘ(𝑛) Θ(1)\nD) Both the array and linked list are Θ(𝑛)\nE) None of the above\n5. What is the worst-case time complexity of accessing an element at an arbitrary index in an array vs. a singly-linked list with a head pointer\nbut NO tail pointer?\nA) Both the array and linked list are Θ(1)\nB) The array is but the linked list is Θ(𝑛)Θ(1)\nC) The array is but the linked list isΘ(𝑛) Θ(1)\nD) Both the array and linked list are Θ(𝑛)\nE) None of the above\n6. For which of the following containers is it possible to insert a new element another existing element in constant time, provided thatbefore\nyou are given a pointer to the existing object that you want to insert before?\nI. Vector\nII. Singly-linked list\nIII. Doubly-linked list\nA) I only\nB) II only\nC) III only\nD) II and III only\nE) I, II, and III", "word_count": 606, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "56ee1a35-5818-5b95-a780-1b6051acfc31", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 242, "real_page_number": null, "text": "230\nChapter 8. Linked Lists\n7. Which of the following operations has a better time complexity when performed on a doubly-linked list instead of a singly-linked list?\nAssume that the lists are NOT implemented with a tail pointer.\nI. Inserting an element at the back of the list\nII. Inserting a node after another node, given the other node’s pointer\nIII. Erasing an element at the back of the list\nIV. Erasing an element when given its pointer\nA) IV only\nB) I and III only\nC) II and IV only\nD) III and IV only\nE) I, II, III, and IV\n8. You are trying to implement a queue that keeps track of students who go to EECS 281 office hours. When a student arrives, they put their\ninformation into the program and get sent to the back of the line. When an instructor is available, they query the program for the student who\nhas been waiting the longest. The student is then removed from the queue after they receive help. There is no limit on the number of students\nthat end up waiting for office hours. If an efficient time complexity is the sole concern, and these are the only behaviors that need to be\nStudentsupported, which of the following data structures would be best for storing the objects for this program?\nA) A singly-linked list with both head and tail pointers\nB) A doubly-linked list with only a head pointer\nC) A fixed size array\nD) A dynamic array (i.e., vector)\nE) All of the above data structures are equally efficient\n9. You currently have a singly-linked list that only has a head pointer. For which of the following operations would adding in a tail pointer\nthe worst-case time complexity of that operation? Assume that the most efficient algorithm is used.improve\nA) Reversing the linked list\nB) Removing the last element in the list\nC) Finding an element in the list\nD) Removing the first element in the list\nE) None of the above\n10th 10th10. What are the time complexities of finding the element in a singly-linked list and finding the element in a singly-linked list?to last\nLet 𝑛be the number of nodes in the linked list. You may assume that the list contains more than 10 elements.\nA) andΘ(1) Θ(1)\nB) and Θ(𝑛)Θ(1)\nC) andΘ(𝑛) Θ(1)\nD) andΘ(𝑛) Θ(𝑛)\nE) None of the above\n11. Consider the following circular singly-linked list of 𝑛students, which represents an office hours queue.\nAlice\nBob\nCathy\nDrew\n...\nother students\nYvonne\nZach\nptr\nptr ptrThe pointer is the only way to access elements in the queue. As shown above, is currently pointing to the student named Zach in\nthe line. You are told that both adding and removing students from the queue always take time. Once a student is in the queue, theyΘ(1)\ncannot leave until they receive help. Under this setup, which of the following students could potentially be the next in line to be helped?\nI. Alice\nII. Yvonne\nIII. Zach\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) II and III only\n12. Suppose you have a doubly-linked list that supports a head and tail pointer. How many pointers are modified when an element is inserted\ninto the middle of the list? Assume that there are elements on both sides of the insertion position.\nA) 0\nB) 1\nC) 2\nD) 4\nE) 6", "word_count": 585, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7641a91a-8eeb-50a9-8f3a-cbf4b9407dd8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 243, "real_page_number": null, "text": "8.7 Summary of List Complexities\n231\n13. Which of the following CANNOT be done in time? Assume that the lists mentioned in the answers support both head and tail pointers.Θ(1)\nA) Inserting an element at the front of a singly-linked list\nB) Inserting an element at the back of a singly-linked list\nC) Deleting an element at the front of a singly-linked list\nD) Deleting an element at the back of a singly-linked list\nE) None of the above\n14. Consider the following function, which detects whether a cycle exists in a singly-linked list and removes the cycle if there is one.\n1\nvoid remove_list_cycle(Node* head) {\n2\nif nullptr nullptr)(head == || head->next == {\n3\nreturn;\n4\n} // if\n5\n6\nNode* slow = head->next;\n7\nNode* fast = head->next->next;\n8\nwhile nullptr nullptr)(slow != && fast != {\n9\nif (slow == fast) {\n10\nbreak;\n11\n} // if\n12\n13\nslow = slow->next;\n14\nfast = fast->next->next;\n15\n} // while\n16\n17\nif (slow == fast) {\n18\nslow = head;\n19\nwhile (slow->next != fast->next) {\n20\nslow = slow->next;\n21\nfast = fast->next;\n22\n} // while\n23\n24\nnullptr;fast->next =\n25\n} // if\n26\n} // remove_list_cycle()\nThe above implementation may have a bug. Which of the following linked lists, when passed into this function, would expose this bug?\nA) A singly-linked list that has a cycle with an even number of nodes\nB) A singly-linked list that has a cycle with an odd number of nodes\nC) A singly-linked list that has an even number of nodes, but no cycle\nD) A singly-linked list that has an odd number of nodes, but no cycle\nE) None of the above, there is actually no bug\n15. You have the head pointers of two singly-linked lists that converge to become a single linked list. An illustration is shown below:\nhead1\nhead2\nnullptr\nThe length of the first list is 𝑚, and the length of the second list is 𝑛. There is no relationship between 𝑚and 𝑛; that is, you cannot assume\nthat 𝑚is larger than 𝑛or vice versa. What is the worst-case time complexity of finding the intersecting node between the two lists (i.e., the\nfirst node that is shared by both lists) if you use the most efficient algorithm?\nA) Θ(𝑚+𝑛)\nΘ(𝑛2)B)\nC) Θ(𝑚𝑛)\nD) Θ(min(𝑚,𝑛)))\nΘ(min(𝑚𝑛2,𝑚2𝑛)))E)\n16. Which of the following statements is FALSE?\nA) The nodes of a linked list are not necessarily stored sequentially in memory on the heap\nB) Singly- and doubly-linked lists that store 𝑛elements both have memory overheadΘ(𝑛)\nC) Given a head pointer, identifying whether or not a linked list is circular takes time for both singly- and doubly-linked listsΘ(𝑛)\nD) An element can be inserted at any position in a doubly-linked list in constant time, as long as the position to insert after is provided\nusing a node pointer\nE) None of the above", "word_count": 500, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0d2a3c0d-bc6e-50d5-843c-c7c82f3931b6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 244, "real_page_number": null, "text": "232\nChapter 8. Linked Lists\n17. You are given two singly-linked lists of size 𝑛containing integers. Both lists are sorted. What is the worst-case time complexity of merging\nthe two lists into a single, sorted linked list if you use the most efficient algorithm? Assume that both lists support a head and tail pointer.\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n18. For which of the following situations would a linked list be preferable to a vector?\nA) When lower memory overhead is needed\nB) When fast sequential traversal is needed\nC) When pointers and iterators to elements in the container cannot be invalidated\nD) When random access is neededΘ(1)\nE) None of the above\n19. Suppose you want to a find an element given its value, either in an array or in a singly-linked list that supports a head pointer. Which of the\nfollowing is TRUE about the worst-case time complexity of accomplishing this on these two container types?\nA) Finding this element takes for the array and time for the linked listΘ(1) Θ(1)\nB) Finding this element takes for the array and time for the linked listΘ(𝑛)Θ(1)\nC) Finding this element takes for the array and time for the linked listΘ(𝑛) Θ(1)\nD) Finding this element takes for the array and time for the linked listΘ(𝑛) Θ(𝑛)\nE) None of the above\n20. Given a linked list with 𝑛nodes, which of the following statements is TRUE?\nA) The first element in a singly-linked list can be removed in timeΘ(1)\nB) The last element in a singly-linked list can be removed in timeΘ(1)\nC) Finding an element in a doubly-linked list takes worst-case time if the list supports both head and tail pointersΘ(log(𝑛))\nD) The time complexity of reversing a doubly-linked list with both head and tail pointers is better than the time complexity of reversing\na doubly-linked with only a head pointer\nE) More than one of the above\nLinked lists support a special operation known as splicing, which can be used to transfer elements from one list into another. For example,21.\nlist1 list2:consider the following two lists, and\n183\n203\n370\nlist1\nnullptr\n280\n281\nlist2\nnullptr\nlist2 list1 list2 list1We can splice the entirety of into by transferring all the elements in into at any given position, as shown.\nlist2After this operation, would become empty.\n183\n203\n280\n281\n370\nlist1\nnullptr\nSuppose you are given two lists that support tail pointers, one of length 𝑚and one of length 𝑛, and an iterator pointing to thedoubly-linked\nelement that the spliced elements should be inserted before (in the list of length 𝑛). What is the time complexity of splicing the entirety of\nthe list of length 𝑚into the list of length 𝑛at the position before the given iterator, if you use the most efficient implementation strategy?\nA) Θ(1)\nB) Θ(𝑚)\nC) Θ(𝑛)\nD) Θ(𝑚+𝑛)\nE) Θ(𝑚𝑛)", "word_count": 496, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9edc951a-8f95-5d7d-9201-6157f65a7316", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 245, "real_page_number": null, "text": "8.7 Summary of List Complexities\n233\n≤right.left right left22. You are given the head of a singly-linked list and two valid indices and (1-indexed), where Reverse the\nleft right,nodes of the list from position to position inclusive, and return the reversed linked list. For example, given the following list\nleft = 2 right = 4,and and you would reverse the list as follows:\n183\n203\n280\n281\n370\nhead\nnullptr\n183\n281\n280\n203\n370\nhead\nnullptr\nThe function header is provided below:\nstruct Node {\nint32_t val;\nNode* next;\nNode(int32_t next{nullptr}val_in) : val{val_in}, {}\n};\nuint32_t uint32_tNode* reverse_list(Node* head, left, right);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of the list.Θ(𝑛) Θ(1)\nYou are given the head of a singly-linked list. Implement a function that deletes all duplicates in the sorted list so that each element23. sorted\nonly appears once, and then returns the sorted linked list without duplicates. For example, given the following list:\n183\n280\n280\n281\n281\nhead\nnullptr\nyou would return the following list without any duplicates:\n183\n280\n281\nhead\nnullptr\nThe function header is provided below:\nstruct Node {\nint32_t val;\nNode* next;\nNode(int32_t next{nullptr}val_in) : val{val_in}, {}\n};\nNode* remove_duplicates(Node* head);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of the list.Θ(𝑛) Θ(1)\nnullptr24. You are given the head of a singly-linked list. Implement a function that returns the node where a cycle begins, or if there is no\n280.cycle. For example, given the following list, you would return the node that stores the value\n183\n203\n280\n281\n370\nhead\nThe function header is provided below:\nstruct Node {\nint32_t val;\nNode* next;\nNode(int32_t next{nullptr}val_in) : val{val_in}, {}\n};\nNode* first_node_in_cycle(Node* head);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of the list.Θ(𝑛) Θ(1)", "word_count": 329, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "281f4eb7-3f8c-5b15-8969-fe4c5e6364b5", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 246, "real_page_number": null, "text": "234\nChapter 8. Linked Lists\n𝑘th25. You are given the head of a singly-linked list and an integer 𝑘. Implement a function that swaps the value of the node with the value of\n𝑘ththe node end, and then returns the head of this modified list. For this problem, the list is 1-indexed. For example, if you arefrom the\ngiven the following list with 2:𝑘=\n183\n203\n280\n281\n370\nhead\nnullptr\nyou would swap the 2nd value in the list (203) with the 2nd to last value in the list (281), as shown:\n183\n281\n280\n203\n370\nhead\nnullptr\nThe function header is provided below:\nstruct Node {\nint32_t val;\nNode* next;\nNode(int32_t next{nullptr}val_in) : val{val_in}, {}\n};\nuint32_tNode* swap_nodes(Node* head, k);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of the list.Θ(𝑛) Θ(1)\n26. You are given the head of a singly-linked list, which contains a series of integers that separated by zeros. The first and last value in this list\nare guaranteed to be zero. Implement a function that, for every two consecutive zeros in the list, merges all the nodes in between the zeros\ninto a single node whose value is the sum of the merged nodes. This new list is then returned. The modified list should not include any\nzeros; you may assume that the original list will not have two consecutive zeros. For example, given the following list:\n0\n5\n3\n9\n0\n4\n6\n0\nhead\nnullptr\nyou would return the following list (where the first node has a value of 5 + 3 + 9 = 17, and the second node has a value of 4 + 6 = 10):\n17\n10\nhead\nnullptr\nThe function header is provided below:\nstruct Node {\nint32_t val;\nNode* next;\nNode(int32_t next{nullptr}val_in) : val{val_in}, {}\n};\nNode* sum_nodes_between_zeros(Node* head);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of the list. The solution listΘ(𝑛) Θ(𝑛)\nshould be returned as a brand new list, so the original list passed into the input should not be modified.\nChapter 8 Exercise Solutions\nint32_t1. The correct answer is (B). An has a size of 4 bytes. For a linked list of 47 integers, you have to store the Fibonacci number\nitself (with size 4 bytes) along with 2 pointers (8 bytes each). Hence, the total storage of 47 integers in a linked list is 47 (8 + 8 + 4), or×\n940 bytes. For an array, you only have to store the number and not the pointers, so an array of 47 numbers would take up 47×4, or 188\nbytes. The difference is thus 940 - 177 = 752 bytes. You can also think of this in terms of the additional storage needed to store a value in a\ndoubly-linked list compared to an array: in the linked list, each value would have to use 16 additional bytes of memory to store 2 pointers,\nso the extra memory required for the linked list of 47 values is bytes.47×16\n2. The correct answer is (D). You cannot remove the last element in a singly-linked list in constant time, regardless of whether you have a tail\nnext nullptr,pointer or not. This is because you have to set the pointer of the node before the one being deleted to which you cannot\naccess in constant time if the list is singly-linked. Option (A) is true, because you need to visit every value in the list during a traversal,\nwhich takes since it takes constant time to visit each element. Option (B) is true if the element you want to find is the last one youΘ(𝑛)\nencounter in the list (or if it does not exist in the list). Option (C) is correct because, if you are given a pointer to the element to insert at,\nyou can set its internal pointers after the insertion without have to do a separate traversal.\n3. The correct answer is (B). Appending an element to the back of an array, assuming no reallocation, takes constant time. Appending an\nelement to a linked list takes constant time if there is a tail pointer, which is not provided in this problem — without this tail pointer,only\nyou would have to perform a linear traversal to get to the point of insertion, which takes time.Θ(𝑛)", "word_count": 745, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "63b1cdc9-080a-5c6d-a7e8-d3c80391474c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 247, "real_page_number": null, "text": "8.7 Summary of List Complexities\n235\n4. The correct answer is (D). Without a tail pointer, insertion into the linked list still takes in the worst case, since you may traverse theΘ(𝑛)\nentire list to find the point of insertion. However, since insertion is allowed at any point, the time complexity for the array also increases to\nΘ(𝑛), not only to find the position of insertion, but to shift all subsequent elements to make space in the array.\n5. The correct answer is (B). Since elements are stored contiguously in memory in an array, you can access an element at any index of an\narray in constant time using pointer arithmetic. This is not true for linked lists, which only support sequential access, so you would need to\niterate to the index you want to access.\n6. The correct answer is (C). The doubly-linked list is the only of the three containers that can support constant time insertion before any\ngiven object in the container. For the vector, you would have to shift elements over after the insertion, which could take linear time. For\nnextthe singly-linked list, you would have to update the pointer of the object before the point of insertion, which you cannot access in\nprevconstant time from the given pointer (due to the absence of a pointer).\n7. Thecorrect answer is(A).Insertion atthebackofa listtakes timeforboth singly-and doubly-linkedlistsif notail pointeris provided.Θ(𝑛)\nInserting an element after another node, when given the other node’s pointer, takes time for both singly- and doubly-linked lists.Θ(1)\nErasing an element at the back of the list takes time for both singly- and doubly-linked lists if no tail pointer is provided. Erasing anΘ(𝑛)\nelement when given its pointer takes time in a doubly-linked list but time in a singly-linked list, since there is no way to accessΘ(𝑛)Θ(1)\nnext prevthe node before the one being deleted in constant time (you will need to update the pointer of this node, but you do not have a\npointer to get there in a singly-linked list).\n8. The correct answer is (A). There are two main things that this container needs. First, you need a container that supports efficient insertion\nfrom one end and efficient removal from the other. Second, the size of the container can grow without bound, and thus cannot be fixed.\nFrom this, both the vector and fixed size array would not work, since removal from positions not at the end may take linear time. The\ndoubly-linked list is also not ideal because it does not have a tail pointer, so it can only support constant time insertion and removal from\nthe front. This leaves us with the singly-linked list with both head and tail pointers, which supports all the operations we need in constant\ntime (we can append new students to the back and remove from the front, both of which take constant time with the tail pointer).\n9. The correct answer is (E). None of the operations would be improved with the presence of a tail pointer. Reversing the linked list can be\ndone in without the tail pointer (see section 8.4), and there is no way to improve this time complexity. Removing an element at theΘ(𝑛)\nback takes linear time for a singly-linked list regardless of whether a tail pointer is supported or not, because you will need to update the\nnext pointer of the node preceding the one that is deleted (which you cannot access in constant time for a singly-linked list). Finding an\nelement in the list takes in the worst-case regardless of whether a tail pointer is present, since you might need to iterate over all theΘ(𝑛)\nelements (which does not depend on the presence of a tail pointer). Removing the first element in the list takes time even without aΘ(1)\ntail pointer present.\n10th10. The correct answer is (B). Finding the element in the singly-linked list requires iterating 9 positions from the head node, which takes\n10thconstant time. Finding the to last element requires iterating positions from the head node, which takes linear time.𝑛−9\nptr11. The correct answer is (B). Since is the only way to access the queue, all insertions and removals from the queue must take place\nbetween Yvonne and Zach. Alice and Zach cannot be removed from the singly-linked list in constant time (to remove Zach, you would\nnextneed to access Alice’s pointer, which requires a linear traversal of all the students in the queue). Thus, the only student that can be\nremoved in constant time in Yvonne, so she must be the next in the queue if removing and adding students always takes constant time. This\nptrmeans that Zach is the last student in the queue. In fact, using this circular linked list setup, points to the last student in the queue,\nsince it takes constant time to insert a new student directly after this position.\nprev12. The correct answer is (D). Four pointers are modified: of the new node is set to the previous node before the insertion position,\nnext next prevof the new node is set to the next node after the insertion position, of the previous node is set to the new node, and of\nthe next node is set to the new node.\n13. The correct answer is (D). Deleting an element at the back of a singly-linked list takes time even if a tail pointer is present, since youΘ(𝑛)\nnextneed to update the pointer of the node before the deleted node, which you cannot access in constant time.\nfast14. The correct answer is (D). If the provided list has odd length but no cycle, then would end up referencing the last node in the list\nwhile fast->next->next fast->nextwithin the loop on line 8. This causes on line 14 to produce undefined behavior, since is a\nnullptr that is dereferenced.\nThe correct answer is (A). This takes time and uses auxiliary space in the worst case. First, traverse the two linked lists to15. Θ(𝑚+𝑛) Θ(1)\n|𝑚−𝑛|find the values of 𝑚and 𝑛. Then, go back to the heads of the linked list, and traverse nodes on the longer list. After this, iterate over\nthe remaining nodes of the lists in lock step and compare the nodes until you find the first shared node.\n16. The correct answer is (C). The time complexity of determining whether a doubly-linked list is circular is Θ(1), since you can just check if\nprevthe of the head node is the last node of the list.\n17. The correct answer is (C). Since the two lists are sorted, you can solve this problem in linear time by initializing two pointers to the\nbeginning of the two lists, appending the smaller element of the two pointers to a new, merged list, and incrementing the pointer of the list\nwhose element was just copied over. This is repeated until both pointers reach the end of their corresponding list.\n18. The correct answer is (C). Unlike data in vectors, data in a linked list do not get reallocated as the container grows in size. This can be\nuseful if you want to ensure the validity of pointers and references to objects in the list throughout the lifetime of the container. However,\nthis comes at the cost of the other three options: linked lists take up more memory to store the same amount of data, does not provide fast\nsequential access (due to the inability to take advantage of caching, which allows contiguously memory to be accessed faster sequentially),\nand also does not provide random access.Θ(1)", "word_count": 1296, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7c52c0fe-895a-5e46-8ee1-b6d020aa45a1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 248, "real_page_number": null, "text": "236\nChapter 8. Linked Lists\n19. The correct answer is (D). In the worst-case, you would have to visit all the values in the container before you find the one you want. This\ndoes not matter if the container is an array or singly-linked list: the traversal would take time.Θ(𝑛)\n20. The correct answer is (A). Only option (A) is true. Option (B) is false because deleting the last element in a singly-linked list takes Θ(𝑛)\ntime. Option (C) is false because finding an element takes worst-case time. Option (D) is false because reversing a doubly-linked listΘ(𝑛)\ncan be done in time regardless of whether a tail pointer is present.Θ(𝑛)\n21. The correct answer is (A). To splice one list into another, you just need to move some pointers around so that the nodes before and after\nthe point of insertion (along with the end nodes of the list being spliced) are updated correctly. Since we have a doubly-linked list with tail\npointers, accessing these nodes to update takes constant time, without needing to iterate over the remaining nodes of the two lists. Because\nof this, the splicing operation can be done in time, irrespective of the lengths of the two lists involved in the splice.Θ(1)\n22. This problem is very similar to the original problem of reversing the entire linked list that we discussed in section 8.4. The only difference\nis that we only want to reverse a subsection of the list rather than the entire list. To accomplish this, we will follow the same logic as the\noptimized implementation in section 8.4.2, with the following additional steps:\nleft1. Before reversing, we will first traverse over the first nodes. This can be done using a counter that keeps track of the number of\nleft rightpositions we have iterated. In the following solution, we will use the values of and themselves as our counter variables.\nleft right, right,2. After reversing the nodes between and we will attach the nodes afterward the new node at the position of\nleft.which should be the node that was originally at the position of\nOne implementation of this solution is shown below:\n1\nuint32_t uint32_tNode* reverse_list(Node* head, left, right) {\n2\n// null check\n3\nif nullptr)(head == {\n4\nreturn nullptr;\n5\n} // if\n6\n7\nnullptr;Node* prev =\n8\nNode* curr = head;\n9\n10\n// iterate forward \"left\" positions to get to the position of the first node to reverse\n11\nwhile (left > 1) {\n12\nprev = curr;\n13\ncurr = curr->next;\n14\n--left;\n15\n--right;\n16\n} // while\n17\n18\n// store node directly before the first position reversed\n19\nNode* node_before_reverse = prev;\n20\n21\n// store first node to be reversed, will be final node in modified range after reversing\n22\nNode* first_node_reversed = curr;\n23\n24\n// start reversing the linked list up to \"right\"\n25\nwhile (right > 0) {\n26\nNode* next = curr->next;\n27\ncurr->next = prev;\n28\nprev = curr;\n29\ncurr = next;\n30\n--right;\n31\n} // while\n32\n33\n// update node before reversal position to point to first node of the reversed section\n34\n// if this node is nullptr, set head of entire reversed list to last node after reversing\n35\nif nullptr)(node_before_reverse != {\n36\nnode_before_reverse->next = prev;\n37\n} // if\n38\nelse {\n39\nhead = prev;\n40\n} // else\n41\n42\n// set the next of the last node of the reversed section to the remaining nodes of the list\n43\nfirst_node_reversed->next = curr;\n44\nreturn head;\n45\n} // reverse_list", "word_count": 613, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4045d88a-8100-5823-a603-135b777d7038", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 249, "real_page_number": null, "text": "8.7 Summary of List Complexities\n237\n23. Since the list is sorted, we can simply iterate over the list and check if there are two adjacent nodes that share the same value. If there are,\nsimply point the next pointer of the first node to the next pointer of the second node (this essentially removes the second duplicate value\nfrom the list). One implementation of this solution is shown below:\n1\nNode* remove_duplicates(Node* head) {\n2\nNode* curr = head;\n3\nwhile (curr && curr->next) {\n4\n// two duplicate values, remove the second duplicate from the list to return\n5\nif (curr->val == curr->next->val) {\n6\ncurr->next = curr->next->next;\n7\ncontinue;\n8\n} // if\n9\n// move forward with iterating over the list\n10\ncurr = curr->next;\n11\n}\n12\n13\nreturn head;\n14\n} // reverse_list\nThe is the same as the linked-list cycle problem we solved in example 8.5 using Floyd’s cycle-finding algorithm, with the added complexity24.\nof returning the node at which the cycle begins. To solve this problem, we can use the same implementation we had before. However, once\nthe slow and fast pointers meet (which indicates the existence of a cycle), we will reset the slow pointer at the head of the original list and\nthen increment both the slow and fast pointers at the same speed until they meet again. The node they meet at must be the first node in the\ncycle. An implementation is shown below:\n1\nNode* first_node_in_cycle(Node* head) {\n2\n// use two pointers, where one moves faster than the other;\n3\n// if the fast pointer catches up to the slow pointer.\n4\n// then there exists a cycle\n5\nNode* fast = head;\n6\nNode* slow = head;\n7\nwhile (fast && fast->next) {\n8\nslow = slow->next;\n9\nfast = fast->next->next;\n10\nif (fast == slow) {\n11\nslow = head;\n12\nwhile (slow != fast) {\n13\nslow = slow->next;\n14\nfast = fast->next;\n15\n} // while\n16\nreturn slow;\n17\n} // if\n18\n} // while\n19\n20\n// fast pointer reached end without meeting slow, so no cycle\n21\nreturn nullptr;\n22\n} // first_node_in_cycle ()\n25. There are two ways to approach this problem, both of which share a time complexity. The first approach is the simplest, as it traversesΘ(𝑛)\n𝑘th 𝑘ththe list twice: during the first pass, we find the node, and on the second pass, we find the node from the end. An implementation of\nthis simple solution is shown below:\n1\nuint32_tNode* swap_nodes(Node* head, k) {\n2\nNode* curr = head;\n3\nNode* kth = head;\n4\nNode* kth_to_last = head;\n5\nint32_t count = 0;\n6\nwhile nullptr)(curr != {\n7\n++count;\n8\nif (count == k) {\n9\nkth = curr;\n10\n} // if\n11\ncurr = curr->next;\n12\n} // while\n13\n14\ncurr = head;\n15\nfor (int32_t i = 0; i < count; ++i) {\n16\nif (i == count - k) {\n17\nkth_to_last = curr;\n18\nbreak;\n19\n} // if\n20\ncurr = curr->next;\n21\n} // for i\n22\n23\nstd::swap(kth->val, kth_to_last->val);\n24\nreturn head;\n25\n} // swap_nodes()", "word_count": 538, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3b2d5f36-a7b6-5d3c-ad33-75ef9d4806c8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 250, "real_page_number": null, "text": "238\nChapter 8. Linked Lists\nAlthough it meets the time complexity requirements, this is actually not the most efficient solution. It turns out that you can solve this\nproblem in only a single pass using the two pointer technique. To do so, we will keep track of a slow and fast pointer. The fast pointer will\n𝑘thfirst be moved positions forward to reach the node. We will keep track of this node. Then, we will follow the strategy described in𝑘−1\nexample 8.3, incrementing both the slow and fast pointer in tandem until the fast pointer reaches the end. At this point, the slow pointer will\n𝑘th 𝑘thbe pointing the to last node. Using the node that we stored earlier, we can switch the two values and return the modified list. An\nimplementation is shown below:\n1\nuint32_tNode* swap_nodes(Node* head, k) {\n2\nNode* slow = head;\n3\nNode* fast = head;\n4\nfor (int32_t i = 0; i < k - 1; ++i) {\n5\nfast = fast->next;\n6\n} // for i\n7\n8\nNode* kth = fast;\n9\nwhile (fast->next) {\n10\nslow = slow->next;\n11\nfast = fast->next;\n12\n} // while\n13\n14\nstd::swap(kth->val, slow->val);\n15\nreturn head;\n16\n} // swap_nodes()\nTo solve this problem, one solution is to traverse the list with a running counter that tracks the sum encountered so far in between zeros.26.\nWhenever a zero is encountered, the counter is reset, and the previous sum is appended to the list. One implementation is shown below:\n1\nNode* sum_nodes_between_zeros(Node* head) {\n2\nnullptr;Node* summed_list =\n3\nnullptr;Node* new_head =\n4\n5\nint32_t sum = 0;\n6\nwhile nullptr)(head != {\n7\nif (head->val == 0) {\n8\nif (sum != 0) {\n9\nnewNode* next_node = Node{sum};\n10\n// is the first node of the merged list\n11\nif nullptr)(summed_list == {\n12\nnew_head = summed_list = next_node;\n13\n} // if\n14\nelse {\n15\nsummed_list->next = next_node;\n16\nsummed_list = summed_list->next;\n17\n} // else\n18\n}\n19\n// reset counter\n20\nsum = 0;\n21\n} // if\n22\nelse {\n23\nsum += head->val;\n24\n} // else\n25\n26\nhead = head->next;\n27\n} // while\n28\n29\nreturn new_head;\n30\n} // swap_nodes()", "word_count": 387, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f302f313-f181-51b1-bd51-051f0db3e1d3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 251, "real_page_number": null, "text": "Chapter 9\nStacks and Queues\n9.1\nIntroduction to Stacks\nA stack is a one-ended linear data structure that supports last-in, first-out (LIFO) data access. When you retrieve elements out of a stack, you\nremove the elements in the opposite order in which they were inserted.\nFor a real-life example of how a stack works, consider a stack of books on a table. When you add a book to the stack, you add it to the very\ntop of the pile. When you retrieve a book from the stack, you must remove the book on top — the one you most recently added. You cannot\naccess the book at the very bottom of the stack without removing all the other books on top of it.\n.push(), .top(),A stack works in a similar manner. When you call you add an element to the top of the stack. When you call you\n.pop(),retrieve the most recently added element out of the stack. When you call you remove the most recently added element from the stack.\nThe interface of a stack:\nFunction\nBehavior\n.push(val)\nvalAdds to the top of the stack\n.pop()\nRemoves top element from the stack\n.top()\nReturns a reference to the top element in the stack\n.size()\nReturns the number of elements in the stack\n.empty()\nReturns whether the stack is empty\n.push()\n.pop()\n.top()\nStacks have numerous practical use cases in programming. For instance, text editors use a stack’s behavior to implement \"undo\" buttons\n'{'that allow you to revert changes. Compilers also use stacks to check for matching parentheses in code (e.g., making sure every has a\n'}'matching in the right place). Stacks also play an important role in recursion (by keeping track of the data of previous function calls) and\ngraph-searching algorithms (such as a depth-first search, which will be covered in chapter 19).", "word_count": 310, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "81334730-5df4-5c64-8d1b-9822b10f1d96", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 252, "real_page_number": null, "text": "240\nChapter 9. Stacks and Queues\n¸ 9.1.1\nImplementing a Stack Using an Array\nHow would you implement a stack container in memory? One strategy would be to use an array as the underlying container and keep a pointer\nto the first empty position past the last element:\n5\n3\n2\nbase_ptr\ntop_ptr\nTop of stack\n(where elements\nare added/removed)\ntop_ptrIf you want to push an element into the stack, you would add the element to the position pointed to by and then increment the\ntop_ptr .push()pointer (allocating new space if needed). If no reallocation is needed, the time complexity of a single is Θ(1). If\nreallocation is involved, all 𝑛elements have to be copied over, so the time complexity would be Θ(𝑛), where 𝑛is the size of the stack.\ntop_ptr.If you want to pop an element off the stack, you would just need to decrement This takes time.Θ(1)\ntop_ptr - 1.If you want to retrieve the top element, you would just need to dereference This takes time. Note that we could checkΘ(1)\nstd::stack<>whether an element actually exists in the stack beforehand, but this check is omitted by the STL’s implementation. This is for\n.top()the sake of efficiency; the operation would be slower if a validity check were made every time it is called. By omitting the validity\n.top()check, we leave it to the programmer to ensure that they are not calling on an empty stack.\ntop_ptr - base_ptr.If you want to compute the size of the stack, you could return Because elements in an array are contiguous in memory,\nthe difference between the two pointers is equal to the number of elements in the array. This operation takes time.Θ(1)\nbase_ptr top_ptr top_ptrIf you want to check whether the stack is empty, you could check if and are equal. Because points to the first\ntop_ptravailable position in the stack, if points to the first position, the first position must be empty. This check also takes time.Θ(1)\nIn summary, if you implement a stack using an array as the underlying container, you can achieve the following time complexities:\nMethod\nImplementation\nComplexity\n.push(val)\ntop_ptr, top_ptrAdd new element at the position pointed to by then increment\nif no reallocation,Θ(1)\ninvolved1if reallocationΘ(𝑛)\n.pop()\ntop_ptrDecrement\nΘ(1)\n.top()\ntop_ptr - 1Dereference\nΘ(1)\n.size()\ntop_ptr - base_ptrCalculate\nΘ(1)\n.empty()\nbase_ptr == top_ptrCheck if\nΘ(1)\nThe code for the array-based stack implementation is shown below:\n1\ntemplate <typename T>\n2\nclass Stack {\n3\nT* data;\n// pointer to array\n4\nT* top_ptr;\n// pointer to first open position\n5\nsize_t capacity;\n// array capacity\n6\npublic:\n7\nStack(size_t num = 4);\n// array capacity defaults to 4 if not specified\n8\n~Stack();\n9\nvoid push(T val);\n10\nvoid pop();\n11\nT& top();\n12\nsize_t size();\n13\nbool empty();\n14\n};\n15\n16\ntemplate <typename T>\n17\nStack<T>::Stack(size_t num) : capacity{num} {\n18\nnew nullptr);data = (num ? T[num] :\n// allocate array memory\n19\ntop_ptr = data;\n// set top_ptr to top of data\n20\n} // Stack()\n21\n22\ntemplate <typename T>\n23\nStack<T>::~Stack() {\n24\ndelete[] data;\n25\n} // ~Stack()\n1Using .push()amortizedanalysis, isanamortized operation. Thisconceptwillbediscussedinchapter12.Θ(1)", "word_count": 556, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b39fbe7e-0c62-5b5f-ba3f-fca5db83029b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 253, "real_page_number": null, "text": "9.1 Introduction to Stacks\n241\n27\ntemplate <typename T>\n28\nvoid Stack<T>::push(T val) {\n29\nif (top_ptr - data >= capacity) {\n// if current array is full\n30\nnewT* temp = T[capacity 2];*\n// double capacity of array\n31\nfor (size_t i = 0; i < capacity; ++i) {\n// copy elements over\n32\ntemp[i] = data[i];\n33\n} // for i\n34\ntop_ptr = &temp[capacity];\n// reset top_ptr to new array\n35\ncapacity *= 2;\n// update capacity value\n36\nstd::swap(temp, data);\n// set data to new array\n37\ndelete[] temp;\n// deallocate old array\n38\n} // if\n39\n*top_ptr++ = val;\n// writes new value to top_ptr\n40\n} // push()\n41\n42\ntemplate <typename T>\n43\nvoid Stack<T>::pop() {\n44\n--top_ptr;\n45\n} // pop()\n46\n47\ntemplate <typename T>\n48\nT& Stack<T>::top() {\n49\nreturn *(top_ptr - 1);\n50\n} // top()\n51\n52\ntemplate <typename T>\n53\nsize_t Stack<T>::size() {\n54\nreturn top_ptr - data;\n55\n} // size()\n56\n57\ntemplate <typename T>\n58\nbool Stack<T>::empty() {\n59\nreturn data == top_ptr;\n60\n} // empty()\n¸ 9.1.2\nImplementing a Stack Using a Linked List\nWe could also use a linked list to implement our stack. A singly-linked list is sufficient for implementing a stack — there is no need to store\nprev headpointers, since we do not need them for any stack operations. In our linked list, we will treat the as the top of the stack. This is\nbecause it is more efficient to add and remove elements from the head than it is to add and remove elements from the tail.\n2\n3\n5\nnullptr\nhead\nTop of stack\n(where elements\nare added/removed)\nIn this case, if we want to push an element into the stack, we would insert it at the beginning of the list. The amount of work required to insert\nan element at the beginning of a list does not depend on the size of the list, so the time complexity of pushing an element is Θ(1).\nhead headIf we want to pop an element off the stack, we would delete the element at the pointer. This requires us to set the pointer to\nhead->next head.and deallocate the old Since we are only moving some pointers around, this takes time.Θ(1)\nheadIf we want to retrieve the top element, we would have to return a reference to the data stored in the node. Since we already have access to\nheadthe node, we can access its data in time.Θ(1)\nsizeIf we want to return the size of the stack, we could count the number of nodes each time is called; this would result in a operation.Θ(𝑛)\nHowever, we could also store an additional member variable that stores the size. If we keep track of the size internally, we would not need to\ntraverse the list each time we want the size of the stack; we could just check the value of our size variable. Even though this approach adds a bit\nmore work to other operations (insertions and deletions would have to modify size), it allows us to retrieve the size of the list in time.Θ(1)\nhead == nullptr size == 0If we want to check if the stack is empty, we could just check to see if (or check if variable if we have a\nsizeseparate variable). This check takes time.Θ(1)\nIn summary, if you implement a stack using a list as the underlying container, you can achieve the following time complexities:\nMethod\nImplementation\nComplexity\n.push(val)\nvalInsert to the front of list\nΘ(1)\n.pop()\nDelete node at the front of list\nΘ(1)\n.top()\nheadReturn reference to data of node\nΘ(1)\n.size()\nsizeTrack internally as a member variable\nΘ(1)\n.empty()\nhead nullptrCheck if is\nΘ(1)", "word_count": 646, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "603ce253-7631-5d73-8959-9f29bd5a1ed1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 254, "real_page_number": null, "text": "242\nChapter 9. Stacks and Queues\nThe code for the list-based stack implementation is shown below:\n1\ntemplate <typename T>\n2\nclass Stack {\n3\nstruct Node {\n4\nT val;\n5\nNode* next;\n6\nnext{nullptr}Node(T val_in) : val{val_in}, {}\n7\n};\n8\nNode* head;\n9\nsize_t sz;\n10\npublic:\n11\nStack();\n12\n~Stack();\n13\nvoid push(T val);\n14\nvoid pop();\n15\nT& top();\n16\nsize_t size();\n17\nbool empty();\n18\n};\n19\n20\ntemplate <typename T>\n21\nhead{nullptr},Stack<T>::Stack() : sz{0} {}\n22\n23\ntemplate <typename T>\n24\nStack<T>::~Stack() {\n25\nNode* temp;\n26\nwhile nullptr)(head != {\n// destructor goes through\n27\ntemp = head->next;\n// list and deallocates\n28\ndelete head;\n// all the nodes\n29\nhead = temp;\n30\n} // while\n31\n} // ~Stack()\n32\n33\ntemplate <typename T>\n34\nvoid Stack<T>::push(T val) {\n35\nnewNode* new_node = Node{val};\n// insert node at head\n36\nnew_node->next = head;\n37\nhead = new_node;\n38\n++sz;\n39\n} // push()\n40\n41\ntemplate <typename T>\n42\nvoid Stack<T>::pop() {\n// remove node at head\n43\nNode* temp = head;\n44\nhead = temp->next;\n45\n--sz;\n46\ndelete temp;\n47\n} // pop()\n48\n49\ntemplate <typename T>\n50\nT& Stack<T>::top() {\n51\nreturn head->val;\n52\n} // top()\n53\n54\ntemplate <typename T>\n55\nsize_t Stack<T>::size() {\n56\nreturn sz;\n57\n} // size()\n58\n59\ntemplate <typename T>\n60\nbool Stack<T>::empty() {\n61\nreturn nullptr;head ==\n62\n} // empty()\nWhich stack implementation is better, the one that uses an array or the one that uses a linked list? It turns out that both containers support all\nstack operations in time. However, if you were to time these implementations, you would find that the array implementation is slightlyΘ(1)\nfaster than the linked list implementation. This is because, even though both implementations run in time, the constant factor of the arrayΘ(1)\nimplementation is smaller. The linked list implementation also has higher memory overhead compared to the array (since additional pointers\nneed to be stored with each element).", "word_count": 347, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e543425f-cfa9-5559-9143-8fcbb92ad020", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 255, "real_page_number": null, "text": "9.2 The STL Stack Container\n243\n9.2\nThe STL Stack Container\n<stack>TheC++standardtemplatelibraryprovidesapre-implementedstackcontainerforyouinthe library. Tousethisstackinyourprogram,\n#include <stack> std::stack<>. std::stack<>you will need to at the top of your code file and declare an object of type A\nsupports the following operations:\nFunction\nBehavior\n.push(val)\nvalAdds to the top of the stack\n.pop()\nRemoves top element from the stack (undefined behavior if empty)\n.top()\nReturns a reference to the top element in the stack (undefined behavior if empty)\n.size()\nReturns the number of elements in the stack\n.empty()\nChecks if the stack is empty\n.top() .pop()In C++, the operation returns a reference to the top element in the stack but does not remove it. The operation removes the\ntop element but does not return it. This was designed with speed in mind; in cases where you only want to do one of these operations, there is no\n.top() .pop().point in doing both. If you want to retrieve and remove the top element in a stack, you must call both and\nstd::stack<>:The following code provides an example that utilizes a\n1\nstd::stack<int32_t> s;\n// initializes a stack with variable name 's'\n2\ns.push(5);\n// pushes 5 onto the stack\n3\ns.push(3);\n// pushes 3 onto the stack\n4\nint32_t x = s.top();\n// x stores the value 3\n5\ns.pop();\n// 3 is removed from the stack\n6\ns.top() = 4;\n// the top element is changed from 5 to 4\n7\ns.pop();\n// 4 is removed from the stack\n8\nstd::cout << s.size() << '\\n';\n// stack is empty, so this prints out 0\n9.3\nIntroduction to Queues\nA queue is linear data structure that supports first-in, first-out (FIFO) data access. When you retrieve elements out of a queue, you remove the\nelements in the same order in which you inserted them.\nFor a real-life example of how a queue works, think of an office hours queue where students wait in line for help. When a student adds\nthemselves to the queue, they put themselves at the end of the line. If an instructor is available, they get the student who has been in the queue\nthe longest. A queue works the same on data. When you an element, you add the element to the of the queue. When youenqueue back dequeue\n.push() .pop().an element, you remove the element at the of the queue. In C++, elements are enqueued using and dequeued usingfront\nThe interface of a queue:\nFunction\nBehavior\n.push(val)\nvalAdds to the back of the queue\n.pop()\nRemoves top element from the front of the queue\n.front()\nReturns a reference to the element at the front of the queue\n.size()\nReturns the number of elements in the queue\n.empty()\nChecks if the queue is empty\n.pop()\n.push()\n.front()\n.top() .front()It is important to remember that stacks use to retrieve the next element, while queues use to retrieve the next element.\nThe other method names of the two containers are identical.\n¸ 9.3.1\nImplementing a Queue Using an Array (Circular Buﬀer)\nLike with stack, a queue can be implemented using an array or a linked list as the underlying structure. However, the implementation of a queue\nis slightly more complicated, since operations need to be supported on ends of the underlying container. One method for implementing aboth\nqueue using an array is to use a circular buffer. In a circular buffer, we allow elements to loop off the end of the array to support efficient\nfront_ptr,deletions and insertions on opposing ends of the queue. To track the order of elements in a circular buffer, two pointers are used:\nback_ptr,which points to the element at the front of the queue, and which points to the first open position at the back of the queue.\nConsider the following array, which represents our circular buffer:\n1\n2\n3\nfront_ptr\nback_ptr\n1 3Here, the value is at the front of the queue (and is next to be popped off), and the value is at the back of the queue (the element most recently\nfront_ptr back_ptr,added to the queue). We know this because of the positions of and which indicate the two ends of our queue.", "word_count": 724, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b5b397e8-7a94-5d7b-8b8a-87e21ab8d69d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 256, "real_page_number": null, "text": "244\nChapter 9. Stacks and Queues\nBecause a queue allows elements to be modified at both ends of the container, we have to be a bit more careful when storing our elements. With\ntop_ptran array-based stack implementation, we knew that if reached the end of the array, the stack was full and we needed to allocate more\nspace. This assumption cannot be made with a queue, however, since the first element is not guaranteed to be in the leftmost position of the\n1array! For instance, if we popped an element off the above queue, would get removed and the first element would be located at the second\nposition of the array. This is where the circular nature of the circular buffer comes in. Every time an element is pushed in, we add the element to\nback_ptr back_ptr back_ptrthe position pointed to by and increment by one. However, if ends up off the end of the array, we wrap\nback to the beginning of the array and check if the position is available. This process is illustrated below:around\n3\n5\n2\nfront_ptr\nback_ptr\n.pop(), front_ptrSuppose we call which removes the element at the front of the queue. Since is a pointer that points to the element at the\nfront_ptrfront of the queue, we can \"remove\" the front element by moving forward one position.\n3\n5\n2\nfront_ptr\nback_ptr\n.pop()\n3\n5\n2\nfront_ptr\nback_ptr\n3Notice here that the value was not physically deleted from the array. This is because our queue, if implemented correctly, has no way of\n3accessing this since it is no longer a valid entry in our queue (which only looks at the values between the front and back pointers). As a result,\nthere is no need to do the additional work of clearing this value out, as the queue should never access it anyway. We will only reset this value\nwhen need to access index 0 of our array again.\n9 .push(9). back_ptrNow, suppose we push the value to the back of the queue by making a call to Since points to the first open\n9 back_ptrposition in the queue, we can write to this location and increment by one position. Since a circular buffer wraps around,\nback_ptr ends up pointing to index 0 of the array.\n3\n5\n2\nfront_ptr\nback_ptr\n.push(9)\n3\n5\n2\n9\nfront_ptr\nback_ptr\n4 .push(4)), 4 back_ptrIf were to push the value into the queue (by calling we would write to the value pointed to by and increment\nback_ptr forward one position. This is shown below.\n3\n5\n2\n9\nfront_ptr\nback_ptr\n.push(4)\n4\n5\n2\n9\nfront_ptr\nback_ptr\nfront_ptr back_ptrWith this procedure, will always point to the element at the front of the queue, and will always point to the next\nback_ptr front_ptr,available position in the array. If incrementing causes it to point to the same position as that means the array is filled\nto capacity, and reallocation would be needed to support any more elements. During reallocation, the elements in the array are copied to a larger\narray from front to back, so that the front element ends up at the first position of the new array. For example, if reallocation were done on the\nabove array, the elements would be copied to a new array in the order [5, 2, 9, 4], since 5 is the element at the front.\nThe operations for a circular buffer, along with their complexities, are shown below:\nMethod\nImplementation\nComplexity\n.push(val)\nval back_ptr back_ptr,Insert at position pointed to by and increment wrap-\nback_ptrping around to the beginning of the array if necessary. If is ever incre-\nfront_ptr,mented to the position of a larger array is allocated and elements are\ncopied over in order from front to back.\nif no reallocation,Θ(1)\ninvolved2if reallocationΘ(𝑛)\n.pop()\nfront_ptrIncrement\nΘ(1)\n.front()\nfront_ptrDereference and return its value\nΘ(1)\n.size()\nback_ptr front_ptr,Iftheaddressof islargerthanthatof returnthedifference\nbetween the two pointers; otherwise, add the capacity of the array to the difference\nback_ptr front_ptr)(i.e., return array capacity + -\nΘ(1)\n.empty()\nback_ptr == front_ptrCheck if\nΘ(1)\n2Using .push()amortizedanalysis, isanamortized operation. Thisconceptwillbediscussedinchapter12.Θ(1)", "word_count": 723, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "19a9a6ef-9fbb-591f-8bba-a8df64d543f3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 257, "real_page_number": null, "text": "9.3 Introduction to Queues\n245\nThe following code implements a queue using a circular buffer:\n1\ntemplate <typename T>\n2\nclass Queue {\n3\nT* data;\n4\nT* front_ptr;\n5\nT* back_ptr;\n6\nsize_t capacity;\n7\npublic:\n8\n// array capacity defaults to 4 if not specified\n9\nQueue(size_t num = 4);\n10\n~Queue();\n11\nvoid push(T val);\n12\nvoid pop();\n13\nT& front();\n14\nsize_t size();\n15\nbool empty();\n16\n};\n17\n18\ntemplate <typename T>\n19\nQueue<T>::Queue(size_t num) : capacity{num} {\n20\nnew nullptr);data = (num ? T[num] :\n21\nfront_ptr = back_ptr = data;\n22\n} // Queue()\n23\n24\ntemplate <typename T>\n25\nQueue<T>::~Queue() {\n26\ndelete[] data;\n27\n} // ~Queue()\n28\n29\ntemplate <typename T>\n30\nvoid Queue<T>::push(T val) {\n31\n*back_ptr++ = val;\n// write val to back_ptr\n32\nif (back_ptr >= data + capacity) {\n// loop around if off end\n33\nback_ptr = data;\n34\n} // if\n35\nif (back_ptr == front_ptr) {\n// if back hits front, reallocate\n36\nnewT* temp = T[capacity 2];*\n// double capacity of array\n37\nfor (size_t i = 0; i < capacity; ++i) {\n38\ntemp[i] = *front_ptr++;\n// write data starting from front\n39\nif (front_ptr == data + capacity) {\n// loop around if off end\n40\nfront_ptr = data;\n41\n} // if\n42\n} // for i\n43\nfront_ptr = temp;\n// reset front_ptr to new array\n44\nback_ptr = &temp[capacity];\n// reset back_ptr to new array\n45\ncapacity *= 2;\n// update capacity\n46\nstd::swap(temp, data);\n// set data to new array\n47\ndelete[] temp;\n// deallocate old array\n48\n} // if\n49\n} // push()\n50\n51\ntemplate <typename T>\n52\nvoid Queue<T>::pop() {\n53\n++front_ptr;\n54\nif (front_ptr >= data + capacity) {\n// loop around if off end\n55\nfront_ptr = data;\n56\n} // if\n57\n} // pop()\n58\n59\ntemplate <typename T>\n60\nT& Queue<T>::front() {\n61\nreturn *front_ptr;\n62\n} // front()\n63\n64\ntemplate <typename T>\n65\nsize_t Queue<T>::size() {\n66\nif (back_ptr >= front_ptr) {\n67\nreturn back_ptr - front_ptr;\n68\n} // if\n69\nelse {\n70\nreturn capacity + back_ptr - front_ptr;\n71\n} // else\n72\n} // size()\n73\n74\ntemplate <typename T>\n75\nbool Queue<T>::empty() {\n76\nreturn back_ptr == front_ptr;\n77\n} // empty()", "word_count": 395, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f1c8cd53-3275-5695-9465-6c05f3f4db6b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 258, "real_page_number": null, "text": "246\nChapter 9. Stacks and Queues\n¸ 9.3.2\nImplementing a Queue Using a Linked List\nA queue can also be implemented with a linked list as the underlying container. The process is similar to implementing a stack with a linked list;\nthe only major difference is that elements are added to the of the linked list rather than the head:tail\n5\n2\n9\n4\nnullptr\nhead\nfront of queue\n(where elements are removed)\ntail\nback of queue\n(where elements are added)\n.push() .pop()When is called, an element is added to the back of the linked list. With a tail pointer, this can be done in time. WhenΘ(1)\nis called, the element at the front of the linked list is removed. The operations for a list-based queue are shown below with their complexities:\nMethod\nImplementation\nComplexity\n.push(val)\nvalInserts to the back of the list\nif tail pointer,Θ(1)\nif no tail pointerΘ(𝑛)\n.pop()\nheadDelete node of the list\nΘ(1)\n.front()\nheadReturn reference to data in node\nΘ(1)\n.size()\nTrackandupdatesizeinternallywhenothermethodsmodifythelist,orcount\nnodes each time\nif size stored internally,Θ(1)\nif nodes counted every timeΘ(𝑛)\n.empty()\nhead == nullptrCheck if\nΘ(1)\nThe following code implements aqueue using a list:singly-linked\n1\ntemplate <typename T>\n2\nclass Queue {\n3\nstruct Node {\n4\nT val;\n5\nNode* next;\n6\nnext{nullptr}Node(T val_in) : val{val_in}, {}\n7\n};\n8\nNode* head;\n9\nNode* tail;\n10\nsize_t sz;\n11\npublic:\n12\nQueue();\n13\n~Queue();\n14\nvoid push(T val);\n15\nvoid pop();\n16\nT& front();\n17\nsize_t size();\n18\nbool empty();\n19\n};\n20\n21\ntemplate <typename T>\n22\nhead{nullptr}, tail{nullptr},Queue<T>::Queue() : sz{0} {}\n23\n24\ntemplate <typename T>\n25\nQueue<T>::~Queue() {\n26\nNode* temp;\n27\nwhile nullptr)(head != {\n28\ntemp = head->next;\n29\ndelete head;\n30\nhead = temp;\n31\n}\n32\n}\n33\n34\ntemplate <typename T>\n35\nvoid Queue<T>::push(T val) {\n36\n// insert node at tail\n37\nnewNode* new_node = Node{val};\n38\nif nullptr)(tail != {\n39\ntail->next = new_node;\n40\n} // if\n41\nelse {\n42\nhead = new_node;\n43\n} // else\n44\ntail = new_node;\n45\n++sz;\n46\n} // push()", "word_count": 378, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9493b4a9-e6a6-5359-83cd-6bba9efcf470", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 259, "real_page_number": null, "text": "9.4 The STL Queue Container\n247\n48\ntemplate <typename T>\n49\nvoid Queue<T>::pop() {\n50\n// remove node at head\n51\nNode* temp = head;\n52\nhead = temp->next;\n53\n--sz;\n54\ndelete temp;\n55\nif nullptr)(head == {\n56\nnullptr;tail =\n57\n} // if\n58\n} // pop()\n59\n60\ntemplate <typename T>\n61\nT& Queue<T>::front() {\n62\nreturn head->val;\n63\n} // front()\n64\n65\ntemplate <typename T>\n66\nsize_t Queue<T>::size() {\n67\nreturn sz;\n68\n} // size()\n69\n70\ntemplate <typename T>\n71\nbool Queue<T>::empty() {\n72\nreturn nullptr;head ==\n73\n} // empty()\n9.4\nThe STL Queue Container\n<queue>The C++ standard template library provides a pre-implemented queue container for you in the library. To use this queue in your\n#include<queue> std::queue<>. std::queue<>program,youwillneedto atthetopofyourcodefileanddeclareanobjectoftype A\nsupports the following operations:\nFunction\nBehavior\n.push(val)\nvalAdds to the back of the queue\n.pop()\nRemoves the element at the front of the queue (undefined behavior if empty)\n.front()\nReturns a reference to the element at the front of the queue (undefined behavior if empty)\n.size()\nReturns the number of elements in the queue\n.empty()\nChecks if the queue is empty\nstd::stack<>, std::queue<> .front()Similar to a a separates element retrieval and removal into two different methods. A call to\n.pop()returns the element at the front of the queue, but does not remove it. In contrast, a call to removes the element at the front, but does not\nstd::queue<>:return it. The following code goes through an example that utilizes a\n1\nstd::queue<int32_t> q;\n// initializes a queue with variable name 'q'\n2\nq.push(5);\n// pushes 5 onto the queue\n3\nq.push(3);\n// pushes 3 onto the queue\n4\nint x = q.front();\n// x now stores the value 5\n5\nq.pop();\n// 5 is removed from the queue\n6\nq.front() = 4;\n// the front element is changed from 3 to 4\n7\nq.pop();\n// 4 is now removed from the queue\n8\nstd::cout << q.size() << '\\n';\n// queue is empty, so this prints out 0\n9.5\nSolving Problems Using Stacks and Queues\nStacksandqueuescanbequiteusefulinsolvingseveraldifferenttypesofprogrammingproblems. Inthissection,wewillexploresomeproblems\nthat utilize these containers.\n¸ 9.5.1\nImplementing a Queue Using Two Stacks\nExample 9.1 .top(), .push(), .pop(), .size().You are given two stacks, each of which supports and If you are given no other\n.front(), .push(), .pop(), .size()?containers, how can you use the two stacks to implement a queue that supports and\nThe thing to notice about a stack is that it is a container. Since insertion and removal are both done on the same end, there is no wayone-ended\nto access elements at the bottom of a stack. On the other hand, a queue must manage data on both ends. This is why a single stack cannot be\nused to simulate a queue, since there is only one place to insert and remove data.", "word_count": 519, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "03ae7561-df7f-5ba5-8529-2192e1eadbf8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 260, "real_page_number": null, "text": "248\nChapter 9. Stacks and Queues\nHowever, this changes if you are given two stacks instead of one. With two stacks, there are now two places where data can be added or removed:\nthe top of stack 1, and the top of stack 2. As a result, we can treat one stack as the \"front\" of our queue (where elements are removed), and the\nother stack as the \"back\" of our queue (where elements are inserted). An illustration of this idea is shown below:\nStack 1 (Front)\nStack 2 (Back)\nBack of Queue\nFront of Queue\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\nstackFront stackBackA summary of these operations is shown below, where is the front stack and is the back stack:\n.push(x): stackBack.push(x);\nreturn.front(): stackFront.top();\n.pop()\n: stackFront.pop();\nreturn.size() : stackFront.size() + stackBack.size();\nHowever, this implementation has a slight problem. Suppose we push 1 into our queue and immediately try to pop it. Using the operations\nabove, we would push 1 into our back stack and then pop an element out of our front stack.\nStack 1 (Front)\nStack 2 (Back)\n1\nBack of Queue\nFront of Queue\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\n.pop() .front()However, there is nothing to pop since our front stack is empty! In fact, our queue’s and methods do not work if all the\nelements are in the back stack.\n.pop() .front()To solve this, we will need to move the elements from the back stack to the front stack if or is ever invoked whileall\nthe front stack is empty. We need to remove every element from the back stack because the next element we want to retrieve from our queue is\nat the bottom of this back stack, and it cannot be accessed before all the elements on top of it are removed first.\nQueueWithTwoStacks::front() {\nif (stackFront.empty()) {\nwhile (!stackBack.empty()) {\nstackFront.push(stackBack.top());\nstackBack.pop();\n} // while\n} // if\nreturn stackFront.top();\n} // front()\nQueueWithTwoStacks::pop() {\nif (stackFront.empty()) {\nwhile (!stackBack.empty()) {\nstackFront.push(stackBack.top());\nstackBack.pop();\n} // while\n} // if\nstackFront.pop();\n} // pop()\nLet’s look at how this process works with the following example:\n1\nQueueWithTwoStacks<int> q;\n2\nq.push(1);\n3\nq.push(2);\n4\nq.push(3);\n5\nq.pop();\n6\nq.push(4);\n7\nq.push(5);\n8\nq.pop();\n9\nq.pop();\n10\nq.pop();\nFirst, we push 1, 2, and 3 to the back of our queue. This is done by pushing these three elements into our back stack:\nStack 1 (Front)\nStack 2 (Back)\n1\n2\n3\nBack of Queue\nFront of Queue\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2", "word_count": 455, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "436d7741-425a-5580-9f02-020fe1f68251", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 261, "real_page_number": null, "text": "9.5 Solving Problems Using Stacks and Queues\n249\nNext, we pop out an element (which should be 1, since it has been in our queue the longest). Since the front stack is empty, we first transfer all\nthe elements from the back stack into the front stack:\nStack 1 (Front)\nStack 2 (Back)\nBack of Queue\nFront of Queue\n3\n2\n1\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\nWe then pop out the next element in the queue by popping from the front stack:\nStack 1 (Front)\nStack 2 (Back)\nBack of Queue\nFront of Queue\n3\n2\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\nWe then push 4 and 5 into our queue, which is added to the back stack:\nStack 1 (Front)\nStack 2 (Back)\n4\n5\nBack of Queue\nFront of Queue\n3\n2\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\nWe then pop out three elements from the queue. Since the front stack is not empty, we can just pop out 2 and 3 without moving anything:\nStack 1 (Front)\nStack 2 (Back)\n4\n5\nBack of Queue\nFront of Queue\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\nHowever, for our third pop, the front queue is empty. Thus, we will have to transfer all the elements from the back stack to the front stack before\npopping out the next element (which is the value 4 in this case):\nStack 1 (Front)\nStack 2 (Back)\nBack of Queue\nFront of Queue\n5\n4\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\nStack 1 (Front)\nStack 2 (Back)\nBack of Queue\nFront of Queue\n5\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\n.push() .size() .front() .pop()What’s the worst-case time complexity of this queue? Here, and both take time, but and bothΘ(1)\ntake worst-case time, where 𝑛is the size of the queue. This is because these functions may be responsible for moving all the elements fromΘ(𝑛)\n.front() .pop()the back stack to the front stack. It may seem that this queue is pretty terrible, since and should not be taking linear time!\n.front() .pop()However, looks can be deceiving — even though the worst-case time complexity of a single or call may be Θ(𝑛), this can\nonly happen if the front stack is empty. Using something known as amortized analysis, we can prove that this worst-case scenario happens so\n.front() .pop()rarely that both and can essentially be treated as operations. This concept will be covered in a later chapter.Θ(1)", "word_count": 470, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d748e5b3-c859-50c5-afd9-8b143187b5c3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 262, "real_page_number": null, "text": "250\nChapter 9. Stacks and Queues\n¸ 9.5.2\nSorting a Stack\nExample 9.2 Suppose you are given a stack of values, and you are told to sort this stack using only the supported stack operations of\n.top(), .push(), .pop(), .size().and You are given an additional auxiliary stack (which you may use to help you sort the input\nstack), but you are not allowed to use any additional containers. How can you solve this problem?\nThe idea here is to utilize our auxiliary stack to help keep our values sorted. Even though our input stack is not sorted, we can push elements\ninto the auxiliary stack in such a way to keep its elements sorted. This is done by making sure that bigger values are always sent to the bottom of\nthe auxiliary stack. This process is summarized below:\n1. Save the value at the top of the input stack in a variable (we will call this value the current element).\n2. Since we want larger items to be sent to the bottom of the auxiliary stack, move all elements in the auxiliary stack that are smaller than\nthe current element back into the input stack.\n3. Once the auxiliary stack only contains elements larger than the current element, push the current element into the auxiliary stack.\nThis algorithm is illustrated in the code below:\n1\nwhile (!input.empty()) {\n2\ncurr = input.top();\n3\ninput.pop();\n4\nwhile (!aux.empty() && aux.top() < curr) {\n5\ninput.push(aux.top());\n6\naux.pop();\n7\n} // while\n8\naux.push(curr);\n9\n} // while\ninput aux currLet’s look at this algorithm in action. In the figure below, represents the input stack, represents the auxiliary stack, and\nrepresents the current element.\n4\n1\n3\n5\n2\ninput\naux\ncurr\nFirst, we look at the element at the top of the input stack, 2. Since the auxiliary stack is empty, we can immediately push 2 to into our auxiliary\nstack without any problems.\n4\n1\n3\n5\n2\ninput\naux\ncurr\n4\n1\n3\n5\ninput\naux\ncurr\n2\n4\n1\n3\n5\ninput\n2\naux\ncurr", "word_count": 349, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "be90b378-dfcd-53d3-8dfa-6468d53e5fbb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 263, "real_page_number": null, "text": "9.5 Solving Problems Using Stacks and Queues\n251\nauxThe next element to be considered is 5, which is at the top of the input stack. We want to insert 5 into so that it is in the correct sorted\naux. input, aux,position relative to the other values in To do this, we will first move all elements smaller than 5 back into push 5 into and\naux. input, aux, aux.reinsert the elements we moved back into In this case, we would move 2 back to add 5 into and readd 2 into\n4\n1\n3\ninput\n2\naux\ncurr\n5\nSet 5 (top of input\nstack) as current value\n4\n1\n3\n2\ninput\n5\naux\ncurr\nauxMove all values less than 5\ninput auxinto and add 5 to\n4\n1\n3\ninput\n5\n2\naux\ncurr\nReadd previously removed\nelements back into auxiliary stack\nThe next element to be considered is 3. To insert 3 into correct sorted position, we first move 2 from the auxiliary stack to the input stack. Then,\nwe insert 3 into the auxiliary stack. Lastly, we add 2 back into the auxiliary stack.\n4\n1\ninput\n5\n2\naux\ncurr\n3\nSet 3 (top of input\nstack) as current value\n4\n1\n2\ninput\n5\n3\naux\ncurr\nauxMove all values less than 3\ninput auxinto and add 3 to\n4\n1\ninput\n5\n3\n2\naux\ncurr\nReadd previously removed\nelements back into auxiliary stack\naux, auxThe next element to be considered is 1. Since 1 is already smaller than all the elements in we can just push 1 into directly.\n4\n1\ninput\n5\n3\n2\naux\ncurr\n4\ninput\n5\n3\n2\naux\ncurr\n1\n4\ninput\n5\n3\n2\n1\naux\ncurr\nThe next element to be considered is 4. To insert 4 into correct sorted position, we first move 1, 2, and 3 from the auxiliary stack to the input\nstack. Then, we insert 4 into the auxiliary stack. Lastly, we add 1, 2, and 3 back into the auxiliary stack.\ninput\n5\n3\n2\n1\naux\ncurr\n4\nSet 4 (top of input\nstack) as current value\n1\n2\n3\ninput\n5\n4\naux\ncurr\nauxMove all values less than 4\ninput auxinto and add 4 to\ninput\n5\n4\n3\n2\n1\naux\ncurr\nReadd previously removed\nelements back into auxiliary stack", "word_count": 405, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "70100997-9f89-5184-b380-e15332f7a040", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 264, "real_page_number": null, "text": "252\nChapter 9. Stacks and Queues\nΘ(𝑛2),auxAfter this algorithm runs to completion, the values in will be sorted. The worst-case time complexity for this algorithm is when the\nauxinput stack is already sorted. When this happens, you will have to move all the elements out of whenever you want to push the current\naux,element in. For instance, to push the second value into you would need to move out the first element and later readd it in. To push the third\n𝑛thaux, aux,value into you would need to move out the first second elements and later readd them in. To push the element into you wouldand\nneed to move out all elements before it and later readd them in. The total number of elements moved in this case would be 1 + 2 + +𝑛−1 …\nΘ(𝑛2).(𝑛−1), which is The auxiliary space used by this algorithm is Θ(𝑛), since an additional stack of size 𝑛was used to solve this problem.\n¸ 9.5.3\nSorting a Queue\nExample 9.3 Suppose you are given a queue of values, and you are told to sort this queue using only the supported queue operations of\n.front(), .push(), .pop(), .size().and Using no additional containers, how can you solve this problem?\nThis is similar to the previous problem, but this time you are asked to sort a queue rather than a stack. This difference in container actually\nmakes quite a difference. Unlike a stack, a queue is not single-ended — if you insert elements into a stack, the element at the bottom of the stack\ncannot be retrieved without first removing and storing the elements above it elsewhere (think about a stack of books: to remove the book at\nthe bottom of the stack, you must first move all the books on top of it). This was why we needed an auxiliary stack for the previous sorting\nproblem. However, to reorder elements in a queue, there is no need to keep track of an auxiliary container. Because data in a queue is inserted\nand removed on different ends, all elements can eventually be accessed using auxiliary space by just popping each element off the front ofΘ(1)\nthe queue and repushing it onto the back.\nSuppose we are given the following unsorted queue:\nfront\nback\n2\n5\n3\n1\n4\n.push(), .pop(), .front(), .size(),Assuming that this queue only supports and we can devise the following algorithm to sort it:\n1. Make multiple passes through the queue by popping elements off the front and reinserting them on the back. During each pass, keep\ntrack of the smallest value you have seen so far. If you encounter an element that is the smallest you have seen so far, do not push it to the\nback until a smaller element is found or the pass completes.\n2. At the end of each pass, push the minimum value found to the back of the queue. This value is now in sorted order and should be ignored\nfor all future passes.\n3. Keep on repeating steps 1 and 2 until the queue becomes sorted.\nLet’s look at this algorithm in action:\nfront\nback\n2\n5\n3\n1\n4\nStore the first value in a separate variable that keeps track of the smallest value you have seen on the first pass. After doing so, pop this value off\nthe queue.\nfront\nback\n5\n3\n1\n4\n2\ncurr_min\nRetrieve the next value from the queue and compare it with the current minimum value encountered. In this case, 5 is not less than 2, so pop it\noff and push it to the back.\nfront\nback\n3\n1\n4\n5\n2\ncurr_min\nThe next value in the queue, 3, is also not less than 2, so it also gets popped off and pushed to the back.\nfront\nback\n1\n4\n5\n3\n2\ncurr_min\nThe next value in the queue is 1, which is less than 2. Because we found a new smallest element, we push 2 to the back of the queue and replace\ncurr_min with 1.\nfront\nback\n4\n5\n3\n2\n1\ncurr_min\nThe next value in the queue is 4, which is not less than 1. It gets popped off and pushed to the back.\nfront\nback\n5\n3\n2\n4\n1\ncurr_min", "word_count": 724, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "97ce1ba7-2142-5bcf-9c53-470ed958a65d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 265, "real_page_number": null, "text": "9.5 Solving Problems Using Stacks and Queues\n253\ncurr_minWe have now completed our first pass of the queue. Since is 1 at the end of this first pass, 1 must be the smallest element in the\nqueue. We push 1 to the back of the queue and keep its position fixed for the remainder of the algorithm.\nfront\nback\n4\n5\n3\n2\n1\ncurr_minNow, let’s start our second pass. Initialize to the first value encountered on the second pass, which in this case is 4.\nfront\nback\n5\n3\n2\n1\n4\ncurr_min\nThe next value in the queue, 5, is not less than 4. It gets popped off and pushed to the back.\nfront\nback\n3\n2\n1\n5\n4\ncurr_min\ncurr_minThe next value in the queue, 3, is less than 4. Thus, we push 4 to the back and replace with 3.\nfront\nback\n2\n1\n5\n4\n3\ncurr_min\ncurr_minThe next value in the queue, 2, is less than 3. Thus, we push 3 to the back and replace with 2.\nfront\nback\n1\n5\n4\n3\n2\ncurr_min\nThe next value in the queue, 1, has already been considered. Thus, we ignore it, and it gets popped off and pushed to the back.\nfront\nback\n5\n4\n3\n1\n2\ncurr_min\ncurr_minWe have finished our second pass of the queue. Since the value of is 2 at the end of this second pass, 2 must be the second smallest\nvalue in the queue. We push it to the back and fix its position.\nfront\nback\n5\n4\n3\n1\n2\nWe can continue this process, which is shown below (read from left to right first, then top to bottom). On the third pass, 3 gets fixed in position.\nThen, on the fourth pass, 4 gets fixed in position.\nfront\nback\n4\n3\n1\n2\n5\ncurr_min\nfront\nback\n3\n1\n2\n5\n4\ncurr_min\nfront\nback\n1\n2\n5\n4\n3\ncurr_min\nfront\nback\n2\n5\n4\n1\n3\ncurr_min\nfront\nback\n5\n4\n1\n2\n3\ncurr_min\nfront\nback\n1\n2\n5\n4\n3\ncurr_min\nfront\nback\n5\n4\n1\n2\n3\nfront\nback\n4\n1\n2\n3\n5\ncurr_min\nfront\nback\n1\n2\n3\n5\n4\ncurr_min\nfront\nback\n2\n3\n5\n1\n4\ncurr_min\nfront\nback\n3\n5\n1\n2\n4\ncurr_min\nfront\nback\n5\n1\n2\n3\n4\ncurr_min\nfront\nback\n1\n2\n5\n4\n3\ncurr_min\nfront\nback\n5\n1\n2\n3\n4", "word_count": 418, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "403ab70b-f43c-50b4-b213-fc9b69ed4e2f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 266, "real_page_number": null, "text": "254\nChapter 9. Stacks and Queues\nAt this point, 5 is the last element that hasn’t been fixed in position. Because of this, we know that 5 must be the largest element in the queue;\nwe can just pop it off and push it to the end.\nfront\nback\n1\n2\n3\n4\n5\nΘ(𝑛2).Our queue is now sorted. The time complexity of this algorithm is This is because we complete approximately 𝑛passes of the queue,\nwhere each pass makes 𝑛comparisons. As a result, the number of elements we check throughout the lifetime of the algorithm is 𝑛),Θ(𝑛×\nΘ(𝑛2).or The auxiliary space used by this algorithm is Θ(1). This is because we did the sorting in-place, using just the input queue itself to\ncomplete our algorithm. We did not have to allocate additional memory to solve this problem!\n¸ 9.5.4\nEvaluating Reverse Polish Notation\nExample 9.4 is a mathematical notation in which operators follow their operands. For example, if we wanted toReverse Polish notation\nadd together 1 and 2, reverse Polish notation would express this addition as \"1 2 +\" instead of \"1 + 2\". As another example, the expression\n\"(3 - 4) * 5\" is written as \"3 4 - 5 *\" in reverse Polish notation: the \"3 4 -\" indicates that 3 and 4 should be subtracted, and combining it with\n\"5 *\" indicates the this result should be multiplied by 5.\nSuppose you are given a valid arithmetic expression in reverse Polish notation (in the form of a vector of strings), with the following valid\n\"+\", \"-\", \"*\", \"/\".operators: and Write a function that evaluates the expression’s result. You may assume that the expression always\nevaluates to a solution, and there will never be a situation where you will divide by zero.\n[\"2\", \"3\", \"*\", \"4\", \"5\", \"*\", \"+\"],Example: Given you would return 26, since that is the result of (2 * 3) + (4 * 5).\nIn this problem, each operator acts upon the results of the operands that precede it (for example, the \"+\" in \"2 3 * 4 5 * +\" multiplies the result\nof \"2 * 3\" with the result of \"4 * 5\"). Thus, whenever we encounter an operator in our input, we would need to identify the values of the most\nrecent expressions encountered so far (e.g., to apply the \"+\" operator in our example, we would need to know the values of \"2 3 *\" and \"4 5 *\").\nA stack would therefore be a good container choice for solving this problem, since it provides this LIFO behavior for identifying these values.\nTo solve this problem, we will iterate over the input values. Every time we encounter a number, we would push it into a stack. Every time\nwe encounter an operator symbol, we would take out the two values at the top of the stack, apply the operator on these values, and push the result\nback into the stack. Once we reach the end of the input, the remaining value left in the stack must be the solution of our original expression.\n[\"2\", \"3\", \"*\", \"4\", \"5\", \"*\", \"+\"].Consider our example input of The first value is the number 2, so we push it onto a stack.\n2\nStack\nThe next value is the number 3, so we push it onto the stack.\n2\n3\nStack\nThe next value is the operator \"*\", which indicates that we should multiply the results of the previous two expressions. Thus, we will take out\nthe top two values in our stack (2 and 3), multiply them together, and push in the result of 6 back onto the stack.\n6\nStack\nThe next value is the number 4, so we push it onto the stack.\n6\n4\nStack", "word_count": 632, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c78593dd-9e9c-5991-80fe-15f35726d0e9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 267, "real_page_number": null, "text": "9.5 Solving Problems Using Stacks and Queues\n255\nThe next value is the number 5, so we push it onto the stack.\n6\n4\n5\nStack\nThe next value is the operator \"*\", which indicates that we should multiply the results of the previous two expressions. Thus, we will take out\nthe top two values in our stack (4 and 5), multiply them together, and push in the result of 20 back onto the stack.\n6\n20\nStack\n\"+\",The next value is the operator which indicates that we should add the results of the previous two expressions. Thus, we will take out the top\ntwo values in our stack (6 and 20), add them together, and push in the result of 26 back onto the stack.\n26\nStack\nThere are no more values to process in our input, so the remaining value in the stack, 26, must be our final solution.\nThis solution to the problem is implemented in the code below:\n1\n// helper function that applies an operator on two values and returns the result\n2\nint32_t apply_operation(int32_t int32_t chara, b, op) {\n3\nswitch (op) {\n4\ncase return'+': a + b;\n5\ncase return'-': a - b;\n6\ncase return'*': a b;*\n7\ncase return'/': a / b;\n8\ndefault: throw std::invalid_argument{\"Invalid operation\"};\n9\n} // switch\n10\n} // apply_operation()\n11\n12\nint32_t evaluate_reverse_polish_notation(std::vector<std::string>& tokens) {\n13\nstd::stack<int32_t> values;\n14\nfor (const std::string& token : tokens) {\n15\nif ((token == \"+\" || token == \"-\" || token == \"*\" || token == \"/\")) {\n16\nint32_t value1 = values.top();\n17\nvalues.pop();\n18\n19\nint32_t value2 = values.top();\n20\nvalues.pop();\n21\n22\nvalues.push(apply_operation(value2, value1, token[0]));\n23\n} // if\n24\nelse {\n25\nvalues.push(std::stoi(token));\n// stoi() converts string to integer\n26\n} // else\n27\n} // for token\n28\nreturn values.top();\n29\n} // evaluate_reverse_polish_notation()\nSince the algorithm traverses over each value in the input vector once, its time complexity is Θ(𝑛), where 𝑛is the size of the input vector. The\nauxiliary space used by the algorithm is also Θ(𝑛), due to the additional stack that we allocated to build up our solution (whose size depends on\nthe number of values we have to process).\nThere are certainly more problems that can be solved using stacks and queues. In fact, you will see these containers again when graphs and\nsearching algorithms are covered in chapter 19. You have probably already encountered several use cases where stacks may be useful throughout\nyour programming career: they play an important role in storing data during recursive calls, and they can be used by compilers and text editors to\ncheck the validity of parentheses. A queue is also a very important container type in programming. Just as a preview for what is to come, queues\ncan be used to discover the shortest path between two nodes of a graph (known as a breadth-first search). They also play a vital role in operating\nsystems, allowing your computer to manage CPU and disk usage for different system processes (which is beyond the scope of this class).", "word_count": 523, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4211449e-e92f-500f-bb46-009943a4d43c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 268, "real_page_number": null, "text": "256\nChapter 9. Stacks and Queues\n9.6\nThe STL Deque Container\nA deque (pronounced as \"deck\"), or a double-ended queue, is an STL container that supports both the functionality of a stack and a queue.\nUnlike a vector, where efficient insertion and deletion is only supported at the back of the container, a deque supports efficient insertion and\ndeletion operations on ends of the container.both\n.push_front()\n.pop_front()\n.push_back()\n.pop_back()\n(.push_front()According to the C++ standard, a deque must support time insertion and removal from both the front andΘ(1)\n.pop_front()) (.push_back() .pop_back()). operator[],andtheback and Dequesmustalsosupport randomaccessusingΘ(1)\nand insertion and deletion from either end of a deque must invalidate pointers or references to the rest of the elements in the container.never\nHow can a deque be implemented in memory? Since deques must support constant time insertion and deletion from both ends, a linked list\ntailimplementation may seem tempting at first (as lists support constant time insertion and deletion, assuming that a pointer is included).\nhead\n0\n1\n2\n3\n4\ntail\noperator[]However, this approach does not fully satisfy the C++ standard. This is because a linked list deque cannot support in time.Θ(1)\n𝑛thAs mentioned previously, lists do not support random access; if you want to retrieve the element of a list, you would have to iterate through\nall elements before it. A list is therefore unsuitable for implementing a deque that supports random access.𝑛−1\nIf linked lists do not work because they cannot provide random access, what about an array? Even though arrays cannot support constant\ntime insertions or deletions from the front, we can remove this problem by using a instead. Just like with our circular buffercircular array\nqueue implementation, we will use two pointers to keep track of the front and back of our deque. This allows us to remove and insert elements\nto the front of our deque without having to shift elements over in our underlying array.\nback_ptr\nfront_ptr\n2\n3\n4\n0\n1\nIt turns out that the circular array approach does not satisfy the C++ standard either. Even though an array provides random access in constant\ntime, pointer invalidation now becomes a concern. Recall that a deque cannot invalidate any pointers if values are inserted or deleted from the\nfront or back of the container. However, the circular array approach would occasionally require reallocation, which would cause all elements to\nmove to a new location in memory (and thus would invalidate existing pointers).\nItturnsoutthatthecomplexfunctionalityofdequesmakethemabitmoredifficulttoimplement. Wewilldiscussonecommonimplementation\nsection.3in this Behind the scenes, a deque is implemented as an array of pointers to other arrays, where each inner array has a constant size\n(usually a power of two). This is shown in the illustration below:\narray of pointers\nthese positions\nare allocated if\nadditional space\nis needed before\nthe first array or\nafter the last array\nundef\nundef\nundef\nundef\nundef\nundef\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nundef\nundef\n.push_back()\n.push_front()\ndeque index 0\ninner array 0\ninner array 1\ninner array 2\ninner array 3\nThe outer array stores pointers to constant-size inner arrays, and the inner arrays store the deque’s data. The behavior of the inner arrays change\ndepending on whether the array is at the front or back of the deque. For the last array in the deque, data is added toward the back of the array\n12 11(e.g., if were pushed to the back of the deque, it would be added to the position directly after in inner array 3). For the first array in the\n12 0).deque, data is added toward the front of the array (e.g., pushing to the front of the deque would add it to the position directly before\n3Theimplementationofadequemaybedifferentacrosslibraries,aslongastheimplementationadherestotheC++standardsforadeque. Theimplementation\ncoveredinthissectionisusedbytheGCCstandardlibraryimplementation.", "word_count": 694, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3a6d76d6-e1b7-5919-960d-11dbbf7539b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 269, "real_page_number": null, "text": "9.6 The STL Deque Container\n257\nIf either the inner array at the beginning or end is fully filled, a new inner array is allocated at the next available position of the outer array. For\n12, 13, 14instance, the deque would look like this if and were pushed to the back:\narray of pointers\nthese positions\nare allocated if\nadditional space\nis needed before\nthe first array or\nafter the last array\nundef\nundef\nundef\nundef\nundef\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nundef\nundef\nundef\n.push_back()\n.push_front()\ndeque index 0\ninner array 0\ninner array 1\ninner array 2\ninner array 3\ninner array 4\nIf the entire outer array fills up (i.e., there is no space to allocate new inner arrays at either end), then the outer array is reallocated to a larger\ncapacity. This is very similar to how a vector behaves when its underlying array fills up. However, unlike a vector, the additional space from\nreallocation is split between both ends of the deque’s outer array. This allows insertion to be a constant time operation on both ends of the deque.\nThis implementation of a deque solves both of the problems we had when using a circular array and a linked list. Pointer invalidation is no\nlonger an issue when inserting or deleting elements from either end of the container: since the inner arrays do not get reallocated, elements in the\ndeque stay at the same memory location throughout its lifetime (assuming that they do not get deleted or moved). This layout in memory also\nallowsthedequetosupport randomaccess,whichwasnotpossiblewithalinkedlist. Givenanindex,wecanquicklyfindthecorrespondingΘ(1)\nelement in the deque using some simple arithmetic. For instance, suppose we wanted to retrieve the element at index 7 of the following deque:\narray of pointers\nthese positions\nare allocated if\nadditional space\nis needed before\nthe first array or\nafter the last array\nundef\nundef\nundef\nundef\nundef\nundef\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nundef\nundef\n.push_back()\n.push_front()\ndeque index 0\ninner array 0\ninner array 1\ninner array 2\ninner array 3\nTo accomplish this, we first add the index we want (in this case, 7) to the number of unused slots in the first inner array: 7 + 2 = 9. Then,\n• Dividing this result by the capacity of each inner array gives us the array that the target value is in.\n– In this case, we get 9 / 4 = 2 using integer division (decimals are truncated). This means the element at index 7 is located within\ninner array 2.\n• Taking the modulo of the original result with the capacity of the inner array gives us the position of the element within its inner array.\n– In this case, we get 9 % 4 = 1. This means the element at index 7 is located at index 1 of its inner array.", "word_count": 511, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1ff2ff10-575b-5c53-814d-076a9c3e7509", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 270, "real_page_number": null, "text": "258\nChapter 9. Stacks and Queues\nThus, the element at index 7 of the provided deque can be found at index 1 of inner array 2 (using zero indexing):\narray of pointers\nthese positions\nare allocated if\nadditional space\nis needed before\nthe first array or\nafter the last array\nundef\nundef\nundef\nundef\nundef\nundef\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nundef\nundef\n.push_back()\n.push_front()\ndeque index 0\ninner array 0\ninner array 1\ninner array 2\ninner array 3\nstd::deque<>. std::deque<>,The STL provides an implementation of a deque for you, defined as a To use a you will need to include\n<deque>the library at the beginning of your code. A summary of deque operations is shown below:\ntemplate <typename T>\nstd::deque<T>();\nT,Default constructor for deque that holds elements of type creates an empty deque without any elements.\n// initializes an empty deque of integers\nstd::deque<int32_t> d1;\ntemplate <typename T>\nstd::deque<T>(std::initializer_list<T> init);\nInitializes the deque with the contents of the initializer list.\n// initializes d2 and d3 to have contents of {1, 2, 3}\nstd::deque<int32_t> d2{1, 2, 3};\nstd::deque<int32_t> d3 = {1, 2, 3};\ntemplate <typename T>\nstd::deque<T>(const std::deque<T>& other);\notherCopy constructor, copies the contents of into the constructed deque.\n// initializes d4 with the contents of d3, or {1, 2, 3}\nstd::deque<int32_t> d4{d3};\ntemplate <typename T>\nstd::deque<T>(size_t sz);\nsz sz.Creates a deque of elements, where each element is value initialized; size equal to\n// initializes d5 as a deque with size 5, each element initialized to 0\nstd::deque<int32_t> d5(5);\ntemplate <typename T>\nstd::deque<T>(size_t sz, T& val);\nsz val; sz.Creates a deque of elements, where each element is initialized to a value of size equal to\n// initializes d6 as a deque with size 5, each element initialized to 1\nstd::deque<int32_t> d6(5, 1);\ntemplate <typename typenameT, InputIterator>\nstd::deque<T>(InputIterator begin_iter, InputIterator end_iter);\n[begin_iter, end_iter)Creates a deque with all elements in the iterator range — inclusive begin but exclusive end. Both\nbegin_iter end_iterand are input iterators.\n// initializes d7 with the first two elements of d3, or {1, 2}\nstd::deque<int32_t> d7(d3.begin(), d3.begin() + 2);", "word_count": 358, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "799c8ee5-27b8-5f4e-9376-8b1f2a7dfc7f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 271, "real_page_number": null, "text": "9.6 The STL Deque Container\n259\ntemplate <typename T>\nsize_t std::deque<T>::size();\nReturns the number of elements in the deque.\ntemplate <typename T>\nbool std::deque<T>::empty();\nReturns whether the deque is empty.\ntemplate <typename T>\nT& std::deque<T>::front()\nReturns a reference to the first element (undefined behavior if deque is empty).\ntemplate <typename T>\nT& std::deque<T>::back()\nReturns a reference to the last element (undefined behavior if deque is empty).\ntemplate <typename T>\nvoid std::deque<T>::push_back(const T& val);\nvalPushes to the back of the deque, increasing size by 1.\ntemplate <typename typename...T, Args>\nT& std::deque<T>::emplace_back(Args&&... args);\nInserts a new element at the back of the deque, right after the current last element. The new element is constructed in place using arguments\n(args).for its constructor Returns a reference to the emplaced element since C++17.\ntemplate <typename T>\nvoid std::deque<T>::pop_back();\nRemoves the last element in the deque, reducing size by 1 (undefined behavior if deque is empty).\ntemplate <typename T>\nvoid std::deque<T>::push_front(const T& val);\nvalPushes to the front of the deque, increasing size by 1.\ntemplate <typename typename...T, Args>\nT& std::deque<T>::emplace_front(Args&&... args);\nInsertsanewelementatthefrontofthedeque,rightbeforethecurrentfirstelement. Thenewelementisconstructedinplaceusingarguments\n(args).for its constructor Returns a reference to the emplaced element since C++17.\ntemplate <typename T>\nvoid std::deque<T>::pop_front();\nRemoves the first element in the deque, reducing size by 1 (undefined behavior if deque is empty).\ntemplate <typename T>\nconstiterator std::deque<T>::insert(iterator position, T& val);\nval posInserts directly before the element pointed to by the iterator and returns an iterator to the newly added element.\ntemplate <typename T>\nsize_t constiterator std::deque<T>::insert(iterator position, n, T& val);\nn val posInserts copies of directly before the element pointed to by the iterator and returns an iterator to the first new element added.\ntemplate <typename typenameT, InputIterator>\niterator std::deque<T>::insert(iterator position, InputIterator first, InputIterator last);\n[first, last) posInserts a copy of all elements in the iterator range directly before the iterator and returns an iterator to the first new\nelement added.\ntemplate <typename T>\niterator std::deque<T>::insert(iterator position, std::initializer_list<T> init);\nposInserts the elements in the initializer list into the deque directly before the iterator and returns an iterator to the first new element added.\ntemplate <typename typename...T, Args>\niterator std::deque<T>::emplace(const_iterator pos, Args&&... args);\npos.Insertsanewelementdirectlybeforetheelementatposition Thenewelementisconstructedinplaceusingargumentsforitsconstructor\n(args). An iterator pointing to the emplaced object is returned.\ntemplate <typename T>\niterator std::deque<T>::erase(iterator pos);\nposErases the element pointed to by the iterator and returns an iterator to the element following the one that was erased.\ntemplate <typename T>\niterator std::deque<T>::erase(iterator first, iterator last);\n[first, last)Erases all elements in the iterator range and returns an iterator to the element following the last element that was erased.\ntemplate <typename T>\nvoid std::deque<T>::clear();\nRemoves all elements in the container, leaving it with a size of 0.\ntemplate <typename T>\nstd::deque<T>::operator[](size_tT& n);\nnReturns a reference to the element at index of the deque.", "word_count": 516, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d51c5c04-fe2f-59ab-8922-de8595a2ce3f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 272, "real_page_number": null, "text": "260\nChapter 9. Stacks and Queues\n.begin()The iterator operations for a deque are similar to those of a vector, where returns an iterator to the beginning of the deque, and\n.end() returns an iterator one past the end of the deque. Deques also support reverse and constant iterators, which can be retrieved using\n.rbegin(), .rend(), .cbegin(), .cend().and\nFunction\nBehavior\n.begin()\nReturns a random access iterator to the first element in the deque\n.end()\nReturns a random access iterator to the position one past the last element in the deque\n.cbegin()\nReturns a random access iterator to the first element in the dequeconstant\n.cend()\nReturns a random access iterator to the position one past the last element in the dequeconstant\n.rbegin()\nReturns a iterator to the last element in the dequereverse\n.rend()\nReturns a iterator to the position one before the first element in the dequereverse\n.crbegin()\nReturns a constant reverse iterator to the last element in the deque\n.crend()\nReturns a constant reverse iterator to the position one before the first element in the deque\nDequesaresimilartovectorsinthatinsertionsanddeletionsfromthemiddleofthecontainerrequireelementstobeshiftedafterthemodification\npoint (to open up a space for insertion, or to bridge a gap for deletion). However, in contrast to a vector, a deque’s reallocation may exhibit better\noverall performance. In a vector, reallocation forces all data in the original array to be copied to a new, larger array. For a deque, reallocation is\nonly done on the outer array of pointers, so only the contents of this outer array are copied during reallocation. This is because the actual data\nresides in the inner arrays, which have fixed capacity and do not need to be reallocated.\noperator[] operator[]However, even if reallocation may be more efficient for a deque, is faster for a vector. Despite the fact that is\na operation for both vectors and deques, the constant term for a deque is larger. This is because a vector only needs to perform a simpleΘ(1)\narr[i] == *(arr + i)),addition to access a value at any index (i.e., but deques require a bit more math to get the memory address of the\nelement we want. In addition, elements in a deque are not always contiguous in memory (as shown by the illustrations in this section). As a\nresult, iteration through a deque is typically slower than iteration through a vector. This is because vectors are better at exploiting caching: a\nphenomenon that allows elements in a sequence to be accessed faster if they are closer together in memory.\n9.7\nContainer Adaptors\nVectors, lists, and deques fall into a category of containers known as containers, whose data can be sequentially accessed in ansequence\nestablished order. On the other hand, stacks and queues (and priority queues, which will be covered in the next chapter) belong to a class of\ncontainers known as container adaptors. Container adaptors are not full container classes on their own, but rather interfaces that are adapted\nfor specific needs by modifying or restricting the functionality of a standard container. In the STL, container adaptors do not support iteration.\nAt the beginning of this chapter, we looked at how stacks and queues can be implemented using arrays and linked lists. However, by default,\nthe STL implementations of stacks and queues use neither approach. Instead, they are implemented using a deque as the underlying container.\nstd::stack<> std::deque<>When you initialize a in your program, you are actually getting a that limits data access to only one end.\nstd::queue<>, std::deque<>Similarly, if you initialize a you are actually getting a that enforces insertion and deletion on different\nends. In other words, both the STL stack and queue are built upon an underlying deque that is \"adapted\" to support only the operations that suit\nthe interface of the desired container.\nThus, if you want to use a stack or a queue in your program, there is functionally no difference between initializing a stack or queue and\ninitializing a deque, as a deque can be used to emulate the functionality of both stacks and queues. If you want a stack, you can initialize a\n.push_back() .pop_back(). .push_back()deque and only use and If you want a queue, you can initialize a deque and only use and\n.pop_front(). However, it is generally good practice to explicitly declare the container type that matches your needs (i.e., either a stack or\na queue, rather than a deque), as it improves the readability of code and reduces the likelihood of mistakes (such as inserting or removing data\nfrom the wrong end); a deque should only be chosen if you need double-ended queue behavior that is not supported by a regular stack or queue.\nAdequeisnottheonlyunderlyingcontainerthatcanbeused. Youcanalsoinitializeastackoraqueueusingadifferentunderlyingcontainer,\n.push_back() .pop_back(),such as a vector or a list. The underlying container of a stack must support and while the underlying\n.push_back() .pop_front().container of a queue must support and A table of possible underlying containers is shown below:\n(std::stack<>)STL Stack\n(std::queue<>)STL Queue\nDefault Underlying Container\nstd::deque<> that only permits\n.push_back() .pop_back()and\nstd::deque<> that only permits\n.push_back() .pop_front()and\nOptional Underlying Container\nstd::list<> that only permits\n.push_back() .pop_back()and\nstd::list<> that only permits\n.push_back() .pop_front()and\n(must be explicitly specified)\n— OR —\nstd::vector<> that only permits\n.push_back() .pop_back()and\nstd::queue<> std::vector<>Notice that a cannot be initialized with a as the underlying container. This is because vectors do not\n.pop_front()support (and thus cannot provide efficient insertions/removals at different ends of the container).\nTo explicitly specify the underlying container for a stack or a queue, you will need to include the container type directly after the type of the\ndata. For example, the following line of code initializes a stack that uses a vector as its underlying container:\nstd::stack<int32_t, std::vector<int32_t>> s;\nSimilarly, the following code initializes a queue that uses a list as its underlying container:\nstd::queue<int32_t, std::list<int32_t>> q;\nstd::deque<>If you do not explicitly specify the underlying container, a will be used by default. In this class, there is really no reason to\never use anything other than a deque as the underlying container of a stack or a queue. For stacks and queues, a deque is the ideal underlying\ncontainer for most use cases — hence why the STL uses it as the default!", "word_count": 1089, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5330c60c-fb11-535b-adcb-d63741058bad", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 273, "real_page_number": null, "text": "9.7 Container Adaptors\n261\nChapter 9 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\nstd::deque<> std::vector<>?1. Which of the following statements describes a benefit of using a over a\nA) Deques store their internal data contiguously in memory, while vectors do not\noperator[]B) Using is typically faster on a deque than a vector\nC) Deques allow values to be efficiently added to both ends of the container, while vectors only support efficient insertion on one end\nD) The worst-time complexity of finding an arbitrary element is better with a deque than with a vector\nE) More than one of the above\n2. Consider the following snippet of code:\n1\nint main () {\n2\nstd::stack<int32_t> s;\n3\ns.push(12);\n4\ns.push(15);\n5\ns.push(18);\n6\ns.push(21);\n7\ns.push(24);\n8\ns.pop();\n9\nstd::cout << s.top() << '\\n';\n10\n} // main()\nWhat does this code output?\n12A)\n15B)\n18C)\n21D)\n24E)\n3. Which of the following is NOT a disadvantage of using a linked list over an array to implement a stack?\nA) The time complexity of pushing in an element is for an array and for a linked listΘ(𝑛)Θ(1)\nB) If you do not internally track the size of a linked list, the time complexity of finding the size of your stack becomes Θ(𝑛)\nC) Using a linked list is less efficient than using an array, as a linked list must allocate memory for each node individually\nD) Using a linked list results in higher memory overhead\nE) None of the above\n4. Which of the following real-life situations is most similar to how a queue works?\nA) Looking for a book on a shelf in the library, where books are sorted alphabetically by title\nB) Selecting an open seat in the front of the classroom on the first day of class\nC) Searching for an exam room that is assigned by student ID\nD) Waiting in line to get seated at a restaurant\nE) Looking through the midterm exam and answering questions in order of increasing difficulty\nstd::queue<>5. What is the best possible complexity of removing the most recently added element in a of size 𝑛and returning thetime\nqueue with all of the other elements in their original order?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\nstd::queue<>6. What is the best possible complexity of removing the most recently added element in a of size 𝑛andauxiliary space\nreturning the queue with all of the other elements in their original order?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n7. Suppose you are using a circular buffer with an array to implement a queue. In your implementation, you store two pointers: a front pointer\nthat points to the first element in the queue and a back pointer that points one past the last element in the queue. These pointers wrap around\nthe array as needed. Which of the following indicates that your circular buffer is full, and that you should reallocate your data to a larger array?\nA) Incrementing the front pointer causes it to point one past the end of the array\nB) Incrementing the back pointer causes it to point one past the end of the array\nC) Incrementing the front iterator causes it to point to the same location as the back pointer\nD) Incrementing the back iterator causes it to point to the same location as the front iterator\nE) More than one of the above", "word_count": 625, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "2b2c8e15-bb1b-5726-a9d7-87a1fa79c5ab", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 274, "real_page_number": null, "text": "262\nChapter 9. Stacks and Queues\n8. Consider the following snippet of code:\n1\nint main () {\n2\nMYSTERY_CONTAINER<int32_t> eecs;\n3\neecs.push(203);\n4\neecs.push(370);\n5\neecs.push(281);\n6\nstd::cout << eecs.top() << \", \";\n7\neecs.pop();\n8\neecs.push(280);\n9\neecs.push(376);\n10\neecs.push(183);\n11\nstd::cout << eecs.top() << \", \";\n12\neecs.pop();\n13\nstd::cout << eecs.top();\n14\neecs.pop();\n15\n} // main()\n\"281, 183, 376\", eecs?You are told that this code compiles. If the output of this code is what is a valid container type for\nstd::deque<int32_t>A)\nstd::stack<int32_t>B)\nstd::queue<int32_t>C)\nstd::priority_queue<int32_t>D)\nE) None of the above\nTwo retail companies, Darget and Paolmart, keep track of their inventory in different ways. Darget tracks its inventory using the FIFO (first9.\nin, first out) method, assuming that the goods added to inventory first are also the first removed from inventory for sale. Paolmart tracks its\ninventory using the LIFO (last in, first out) method, assuming that the goods added to inventory last are the first removed from inventory for\nsale. The two companies want to use an STL data structure to efficiently keep track of their inventories, and they come to you for advice on\nwhich data structure to choose. What should you tell these two companies?\nA) Darget should use a queue, and Paolmart should use either a stack or a vector\nB) Darget should use a stack, and Paolmart should use a queue or a vector\nC) Paolmart should use a queue, and Darget should use either a stack or a vector\nD) Paolmart should use a stack, and Darget should use either a queue or a vector\nE) Both Darget and Paolmart can efficiently use stacks, queues, and vectors to track their inventory\n10. Which of the following data structures CANNOT be efficiently used as the underlying container for a queue?\nstd::vector<>I.\nstd::list<>II.\nstd::deque<>III.\nA) I only\nB) II only\nC) III only\nD) I and II only\nE) I and III only\n11. Which of the following statements is TRUE about container adaptors?\nA) Container adaptors rely on the functionality of a standard container (vector, list, deque, etc.) to provide a specific interface\nB) Stacks and queues are the only two container adaptors that exist in the STL library\nC) Container adaptors can only be used on containers that provide constant time random access\nD) Container adaptors will always support the full functionality of their underlying container\nE) More than one of the above\n12. If you were using a deque in place of a queue in your program, which of the following statements is/are TRUE?\n.push_front() .pop_front()A) If you use to add data to your queue, you should use to remove data from the queue\n.push_back() .pop_back()B) If you use to add data to the queue, you should use to remove data from the queue\n.push_back() .pop_front()C) If you use to add data to the queue, you should use to remove data from the queue\n.push_front()D) should never be used on your deque if you want to emulate the behavior of a queue\nE) More than one of the above\n13. Which of the following operations CANNOT always be done on a deque in time?Θ(1)\nA) Inserting an element at the front of the deque\nB) Inserting an element in the middle of the deque\nC) Inserting an element at the back of the deque\nD) Removing an element at the front of the deque\nE) Removing an element at the back of the deque", "word_count": 589, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "40dd4d1e-df4d-56e0-b44f-2c11b07e3a0d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 275, "real_page_number": null, "text": "9.7 Container Adaptors\n263\n14. WhichofthefollowingisNOTadownsideofusingadoubly-linkedlisttoimplementadequeinsteadofthesegmentedfixed-sizearray-based\napproach discussed in section 9.6 (replicated below)?\narray of pointers\nthese positions\nare allocated if\nadditional space\nis needed before\nthe first array or\nafter the last array\nundef\nundef\nundef\nundef\nundef\nundef\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nundef\nundef\n.push_back()\n.push_front()\ndeque index 0\ninner array 0\ninner array 1\ninner array 2\ninner array 3\nA) Without a tail pointer, the linked list deque cannot be used as the underlying container for a stack that supports time push and popΘ(1)\nB) Without a tail pointer, the linked list deque cannot be used as the underlying container for a queue that supports time push and popΘ(1)\noperator[]C) The linked list deque cannot support timeΘ(1)\nD) The linked list deque may consume more memory due to the need to store two additional pointers for each element in the container\nE) None of the above\n15. If the integers 1, 2, 3, and 4 are pushed into a queue in this order and then popped out one at a time, in what order will they be removed?\nA) 1, 2, 3, 4\nB) 1, 2, 4, 3\nC) 4, 3, 2, 1\nD) 4, 3, 1, 2\nE) None of the above\n16. Which of the following is a computational application of a queue?\nA) Queues can be used by a program to simulate recursive calls\nB) Queues can be used by an internet browser to store webpage data for the back and forward buttons\nC) Queues can be used by a compiler to check for matching braces in a program’s code\nD) Queues can be used by the operating system to allocate scarce resources among different running processes\nE) None of the above\n17. Consider the function:\nT return_fifth_element(std::stack<T>& s);\nwhich returns the fifth oldest element in a stack. For example, if a stack has five elements, the fifth oldest element is the element at the\nstd::stack<>top of the stack. The implementation of this function only uses other public methods provided in the interface, and it\ndoes not access any of the stack’s internal data structures directly. When the function returns, the stack must contain the same elements as\norder. The fifth oldest element is NOT deleted after the function’s completion; only its value is returned. If there arebefore, in the same\nfewer than five elements, the newest element in the stack is returned. What is the worst-case time and auxiliary space complexities for the\nreturn_fifth_element() function, if the most efficient implementation is used? Let 𝑛represent the size of the input stack.\nA) Runtime: Θ(1), Space: Θ(1)\nB) Runtime: Θ(1), Space: Θ(𝑛)\nC) Runtime: Θ(𝑛), Space: Θ(1)\nD) Runtime: Θ(𝑛), Space: Θ(𝑛)\nE) None of the above", "word_count": 489, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4f308bb1-a9a6-57aa-a45e-cf1e9936111c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 276, "real_page_number": null, "text": "264\nChapter 9. Stacks and Queues\n18. Consider the following snippet of code:\n1\nstruct MysteryContainer {\n2\nstd::queue<int32_t> q;\n3\n4\nvoid push(int32_t x) {\n5\nq.push(x);\n6\nfor (int32_t i = 0; i < q.size() - 1; ++i) {\n7\nq.push(q.front());\n8\nq.pop();\n9\n} // for i\n10\n} // push()\n11\n12\nvoid pop() { q.pop(); }\n13\nint32_t& returntop() { q.front(); }\n14\nbool returnempty { q.empty(); }\n15\n};\nThe above code uses a queue to implement the behavior of which data structure?\nA) Queue\nB) Stack\nC) Priority Queue\nD) Deque\nE) None of the above\nMysteryContainerConsider the implemented in the code from the previous question. Suppose you wanted to push 𝑛elements into19.\nMysteryContainer. MysteryContainer?the What is the time complexity of pushing these 𝑛elements into the\nA) Θ(1)\nB) Θ(𝑛)\nΘ(𝑛2)C)\nΘ(𝑛3)D)\nE) None of the above\n20. Which one of the following statements is TRUE?\nstd::deque<> std::stack<>,A) If a is used as the underlying container for a then inserting an element into the middle of the\nstack takes timeΘ(1)\nstd::vector<> std::stack<>,B) If a is used as the underlying container for a then inserting an element into the middle of the\nstack takes timeΘ(1)\nstd::vector<> std::queue<>,C) If a is used as the underlying container for a then inserting an element into the middle of the\nqueue takes timeΘ(1)\nstd::list<> std::stack<>,D) If a is used as the underlying container for a then inserting an element into the middle of the\nstack takes timeΘ(1)\nE) None of the above\n21. Consider the following code:\n1\nvoid mystery_function(std::queue<int32_t>& q) {\n2\nstd::stack<int32_t> s;\n3\nwhile (!q.empty()) {\n4\nint32_t x = q.front();\n5\nq.pop();\n6\ns.push(x);\n7\n} // while\n8\nwhile (!s.empty()) {\n9\nint32_t y = s.top();\n10\ns.pop();\n11\nq.push(y);\n12\n} // while\n13\n} // mystery_function()\nmystery_function()What does do?\nA) It pops off the element at the back of the input queue\nB) It pops off the element at the front of the input queue\nC) It removes all elements in the input queue\nD) It reverses the input queue\nE) None of the above\n22. Which one of the following statements is most accurate about the STL stack container?\nstd::stack::push()A) The member function will remove data off the top of the stack\nB) Elements are added to and removed from a stack in first in, first out (FIFO) order\nstd::stack::pop()C) The member function can be used to directly retrieve data from a stack\nstd::stack::back()D) The member function allows a user to access the oldest element in the stack\nE) Stacks can be implemented using either a vector or a linked list as its underlying container", "word_count": 464, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bd39695f-ea44-5bcc-815e-8a1925fbda6f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 277, "real_page_number": null, "text": "9.7 Container Adaptors\n265\nstd::stack<> std::queue<>,23. If you want a container that only needs to support the functionality of a or which of the following are\nstd::deque<>?valid reasons why it may be preferable to select one of these two containers instead of a\nI. Stacks and queues typically perform faster than deques, since they are container adaptors that restrict the interface of their\nunderlying container, and thus do not need to support other time consuming operations\nstd::stack<> std::queue<>II. If you only need the behavior of a stack or queue, explicitly using a or instead of a\nstd::deque<> better conveys your intent and may make your code easier to understand\nIII. Because they are container adaptors that restrict the interface of their underlying container, stacks and queues may be safer to\nuse since they are less prone to programming mistakes (such as popping or pushing from the wrong end)\nA) II only\nB) I and II\nC) I and III only\nD) II and III only\nE) I, II, and III\n24. You want to reverse a singly-linked list. If you are only allowed to use one of these STL data structures in addition to the list you want to\nreverse, which of the following data structures is least useful?\nA) Stack\nB) Queue\nC) Deque\nD) Vector\nE) All of the above data structures are equally useful\nstd::queue::back()25. The STL’s queue supports the member function, which returns a reference to the last element in the queue in\nstd::queue<>time. Knowing this, what is the time complexity of removing the most recently added element in a of size 𝑛?Θ(1)\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n26. Suppose you need a container that optimizes for speed of sequential access over a fixed set of elements. Which of the following STL\ncontainers would be the best to use?\nstd::vector<>A)\nstd::deque<>B)\nstd::stack<>C)\nstd::queue<>D)\nstd::list<>E)\n27. Which of the following STL data structures can be used to efficiently simulate the behavior of a recursive algorithm without needing a\nfunction that makes a recursive call itself?\nI. Vector\nII. Queue\nIII. Stack\nA) II only\nB) III only\nC) I and III only\nD) II and III only\nE) I, II, and III\n28. Which of the following statements is FALSE regarding stacks and queues?\nA) The STL stack interface gives direct access to only the most recently inserted element, and not everything else inside the stack\nB) A vector can be efficiently used as an underlying container for implementing a stack\nC) A deque can be used to simulate the behavior of both stacks and queues\n.size()D) If a singly-linked list is used to implement a queue, then the member function would always take timeΘ(𝑛)\nE) None of the above (i.e., all of the above statements are true)\n29. A queue is implemented using a non-circular singly-linked list that only supports a head and tail pointer. Let 𝑛be the number of nodes in the\n.push() .pop()queue. The operation is implemented by inserting a new node at the head of the list, and the operation is implemented\n.push() .pop()by deleting the node at the tail of the list. What is the time complexity of and for this queue, if the most efficient\nimplementation is used?\nA) Push: Θ(1), Pop: Θ(1)\nB) Push: Θ(1), Pop: Θ(𝑛)\nC) Push: Θ(𝑛), Pop: Θ(1)\nD) Push: Θ(𝑛), Pop: Θ(𝑛)\nE) None of the above", "word_count": 582, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "daa8b081-98b9-50f7-a9a5-67e496261420", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 278, "real_page_number": null, "text": "266\nChapter 9. Stacks and Queues\nfront_idx back_idx30. Suppose you are implementing a queue using a circular buffer, using two pointers and (with the implementation\nas defined in section 9.3, and replicated below).\narr\n1\n2\n3\nfront_ptr\nback_ptr\narrIf the underlying array has a size of 16, which of the following configurations is NOT possible?\nfront_idx arr[9] back_idx arr[9]A) points to and points to\nfront_idx arr[14] back_idx arr[11]B) points to and points to\nfront_idx arr[15] back_idx arr[16]C) points to and points to\nfront_idx arr[5] back_idx arr[8]D) points to and points to\nE) All of the above configurations are possible\n31. Consider the map below:\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nYou want to find a path from your starting position (S) to your house (H). Cells marked with a # are walls that are inaccessible. Assume\nstd::stack<>that you use a stack-based pathfinding scheme and add non-visited cells into a in the following direction order until you\nreach H: north, east, south, west. Which of the following correctly gives the order in which the cells labeled 𝑤, 𝑥, 𝑦, and 𝑧are encountered?\nFor this problem, consider a cell as encountered when it is first pushed onto the pathfinding stack, and assume the search is terminated once\nH is encountered.\nA) 𝑤,𝑥,𝑦,𝑧\nB) 𝑥,𝑦,𝑤,𝑧\nC) 𝑥,𝑤,𝑦,𝑧\nD) 𝑦,𝑧,𝑥,𝑤\nE) Not all of 𝑤, 𝑥, 𝑦, and 𝑧are encountered before the destination (H) is found\n32. Which of the following statements is/are TRUE?\nstd::queue<> std::deque<>I. By default, a uses a as its underlying container\nstd::stack<> std::vector<>II. By default, a uses a as its underlying container\nstd::queue<> std::list<>III. One difference between a that uses a as its underlying container versus one that uses a\nstd::vector<> is that the list-based queue does not support time random access, but the vector-based queue doesΘ(1)\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) II and III only\n33. Given two data structures of the same type and size, we define the two data structures as the process of creating a third datainterleaving\nstructure of the with double the size, and where popping elements interleaves the result of popping from each of the two datasame type\nstructures individually. For example, if popping from container A until it’s empty yields [4, 8, 10], and popping from container B until it’s\nempty yields [5, 9, 3], then interleaving A and B would produce a container that yields [4, 5, 8, 9, 10, 3] when values are popped. If you are\nNOT allowed to push elements into the two containers you want to interleave (i.e., A and B), which of the following statements is TRUE?\nstd::vector<>I. Interleaving two containers of size 𝑛can be done in timeΘ(𝑛)\nstd::stack<>II. Interleaving two containers of size 𝑛can be done in timeΘ(𝑛)\nstd::queue<>III. Interleaving two containers of size 𝑛can be done in timeΘ(𝑛)\nA) I only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III", "word_count": 555, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e693dbac-7e0a-58a5-b8b1-ec22a1c5011e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 279, "real_page_number": null, "text": "9.7 Container Adaptors\n267\nQueueWithStacks34. Implement the following templated class, which represents a queue using two stacks. The following member\nfunctions should be implemented:\n.size():• returns the size of the queue time\n.empty():• returns true if the queue is empty and false otherwise\n.push():• inserts an element at the back of the queue\n.pop():• removes an element at the front of the queue\nstd::stack<> .size(),.empty(),.push(), .pop().Youmayuseany methods,including and However,youareNOTallowed\nstd::queue<>to use in any capacity.\n1\ntemplate <typename T>\n2\nclass QueueWithStacks {\n3\nprivate:\n4\n// These two stacks will be used to simulate the behavior of a queue\n5\n// You may add other member variables here\n6\nstd::stack<T> stack1;\n7\nstd::stack<T> stack2;\n8\n9\npublic:\n10\n// These methods should be implemented (as efficiently as possible under the constraints)\n11\nsize_t constsize() {\n12\n13\n}\n14\n15\nbool constempty() {\n16\n17\n}\n18\n19\nvoid push(const T& val) {\n20\n21\n}\n22\n23\nvoid pop() {\n24\n25\n}\n26\n};\nBrowserTab homepageImplement the following class, which simulates a browser tab. You will start on the given (initialized in the35.\nurlconstructor), and you can visit different strings. You are also able to go back or move forward in the URL history a given number of\nsteps. Feel free to add any member variables that you may find useful.\n1\nclass BrowserTab {\n2\nprivate:\n3\n// Add any member variables that you may find useful here\n4\n5\npublic:\n6\n// This constructor inits the BrowserTab with the homepage of the browser\n7\nBrowserTab(const std::string& homepage) {\n8\n9\n}\n10\n11\n// This function visits the given url from the current page\n12\n// Calling this function clears up all the forward history.\n13\nvoid visit(const std::string& url) {\n14\n15\n}\n16\n17\n// This function moves back in history a total of \"steps\" steps\n18\n// (or as far back as possible if the number of steps specified\n19\n// exceeds the history length) - return the url you end on after\n20\n// moving back in history\n21\nback(uint32_tstd::string steps) {\n22\n23\n}\n24\n25\n// This function moves forward in history a total of \"steps\" steps\n26\n// (or as far forward as possible if the number of steps specified\n27\n// exceeds the history length) - return the url you end on after\n28\n// moving forward in history\n29\nforward(uint32_tstd::string steps) {\n30\n31\n}\n32\n};", "word_count": 434, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d6ad888a-ef6c-548c-919f-57c5cd07aa6e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 280, "real_page_number": null, "text": "268\nChapter 9. Stacks and Queues\nflip()36. You are given a stack of integers. Implement the following method, which flips the order of the elements in the input stack using\nand no other STL or C-style container (although primitive variables are okay).only one auxiliary stack\nvoid flip(std::stack<int32_t>& input);\nΘ(𝑛2)You should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of the input stack.Θ(𝑛)\nSuppose you are given a vector of integers and a number 𝑘, and you consider 𝑘or more copies of the same value in a row to constitute37.\na \"group\". You need to sum up any values that are left over after forming and removing groups; the values left over are known as the\nremnants. For example, given and the following vector:𝑘=3\n[3, 5, 5, 2, 2, 2, 5, 5, 5, 2, 3, 4, 4, 4, 3]\nStarting from the left, there is only one copy of 3 (not a group since there are less than copies of this value), followed by two 5’s (also𝑘=3\nnot a group). These are followed by three 2’s, which do form a group, so they are removed from the list:\n2, 2, 2,[3, 5, 5, 5, 5, 5, 2, 3, 4, 4, 4, 3]\n[3, 5, 5, 5, 5, 5, 2, 3, 4, 4, 4, 3]\nNow, there are five copies of the value 5, which do form a group (since the number of 5’s is greater than 3). Thus, these 5’s are removed𝑘=\nfrom the list, and the remaining values to be considered are:\n5, 5, 5, 5, 5,[3, 2, 3, 4, 4, 4, 3]\n[3, 2, 3, 4, 4, 4, 3]\nThe cluster of 4’s is a group, so removing these leaves us with:\n4, 4, 4,[3, 2, 3, 3]\n[3, 2, 3, 3]\nEventhoughthereare𝑘copiesofthevalue3remaining,theyarenotconsecutiveandthusdonotconstituteasagroup. Thesefourremaining\nvalues are the remnants of the original input vector.\nsum_remnant_values() numsImplement the function, which takes in a vector of integers and an integer 𝑘, and returns the sum of\nall remnant values, using the procedure illustrated above. For the given example, the function would return 3 + 2 + 3 + 3 = 11.\nint32_t sum_remnant_values(const std::vector<int32_t>& int32_tnums, k);\nnumsYoumayassumethat isnotemptyandthat 1. Notethatyoushouldonlyreturnthesumoftheremnants; youshouldnotphysically𝑘>\nremove values from the input vector itself. You should implement your solution in worst-case time and auxiliary space, whereΘ(𝑛) Θ(𝑛) 𝑛\nis the size of the input vector.\nstr '(' ')'.38. You are given a string that consists of the characters and Implement a function that returns the minimum number of\n'(' or')')parentheses (either that need to be added to the string so that the resulting string is balanced. For example, given the input\n\"()()()(())\"\"()))((\",string you would return 4, since a minimum of 4 new parentheses are need to balance the string: .\nint32_t min_add_to_make_string_valid(const std::string& str);\nYou should implement your solution in worst-case time and auxiliary space, where 𝑛is the size of the input list.Θ(𝑛) Θ(𝑛)\nMovingAverageCalculator39. Implementthefollowing class,whichcanbeusedtocalculatetheaverageofthe𝑘mostrecentvaluesthat\n.calculate_k_average()wereadded. Forexample,ifacalculatorisinitializedwith 3,andthevaluesof2and4wereadded,then𝑘=\n.calculate_k_average()would return a value of (2 + 4) / 2 = 3. If the value 6 were then added afterward, then would return a\n.calculate_k_average()value of (2 + 4 + 6) / 3 = 4. Then, if the value 8 were added, would return (4 + 6 + 8) / 3 = 6 (note that\nthe 2 is no longer considered since it is now the 4th most recently added value, which exceeds our limit of 3).𝑘=\n1\nclass MovingAverageCalculator {\n2\nprivate:\n3\n// Add any member variables that you may find useful here\n4\n5\npublic:\n6\n// This constructor inits the MovingAverageCalculator with a limit of k\n7\nMovingAverageCalculator(uint32_t k) {\n8\n9\n}\n10\n11\n// Adds a value to be considered\n12\nvoid add_value(double val) {\n13\n14\n}\n15\n16\n// Calculates the average of the k most recent values that were added\n17\n// If less than k values were added, return the average of all the values\n18\ndouble constcalculate_k_average() {\n19\n20\n}\n21\n};", "word_count": 768, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "642e70e7-0057-5d9c-86bb-7eba1230c70c", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 281, "real_page_number": null, "text": "9.7 Container Adaptors\n269\nChapter 9 Exercise Solutions\n1. The correct answer is (C). Deques allow values to be inserted efficiently at both ends of the container, while vectors only allow efficient\ninsertions at the back. Option (A) is incorrect because vectors do store their data contiguously in memory. Option (B) is incorrect because\noperator[]the contiguity of elements in a vector allows to be implemented with simple pointer addition (deques also require arithmetic,\nbut the more advanced internal structure of a deque results in slightly more complicated arithmetic compared to a vector). Option (D) is\nincorrect because finding an arbitrary element is worst-case regardless of which data structure you choose.Θ(𝑛)\n.pop()2. The correct answer is (D). Five elements are pushed onto the stack. The first on line 8 removes the most recently added element,\n24. .top() 21.or Then, the call to on line 9 retrieves the most recent element currently on the stack, which is\n3. The correct answer is (A). The time complexity of pushing an element into a stack implemented with a linked list is also Θ(1), since you\ncansimplyattachtheelementtothebeginningofthelist. Alloftheremainingoptionsarevaliddisadvantagesofalinkedlistimplementation\nover an array one.\n4. The correct answer is (D). The situation described in answer choice (D) is most similar to how a queue works, since those who are in line\nfirst are seated first.\nstd::queue<>,5. The correct answer is (C). To access the most recently added element in a you must remove the elements that are𝑛−1\nbefore it, which takes time.Θ(𝑛)\n6. The correct answer is (A). You do not need any additional space to complete this process: simply pop off the front element and readd it to\nthe back of the queue. Continue doing this for elements before you reach the final element; at this point, you can take the element out𝑛−1\nand the other elements will still be in their original order.\n7. The correct answer is (D). When you push back an element into the circular buffer, you increment the back iterator; if this back iterator\nends up at the same position as the first element in the circular buffer (the front iterator), you would know that your circular buffer is full.\n8. The correct answer is (B). When 203, 370, and 281 are pushed into the container, 281 is the first to be retrieved. When 280, 376, and 183\nare added to the container, 183 is the first to be retrieved. After 183 is popped off, 376 is the next to be retrieved. Notice that the top value\nis always the most recent element inserted into the container; thus, of the options provided, the container must be a stack. Notice that a\n.push() .pop()deque would not work here, since they do not support and methods without specifying the end you want to push or pop\nMYSTERY_CONTAINER std::deque.from, so the code would not compile if were a\n9. The correct answer is (A). Since Darget needs to insert inventory from one end of a container and remove it from the other, it should use a\nqueue, which can be used to efficiently support FIFO behavior. On the other hand, Paolmart needs to insert and remove inventory from the\nsame end of a container (since the newest element is removed first), which can be efficiently done using a stack or a vector.\n10. The correct answer is (A). Queues need to support efficient insertion and removal from both ends of its underlying container, since\nstd::list<>the side in which you insert an element is not the side you should pop elements out of. This can be done with a or a\nstd::deque<>, std::vector<>,whichcansupport insertionandremovalfrombothendsofthecontainer, butnotwitha whichΘ(1)\nonly supports insertion and removal from the back.Θ(1)\n11. The correct answer is (A). Only option (A) is a true statement, since it describes the functionality of a container adaptor. Option (B)\nis false because stacks and queues are not the only container adaptors on the STL (priority queues are another one). Option (C) is false\nbecause random access is not a prerequisite for an underlying container: for containers like stacks and queues where random access is not\nstd::list<>necessary and you only need efficient access at the ends of the container, you can use a as the underlying container. Option\n(D) is false, since container adaptors may restrict or modify the functionality of its underlying container to support the desired behavior.\n12. The correct answer is (C). If you want to emulate the behavior of a queue using a deque, then you should insert and remove elements from\nopposite ends of the deque.\n13. The correct answer is (B). Deques support insertions and removals from the ends of the container, but not the middle. Inserting anΘ(1)\nelement into the middle of the deque may require elements to be shifted in memory, which could take time.Θ(𝑛)\n14. The correct answer is (B). Without a tail pointer, a linked list (regardless of whether it is doubly- or singly-linked) cannot support Θ(1)\ninsertion or removal from the back. To implement the queue, you would need a way to insert and remove elements in from both endsΘ(1)\nof the container, which is not supported with a linked list deque with no tail pointer.\n15. The correct answer is (A). The order in which elements are inserted into a queue is the same order in which they are removed.\nThe correct answer is (D). Queues are often used to allocate scarce resources among different running processes by providing them in16.\nfirst in, first out order for different processes (e.g., if two processes need a shared resource, and one gets there before the other, then the\nfirst process would get the resource first). The other three applications are better suited for a stack, which supports efficient insertions and\nremovals from the same end.\nstd::stack<>17. The correct answer is (D). Since you are only allowed to use public methods provided in the interface, you would not\nbe able to access the fifth oldest element without popping out all but five of the elements in the stack. Since the order of elements in the\nstack cannot be changed either, you would need to store the values you popped as well, in an external data structure. Because of this, the\nworst-case time and auxiliary space complexities of finding the fifth oldest element in a stack are both Θ(𝑛).\n18. The correct answer is (B). This class uses a queue to implement the behavior of a stack. This is because, after every insertion, all the\npreviously existing elements in the queue are popped out and reinserted back in the queue. This ensures that the most recently insert element\nis always at the front of the queue, which essentially recreates the functionality of a stack.", "word_count": 1170, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "17bf8d42-4ab2-5a62-8d8f-787ba325d81b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 282, "real_page_number": null, "text": "270\nChapter 9. Stacks and Queues\n19. The correct answer is (C). Each insertion takes time, since all the elements in the queue are removed and inserted back in. To insertΘ(𝑛)\nΘ(𝑛2).𝑛elements, the total time complexity would become 𝑛×Θ(𝑛)=\n20. The correct answer is (E). None of the statements are true. The STL stack and queue do not support constant-time insertions in the middle\nof the container, regardless of what the underlying container is.\n21. The correct answer is (D). This function takes all the elements in a queue and inserts them into a stack. Then, it takes the elements in the\nstack and adds it back to the queue. Since stacks provide LIFO access, the last elements of the original queue are taken out of the stack and\nreinserted back into the queue first, essentially reversing the order of the queue’s contents.\n22. The correct answer is (E). The underlying container of a stack needs to support insertion and removal from the same end of theΘ(1)\ncontainer, which works for both vectors and linked lists. Option (A) is false since pushing an element inserts it into the stack instead of\nremoving it. Option (B) is false because elements are added to and removed from a stack in last in, first out (LIFO) order. Option (C) is\nfalse because popping an element out of an STL stack does not return a reference to the removed value. Option (D) is false because no such\nmember exists to access the oldest element in the stack.\nstd::stack<> std::queue<>,23. The correct answer is (D). If you only need the functionality of a or it may be preferable to use one\nstd::deque<>of these container adaptors rather than a since it makes clear in your code what the container is designed to be used for\nstd::queue<>, .push() .pop()and also lowers the likelihood of programming mistakes (e.g., if you use a you can just use and\nstd::deque<>,and the container handles it properly for you; for a you would need to keep track of which direction you are inserting\nand removing elements yourself). Statement I is false because stacks and queues are essentially the same as deques, just with a restricted\ninterface, so there is no difference in performance.\n24. The correct answer is (B). To build a singly-linked list in reverse order, one common strategy is need to start from the last node and build\nthe list by inserting nodes from the back to the front. This can be done using a container that supports efficient last in, first out (LIFO)\naccess (e.g., push all the nodes into the container, take them out one at a time in LIFO order, and the attach them to the reversed list). Of the\nfour containers provided, all support efficient LIFO access except the queue, which only supports efficient first in, first out (FIFO) access.\n25. The correct answer is (C). The time complexity it takes to the most recently added element is irrelevant here, since you are notaccess\nstd::queue<>,changing the contents of the queue. To the most recently added element from a you have no choice but to firstremove\nremove and reinsert all the elements before it, which would take time.Θ(𝑛)\n26. The correct answer is (A). Sequential access is fastest if the elements to access are stored contiguously in memory. Stacks and queues do\nstd::vector<>not support sequential access at all. Of the remaining three containers, only the guarantees that all of its elements are\nstored contiguously in memory.\n27. The correct answer is (C). To emulate the stack frames of a recursive call without performing any recursion, you will need a container that\nsupports efficient LIFO access (since with recursion, the item you put on the stack frame most recently is the one you want to retrieve first).\nThis can only be done with a vector and a stack, and not a queue.\n28. The correct answer is (D). There is nothing that prevents the queue from keeping track of its size internally, even if it uses a singly-linked\nlist as its underlying container. In this case, the time complexity of accessing the queue’s size would be Θ(1), not Θ(𝑛).\n29. The correct answer is (B). Inserting an element at the front of a singly-linked list takes time, and removing an element at the back ofΘ(1)\na singly-linked list takes time, regardless of whether a tail pointer is supported. Since push acts upon the front and pop acts upon theΘ(𝑛)\nback, the time complexity of push would be Θ(1), and the time complexity of pop would be Θ(𝑛).\n30. The correct answer is (C). Only configuration (C) is impossible, since the back pointer points past the end of the array. In a circular buffer,\nthe back pointer would circle back to the front if it ever moves past the end, so if the last element is at the last index of the array, then\nback_idx would end up at index 0, and not 16.\n31. The correct answer is (D). During the first iteration, D5, E4, and D3 are inserted into the stack, in this order.\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nStack\nD5\nE4\nD3\nD3 is now at the top of the stack, so on the next iteration, we take it out and add the unvisited cells of E3 (𝑦) and D2 into the stack.\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nStack\nD5\nE4\nE3\nD2", "word_count": 992, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c0ce1862-f889-55ec-95c1-8de2bad4721a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 283, "real_page_number": null, "text": "9.7 Container Adaptors\n271\nD2 is now at the top of the stack, so on the next iteration, we take it out and add the unvisited cells of C2 and E2 into the stack.\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nStack\nD5\nE4\nE3\nC2\nE2\nContinuing this process, we eventually encounter cell 𝑧next.\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nStack\nD5\nE4\nE3\nC2\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nStack\nD5\nE4\nE3\nB2\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nStack\nD5\nE4\nE3\nB3\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nStack\nD5\nE4\nE3\nB4\nThe next cell we encounter is 𝑥.\n1\n2\n3\n4\n5\n6\n7\nA\nB\nC\nD\nE\nF\n#\n#\n#\n#\n#\n#\n#\n#\n𝑧\n#\nH\n#\n#\n#\n#\n#\n#\n#\nS\n𝑤\n#\n#\n𝑦\n𝑥\n#\n#\n#\n#\n#\n#\n#\n#\nStack\nD5\nE5\nLastly, while not explicitly shown, we encounter cell 𝑤before reaching our destination H.\nstd::stack<> std::queue<> std::deque<>32. The correct answer is (A). Both and use a as its default underlying container.\nstd::queue<>Statement III is false because a does not support random access\n33. The correct answer is (E). All three containers can be interleaved in Θ(𝑛). For the vector, you can just iterate over the two input vectors\nsimultaneously and alternate pushing their values into the interleaved vector. For the queue, you can alternate popping out elements from\nthe two input queues and push them into the interleaved queue. The stack is a bit more tricky since you only have access to the most recently\nadded element, so the items at the top of the two stacks must be pushed into the interleaved stack last. This requires the use an additional", "word_count": 490, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8a794c56-3e6d-5a58-b137-58dbe2ea5219", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 284, "real_page_number": null, "text": "272\nChapter 9. Stacks and Queues\nstack; items are popped out of the two input stacks and alternately pushed into this additional stack first, and then these elements are popped\nback out and pushed into the interleaved stack (which reverses the order of elements to support the stack’s LIFO processing order).\n34. To solve this problem, we can just use the strategy discussed in example 9.1. We will treat one stack as the front of our queue, and the other\nstack as the back of our queue. If pop is ever invoked while the front stack is empty, we would move all the elements from the back stack\ninto the front stack. An implementation is shown below:\n1\ntemplate <typename T>\n2\nclass QueueWithStacks {\n3\nprivate:\n4\nstd::stack<T> stack1;\n// front stack\n5\nstd::stack<T> stack2;\n// back stack\n6\npublic:\n7\nsize_t constsize() {\n8\nreturn stack1.size() + stack2.size();\n9\n} // size()\n10\n11\nbool constempty() {\n12\nreturn stack1.empty() && stack2.empty();\n13\n} // empty()\n14\n15\nvoid push(const T& val) {\n16\nstack2.push(val);\n17\n} // push()\n18\n19\nvoid pop() {\n20\nif (stack1.empty()) {\n21\nwhile (!stack2.empty()) {\n22\nstack1.push(stack2.top());\n23\nstack2.pop();\n24\n} // while\n25\n} // if\n26\nstack1.pop();\n27\n} // pop()\n28\n};\nOne way to solve this problem is to use two stacks, one storing back pages and the other storing forward pages. Whenever you visit a page,35.\nadd it to the back stack and clear out the forward stack. Whenever you want to go back by 𝑘pages, pop 𝑘pages out of the back stack and\ninto the forward stack, and then set the current page to the one at the top of the back stack. Whenever you want to go forward by 𝑘pages,\npop 𝑘pages out of the forward stack and into the back stack, and then set the current page to the one at the top of the forward stack. One\nimplementation is shown below:\n1\nclass BrowserTab {\n2\nprivate:\n3\nstd::stack<std::string> pages_back, pages_forward;\n4\nstd::string current_page;\n5\npublic:\n6\nBrowserTab(const std::string& homepage) {\n7\ncurrent = homepage;\n8\n} // BrowserTab()\n9\n10\nvoid visit(const std::string& url) {\n11\npages_forward = std::stack<std::string>();\n12\npages_back.push(current);\n13\ncurrent = url;\n14\n} // visit()\n15\n16\nback(uint32_tstd::string steps) {\n17\nwhile (--steps >= 0 && !pages_back.empty()) {\n18\npages_forward.push(current);\n19\ncurrent = pages_back.top();\n20\npages_back.pop();\n21\n} // while\n22\nreturn current;\n23\n} // back()\n24\n25\nforward(uint32_tstd::string steps) {\n26\nwhile (--steps >= 0 && !pages_forward.empty()) {\n27\npages_back.push(current);\n28\ncurrent = pages_forward.top();\n29\npages_forward.pop();\n30\n} // while\n31\nreturn current;\n32\n} // forward()\n33\n};", "word_count": 449, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "947bcb9c-4669-5517-bc1c-d137631b83b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 285, "real_page_number": null, "text": "9.7 Container Adaptors\n273\n36. One unique challenge of this problem is that you are restricted to only a single auxiliary stack; it is trivial to reverse the order of the elements\nin another stack (by just popping the elements out of the input stack and pushing it into the other stack), but ensuring that the elements are\nreversed in the input stack itself is trickier. A general solution to this problem is to repeat the following process from to 𝑛−1:𝑘= 𝑘=0\n1. Pop off the top of the input stack and store it in a local variable.\n2. Pop elements from the input stack and push it into the auxiliary stack.𝑛−𝑘−1\n3. Push the element that was popped off in step 1 back onto the input stack.\n4. Pop all elements in the auxiliary stack back into the input stack.\nThis solution is implemented in code as follows:\n1\nvoid flip(std::stack<int32_t>& input) {\n2\nstd::stack<int32_t> aux;\n3\nsize_t n = input.size();\n4\n5\nfor (size_t k = 0; k < n; ++k) {\n6\nint32_t next = input.top();\n7\ninput.pop();\n8\n9\nfor (size_t j = 0; j < (n - k - 1); ++j) {\n10\naux.push(input.top());\n11\ninput.pop();\n12\n} // for j\n13\n14\ninput.push(next);\n15\n16\nwhile (!aux.empty()) {\n17\ninput.push(aux.top());\n18\naux.pop();\n19\n} // while\n20\n} // for k\n21\n} // flip()\nYou can use a LIFO container such as a stack or vector to keep track of groups. To do so, iterate over the input values and check the most37.\nrecently added element to see if it is equal to the new value to be added; if it is, increment a counter associated with that element. Once the\ncounter exceeds 𝑘, then you have a group that can be removed — by removing groups while iterating over the values, you are able to handle\ngroups larger than 𝑘that are only formed after removing other groups in the input. One implementation is shown below:\n1\nint32_t sum_remnant_values(const std::vector<int32_t>& int32_tnums, k) {\n2\nstruct Counter {\n3\nint32_t value{};\n4\nint32_t count{};\n5\n};\n6\n7\nstd::stack<Counter> s;\n8\nint32_t sum = 0;\n9\n10\nfor (int32_t num : nums) {\n11\nsum += num;\n12\nif (s.empty()) {\n13\ns.push({ num, 1 });\n14\n} // if\n15\nelse if (s.top().value == num) {\n16\n++s.top().count;\n17\n} // else if\n18\nelse {\n19\nif (s.top().count >= k) {\n20\nsum -= s.top().value s.top().count;*\n21\ns.pop();\n22\n} // if\n23\nif (s.empty() || s.top().value != num) {\n24\ns.push({ num, 1 });\n25\n} // if\n26\nelse {\n27\n++s.top().count;\n28\n} // else\n29\n} // else\n30\n} // for\n31\n32\n// check one more time in case last element caused group to exceed size of k\n33\nif (s.top().count >= k) {\n34\nsum -= s.top().value s.top().count;*\n35\n} // if\n36\n37\nreturn sum;\n38\n} // sum_remnant_values()", "word_count": 502, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3fa3fc94-32f9-5b83-ae92-de335ad82ae6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 286, "real_page_number": null, "text": "274\nChapter 9. Stacks and Queues\n38. One way to solve this problem is to keep counters for the number of left and right parentheses that need to be added for the string to be\nbalanced. We will iterate over the string and count the number of left and right parentheses that are required using the following procedure:\n• If a left parenthesis is encountered, increment the number of right parenthesis that are needed (since one will be needed to match this\nleft parenthesis).\n• If a right parenthesis is encountered:\n– If the right parenthesis counter is greater than zero, decrement it.\n– Otherwise, increment the left parenthesis counter, since a new left parenthesis will be needed to match this right parenthesis.\nAn implementation of this solution is shown below:\n1\nint32_t min_add_to_make_string_valid(const std::string& str) {\n2\nint32_t left = 0, right = 0;\n3\nfor (char c : str) {\n4\nif (c == '(') {\n5\n++right;\n6\n} // if\n7\nelse if (right > 0) {\n8\n--right;\n9\n} // else if\n10\nelse {\n11\n++left;\n12\n} // else\n13\n} // for c\n14\n15\nreturn left + right;\n16\n} // min_add_to_make_string_valid()\nThis can also be done using a stack. Whenever we encounter a left parenthesis, we push it onto the stack. Whenever we encounter a right\nparenthesis, we pop it from the stack (or, if the stack is empty, then the right parenthesis has no matching left parenthesis, so we will\nincrement a separate counter). This solution is shown below:\n1\nint32_t min_add_to_make_string_valid(const std::string& str) {\n2\nint32_t left = 0;\n3\nstd::stack<char> s;\n4\nfor (char c : str) {\n5\nif (c == '(') {\n6\ns.push(c);\n7\n} // if\n8\nelse {\n9\nif (!s.empty()) {\n10\ns.pop();\n11\n} // if\n12\nelse {\n13\n++left;\n14\n} // else\n15\n} // else\n16\n} // for c\n17\n18\nreturn left + s.size();\n19\n} // min_add_to_make_string_valid()\n39. This problem can be solved using a queue that stores the elements in the window. Each new element is pushed into the back of the queue,\nand whenever the capacity exceeds 𝑘, we pop the oldest element out from the front of the queue. An implementation is shown below:\n1\nclass MovingAverageCalculator {\n2\nprivate:\n3\nstd::queue<double> window;\n4\ndouble sum{};\n5\nuint32_t window_size{};\n6\npublic:\n7\nMovingAverageCalculator(uint32_t k) {\n8\nwindow_size = k;\n9\n} // MovingAverageCalculator()\n10\n11\nvoid add_value(double val) {\n12\nif (window.size() == window_size) {\n13\nsum -= window.front();\n14\nwindow.pop();\n15\n} // if\n16\nsum += val;\n17\nwindow.push(val);\n18\n} // add_value()\n19\n20\ndouble constcalculate_k_average() {\n21\nreturn sum / window.size();\n22\n} // calculate_k_average()\n23\n};", "word_count": 463, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d9f80f1c-ea6e-5522-8ae7-4c24cd5d173a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 287, "real_page_number": null, "text": "Chapter 10\nPriority Queues and Heaps\n10.1\nIntroduction to Priority Queues\nSo far, we have covered two types of container adaptors, stacks and queues, that allow data to be accessed based on the order of insertion. In a\nstack, elements are accessed in last-in, first-out (LIFO) order. In a queue, elements are accessed in first-in, first-out (FIFO) order. However,\nthere are situations where retrieving data isn’t as simple as taking out the oldest or newest element in a container. For example, if you were\nimplementing a program that simulated an emergency call center, you would want to dispatch the calls in order of how urgent they are, not by\nthe order in which the calls were received. Similarly, if you wanted to implement a matching algorithm for a stock exchange, you would want to\nmatch the highest price a buyer is willing to pay for an asset with the lowest price a seller is willing to sell it for. The trades that are conducted\nare not determined by the order in which buyers and sellers enter the market, but rather the prices they are willing to buy or sell for.\nIn these examples, the order of data access is based on some priority value. For the call center, the priority of a call is determined by its\nurgency. For the stock exchange, the priority of a trader is determined by the price at which they are willing to buy or sell stock.\nTo access data based on a priority value, we would need a container that stores elements in a way such that the element with the highest\npriority is always easily accessible. A container type that provides this functionality is the priority queue. A priority queue is a data structure\nthat allows the user to extract or pop the element with the highest priority in the container, where priority is determined by a value that can be\noperator<).compared (e.g., using The interface of a priority queue is as follows:\nFunction\nBehavior\n.push(val)\nvalPushes the element into the priority queue\n.pop()\nRemoves the element with the highest priority from the priority queue\n.top()\nReturns a reference to the element with the highest priority\n.size()\nReturns the number of elements in the priority queue\n.empty()\nReturns whether the priority queue is empty\nFor instance, if you pushed the elements 1, 5, 2, 8, and 4 into a priority queue, where priority is determined by an element’s value (with larger\n.pop()numbers have higher priorities), calling on this priority queue would remove the element 8, since it has the highest value.", "word_count": 431, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1aa88fc7-8ab3-5b6c-af28-a66fc6bf01de", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 288, "real_page_number": null, "text": "276\nChapter 10. Priority Queues and Heaps\nHow would you go about implementing a priority queue in memory? There are several different ways we can go about this, which we will\ndiscuss in this chapter. A summary of four priority queue implementation methods is shown in the table below:\n.push()Time Complexity of\n.pop()Time Complexity of\nArray of Linked Lists\n(for priorities of small integers)\nΘ(1)\nΘ(1)\nUnsorted Sequence Container\nΘ(1)\nΘ(𝑛)\nSorted Sequence Container\nΘ(𝑛)\nΘ(1)\nHeaps\nΘ(log(𝑛))\nΘ(log(𝑛))\n10.2\nList and Sequence Container Implementations\n¸ 10.2.1\nArray of Linked Lists Implementation\nIf your data can only take on a small number of priority values, you can use an array of linked lists to implement a priority queue that supports\n.push() .pop().and To do so, set the array’s size to the number of possible priority values and assign a linked list that holds allΘ(1) Θ(1)\nvalues of a certain priority to each index of the array.\nFor instance, suppose the calls in our emergency call center example can only take on one of three severity levels: 0 (not urgent), 1\n(moderately urgent), and 2 (very urgent). We could implement a priority queue by using an array of linked lists that represent each severity level:\n0\n1\n2\nlinked list that stores all calls with low urgency\nlinked list that stores all calls with moderate urgency\nlinked list that stores all calls with high urgency\nWhen an element is pushed into this priority queue, its priority is checked, and it is added to the correct linked list in time. When anΘ(1)\nelement is popped from the priority queue, it is removed from the linked list associated with the highest priority (in this case, the \"high urgency\"\nlinked list at index 2). However, this implementation only works if there are a fixed number of priorities that exist in your data (e.g., low,\nmoderate, and high urgency). As an example, if we wanted to find the buyer with the highest price in our electronic stock exchange example, the\npriority value of an order can take on an infinite number of values (e.g., …, $5.00, $5.01, $5.02, $5.03, …) . For our linked list approach to\nwork in this situation, we would need a separate linked list for every possible price we could encounter, which is impractical.\n¸ 10.2.2\nUnordered Sequence Container Implementation\nInmostcases,anarrayoflinkedlistswillnotworksinceitcannotsupportdatawithmanypossiblepriorityvalues. Analternativeistoimplement\nthe priority queue using an container, or a container that stores its elements in no defined order. This is a commonly doneunordered sequence\nusing a vector as the underlying container for the data in a priority queue.\n.push()To an element into a priority queue that is implemented using an unordered sequence container, the element is simply appended\nto the end of the container. This takes time.Θ(1)\n.pop()To an element from a priority queue that is implemented using an unordered sequence container, a linear search is completed\nto find the element with the highest priority. Since every element in the underlying container must be visited to identify this highest priority\nelement, this search takes worst-case time. Then, once the highest priority element is found, it is removed. If you are using a vector as theΘ(𝑛)\nunderlying container, this requires all elements after the deleted element to shift to the left by one, which also takes time on average.Θ(𝑛)\n16\n22\n15\n11\n.push(19)\n16\n22\n15\n11\n19\n16\n22\n15\n11\n19\n.pop()\n16\n15\n11\n19\nshift left 1\n16\n15\n11\n19\n.pop()Because the container is unordered, the operation can be further optimized. Instead of removing the element from the middle, which\nforces the container to shift all elements after the one that was removed, you can overwrite the element you want to remove with the last element,\n.pop_back()and then on the underlying vector (which, unlike erasing at an arbitrary position, always takes constant time).\n16\n22\n15\n11\n19\n.pop()\n16\n19\n15\n11\n19\n22 19overwrite with\npop back underlying vector\n16\n19\n15\n11\n¸ 10.2.3\nSorted Sequence Container Implementation\nAnother option for implementing a priority queue is to use a to hold the data. Unlike the unordered sequencesorted sequence container\ncontainer, the sorted sequence container must be sorted at all times. This guarantees that the element with the highest priority is located at the\nend of the container, where removal is most efficient for a vector.", "word_count": 762, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fb807a21-1007-5bd3-bfa6-c4b81ef007db", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 289, "real_page_number": null, "text": "10.3 Binary Heaps\n277\n.push()When is called, the element being pushed in must be added at the correct position to maintain the sorted invariant. Because the\nstd::lower_bound(),underlying vector is sorted, this position can be found using (a helpful STL function for this task isbinary search\nwhich can be used to return an iterator to the correct point of insertion — this function will be covered in the next chapter). However, since\n.push()elementsafterthepointofinsertionhavetobeshiftedtomakeroomforthenewelement, thetimecomplexityof forasortedsequence\ncontainer is on average.Θ(𝑛)\n.pop() .pop_back()Whenever is called, the element at the back of the vector is removed using the method (since the element at the\nback of the sorted vector must have the highest priority). This can be done in time.Θ(1)\n13\n17\n24\n33\n42\n.push(19)\n13\n17\n19\n24\n33\n42\n13\n17\n19\n24\n33\n42\n.pop()\n13\n17\n19\n24\n33\nTo summarize, if you were to implement a priority queue using an sequence container, insertion would take time, but removalunsorted Θ(1)\nwould take time. On the other hand, if you were to implement a priority queue using a sequence container, insertion would takeΘ(𝑛) sorted\ntime, but removal would take time.Θ(𝑛) Θ(1)\n.push() .pop()Is it better to have or be more efficient? In the end, it does not matter. If you think about the life cycle of a single\nelement in a priority queue, the total cost involved with inserting and removing that element is regardless of whether you use an unsortedΘ(𝑛)\nor sorted sequence container. This is because, in most cases, elements that are added to a priority queue will eventually be removed. The only\ndifference between the two implementations is the operation takes place.Θ(𝑛)when\nThat being said, a complexity for insertion or removal is not ideal, and there are ways to do better. In the next section, we will introduceΘ(𝑛)\nthe concept of a heap, which can be used to improve the time complexities of these priority queue operations.\n10.3\nBinary Heaps\n¸ 10.3.1\nHeap Deﬁnitions\nA binary heap is a data structure that can be used to implement a priority queue that supports worst-case insertion and removal. SinceΘ(log(𝑛))\nboth insertion and removal can be done in time, the entire life cycle of any element going in and out of a binary heap priority queueΘ(log(𝑛))\nis 2 ×Θ(log(𝑛)), which is also Θ(log(𝑛)). This makes the binary heap implementation of a priority queue more efficient than both sequence\ncontainer implementations we covered earlier (where pushing and popping takes time).Θ(𝑛)\nBefore we go over how heaps work, we will need to introduce the concept of a tree. A tree is a graph (i.e., a set of nodes connected by\nedges) that is connected without cycles (i.e., there are no unreachable vertices or circular paths that begin and end at the same vertex). In a tree,\nany two nodes are connected by single, shortest path. The following terminology can be used to describe different components of a tree:unique\n• A parent is an immediate ancestor of a given node (i.e., the node one level above a given node).\n• A child is an immediate descendant of a given node (i.e., the node(s) one level below a given node).\n• An ancestor is any node closer to the root that is along a connected path upwards from a given node.\n• A descendant is any node further from the root that is along a connected path downwards from a given node. If you want to find the\ndescendants of any node, simply treat the given node as the root of a subtree — the subtree would contain all the descendants of that node.\n• A root node is the \"topmost\" node in a tree. The root is the common ancestor of all nodes in the tree.\n• A leaf node is a node children.without\n• An internal node is a node children (i.e., not a leaf node).with\nThe size of a tree is the number of nodes in the tree. For an empty tree, the size is 0. For a non-empty tree, the size of the tree is the size of the\nroot’s left child + the size of the root’s right child + 1 (for the root itself).\nThe height of a node is defined recursively as the maximum height of the node’s children, plus 1. By definition, the height of a leaf node is 1.\nnullptrAn alternative definition of height is the maximum distance needed to get from a node to a by moving downwards through the tree.\nnullptrThe depth of a node is the opposite of its height. Unlike height, which measures the distance between a node and a at the bottom of\nthe tree, the depth measures the distance between a node and the root. The depth of a node is the depth of its parent, plus 1.\nA binary tree is a special type of tree where each node has at most two children. Binary trees can be further split into the following categories:\n• A proper or full binary tree is a binary tree where every node either has 0 or 2 children. In other words, all non-leaf nodes in a proper\nbinary tree have 2 children.\n• A complete binary tree is a tree in which every depth, except possibly the last, is completely filled. If the last depth (the bottom row) is\nnot completely filled, then all nodes that do exist at that depth must be filled from left to right with no gaps.\nConsider the following tree below. This tree is a binary tree because each node has at most two children.\nA\nC\nF\nB\nE\nD\nH\nG", "word_count": 981, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "00d825ef-9b47-5128-8d7e-962d74ce28ad", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 290, "real_page_number": null, "text": "278\nChapter 10. Priority Queues and Heaps\nThe following statements are true regarding this tree:\n• Node A is the root node of the tree.\n• Nodes E, F, G, and H are leaf nodes because they have no children.\n• Nodes A, B, C, and D are internal nodes because they do have children.\n• Nodes G and H are the children of node D.\n• Node B is the parent of nodes D and E.\n• Nodes A and B are the ancestors of node D.\n• Nodes D, E, G, and H are the descendants of node B.\n• There are 8 nodes in the tree, so the size of the tree is 8.\n• The heights of all the leaf nodes (E, F, G, and H) are each 1. The heights of nodes C and D are 2, the height of node B is 3, and the height\nof node A is 4.\n• The depth of the root node A is 1, and the depths of the other nodes increase by one for each level. Nodes B and C have depths of 2.\nNodes D, E, and F have depths of 3. Nodes G and H have depths of 4.\nNow, let’s look at a few binary trees that depict the property of completeness. The tree below is complete, since every depth is completely filled.\nA\nC\nG\nO\nN\nF\nM\nL\nB\nE\nK\nJ\nD\nI\nH\nThe following binary is also complete. Notice that the lowest level of the tree is not completely filled to capacity. This is okay; per the definition\nof a complete binary tree, the lowest level may be unfilled as long as all the elements on that level are located as far left as possible. Since this is\ntrue for the tree below, the completeness property still holds.\nA\nC\nG\nF\nB\nE\nD\nI\nH\nThe following tree is complete. This is because the second-to-last level is not completely filled (there is a gap between E and F, since C doesnot\nnot have a left child). A complete tree only allows the final depth of the tree to be unfilled, so this tree violates the completeness property.\nA\nC\nF\nB\nE\nD\nH\nG\nThe following tree is also complete. Even though all but the last level is completely filled, the nodes in the last level are located as farnot not\nleft as possible, which violates the completeness property. There is a gap between H and I, as D has no right child.\nA\nC\nG\nF\nB\nE\nI\nD\nH\nNodeHow would a binary tree be represented in memory? One method is to use a node and pointer-based approach, where each object stores\ndata along with pointers to the nodes of its children:\n1\ntemplate <typename T>\n2\nstruct Node {\n3\nT data;\n// data of node\n4\nNode* left;\n// pointer to left child\n5\nNode* right;\n// pointer to right child\n6\n};", "word_count": 507, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9e6a24d7-f81d-55d4-9168-2ee174218ef5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 291, "real_page_number": null, "text": "10.3 Binary Heaps\n279\nThe pointer-based approach is efficient for moving down a tree from parent to child. However, this approach also uses a bit of additional memory,\nsince each node will have to store pointers to its children in addition to its data.\nThe pointer-based approach is not the only possible approach. It turns out that there is another method of storage that works well for\nbinary trees. The completeness property allows these trees to be elegantly collapsed into a form that can be efficiently stored in acomplete\nvector. To convert a complete binary tree into a vector, we append the first row of the tree to the vector, followed by the second row, followed by\nthe third, and so on. The figure below depicts what a complete binary tree would look like after it is flattened into a vector.\nA\nC\nG\nF\nB\nE\nD\nI\nH\nA\nB\nC\nD\nE\nF\nG\nH\nI\n[0] [1] [2] [3] [4] [5] [6] [7] [8] [9]\nNote: There is a reason why index 0 is empty... this will be explained soon.\nThe completeness property ensures efficient vector storage, since it guarantees that there are no gaps in the array. For instance, if node F were\nmissing from the tree (i.e., the tree would not longer be complete), then there would be nothing at index 6. This is inefficient, since the presence\nof empty indices can make the size of a vector much larger than the number of nodes, wasting memory.\nA\nC\nG\nB\nE\nD\nI\nH\nA\nB\nC\nD\nE\nG\nH\nI\n[0] [1] [2] [3] [4] [5] [6] [7] [8] [9]\nNote: There is a reason why index 0 is empty... this will be explained soon.\nWhy is index 0 of the array empty? This is because having the root at index 1 simplifies the math we would need to do to identify the children\nand parent of a node when given its index. Let’s analyze how this works. Consider the following tree, as reproduced from the previous example:\nA\nC\nG\nF\nB\nE\nD\nI\nH\nA\nB\nC\nD\nE\nF\nG\nH\nI\n[0] [1] [2] [3] [4] [5] [6] [7] [8] [9]\nNote: There is a reason why index 0 is empty... this will be explained soon.\nIf we use 1-indexing and store the root at index 1, the following properties are true:\n• Given an index 𝑖of a node in the tree, the index of that node’s parent is 𝑖∕2.\n– In the tree above, the parent of node F (index 6) is node C (index = 3).6∕2\n– In the tree above, the parent of node G (index 7) is node C (index = 3, using integer division).7∕2\n• Given an index 𝑖of a node in the tree, the index of its left child is 2𝑖.\n– In the tree above, the left child of node B (index 2) is node D (index 2 2 = 4).×\n• Given an index 𝑖of a node in the tree, the index of its right child is 2𝑖+1.\n– In the tree above, the right child of node B (index 2) is node E (index 2 2 + 1 = 5).×\n• A node is a leaf node if its index 𝑖is greater than 𝑛∕2, where 𝑛is the size of the tree.\n– In the tree above, node E is a leaf node because its index, 5, is greater than = = 4.𝑛∕2 9∕2\n– In the tree above, node D is not a leaf node because its index, 4, is not greater than = = 4.𝑛∕2 9∕2\n• A node is an internal node if its index 𝑖is less than or equal to 𝑛∕2, where 𝑛is the size of the tree.\n– In the tree above, node D is an internal node because its index, 4, is less than or equal to = = 4.𝑛∕2 9∕2\n– In the tree above, node E is not an internal node because its index, 5, is not less than = = 4.𝑛∕2 9∕2\nRemark: Even though we store a dummy element at index 0 in these examples, this actually isn’t the best way to do things (as it wastes space\npq.size() == pq.data.size() - 1).and makes certain invariants a bit more confusing; e.g., A better option would be to write a few\nprivate member functions that can be used to translate 1-indexing into a 0-indexed vector, as shown below. For simplicity, however, the\nremaining code in this section will assume a dummy element at index 0.\n1\nget_element(size_tT& idx) {\n2\nreturn data[idx - 1];\n3\n} // get_element()\n4\n5\nconst get_element(size_t constT& idx) {\n6\nreturn data[idx - 1];\n7\n} // get_element()", "word_count": 807, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9cbcd086-010d-55a3-9524-b3b150d4512c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 292, "real_page_number": null, "text": "280\nChapter 10. Priority Queues and Heaps\nA tree is considered to be heap-ordered if, for every node in the tree, each of its children has a priority that is to its parent. Inless than or equal\na heap-ordered tree, no node has a priority greater than the root.\nHeap-ordered binary trees can be separated into two categories. A max heap-ordered tree, or a max-heap, is a binary tree where the value of\neach node is greater than or equal to the values of its children (i.e., larger values have higher priority). A min heap-ordered tree, or a min-heap,\nis a tree where the value of each node is less than or equal to the values of its children (i.e., smaller values have higher priority). The following\nare examples of max and min heap-ordered trees:\n25\n18\n5\n1\n14\n7\n9\n4\n2\nMax-Heap\n13\n19\n24\n26\n16\n37\n23\n35\n41\nMin-Heap\nThisleadsustotheformaldefinitionofabinaryheap. Abinaryheapisabinarytree-baseddatastructurethathastwoproperties. Bothproperties\nmust be satisfied for a binary tree to be a binary heap!\n1. The tree must be complete.\n2. The contents of the tree must be heap-ordered.\nA binary heap can be used to efficiently retrieve the highest priority element in a collection of data. Max-heaps allow for easy access to the\nlargest value, while min-heaps allow for easy access to the smallest value. Because of this, binary heaps can be efficiently used as the underlying\nstructure of a priority queue.\nIn this class, binary heaps will be implemented using a vector. The vector approach is preferable to the pointer-based approach in that it\nuses less memory (i.e., no need to store pointers to children) without sacrificing any of the functionality or performance of the heap.\nExample 10.1 Consider the following vectors of data. Which of the following are min-heaps? Which of the following are max-heaps?\nAssume that the root is the first element in the vector.\n[1, 8, 4, 9, 12, 11, 7]a.\n[3, 4, 5, 7, 12, 11, 8, 6, 13]b.\n[13, 10, 6, 8, 7, 4, 2, 8, 1]c.\nTo approach problems like these, draw out the tree first. If the root element is the smallest in the vector, check if the remaining elements satisfy\na min-heap by making sure that no child has a value smaller than that of its parent. On the other hand, if the root element is the largest in the\nvector, check if the remaining elements satisfy a max-heap by making sure that no child has a value larger than that of its parent.\n[1, 8, 4, 9, 12, 11, 7]a. The vector can be converted to the following tree:\n1\n4\n7\n11\n8\n12\n9\nThis is a min-heap, since the value of each parent is less than or equal to the values of its children. 1 is less than or equal to 8 and 4, 8 is less\nthan or equal to 9 and 12, and 4 is less than or equal to 11 and 7.\n[3, 4, 5, 7, 12, 11, 8, 6, 13]b. The vector can be converted to the following tree:\n3\n5\n8\n11\n4\n12\n7\n13\n6\nThis is neither a min-heap nor a max-heap. Notice that 7 has children 6 and 13. Although the rest of the elements follow the min-heap ordering,\nthis heap is not a min-heap because 7 is not smaller than or equal to 6.", "word_count": 593, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6e87df23-ca12-53b6-9afe-f3a1b0a2b676", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 293, "real_page_number": null, "text": "10.3 Binary Heaps\n281\n[13, 10, 6, 8, 7, 4, 2, 8, 1]c. The vector can be converted to the following tree:\n13\n6\n2\n4\n10\n7\n8\n1\n8\nThis is a max-heap, since the value of each parent is greater than or equal to the values of its children. 13 is greater than or equal to 10 and 6, 10\nis greater than or equal to 8 and 7, 6 is greater than or equal to 4 and 2, and 8 is greater than or equal to 8 and 1.\nExample 10.2 Is the following a valid max-heap?\n99\n77\n33\n44\n88\n55\n11\n66\n22\nThe answer is no. Even though this tree is heap-ordered, it is not complete. Remember that completeness is one of the necessary requirements\nof a binary heap!\n¸ 10.3.2\nFix Up\nWhat if the priority of an element in the heap is modified? When this happens, the contents of the heap may need to be reordered to ensure that\nthe heap-ordered criteria is still met.\nThere are two ways an element in the heap can be modified: the priority of the element can either be or decreased. If the priorityincreased\nof an element is increased, you will need to use the fix up approach to bring the element closer to the top of the heap. To do this, continuously\nswap the modified element with its parent until either:\n• you reach the root (this happens if the modified element ends up with a priority that is the highest in the heap).\n• you reach a parent with a higher or equal priority.\nThe following process illustrates what happens when the element 2 in the following max-heap gets updated to 22.\n25\n18\n5\n1\n14\n7\n9\n4\n2\n25\n18\n5\n1\n14\n7\n9\n4\n22\nTo fix the binary heap, the fix up method is used. The new node 22 is continuously swapped with its parent until it either reaches the root or a\nparent that is greater than or equal to 22, whichever comes first.\n25\n18\n5\n1\n14\n7\n9\n4\n22\n25\n18\n5\n1\n14\n7\n22\n4\n9\n25\n18\n5\n1\n22\n7\n14\n4\n9", "word_count": 378, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8da7019e-965a-5b8c-b7eb-e4c6a940f7f4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 294, "real_page_number": null, "text": "282\nChapter 10. Priority Queues and Heaps\nRecall that, in the vector-based implementation of a binary heap, the parent of an element at index 𝑖is located at index 𝑖∕2. The following\nfunction uses this information to implement fix up, given a vector of data and the index of the element whose priority was increased.\n1\n// this function returns whether val1 has a lower priority than val2\n2\ntemplate <typename T>\n3\nbool comp_priority_less(const constT& val1, T& val2);\n4\n5\ntemplate <typename T>\n6\nvoid size_tfix_up(std::vector<T>& data, index) {\n7\n// swap modified element with parent until root is reached or a parent\n8\n// with a greater than or equal priority is found\n9\nwhile (index > 1 && comp_priority_less(data[index / 2], data[index])) {\n10\nswap(data[index], data[index / 2]);\n11\nindex /= 2;\n12\n} // while\n13\n} // fix_up()\nWhat is the worst-case time complexity of the fix up operation? In the worst case, the number of swaps you need to perform is equal to the\nnumber of levels in the tree (which happens if an element is swapped from the bottom-most level to the root). The number of levels in the tree is\napproximately if 𝑛is the size of the heap, so the time complexity of fix up is also Θ(log(𝑛)).log(𝑛)\nWhy is the number of levels in a heap rather than Θ(𝑛)? This is because heaps enforce the completeness property. A completeΘ(log(𝑛))\ntree with 𝑛nodes must have levels, as every level but the last must be completely filled. The only way to have levels is to have aΘ(log(𝑛)) Θ(𝑛)\nsticklike tree where certain nodes only have one child, which would not be complete.\n¸ 10.3.3\nFix Down\nIf the priority of an element in the heap is decreased, you will need to use the fix down approach to lower that element’s position in the heap. To\ndo this, continuously swap the modified element with the child that has the highest priority until either:\n• you reach the bottom of the heap.\n• you reach a position where no child has a higher priority.\nThe following process illustrates what happens when the element 14 in the following max-heap gets updated to 3:\n25\n18\n5\n1\n14\n7\n9\n4\n2\n25\n18\n5\n1\n3\n7\n9\n4\n2\nTo fix the binary heap, the fix down method is used. The new node 3 is continuously swapped with the child of higher priority until it either\nreaches the bottom level of the heap or a position where no child has a higher priority.\n25\n18\n5\n1\n3\n7\n9\n4\n2\n25\n18\n5\n1\n9\n7\n3\n4\n2\n25\n18\n5\n1\n9\n7\n4\n3\n2", "word_count": 460, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c3518830-d3e1-52b4-8144-9499c7f613a5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 295, "real_page_number": null, "text": "10.3 Binary Heaps\n283\nThe code involved with the fix down approach using a vector is shown below. This implementation is slightly more complicated than the fix up\napproach since it requires checks for whether a node actually has a left or right child.\n1\n// this function returns whether val1 has a lower priority than val2\n2\ntemplate <typename T>\n3\nbool comp_priority_less(const constT& val1, T& val2);\n4\n5\ntemplate <typename T>\n6\nvoid size_tfix_down(std::vector<T>& data, index) {\n7\nsize_t heap_size = data.size() - 1;\n8\nwhile (2 index <= heap_size) {*\n9\n// initialize highest priority child to left child\n10\nsize_t larger_child = 2 index;*\n11\n// if right child has higher priority, set larger child to right child\n12\nif (larger_child < heap_size &&\n13\ncomp_priority_less(data[larger_child], data[larger_child + 1])) {\n14\n++larger_child;\n15\n} // if\n16\n// if children all have lower priority, the heap is restored so break out of the loop\n17\nif (comp_priority_less(data[larger_child], data[index])) {\n18\nbreak;\n19\n} // if\n20\n// otherwise, swap the modified element with the largest child\n21\nstd::swap(data[index], data[larger_child]);\n22\n// set new index value for next iteration of loop\n23\nindex = larger_child;\n24\n} // while\n25\n} // fix_down()\nOnce again, the time complexity of fix down depends on the number of swaps that need to be done. Since there are levels in aΘ(log(𝑛))\ncomplete binary tree, the number of swaps required is at worst. Thus, the worst-case time complexity of fix down is Θ(log(𝑛)).Θ(log(𝑛))\n¸ 10.3.4\nInserting and Removing Elements\nNow, let’s consider what happens when we try to insert an element into a binary heap. Since a heap’s underlying vector is not guaranteed to\nbe sorted, we cannot do a binary search to find the correct position of insertion (much like with the sorted sequence container). Since we do\n.push_back()not immediately know where to insert the element, we will first push the element to the back of the vector using (as vectors\nsupport efficient insertion at the back). An example is shown below:\n25\n18\n14\n9\n7\n1\n5\n2\n4\n.push(99)\n25\n18\n14\n9\n7\n1\n5\n2\n4\n99\nIf we look at the contents of the modified heap, adding an element to the back of the underlying vector essentially adds the element to the next\navailable position in the complete binary tree (i.e., the next open position on the bottom level when filling out from left to right).\n25\n18\n5\n1\n14\n7\n99\n9\n4\n2\nAfter inserting the new element, we will have to move it to the correct position to ensure that the tree remains heap-ordered. Because the new\nelement was added to the bottom of the heap, we want to swap it upwards to the correct position. This can be done using the fix up approach\ncovered earlier: the newly added node is repeatedly swapped with its current parent until it is in its correct position.\n25\n18\n5\n1\n14\n99\n7\n9\n4\n2\n25\n18\n5\n1\n99\n14\n7\n9\n4\n2\n99\n18\n5\n1\n25\n14\n7\n9\n4\n2", "word_count": 532, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "97830d86-7d0f-5fb1-bf4c-c21b92839aea", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 296, "real_page_number": null, "text": "284\nChapter 10. Priority Queues and Heaps\nIn summary, to push an element into a priority queue that is implemented using a vector-based binary heap:\n1. Push the element to the back of the underlying vector that stores the data of the binary heap.\n2. Call fix up on the element that was added (to swap it up to the correct position).\n1\ntemplate <typename T>\n2\nvoid constpush(std::vector<T>& data, T& val) {\n3\ndata.push_back(val);\n4\nfix_up(data, data.size());\n5\n} // push()\n.push()What is the time complexity of pushing an element into this priority queue? Since inserts an element at the back of the vector (which\nfix_up() .push()takes time) and then calls (which takes time), the time complexity of is Θ(1+log(𝑛)), or Θ(log(𝑛)).Θ(log(𝑛))Θ(1)\nThe process of removing an element works similarly. To pop an element from the heap, you would remove the element with the highest\npriority. This element is located at the root of the binary heap, or index 1 of its underlying vector (assuming a dummy at index 0). The intuitive\n.pop()approach would be to simply erase the element at index 1 of the vector when is called:\n1\ntemplate <typename T>\n2\nvoid pop(std::vector<T>& data) {\n3\ndata.erase(data.begin() + 1);\n// NOTE: this implementation of pop is incorrect!\n4\n} // pop()\nHowever, this approach does not work! This is because the rest of the heap may be invalidated. Consider the example below: suppose we wanted\nto pop the highest priority element off the following max-heap, and we did so by just erasing the first element in the heap’s underlying vector:\n25\n18\n5\n1\n14\n7\n9\n4\n2\n25\n14\n18\n9\n7\n1\n5\n2\n4\n[0] [1] [2] [3] [4] [5] [6] [7] [8] [9]\nNote: There is a reason why index 0 is empty... this will be explained soon.\n18\n5\n1\n14\n7\n9\n4\n2\n14\n18\n9\n7\n1\n5\n2\n4\n[0] [1] [2] [3] [4] [5] [6] [7] [8] [9]\nNote: There is a reason why index 0 is empty... this will be explained soon.\nAfter erasing the element at index 1, all of the remaining elements will need to shift one position to the left to ensure that the new root node is\npositioned at index 1. This ends up being a operation, which is not what we want! Furthermore, the heap ends up looking like this after theΘ(𝑛)\nshifting process is complete:\n14\n9\n2\n5\n18\n1\n7\n4\n14\n18\n9\n7\n1\n5\n2\n4\n[0] [1] [2] [3] [4] [5] [6] [7] [8]\nNote: There is a reason why index 0 is empty... this will be explained soon.\nThis is not a valid heap! Notice that 14 ends up at the root, but it is not the element with the highest priority (that would be 18). Thus, simply\nerasing the first element does not guarantee that the remaining elements will form a valid heap. A better approach would be to the rootoverwrite\nwith the last element in the heap, and then pop off the last element. Let’s look at how this works:\n25\n18\n14\n9\n7\n1\n5\n2\n4\n.pop()\n4\n18\n14\n9\n7\n1\n5\n2\n4\n(1) overwrite the element to pop\n(2) pop the last element", "word_count": 559, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "07f7a4d7-0573-5ded-9a3b-2f97be950dba", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 297, "real_page_number": null, "text": "10.3 Binary Heaps\n285\nLooking at this from a tree perspective, this procedure takes the last element in a complete tree (the rightmost node on the bottom level of the\ntree) and moves it to the root position:\n4\n18\n5\n1\n14\n7\n9\n4\n2\nOnce again, the heap-ordered invariant is broken, since 4 was moved to the root. However, unlike the previous approach of just erasing the value\nat index 1, this method ensures that the remaining elements are still heap-ordered (since 4 was the only value that changed its position in the\nheap). This allows us to fix the heap by simply calling fix down on the node we swapped to the root, repeatedly swapping it with its highest\npriority child until it is in its correct position.\n4\n18\n5\n1\n14\n7\n9\n2\n18\n4\n5\n1\n14\n7\n9\n2\n18\n5\n4\n1\n14\n7\n9\n2\nIn summary, to pop an element off a priority queue that is implemented using a vector-based binary heap.\n1. Overwrite the root element with the element at the back of the underlying vector.\n2. Pop back the last element of the vector.\n3. Call fix down on the new root element to bring it down to the correct position.\n1\ntemplate <typename T>\n2\nvoid pop(std::vector<T>& data) {\n3\ndata[1] = data.back();\n4\ndata.pop_back();\n5\nfix_down(data, 1);\n6\n} // pop()\n.pop() .pop_back()Since overwrites an element and calls on the underlying vector (both of which take time) and then callsΘ(1)\nfix_down() .pop()(which takes time), the overall time complexity of is Θ(1+log(𝑛)), or just Θ(log(𝑛)).Θ(log(𝑛))\nExample 10.3 Consider the following max-heap. After inserting the value 55 into this max-heap and fixing the heap invariant, what is the\nfinal array representation of this max-heap?\n57\n52\n42\n48\n54\n49\n45\n43\n41\nSince this question involves adding an element to the heap, the new element is inserted at the very back, and fix up is continuously called on that\nelement until it is in the correct position. In this case, 55 is added as the left child of 49 and then fixed up to its position.\n57\n52\n42\n48\n54\n49\n55\n45\n43\n41\n57\n52\n42\n48\n54\n55\n49\n45\n43\n41\n57\n52\n42\n48\n55\n54\n49\n45\n43\n41\nThe final array representation of the max-heap is simply the elements of the heap in level order (i.e., left to right across each level of the tree). In\n[57, 55, 52, 45, 54, 48, 42, 41, 43, 49].this case, the array representation of the heap is", "word_count": 442, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "36e4a2b3-6f61-5cc7-ac47-5a9e210aa2e0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 298, "real_page_number": null, "text": "286\nChapter 10. Priority Queues and Heaps\nExample 10.4 Consider the following max-heap. After popping off the largest element, 57, and fixing the heap invariant, what is the final\narray representation of this max-heap?\n57\n52\n42\n48\n54\n49\n45\n43\n41\nSince this question involves removing an element from the heap, you should overwrite 57 with the last element in the heap, which is 43. Then,\nyou would pop the element at the back and call fix down continuously to move 43 down to the correct position. In this case 43, is swapped with\nits largest child until none of its children have a higher priority than it.\n43\n52\n42\n48\n54\n49\n45\n43\n41\n54\n52\n42\n48\n43\n49\n45\n41\n54\n52\n42\n48\n49\n43\n45\n41\nThe final array representation of the max-heap is a simply the elements of the heap in level order. In this case, the array representation of the\n[54, 49, 52, 45, 43, 48, 42, 41].heap is\nExample 10.5 You are given an empty min-heap priority queue, and you push the following values into the priority queue in this order: 32,\n52, 11, 23, 59, 10. What does the underlying array structure look like after all of these values are inserted?\nRemember that inserting an element into a heap involves adding the element to the back of the heap and continuously calling fix up on that\nelement until it is in the correct position. This process for the given sequence of insertions is detailed below. Note that this problem deals with a\nmin-heap, and not a max-heap!\n1. Insert 32 into the heap. Since the heap is currently empty, this insertion is trivial.\n32\n2. Insert 52 into the heap. In this case, 52 is added in the next open spot, which is the left child of 32. Since 52 has a lower priority than 32, the\nmin-heap is still valid.\n32\n52\n3. Insert 11 into the heap. In this case, 11 is added in the next open spot, which is the right child of 32. Since 11 has a higher priority than 32,\nthe min-heap is no longer valid, and we need to fix it. This is done by calling fix up on 11, which causes it to swap with 32.\n32\n11\n52\n11\n32\n52\n4. Insert 23 into the heap. In this case, 23 is added in the next open spot, which is the left child of 52. Since 23 has a higher priority than 52, the\nmin-heap is no longer valid, and we need to fix it. This is done by calling fix up on 23, which causes it to swap with 52.\n11\n32\n52\n23\n11\n32\n23\n52", "word_count": 461, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2f498c8a-cdcb-5238-bc80-e22881f20a4e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 299, "real_page_number": null, "text": "10.4 Heapify\n287\n5. Insert 59 into the heap. In this case, 59 is added in the next open spot, which is the right child of 23. Since 59 has a lower priority than 23,\nthe min-heap is still valid.\n11\n32\n23\n59\n52\n6. Insert 10 into the heap. In this case, 10 is added in the next open spot, which is the left child of 32. Since 10 has a lower priority than 32, the\nmin-heap is no longer valid, and we need to fix it. This is done by calling fix up on 10, which causes it to swap with 32 and 11.\n11\n32\n10\n23\n59\n52\n11\n10\n32\n23\n59\n52\n10\n11\n32\n23\n59\n52\n[10, 23, 11, 52, 59, 32].We are now done. The final array representation of the min-heap is\n10.4\nHeapify\n¸ 10.4.1\nTop-Down and Bottom-Up Heapify\nIf you were given an array of values in an arbitrary order, how can you turn it into a heap? The simplest approach would be to push the values\ninto a binary heap one at a time, fixing the heap with every operation (like in the previous example). However, there are several inefficiencies\nwith this method. First, the time complexity of this process is Θ(𝑛log(𝑛)), since 𝑛pushes are needed, each taking time. Second, thisΘ(log(𝑛))\nprocess uses additional memory, since you would have to push the contents of the original container into a new container for the heap.Θ(𝑛)\nIt turns out that there is a better way to turn a container of values into a heap using time and auxiliary space — this process isΘ(𝑛) Θ(1)\nknown as heapify. Heapify relies on the idea that, if fix up and fix down can be used to fix the position of any element in a heap, you should be\nable to turn any arbitrary container of data into a heap by just iterating through its contents and calling fix up or fix down on each element.\nIntuitively, this leads us to four possibilities, each of which is summarized below. We can choose whether we want to call fix up or fix down\non each element, and we can choose the direction of iteration. In the explanations below, forward iteration means starting from index 1 of the\nheap’s underlying vector (the root) and iterating to the end; backward iteration means starting at the back of the heap’s underlying vector (the\nrightmost leaf on the lowest level) and iterating to the front.\nHeapify Idea #1: Traverse in the direction, from the top of the heap to the bottom, calling on each element encountered.• forward fix up\n• Heapify Idea #2: Traverse in the direction, from the top of the heap to the bottom, calling on each element encountered.forward fix down\n• Heapify Idea #3: Traverse in the direction, from the bottom of the heap to the top, calling on each element encountered.backward fix up\n• HeapifyIdea#4: Traverseinthe direction,fromthebottomoftheheaptothetop,calling oneachelementencountered.backward fixdown\nUpon first glance, all four methods seem to work equally well. However, things are not as simple as they seem! It turns out that only two of the\nfour implementations work, and of these two, only one can be done in worst-case time.Θ(𝑛)\nLet’s first get rid of the two that do not work, ideas #2 and #3. These two methods may actually produce invalid heaps. Let’s look at\nexamples that break these two methods.\nProof: Traversing a heap in the forward direction (from top to bottom) and calling fix down on each element does always produce a validnot\n[1, 2, 3, 4, 5, 6, 7],heap. Suppose you are given the array and you wanted to create a max-heap. Moving from the top of this heap\n(1) to the bottom (7) and calling fix down on each element produces an invalid heap.\n1\n3\n7\n6\n2\n5\n4\nFirst, we call fix down on the element at index 1, which in this case is 1.\n1\n3\n7\n6\n2\n5\n4\n3\n1\n7\n6\n2\n5\n4\n3\n7\n1\n6\n2\n5\n4", "word_count": 711, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b7a50e16-a66f-52a6-b42c-818b45ad005e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 300, "real_page_number": null, "text": "288\nChapter 10. Priority Queues and Heaps\nNext, we call fix down on the element at index 2, which in this case is 2.\n3\n7\n1\n6\n2\n5\n4\n3\n7\n1\n6\n5\n2\n4\nNext, we call fix down on the element at index 3, which in this case is 7. 7 is already larger than its children, so it doesn’t change its position.\nSimilarly, calling fix down on the elements 4, 2, 6, and 1 does not do anything either, as those nodes are leaves, which do not have any\nchildren. Thus, our final heap looks like this:\n3\n7\n1\n6\n5\n2\n4\nThis is a valid max-heap, since 3 is smaller than 5 and 7. The problem with this approach is that the smallest element, 1, was swappednot\nwith 3, forcing 3 to the top of the heap. However, since we are iterating in the forward direction while only fixing downwards, the 3 is ignored\nand ends up stuck at the top, in the incorrect position.\nProof: Traversing a heap in the backward direction (from bottom to top) and calling fix up on each element does always produce a validnot\n[5, 3, 6, 1, 2, 4, 7],heap. To show this, suppose you are given the array and you wanted to create a max-heap. Moving from the\nbottom of this heap (7) to the top (5) and calling fix up on each element produces an invalid heap.\n5\n6\n7\n4\n3\n2\n1\nFirst, we call fix up on the last element, which in this case is 7.\n5\n6\n7\n4\n3\n2\n1\n5\n7\n6\n4\n3\n2\n1\n7\n5\n6\n4\n3\n2\n1\nNext, we call fix up on the elements 4, 2, 1, 5, 3, and 7, in this order. You can work through this process on your own, but none of these calls\nactually change the position of any nodes in the tree (since they are all smaller than their parent’s value). Thus, our final heap looks like this:\n7\n5\n6\n4\n3\n2\n1\nThis is a valid max-heap, since 6 is larger than 5. The problem with this approach is that the largest element, 7, was swapped with 6,not\nforcing 6 to the back of the heap. However, since we are moving toward the top while only fixing upwards, the 6 is ignored and ends up stuck\nat the bottom, in the incorrect position.\nAs a result, to correctly heapify a container of values, you must fix in the direction you come from! If you are traversing the heap from top to\nbottom, you must call fix on every element in the heap. If you are traversing the heap from bottom to top, you must call fix on everyup down\nheap.1element in the\n1Agoodanalogytothinkofisalayeroffreshsnowontheground. Asyouwalkthroughthesnow,youleavefootprints you,whilethesnowinfrontofyoubehind\nremainsuntouched. Ifyouwantto\"fix\"thesnow,youwouldneedtocleanupthefootprintsbehindyou. Thesameappliestoheapify;ifyouaremovingthrough\ntheheapinonedirection,youmustfixtheelementsintheopposingdirection,sincethatisthedirectioninwhichamesscouldhavebeenmade!", "word_count": 567, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "514d9de6-3112-5923-b362-2f426fffbdb5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 301, "real_page_number": null, "text": "10.4 Heapify\n289\nTo summarize the section up to this point, we discussed the heapify process, which can be used to turn a container of data into a valid heap\nin-place. Even though we introduced four possible implementations, only two of the four can be used to heapify a container and guarantee a\nvalid heap. These two methods are defined as follows:\n• Top-Down Heapify: Traverse from the top of the heap (the root) to the bottom (the leaves), calling fix up on each element encountered.\n• Bottom-UpHeapify: Traversefromthebottomoftheheap(theleaves)tothetop(theroot),callingfixdownoneachelementencountered.\nWhat is the time complexity of the heapify process? As alluded to earlier, these two heapify methods actually have time complexitiesdifferent\nin the worst case. Given a vector of size 𝑛, the top-down heapify method has a worst-case time complexity of Θ(𝑛log(𝑛)). The explanation for\nthis is straightforward: fix up is called 𝑛times, and since each call to fix up takes worst-case time, the overall worst-case complexityΘ(log(𝑛))\nof top-down heapify is Θ(𝑛×log(𝑛)). However, things get a bit bizarre with the bottom-up heapify method. Even though the bottom-up method\nmakes 𝑛calls to fix down, and each fix down takes worst-case time, the worst-case time complexity of the entire bottom-up process isΘ(log(𝑛))\nactually Θ(𝑛). Why is this the case?\nThe reason is that work. Fix up, for example, continuously swaps an elementnot every call to fix up or fix down does the same amount of\nwith its parent until it is in the correct position. Thus, the amount of work required for a single fix up call is greatest when the element to fix up\nis at the bottom of the heap, since this may potentially result in swaps (i.e., one for each level of the tree if the element at the bottom isΘ(log(𝑛))\nswapped all the way to the top). However, the work involved in a fix up call decreases as you move toward the top of the heap, since there are\nfewer levels that an element can potentially move up. Calling fix up on the root is essentially a operation, since the root cannot be fixed upΘ(1)\nany higher (as it is already at the top of the heap).\nFix down, on the other hand, continuously swaps an element with its highest priority child until it reaches the correct position. Thus, the\namount of work required for a single fix down call is greatest when the element to fix down is at the top of the heap, since this may result in\nswaps (which happens if the top node is swapped all the way to the bottom). However, the work involved in a fix down call decreasesΘ(log(𝑛))\nas you move toward the bottom of the heap, as there are fewer levels that an element can potentially move down. Calling fix down on a leaf node\nis essentially a time operation, since a leaf cannot be fixed down any lower (as leaves do not have any children).Θ(1)\nThis distinction is important because In fact, given a binary heapthere are more elements near the bottom of the heap than near the top.\nwith 𝑛nodes, at least half of them must be leaf nodes! As a result, when you heapify a heap, half of the nodes you need to fix up or down are\nleaf nodes. Since fixing up a leaf node is much more expensive than fixing down a leaf node, the bottom-up approach is more efficient. In fact,\nsince calling fix down on a leaf is guaranteed to do nothing, the bottom-up heapify approach can skip half of the tree!\nFrom a mathematical perspective, let’s show that the complexity of the bottom-up approach is Θ(𝑛). First, note that half of the nodes in a\nbinary heap are leaf nodes. Since fix down does nothing (as leaves do not have any children), no work needs to be done for these nodes:\nNumber of Nodes\nMax Levels to Fix (Distance to Leaf)\nMax Swaps Needed\n𝑛∕2\n0\n𝑛∕2×0=0\nNow, let’s consider nodes in the heap that are directly above a leaf node. Because a parent has at most two children and binary heaps are\ncomplete, there are approximately nodes that are exactly one level above a leaf. If fix down were called on any of these nodes, the maximum𝑛∕4\nnumber of levels that they could be swapped down is 1.\nNumber of Nodes\nMax Levels to Fix (Distance to Leaf)\nMax Swaps Needed\n𝑛∕4\n1\n𝑛∕4×1=𝑛∕4\nUsing this same process, we know that there must be nodes that are two levels above a leaf, nodes that are three levels above a leaf,𝑛∕8 𝑛∕16\nand so on. Eventually we will reach the root, which is approximately levels above a leaf. This allows us to complete the following table:log(𝑛)\nNumber of Nodes\nMax Levels to Fix (Distance to Leaf)\nMax Swaps Needed\n𝑛∕2\n0\n𝑛∕2×0=0\n𝑛∕4\n1\n𝑛∕4×1=𝑛∕4\n𝑛∕8\n2\n𝑛∕8×2=2𝑛∕8\n𝑛∕16\n3\n𝑛∕16×3=3𝑛∕16\n𝑛∕32\n4\n𝑛∕32×4=4𝑛∕32\n…\n…\n…\n1 (the root)\nlog(𝑛)\nlog(𝑛)\nThe total work completed during the entire heapify process is equal to the sum of the number of swaps needed at each level. This is equal to\n1𝑛(0+\n2+4\n3+8\n4+16\n1+…), which ends up simplifying to Θ(𝑛). This is because the multiplicative term (0+32\n2+4\n3+8\n4+16\nactually sums+…)32\nconstant.2up to a\n2Thissimplificationwon’tbeexplicitlyprovedinthischaptersinceitinvolvessomecomplexmathematicsoutsidethescopeofthisclass. Youdon’tneedtoknow\nhowtogetthe —youjustneedtoknowthatbottom-upheapifytakes time.Θ(𝑛) Θ(𝑛)", "word_count": 980, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dde33cfd-29ab-571b-bad1-724b6ee68275", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 302, "real_page_number": null, "text": "290\nChapter 10. Priority Queues and Heaps\nExample 10.6 Consider the following unsorted array. Perform a bottom-up heapify operation to turn this array into a valid min-heap.Θ(𝑛)\nWhat are the contents of the array after the heapify operation?\n[54, 66, 23, 74, 62, 33, 19]\nFirst, draw out this unsorted array as a binary tree:\n54\n23\n19\n33\n66\n62\n74\nTo perform a bottom-up heapify, we should start at the last element in the array and iterate to the front, calling fix down on each element we\nencounter. Because fix down has no effect on leaf nodes, we can skip the entire bottom row of this tree.\nWe begin calling fix down on the first node with a child at the back of the heap (index 3), which is 23.\n54\n23\n19\n33\n66\n62\n74\n54\n19\n23\n33\n66\n62\n74\nNext, we call fix down on the node at index 2, which is 66.\n54\n19\n23\n33\n66\n62\n74\n54\n19\n23\n33\n62\n66\n74\nLastly, we call fix down on the root, which is 54.\n54\n19\n23\n33\n62\n66\n74\n19\n54\n23\n33\n62\n66\n74\n19\n23\n54\n33\n62\n66\n74\n[19, 62, 23, 74, 66, 33, 54].The final contents of the array are\n¸ 10.4.2\n(✽)STL Heapify\n<algorithm>It turns out that, to no surprise, the heap operations we have covered so far are already implemented in the STL. The library\nstd::make_heap()provides the function, which can be used to heapify a container.\ntemplate <typename typenameRandomAccessIterator, Compare>\nvoid std::make_heap(RandomAccessIterator first, RandomAccessIterator last, Compare comp);\n[first, last) compRearranges the elements in the iterator range to create a heap. The comparator is optional, and without it, a\nmax-heap is created by default. The optional comparator can be used to create a heap whose priority is based on some other condition,\ncomp(a, b) true a b.where returns if has a lesser priority than\nstd::make_heap()The comparator is optional, and without it, will by default heapify the given container into a max-heap. However, if a\ncomp a b, acomparator is specified, the priorities of elements in the heap are determined using the following rule: for any two elements and\nb comp(a, b) true.has a lower priority than if returns\nstd::less<> std::make_heap()For instance, passing the comparator into would produce a max-heap: using the rules of deter-\na < b, a b std::less<T>(a, b) true a < b.mining priority, we know that if must have a lower priority than because returns if In\nstd::greater<> a > b, a bcontrast, passing in the comparator would create a min-heap: if must have a lower priority than because\nstd::greater<T>(a, b) true.returns\n1\nstd::vector<int32_t> v1 = {4, 6, 2, 9, 7, 3, 8, 1, 5};\n2\nstd::make_heap(v1.begin(), v1.end());\n// creates max-heap\n3\nfor (int32_t val : v1) {\n4\nstd::cout << val << ' ';\n// 9 7 8 6 4 3 2 1 5\n5\n} // for val", "word_count": 508, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d32176a2-c437-5bcc-860f-b1bd6a5a6a5d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 303, "real_page_number": null, "text": "10.5 Pairing Heaps\n291\n1\nstd::vector<int32_t> v2 = {4, 6, 2, 9, 7, 3, 8, 1, 5};\n2\nstd::greater<int>());std::make_heap(v2.begin(), v2.end(),\n// creates min-heap\n3\nfor (int32_t val : v2)\n4\nstd::cout << val << ' ';\n// 1 4 2 5 7 3 8 9 6\n5\n} // for val\nstd::push_heap() std::pop_heap()The STL algorithm library also provides the and methods, which can be used to insert and\nstd::push_heap()remove values from a heap. The method takes the value at the end of a given iterator range and fixes it into the correct\nstd::pop_heap()position in the heap. The method takes the value at the front of an iterator range, swaps it with the value at the back, and\nthen fixes the element that was swapped to the front into its correct heap-ordered position.\ntemplate <typename typenameRandomAccessIterator, Compare>\nvoid std::push_heap(RandomAccessIterator first, RandomAccessIterator last, Compare comp);\nlast - 1 [first, last - 1).Inserts the value at the position into the heap defined by the range The comparator is optional, and the\nmethod defaults to a max-heap if it is not provided.\n1\nstd::vector<int32_t> v3 = {4, 6, 2, 9, 7, 3, 8, 1};\n2\nstd::make_heap(v3.begin(), v3.end());\n// 9 7 8 6 4 3 2 1 5\n3\nv3.push_back(10);\n// 9 7 8 6 4 3 2 1 5 10\n4\nstd::push_heap(v3.begin(), v3.end());\n// 10 9 8 6 7 3 2 1 5 4\ntemplate <typename typenameRandomAccessIterator, Compare>\nvoid std::pop_heap(RandomAccessIterator first, RandomAccessIterator last, Compare comp);\nfirst last - [first, last - 1)Swaps the value at the position with the value at and turns the subrange into a heap. The comparator\nis optional, and the method defaults to a max-heap if it is not provided.\n1\nstd::vector<int32_t> v4 = {4, 6, 2, 9, 7, 3, 8, 1};\n2\nstd::make_heap(v4.begin(), v4.end());\n// 9 7 8 6 4 3 2 1 5\n3\nstd::pop_heap(v4.begin(), v4.end());\n// 8 7 3 6 4 1 2 9\n4\nv4.pop_back();\n// actually removes 9 from the vector\n// 8 7 3 6 4 1 2\nstd::make_heap()Since heapifies a given range of elements, its time complexity is Θ(𝑛), where 𝑛is the number of elements in the range.\nHowever, the other two methods assume that the given iterator range is already in the form of a valid heap, so they only need to fix one value\nstd::push_heap(),to its correct position (the value at the back needs to be fixed up for and the value that was newly swapped to the\nstd::pop_heap()).front needs to be fixed down for Thus, the time complexity of these methods is the same as the time complexity of\nperforming a single fix up or fix down, or Θ(log(𝑛)).\n¸ 10.4.3\nSummary of Binary Heaps\nInsummary,aheapisastructurethancanbeusedtoimplementapriorityqueue. Becauseheapssupportboth pushandpopoperations,Θ(log(𝑛))\nthey are more efficient than the standard sequence container implementations. There are many different types of heaps. The most common heap\nis the binary heap, where each node only supports at most two children. Although a binary heap can be visualized using a tree, it is common to\nstore a heap’s underlying data in a vector. For a tree to be considered a heap, it must be both complete and heap-ordered.\nThe binary heap relies on two important operations: fix up and fix down. If fix up is called on an element, the element is continuously\nswapped with its parent until it is in the correct position. If fix down is called on an element, the element is continuously swapped with its\nlargest-priority child until it is in the correct position. Both operations take time.Θ(log(𝑛))\nTo push an element into a priority queue that is implemented using a binary heap, the element should be added to the back of the heap’s\nunderlying vector, and then fix up should be called on that element.\nTo pop an element from a priority queue that is implemented using a binary heap, the element at the front of the heap’s underlying vector\nshould be overwritten with the element at the back of the vector. The last element should then be removed from the vector, and fix down should\nbe called on the element that was moved to the front.\nTo build a binary heap from a container of data, a bottom-up heapify should be used. To do this, start at the last internal node of the\ncontainer and iterate to the front, calling fix down on each element you encounter. Because fix down is cheaper for leaf nodes, the bottom-up\nheapify approach is more efficient than the top-down approach, as there are more nodes closer to the leaf nodes than there are closer to the root.\n10.5\nPairing Heaps\n¸ 10.5.1\nPairing Heap Structure\nA binary heap is a heap where each node has at most two children. However, binary heaps are not the only type of heaps that can be used to\nimplement a priority queue. In this section, we will discuss a type of heap known as a pairing heap. Pairing heaps are similar to binary heaps\nin that the priority of any node cannot be higher than that of its parent. However, unlike a binary heap, each node in a pairing heap can have\nmore than two children, and the structure of the tree does not need to be complete.\nBecause a node in a pairing heap can have many children, the array-based implementation is no longer feasible (there is no easy way to\ntraverse the branches of the tree using indexing). As a result, pairing heaps are implemented using a pointer-based approach. However, instead\nNodeof having each parent store pointers to each of their children (as was the case in a binary heap), each parent stores a pointer to onesingle\nsiblingof its children, and this child is linked to all of the other children using a pointer. This is done for efficiency purposes: if a node has\n281 children, for example, it would be impractical for that node to keep track of 281 pointers.", "word_count": 1019, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "700eaaa6-972a-5d3d-8c97-4a56731263ce", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 304, "real_page_number": null, "text": "292\nChapter 10. Priority Queues and Heaps\nPairing heaps come in two varieties: max pairing heaps and min pairing heaps. Much like binary heaps, the max pairing heap provides efficient\naccess to the element with the largest value, and the min pairing heap provides efficient access to the element with the smallest value. How\nwould you represent a pairing heap? Consider the following max pairing heap:\n55\n43\n31\n36\n52\n47\n39\nNode child sibling childBehind the scenes, each keeps track of a and pointer, in addition to the data it stores. A node’s pointer points\nsiblingto the leftmost child of the node, and the pointer links together all nodes that share a common parent.\nchild.Here, the root node 55 has three children. However, instead of keeping track of three pointers, 55 only keeps track of 47 as its\nsibling sibling siblingThe remaining children of 55 are linked together using pointers: 47’s is 52, and 52’s is 43. The rest of the\nconnections in the tree are labeled below (C for child, S for sibling):\n55\n43\n31\n36\n52\n47\n39\n55\n43\n31\n36\n52\n47\n39\nS\nS\nS\nC\nC\nC\nNode parentFor our pairing heap to be fully efficient, each will need to store an additional pointer. This pointer can either be a or a\nprevious pointer. Only one of these pointers is needed to complete all of the functionality of a pairing heap.\nNode parentIf each in the pairing heap stores a pointer, this pointer must point to the node’s parent. An example is shown below (the\nparentbolded arrows represent the pointers):\n1\ntemplate <typename T>\n2\nclass Node {\n3\nT data;\n4\nNode* child;\n5\nNode* sibling;\n6\nNode* parent;\n7\npublic:\n8\nexplicit Node(const T& val)\n9\nchild{nullptr},: data{val},\n10\nsibling{nullptr},\n11\nparent{nullptr} {}\n12\n};\nPairing Heap Using Parent Pointers\n55\n43\n31\n36\n52\n47\n39\nNode previousIf each in the pairing heap stores a pointer, this pointer must point to the sibling directly to the left of it. If the node is already\npreviousthe leftmost node among its siblings, its pointer should point to its parent. An example is shown below (the bolded arrows represent\npreviousthe pointers):\n1\ntemplate <typename T>\n2\nclass Node {\n3\nT data;\n4\nNode* child;\n5\nNode* sibling;\n6\nNode* previous;\n7\npublic:\n8\nexplicit Node(const T& val)\n9\nchild{nullptr},: data{val},\n10\nsibling{nullptr},\n11\nprevious{nullptr} {}\n12\n};\nPairing Heap Using Previous Pointers\n55\n43\n31\n36\n52\n47\n39\nparent previousThe or pointer will become useful when we modify elements in the pairing heap (which will be discussed later in this\nsection). Just remember that only of these pointers is enough to build a fully functional priority queue with optimal time complexities. Evenone\nthough having both may speed up things a little bit, the additional memory overhead involved with storing an extra pointer for every node in the\npairing heap is not a worthwhile tradeoff for the small increase in performance.", "word_count": 515, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8711d3ce-adf7-595f-a4ec-4e292bd0cdb2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 305, "real_page_number": null, "text": "10.5 Pairing Heaps\n293\n¸ 10.5.2\nMeld\nmeld(),The fundamental operation of a pairing heap is a method known as which can be used to combine two pairing heaps into a single\nmeld()pairing heap. When is called on two pairing heaps, the function checks the roots of the two trees. The tree with the lower priority root\nis then attached as the leftmost child of the other pairing heap (ties are broken arbitrarily). In the following example, the root of tree A is less\nthan the root of tree B, so melding tree A with tree B would cause tree A to be attached the leftmost child of tree B:\ntreeA\n55\n43\n31\n36\n52\n47\n39\ntreeB\n63\n59\n46\n44\n51\n63\n59\n46\n44\n51\n55\n43\n31\n36\n52\n47\n39\nmeld(treeA, treeB)\nmeld() parent previousThe implementation of differs based on whether each node stores a pointer or a pointer. The following illustrates\nparentthe pointer view:\nMelding Pairing Heaps Using Parent Pointers\n63\n59\n46\n44\n51\n55\n43\n31\n36\n52\n47\n39\n2\n1\n3\nHere, there are three pointers that may need adjustments when melding. These are bolded in the previous diagram:\nsibling1. The of the lower priority root should be set to the leftmost child of the higher priority root.\nparent2. The of the lower priority root should be set to the higher priority root.\nchild3. The of the higher priority root should be set to the lower priority root.\npreviousAlternatively, the following illustrates the pointer view:\nMelding Pairing Heaps Using Previous Pointers\n63\n59\n46\n44\n51\n55\n43\n31\n36\n52\n47\n39\n1\n2\n4\n3\nHere, there are four pointers that may need adjustments when melding. These are bolded in the previous diagram:\nsibling1. The of the lower priority root should be set to the leftmost child of the higher priority root.\nprevious2. The of the higher priority root’s leftmost child should be set to the lower priority root.\nchild3. The of the higher priority root should be set to the lower priority root.\nprevious4. The of the lower priority root should be set to the higher priority root.\nmeld()Because only moves a constant number of pointers around, its runtime is not affected by the size of the heap. Thus, each call to\nmeld() takes time.Θ(1)", "word_count": 402, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "470517ce-4db0-58ca-aafd-673aa29f57e7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 306, "real_page_number": null, "text": "294\nChapter 10. Priority Queues and Heaps\n¸ 10.5.3\nInserting and Removing Elements\nTo insert an element into the pairing heap, you can meld the newly created element with the rest of the pairing heap. If the new element has a\nlower priority than the root of the current pairing heap, add the new element as a child of the pairing heap. Otherwise, if the new element has a\nhigher priority than the current root, add the entire current pairing heap as a child of the new element. This process takes time.Θ(1)\n.push(17)\n18\n16\n14\n13\n12\n15\n17 < 18, so add 17 as leftmost child\n18\n16\n14\n13\n12\n15\n17\n.push(19)\n18\n16\n14\n13\n12\n15\n19 > 18, so add entire heap as leftmost child of 19\n19\n18\n16\n14\n13\n12\n15\nHowever, removing an element is slightly more complicated. Once the root is removed, there may be multiple children available; which one\nbecomes the new root? Furthermore, how should these children be melded together? Consider the following pairing heap:\n19\n17\n11\n16\n14\n13\n12\n15\n18\n17\n10\n17\nIf we were to pop 19 off the pairing heap, we would end up with five separate children:\n17\n18\n17\n10\n15\n16\n14\n13\n12\n17\n11\nA key detail to note is that all of the children are also pairing heaps themselves! As a result, we can rebuild our pairing heap by simply melding\nthe children back together. There are two approaches that can be efficiently used to do this: the multi-pass approach and the two-pass approach.\nIn the multi-pass approach, we complete the following:\n1. Take all the children and break all of their existing sibling connections.\n2. Push all of the children into a queue (this is done concurrently with step 1 while breaking sibling connections).\n3. Take two heaps from the front of the queue, meld them, and push the result to the back of the queue.\n4. Repeat step 3 until only one pairing heap remains.", "word_count": 345, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "71ed75d0-7574-598d-a9e3-d8efb907ebbb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 307, "real_page_number": null, "text": "10.5 Pairing Heaps\n295\nUsing the previous example, we would place the five children into a queue. Then, we would continuously remove two children from the front,\nmeld them together, and push them back into the queue. This is repeated until only one heap is left. The time complexity of this pop operation is\namortized 𝑂(log(𝑛)).\n17\n18\n17\n10\n15\n16\n14\n13\n12\n17\n11\nQueue\nfront\nback\npop first two heaps off the front of the queue, meld\nthem, and push the result to the back of the queue\nfront\nback\n15\n16\n14\n13\n12\n17\n11\n18\n17\n10\n17\npop first two heaps off the front of the queue, meld\nthem, and push the result to the back of the queue\nfront\nback\n17\n11\n18\n17\n10\n17\n16\n14\n13\n12\n15\npop first two heaps off the front of the queue, meld\nthem, and push the result to the back of the queue\nfront\nback\n16\n14\n13\n12\n15\n18\n17\n10\n17\n17\n11\npop first two heaps off the front of the queue, meld\nthem, and push the result to the back of the queue\n(only one left, so this is the completed pairing heap)\n18\n17\n10\n17\n17\n11\n16\n14\n13\n12\n15\nAlternatively, in the two-pass approach, we complete the following:\n1. Take all the children and break all of their existing sibling connections.\n2. Initialize both a queue and a stack (or alternatively, two deques). Push all of the children into the queue (this is done concurrently with\nstep 1 while breaking sibling connections).\n3. Make a left-to-right pass over the trees in the queue and meld them in pairs. Push the resultant trees into the stack. If there are an odd\nnumber of children, the last tree gets placed in the stack without being melded.\n4. Start with the right-most tree and meld the remaining trees into this tree one at a time. This can be done by taking out the tree at the top\nof the stack and continuously melding it with the remaining trees in the stack.", "word_count": 357, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2d4dd5fc-0ad1-587c-80fe-334de01fe37e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 308, "real_page_number": null, "text": "296\nChapter 10. Priority Queues and Heaps\nLike with the multi-pass approach, the two-pass approach can also be done in amortized time. (Note that the stack in the following𝑂(log(𝑛))\nillustration is inverted and flipped on its side.)\n17\n18\n17\n10\n15\n16\n14\n13\n12\n17\n11\nQueue\nfront\nback\nmeld these two trees and\npush it into the stack\nmeld these two trees and\npush it into the stack\nodd one out, so push into\nthe stack without melding\nStack\nbottom\ntop\n18\n17\n10\n17\n16\n14\n13\n12\n15\n17\n11\nmeld the two trees at\nthe top of the stack\nStack\nwhile stack is not empty,\npop tree at top of stack\nand meld with this tree\nbottom\ntop\n18\n17\n10\n17\n17\n11\n16\n14\n13\n12\n15\n18\n17\n10\n17\n17\n11\n16\n14\n13\n12\n15\nthis is the completed pairing heap\nRemark: At this point, you might ask: why do we have to perform multiple passes when reconstructing the pairing heap? Can’t we just list\nout the children and iterate over them once, melding the children together into an accumulated pairing heap?\nIt turns out that such an approach is not as efficient for repeated operations. When you make multiple passes and continuously meld pairs of\nchildren with each pop, over time you will end up with more ideal trees with fewer intermediate children of the root. This allows the pop\noperation to support an amortized time complexity, as mentioned (we will go over amortization in chapter 12). On the contrary, if𝑂(log(𝑛))\nyou simply make a single pass over the children and meld them one-by-one into an accumulated heap, you may eventually end up with a\npairing heap with many immediate children after subsequent pops, which would make the pop operation more expensive (since there are\nmore immediate children to meld together).", "word_count": 315, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "de900870-aef9-50a6-8b2d-6eee60db0722", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 309, "real_page_number": null, "text": "10.5 Pairing Heaps\n297\n¸ 10.5.4\nImplementing the Pairing Heap Copy Constructor\nBecause the pairing heap uses dynamic memory, we will need to implement a custom copy constructor, destructor, and assignment operator.\nThe copy-swap method can be used to implement the assignment operator (make a temporary copy of the other pairing heap and swap the\ncontents of the current pairing heap with the temporary one).\nWhen implementing the copy constructor, we cannot just copy over the root node’s pointer — we must walk through the entire tree and\ncopy over every node. Note that the structure of the heap does not need to be the same between the original and the copy. As long as the copy is\nstill a valid pairing heap that holds the same elements, the copy is valid.\nTo walk through all the elements of a pairing heap, we will utilize a queue to store the elements we have yet to visit. Whenever we visit a\n(child sibling)node in the pairing heap, we push a copy to our new pairing heap and push all of the node’s direct connections and into the\nqueue — this ensures that all nodes in the heap will be pushed into the queue at some point during our algorithm. Then, we use the following\nprocess to make a copy of the pairing heap:\n1. Push the root node of the pairing heap into a queue.\n2. Retrieve the node at the front of the queue and pop it off.\n3. Add all the direct connections of the node into the queue. If the node that was taken out has a child, push the child into the queue. If the\nnode that was taken out has a sibling, push the sibling into the queue.\n4. Push the value of the node that was taken out of the queue into the new pairing heap that is being copy constructed.\n5. Repeat steps 2-5 until the queue is empty.\nFor instance, suppose we wanted to copy the following pairing heap:\n19\n15\n18\n16\n17\nchild siblingFirst, we will copy over the root node, 19. Before we push 19 into our new pairing heap, we must first push its and into our\nnullptr). childqueue (if they aren’t In this case, 19 has a of 18, so 18 gets pushed into the queue. After its connections are pushed into the\nqueue, 19 is pushed into the new pairing heap.\nQueue\nfront\nback\n18\nCopy Constructed Heap\n19\nchild siblingThen, we pop the element at the front of the queue (18). We push 18’s (17) and (15) into the queue and push 18 into our heap.\nQueue\nfront\nback\n17\n15\nCopy Constructed Heap\n19\n18\nsibling child,Continuing this process, we then take out 17, push its (16) into the queue, and push 17 into our heap. 17 has no so only the\nsibling is pushed into the queue.\nQueue\nfront\nback\n15\n16\nCopy Constructed Heap\n19\n18\n17\nWe then take out 15. Since 15 has no children or siblings, nothing is added to the queue. 15 is pushed into the heap.\nQueue\nfront\nback\n16\nCopy Constructed Heap\n19\n18\n17\n15", "word_count": 534, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bcc346fd-fb21-5686-bcf1-b32b0a2f1258", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 310, "real_page_number": null, "text": "298\nChapter 10. Priority Queues and Heaps\nWe then take out 16. Since 16 has no children or siblings, nothing is added to the queue. 16 is pushed into the heap.\nQueue\nfront\nback\nCopy Constructed Heap\n19\n18\n17\n15\n16\nThe queue is empty, so all of the elements have been successfully copied over. Even though the structure of the new, copy constructed pairing\nheap is different from the original heap, this is okay because the new heap is still a valid pairing heap with the same elements as the copy. The\ninternal structure of the new pairing heap does not matter as long as we can still efficiently retrieve the element with the highest priority.\nBecause the copy constructor iterates over all the nodes in a pairing heap and performs a copy, the time complexity of the copy constructor\nis Θ(𝑛), where 𝑛is the number of nodes in the pairing heap that is being copied.\n¸ 10.5.5\nImplementing the Pairing Heap Destructor\nThe destructor works similarly to the copy constructor. The only difference is that, instead of pushing the nodes we visit into a new pairing heap,\neach node is deleted after we take it out of the queue. This ensures that all of the nodes are cleaned up when the heap is destroyed. To destruct a\npairing heap, complete the following process:\n1. Push the root node of the pairing heap into a queue.\n2. Retrieve the element at the front of the queue and pop it off.\n3. Add all the direct connections of the node into the queue. If the node that was taken out has a child, push the child into the queue. If the\nnode that was taken out has a sibling, push the sibling into the queue.\n4. Delete the node that was taken out.\n5. Repeat steps 2-5 until the queue is empty.\nFor instance, suppose we wanted to destruct the following pairing heap:\n19\n15\n18\n16\n17\nFirst, we add all of 19’s direct connections into the queue (here, 18 is added since this is 19’s child). The dynamic memory of 19 is then freed.\nQueue\nfront\nback\n18\nHeap to Destruct\n19\n15\n18\n16\n17\nWe then pop the element at the front of the queue, or 18. We add 18’s direct connections (17 and 15) into the queue before freeing 18’s memory.\nQueue\nfront\nback\n17\n15\nHeap to Destruct\n19\n15\n18\n16\n17\nWe then take out the element at the front of the queue, or 17. We add 17’s direct connections (16) into the queue before freeing 18’s memory.\nQueue\nfront\nback\n15\n16\nHeap to Destruct\n19\n15\n18\n16\n17", "word_count": 450, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d5fe4671-474d-50f8-a5e5-6a56a591e611", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 311, "real_page_number": null, "text": "10.5 Pairing Heaps\n299\nWe then take out 15. Since 15 has no children or siblings, nothing is added to the queue before it is deleted.\nQueue\nfront\nback\n16\nHeap to Destruct\n19\n15\n18\n16\n17\nWe then take out 16. Since 16 has no children or siblings, nothing is added to the queue before it is deleted.\nQueue\nfront\nback\nHeap to Destruct\n19\n15\n18\n16\n17\nThe queue isempty, which meansall thenodes havebeen cleaned up. Similar tothe copydestructor, the destructor iteratesover allthenodes ina\npairing heap to delete them; the time complexity of the destructor is thus also Θ(𝑛), where 𝑛is the initial size of the pairing heap to be destroyed.\n¸ 10.5.6\nUpdating Priorities\nSo far, we have covered the basics of constructing and destructing a pairing heap. However, suppose someone went behind the scenes and\nmessed up all the data in your pairing heap. Now your pairing heap is invalid! How would you go about fixing it?\nOne problem with this situation is that you do not know where the invalid elements could be. Maybe one element is in the wrong place, or\nmaybe all the elements are in the wrong place. As a result, the best way to go about this is to break apart the entire heap and rebuild it. To do\nso, you will need to walk through the entire pairing heap. Like before, this can be done using a queue, which guarantees that each element in\nthe pairing heap is visited once. First, push the root of the invalid pairing heap into a queue and reset the root of the current pairing heap to\nnullptr. Then, repeat the following process until the queue is empty:\n1. Retrieve the node at the front of the queue and pop it off. This node is the root of a subheap of the initial pairing heap, so it may have\nadditional connections (i.e., children and siblings) that will need to be broken (this is different from the copy constructor and destructor\nsince we are rearranging the nodes of an existing pairing heap).\nAdd all the direct connections of this node (i.e., the root of the removed subheap) into the queue. If the node has a child, push the child2.\ninto the queue. If the node has a sibling, push the sibling into the queue.\nchild, sibling parent/previous nullptr.3. Break all of the subheap’s existing connections by setting and to\n4. After breaking all of its connections, meld the removed subheap with the current pairing heap.\nTo illustrate this, suppose you wanted to fix the following invalid pairing heap:\n27\n23\n39\n45\n21\n14\nnullptr.First, push the root (27) into a queue and set the current pairing heap pointer to\nQueue\nfront\nback\n27\nCurrent Heap\nnullptr\nchild sibling, childThen, we take out the node at the front (27) and push in its and if there are any. In this case, 27 has a of 14, so 14\nnullptr).gets pushed into the queue. Then, we break all of 27’s connections and meld it with the new pairing heap (which is currently\nQueue\nfront\nback\n14\nCurrent Heap\n27", "word_count": 539, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "41076154-4c0d-5c0f-aadb-41ddf51abb73", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 312, "real_page_number": null, "text": "300\nChapter 10. Priority Queues and Heaps\nsibling childWe then take 14 out of the queue, push its (39) into the queue (there is no to push in), break all of 14’s connections, and meld\nit with the new pairing heap.\nQueue\nfront\nback\n39\nCurrent Heap\n27\n14\nchild siblingWe then take 39 out of the queue, push its (21) and (23) into the queue, break all of 39’s connections, and meld it with the\nnew pairing heap.\nQueue\nfront\nback\n21\n23\nCurrent Heap\n39\n27\n14\nsiblingWe then take 21 out of the queue, push its (45) into the queue, break all of 21’s connections, and meld it with the new pairing heap.\nQueue\nfront\nback\n23\n45\nCurrent Heap\n39\n27\n14\n21\nWe then take 23 out of the queue. Since 23 has no children or siblings, nothing is pushed into the queue. We break all of 23’s connections and\nmeld it with the new pairing heap.\nQueue\nfront\nback\n45\nCurrent Heap\n39\n27\n14\n21\n23\nWe then take 45 out of the queue. Since 45 has no children or siblings, nothing is pushed into the queue. We break all of 45’s connections and\nmeld it with the new pairing heap.\nQueue\nfront\nback\nCurrent Heap\n45\n39\n27\n14\n21\n23\nSince the queue is now empty, we have successfully fixed the pairing heap. The new pairing heap points to the same data as before, but the\nrelationships between elements are now valid. Because we are iterating over all the nodes of the pairing heap to rebuild it, the time complexity\nof this procedure is Θ(𝑛), where 𝑛is the number of nodes in the pairing heap.", "word_count": 287, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "47b8b35d-fc32-507c-9258-2782f6930b01", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 313, "real_page_number": null, "text": "10.6 The STL Priority Queue Container\n301\n¸ 10.5.7\nIncreasing the Priority of an Existing Element\nNow, suppose someone the priority of a single element in the heap, and you are given the node that is modified. In this case, breakingincreased\nand rebuilding the entire heap is no longer an efficient way to fix the heap. This is because the rest of the heap is still valid, and tearing it apart\nand rebuilding it would be a waste of time. Instead, if the priority of a single element is increased, all you need to do is compare the updated\nparent previousnode with the value of its parent (if there is one). This is where the or pointer comes in.\nIf the modified value still has a lower priority than its parent after the update, then nothing needs to be done since the heap would still be\nvalid. However, if the modified value has a higher priority than its parent, it must be broken off from its siblings and remelded with the root.\nSince we are only considering an increase in priority for this problem, the children of the modified node do not need to be detached, as they\nare guaranteed to remain in a valid position. Had the element’s priority been decreased instead (which we will not deal with in this class), its\nchildren would also need to be detached and remelded back into the tree. Consider the following example, where 38 is updated to 83:\nOriginal Heap\n79\n26\n38\n28\n22\n62\n54\nModified Heap\n79\n26\n83\n28\n22\n62\n54\nBecause the updated value of 83 is larger than the value of its parent, 79, the entire subtree rooted at 83 must be broken off and remelded with\nthe original root:\n79\n26\n62\n54\n83\n28\n22\n83 broken off\nand remelded\n83\n28\n22\n79\n26\n62\n54\nparent previous parentThe implementation of this operation depends on whether a or pointer is used. If a pointer is used, comparing\nparentthe modified element to its parent is fast, since the pointer provides direct access to the parent. However, in order to break the sibling\nconnections of the nodes adjacent to the modified node, you must traverse rightward through the siblings on its level to find the element directly\npreviousto the left of the modified node. On the other hand, if a pointer is used, comparing the modified node with its parent takes longer,\nsince you would need to traverse leftward through the siblings on the same level to find the parent. However, breaking the sibling connections is\npreviousfaster with a pointer, since you have direct access to the sibling directly to the left of the modified node.\npreviousRemark: If you have a pairing heap that is implemented with a pointer, it may be worthwhile to skip the parent check entirely\npreviousand breakandmeldwitheveryupdate. Thisisbecausetheupdatednode’sparentisnoteasilyaccessibleifyouonlyhaveaalways\npointer, and the cost of breaking and melding even when it is not necessary may still be preferable to the cost of iterating over all the siblings\nparentto find the parent. That being said, if you do have a pointer, it is still worthwhile to check if corrective action is needed, since you\nhave direct access to the parent, and it can save you the cost of doing unnecessary work.\n10.6\nThe STL Priority Queue Container\nLike many of the other containers we have discussed, the STL provides a priority queue data structure that is implemented for you. Behind the\nheap.3 <queue>scenes, the STL’s priority queue is typically implemented using a binary To use an STL priority queue, include the library\nstd::priority_queue<>and declare a using the following syntax:\nstd::priority_queue<TYPE, CONTAINER, COMPARATOR> name_of_pq;\nstd::stack<> std::queue<>, std::priority_queue<>Similar to the and the is a container adaptor that is built on top of another\n(CONTAINER).container that stores its data. This container can be specified as the second term in the template definition There are limitations\nto what this underlying container can be — a priority queue’s underlying container must be sequential and support random access, since binary\nheaps rely on efficient indexing to access a node’s children.\nThe third term of the template definition is a comparator that can be used to determine the priority of elements in the priority queue. Given\nstd::make_heap() comp(a, b) truea comparator, the rules for determining priority are the same as it was in the case of — if returns\na b a b.for any two elements and in the priority queue, has a lower priority than\n3Whynotapairingheaporanyothertypeofheap? Eventhoughpairingheapsaretheoreticallymoreefficient,binaryheapsoftenoutperformmorecomplicated\nstd::priority_queue<>heapsduetocaching,sincememoryclosertogetherinmemorycanbesequentiallyaccessedfaster. Also,itshouldbenotedthat\nis to be implemented using a binary heap, as long it conforms to the interface specified by the standard. As always, STL containers arenot guaranteed\nimplementationdefined,anddifferentlibrariesmayusedifferentimplementations(althoughmanyuseabinaryheapofsomeform).", "word_count": 873, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "46281e8d-316e-55f5-8f42-7d57f15d4a63", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 314, "real_page_number": null, "text": "302\nChapter 10. Priority Queues and Heaps\nTYPE) CONTAINEROnly the first term (the is required when declaring an STL priority queue. If the is not specified, the underlying\nstd::vector<>. COMPARATORcontainer defaults to a If the is not specified, the priority queue defaults to a max priority queue using the\nstd::less<> comparator. For example, the following initializes a max priority queue of integers that uses a vector for its underlying data:\nstd::priority_queue<int32_t> max_pq;\nCOMPARATORUnfortunately, the must be the third term in the template definition; if you want to declare anything other than a max priority\nqueue, you must also specify the underlying container, even if you want to use the default. For instance, the following line of code constructs a\nstd::greater<>min priority queue using the comparator:\nstd::priority_queue<int32_t, std::vector<int32_t>, std::greater<int32_t>> min_pq;\nstd::vector<int32_t>Here, the term be included if you want to specify a custom comparator. The following line of code wouldmust\nCOMPARATORcompile, since the cannot be the second term in the template definition.not\nstd::priority_queue<int, std::greater<int>> min_pq;\n// does not compile!\nstd::priority_queue<>The supports the following operations:\nFunction\nBehavior\n.push(val)\nvalAdds to the priority queue\n.emplace(args)\nConstructs a new element in-place (i.e., no copies or moves) using the provided constructor arguments\n.pop()\nRemoves the element with the highest priority in the priority queue\n.top()\nReturns a const reference to the element with the highest priority in the priority queue\n.size()\nReturns the number of elements in the priority queue\n.empty()\nChecks if the priority queue is empty\n.top() constThe function returns a reference to the top element in the priority queue because you are allowed to modify an elementnot\nstd::priority_queue<>once it is in the priority queue. This is because the does not automatically fix itself if an element’s priority is\ninvalidated. If you want to modify the top element of a priority queue, the safest way would be to pop the element out and repush it in with a\nmutablenew value. For larger structs or classes, you can use the keyword to denote member variables that can be modified, but you must be\ncareful not to invalidate the priority queue in the process.\nThe STL priority queue also supports a that can be used to construct a priority queue out of an existing container of data.range constructor\nAs a result, if you have a container of data values that you want to insert into a priority queue, you do not need to push the elements in one by\none. Instead, you can pass in an iterator range when constructing the priority queue, and the range constructor will construct the priority queue\nusing the elements in the range. An example is shown below:\nstd::vector<int32_t> vec = {12, 66, 34, 25, 84, 25, 17, 98, 53};\nstd::priority_queue<int32_t> pq(vec.begin(), vec.end());\nThe above code constructs a priority queue by heapifying the contents of the vector in time. This is more efficient than pushing each of theΘ(𝑛)\nnumbers into the priority queue individually, which would take worst-case time (𝑛pushes that each take time).Θ(𝑛log(𝑛)) Θ(log(𝑛))\nExample 10.7 Consider the following code, which uses the STL priority queue and a custom comparator:\n1\nstruct Comp {\n2\nbool operator() (const int32_t const int32_t constlhs, rhs) {\n3\nreturn abs(lhs - 100) > abs(rhs - 100);\n4\n} // operator()()\n5\n};\n6\n7\nint main() {\n8\nstd::vector<int32_t> vec = {112, 166, 314, 251, 184, 25, 117, 98, 153};\n9\nstd::priority_queue<int32_t, std::vector<int32_t>, Comp> pq(vec.begin(), vec.end());\n10\nstd::cout << pq.top() << '\\n';\n11\n} // main()\nWhat is the output of this code?\nIn this example, the priority queue is given a custom comparator. Which element has the highest priority according to this comparator? To start\noff, let’s look at the first two elements in the vector, 112 and 166. If we passed 112 and 166 into the comparator in this order, the comparator\nfalse abs(112 - 100) abs(166 - 100).would return because is not greater than Thus, 112 must have a lower priority than 166not\nfalse(since = higher priority).\nWhat about the elements 25 and 166 — which one has the higher priority? If we passed 25 and 166 into the comparator in this order, the\ntrue abs(25 - 100) abs(166 - 100).comparator would return because is greater than Thus, 25 must have a lower priority than 166.\na b, comp(a, b)In fact, the priority of an element is determined by its positive difference from 100. Given two numbers and would only\ntrue a b.return if is farther from 100 than Thus, elements that are farther from 100 must have a lower priority than elements that are closer to\n100. Since the code prints out the element with the highest priority, the output must be the value in the vector that is closest to 100, or 98.", "word_count": 809, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "31d80e96-9800-5705-bc8b-f1892dd4c17c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 315, "real_page_number": null, "text": "10.6 The STL Priority Queue Container\n303\nExample 10.8 Consider the following object, which stores information on a stock order:\n1\nstruct StockOrderInfo {\n2\nint32_t price;\n3\nint32_t quantity;\n4\nint64_t timestamp;\n5\nint64_t order_id;\n6\n};\nprice quantity timestampThe member stores the price of an order, the member stores the size of an order, the member stores the\norder_idtime at which an order was placed, and the member stores the order ID. For the sake of simplicity, assume that all values for\nthese four members will be positive integers (this removes the issue of floating-point comparison precision, etc.).\nGiven a set of resting BUY orders that need to be executed, a broker-dealer must satisfy orders in the following priority:\n• Orders with the highest price must be executed first, regardless of quantity or time of order.\n• If there are multiple orders with the same price, the order with the earlier timestamp must be executed first.\n• If there are multiple orders with the same price and time of order, the order with the larger quantity must be executed first.\n• If there are multiple orders with the same price, time of order, and quantity, the order with the smaller order ID must be executed first.\nAssume that all orders have a unique ID.\nThe orders are to be inserted into a priority queue, where orders with higher priority are executed first. Implement a comparator that can be\nused to define this priority queue such that it follows the provided rules above.\na b, true a b.Given two stock orders and our comparator should return if has a lower priority than What determines if a stock order has a\nlower priority?\n• Giventwoorderswiththesameprice,timeoforder,andquantity,thestockorderwiththelargerorderIDhasthelowerpriority. Therefore,\ntruewe return for larger IDs in this case.\n• Given two orders with the same price and time of order, the stock order with the smaller quantity has the lower priority. Therefore, we\ntruereturn for smaller quantities in this case.\n• Given two orders with the same price where no variables match, the stock order with the later timestamp has the lower priority. Therefore,\ntruewe return for larger timestamps in this case.\ntrue• Otherwise, if given two orders, the stock order with the lower price has the lower priority. Therefore, we return for lower prices in\nthis case.\nThe constructor below satifies the rules above (the steps for defining a comparator mirrors the process introduced in section 1.6, so we will not\nbe fully walking through every step here):\n1\nstruct StockOrderComparator {\n2\nbool operator() (const const constStockOrderInfo& lhs, StockOrderInfo& rhs) {\n3\nif (lhs.price == rhs.price) {\n4\nif (lhs.timestamp == rhs.timestamp) {\n5\nif (lhs.quantity == rhs.quantity) {\n6\nreturn lhs.order_id > rhs.order_id;\n7\n} // if\n8\nreturn lhs.quantity < rhs.quantity;\n9\n} // if\n10\nreturn lhs.timestamp > rhs.timestamp;\n11\n} // if\n12\nreturn lhs.price < rhs.price;\n13\n} // operator()()\n14\n};\nWe can then define our priority queue as follows:\nstd::priority_queue<StockOrderInfo, std::vector<StockOrderInfo>, StockOrderComparator> stock_pq;\nSome example code is provided below:\n1\nint main() {\n2\nstd::priority_queue<StockOrderInfo, std::vector<StockOrderInfo>, StockOrderComparator> stock_pq;\n3\nstock_pq.push(StockOrderInfo{.price = 50, .quantity = 30, .timestamp = 12343, .order_id = 1023});\n4\nstock_pq.push(StockOrderInfo{.price = 50, .quantity = 35, .timestamp = 12343, .order_id = 1024});\n5\nstock_pq.push(StockOrderInfo{.price = 50, .quantity = 45, .timestamp = 12344, .order_id = 1025});\n6\nstock_pq.push(StockOrderInfo{.price = 50, .quantity = 45, .timestamp = 12344, .order_id = 1026});\n7\nstock_pq.push(StockOrderInfo{.price = 60, .quantity = 25, .timestamp = 12345, .order_id = 1027});\n8\n9\nstd::cout << \"The orders are executed in the following order: \";\n10\nwhile (!stock_pq.empty()) {\n11\nstd::cout << stock_pq.top().order_id << \" \";\n12\nstock_pq.pop();\n13\n} // while\n14\n} // main()\nThe output of this code is:\nThe orders are executed in the following order: 1027 1024 1023 1025 1026", "word_count": 662, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a7c67e45-d29f-565a-a20e-7723f74b5c64", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 316, "real_page_number": null, "text": "304\nChapter 10. Priority Queues and Heaps\n10.7\nSolving Problems Using Priority Queues\nIn this section, we will explore some problems that can be solved using priority queues.\n¸ 10.7.1\nk Largest Elements in an Array\nExample 10.9 Suppose you are given an array of 𝑛elements, and you want to write a program that prints the 𝑘largest elements in the\narray, in any order. How would you approach this problem?\nThe simplest solution would be to sort the array and print out the 𝑘largest elements. However, this is not the most efficient approach, since\nsorting takes time (we will go over more detail on sorting algorithms in chapter 14). A better solution would be to heapify our dataΘ(𝑛log(𝑛))\nintoa andextractoutthe𝑘largestelements. Heapifyinganarraytakesworst-case time, andpoppingoutthelargestelementtakesΘ(𝑛)max-heap\nworst-case time. Since this algorithm pops out the largest element 𝑘times after heapifying, the overall worst-case time complexity ofΘ(log(𝑛))\nthis algorithm is — from the heapify step, and from popping 𝑘times.Θ(𝑛+𝑘log(𝑛)) Θ(𝑛) Θ(𝑘log(𝑛))\nHowever, there is a better way to solve this problem. This process is a less intuitive, since it uses a to find the 𝑘largest elementsmin-heap\nrather than a max-heap. In this procedure, the first 𝑘elements of the array are added to the min-heap. Then, the remaining 𝑛−𝑘elements are\ncompared with the element at the top of the min-heap. If any of the elements encountered is larger than the top of the min-heap, pop off the top\nvalue and push in the new, larger value. This ensures that the min-heap will always contain the 𝑘highest priority elements that have been seen so\nfar. The worst-case time complexity of this process is Θ(𝑛log(𝑘)); this is because 𝑛pushes and pops may be needed, which each take worst-case\ntime. The auxiliary space used is Θ(𝑘), since the priority queue is capped at a size of 𝑘. This algorithm is illustrated below:Θ(log(𝑘))\n15\n61\n36\n24\n46\n47\n53\n28\nIf we wanted to print the 3 largest elements in this array, we would first push the first 3 elements into a min-heap.\n15\n61\n36\n24\n46\n47\n53\n28\n15\n36\n61\nThen, we would iterate through the remaining elements and compare them with the top element. If an element has a larger priority than the top\nelement of the min-heap, pop off the element at the top and push the current value in.\n15\n61\n36\n24\n46\n47\n53\n28\n24 > 15, so pop 15 out and push 24 in\n24\n36\n61\n15\n61\n36\n24\n46\n47\n53\n28\n46 > 24, so pop 24 out and push 46 in\n36\n46\n61\n15\n61\n36\n24\n46\n47\n53\n28\n47 > 36, so pop 36 out and push 47 in\n46\n47\n61\n15\n61\n36\n24\n46\n47\n53\n28\n53 > 46, so pop 46 out and push 53 in\n47\n53\n61\n15\n61\n36\n24\n46\n47\n53\n28\n28 < 47, so it is not pushed into the min-heap\n47\n53\n61\nThe three largest elements in the array are thus 47, 61, and 53, since these are the elements remaining in the min-heap. The worst-case time\ncomplexity of this algorithm can be further improved from to if the first 𝑘elements were heapified rather thanΘ(𝑛log(𝑘)) Θ(𝑘+(𝑛−𝑘)log(𝑘))\nbeing pushed in one by one.", "word_count": 571, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5205bed9-0b73-564e-8050-150c34bc2e89", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 317, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n305\n¸ 10.7.2\nStreaming Median Algorithm\nIn this section, we will discuss how heaps can be used to efficiently determine the median of a of data. Unlike an array, the data instream\nthe stream must be accessed sequentially, and there is no way to predetermine the number of elements in a stream nor identify the values of\nelements that are yet to be extracted.\nExample 10.10 Devise an algorithm that can be used to determine the median value of all values that you have seen at any point in a given\nstream. For example, consider the following stream of data:\n12 55 74 35 96 52 54 53 63 23 37 ...\nIf you were asked to calculate the median after the first seven numbers in the stream are extracted (12, 55, 74, 35, 96, 52, and 54), you would\nreturn 54, since that is the median of the first seven numbers. If you were asked again to calculate the median after extracting 53, the new\nmedian would be 53.5. How can you accomplish this task efficiently?\nA naïve approach would be to store all the values you have extracted so far in a vector and sort the vector whenever you need to calculate the\nmedian. For example, if you were asked to determine the median after extracting the first seven elements, you would sort the first seven elements\nand retrieve the middle element, as shown:\n12\n35\n52\n54\n55\n74\n96\nHowever, sorting is an process, which can quickly blow up the runtime if you are asked to query the median multiple times. Instead,Θ(𝑛log(𝑛))\na better method would be to use a heap to keep track of the streaming median.\nIf you think back to the previous problem of finding the 𝑘largest elements in an array, we used a min-heap of size 𝑘to keep track of the\nlargest 𝑘elements seen so far. A similar approach can be used to help us find the streaming median: we will instantiate a max-heap aand\nmin-heap, where the former keeps track of the smallest elements, and the latter keeps track of the largest elements. By doing so, we𝑛∕2 𝑛∕2\nensure that the values needed to calculate the median are always accessible at the top of the two heaps. This process can be summarized as\nfollows:\nEvery time you extract an element, determine whether it should be pushed into the max-heap (which stores the smaller half of elements1.\nseen so far) or the min-heap (which stores the larger half of elements seen so far).\nThis is done by checking the values at the top of the heaps: if the new element is larger than the top of the max-heap, push it into•\nthe min-heap; otherwise, push it into the max-heap. If both heaps are empty, you can push the new element into either heap.\n2. If the sizes of the two heaps differ by more than one, you will have to rebalance the heaps to ensure that the median value is still on top.\n• This is done by popping the top element off the larger heap and pushing it into the smaller one.\nTo calculate the median at any point in time, first check the sizes of the two heaps. If they have the same size, there must be an even3.\nnumber of elements, so the median would be the average of the two values at the top of the heaps. Otherwise, there is an odd number of\nelements, so the median would be the value at the top of the heap with the larger size.\nConsider the same stream as above, but this time using the heap approach:\n12 55 74 35 96 52 54 53 63 23 37 ...\nFirst, we declare two heaps: a max-heap that stores the smaller half of values seen, and a min-heap that stores the larger half of values seen. The\nfirst value we extract, 12, can go in any heap, so we will arbitrarily add it to the left one.\nLeft Max-Heap\n12\nRight Min-Heap\nThe next element in the stream, 55, is larger than the top of the left heap, so it gets added to the right heap.\nLeft Max-Heap\n12\nRight Min-Heap\n55\nAt this point, the median is the average of the two top values, or (12 + 55) / 2 = 33.5, since both the left and right heaps are the same size. The\nnext element in the stream, 74, is larger than the value at the top of the left heap, so it gets added to the right heap.\nLeft Max-Heap\n12\nRight Min-Heap\n55\n74", "word_count": 780, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "256276f0-1006-5e92-ab50-c066d8802b74", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 318, "real_page_number": null, "text": "306\nChapter 10. Priority Queues and Heaps\nAt this point, the median is the value at the top of the right heap, or 55, since the right heap has a larger size. The next element in the stream, 35,\nis larger than the value at the top of the left heap, so it gets added to the right heap.\nLeft Max-Heap\n12\nRight Min-Heap\n35\n55\n74\nWe now have an issue — the two heaps differ in size by more than one. When this happens, we rebalance the heaps by moving the top element\nfrom the larger heap (35) into the smaller heap.\nLeft Max-Heap\n35\n12\nRight Min-Heap\n55\n74\nAt this point, the median is the average of the two top values, or (35 + 55) / 2 = 45, since both the left and right heaps are the same size. The\nnext element in the stream, 96, is larger than the value at the top of the left heap, so it gets added to the right heap.\nLeft Max-Heap\n35\n12\nRight Min-Heap\n55\n96\n74\nAt this point, the median is the value at the top of the right heap, or 55, since the right heap has a larger size. The next element in the stream, 52,\nis larger than the value at the top of the left heap, so it gets added to the right heap.\nLeft Max-Heap\n35\n12\nRight Min-Heap\n52\n96\n55\n74\nOnce again, the two heaps differ in size by more than one, so we rebalance the heaps by moving the top element of from the larger heap (52)\ninto the smaller heap.\nLeft Max-Heap\n52\n35\n12\nRight Min-Heap\n55\n96\n74\nAt this point, the median is the value at the average of the two top values, or (52 + 55) / 2 = 53.5, since both the left and right heaps are the\nsame size. We can continue this process for the remaining elements, adding each value to the correct heap, rebalancing if necessary, and then\nretrieving the median by either taking the top value of the larger heap, or an average of the top values if both heaps have the same size.", "word_count": 367, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9f379bb9-d740-5d83-a757-f46b4830517c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 319, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n307\n¸ 10.7.3\nMerging Sorted Arrays\nExample 10.11 Suppose you are given 𝑘sorted arrays of size 𝑛each. Devise an time-efficient algorithm that merges the arrays into a\nsingle larger, sorted array.\nThe naïve approach would be to insert all the elements into an array of size 𝑛𝑘, and then run a sorting algorithm on the entire array. However,\nthis approach is inefficient, since sorting an array of size 𝑛𝑘takes time. Instead, we will look at two better solutions that can beΘ(𝑛𝑘log(𝑛𝑘))\nused to solve this problem, one that uses heaps and one that uses recursion.\nIn the heap solution, we first create a min-heap and insert the first element of each of the 𝑘sorted arrays. Then, while the min-heap has a\nsize greater than zero, remove the element at the top of the heap, push it to the output array, and insert the next element in the array that the\nnewly pushed in element originated from. To illustrate this process, suppose you are given the following four sorted arrays:\n3\n5\n10 16\n2\n4\n8\n12\n1\n7\n1511\n6\n9\n13 14\nFirst, we will declare a min-heap and push in the first value of each of the 𝑘sorted arrays:\nArray 1\n3\n5\n10 16\nArray 2\n2\n4\n8\n12\nArray 3\n1\n7\n1511\nArray 4\n6\n9\n13 14\nMin-Heap\n1\n2\n3\n6\nOutput\nWe then pop the top element off the heap and push it into our output array. Since the element we pushed to the output array belongs to array 3,\nwe push the next element in array 3 into the min-heap.\nArray 1\n3\n5\n10 16\nArray 2\n2\n4\n8\n12\nArray 3\n1\n7\n1511\nArray 4\n6\n9\n13 14\nMin-Heap\n2\n6\n3\n7\nOutput\n1\nThe top element is now 2, which belongs to array 2. Thus, we push 2 into the output vector and push the next value in array 2 into the min-heap.\nArray 1\n3\n5\n10 16\nArray 2\n2\n4\n8\n12\nArray 3\n1\n7\n1511\nArray 4\n6\n9\n13 14\nMin-Heap\n3\n6\n4\n7\nOutput\n1\n2", "word_count": 373, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9361c5b9-d709-53b5-8f1b-f32333de30aa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 320, "real_page_number": null, "text": "308\nChapter 10. Priority Queues and Heaps\nThe top element is now 3, which belongs to array 1. Thus, we push 3 into the output vector and push the next value in array 1 into the min-heap.\nArray 1\n3\n5\n10 16\nArray 2\n2\n4\n8\n12\nArray 3\n1\n7\n1511\nArray 4\n6\n9\n13 14\nMin-Heap\n4\n6\n5\n7\nOutput\n1\n2\n3\nThe top element is now 4, which belongs to array 2. Thus, we push 4 into the output vector and push the next value in array 2 into the min-heap.\nArray 1\n3\n5\n10 16\nArray 2\n2\n4\n8\n12\nArray 3\n1\n7\n1511\nArray 4\n6\n9\n13 14\nMin-Heap\n5\n6\n7\n8\nOutput\n1\n2\n3\n4\nIf we continue pushing and popping, we will eventually push all of the elements into the output array in sorted order. Since the worst-case\ncomplexity of a single push or pop from the heap is (since 𝑘is the size of the heap), and a total of 𝑛𝑘elements are pushed and poppedΘ(log(𝑘))\nfrom the heap, the worst-case complexity of the entire algorithm is Θ(𝑛𝑘log(𝑘)).\nAn alternative solution would be to recursively merge the arrays two at a time until there is only one array remaining. Merging two sorted\narrays is a process (we will cover the details of the merging process in chapter 13).Θ(𝑛)\n3\n5 10 16\n2\n4\n8 12\n2\n3\n4\n5\n8 10 1612\n1\n7 1511\n6\n9 13 14\n1\n6\n7\n9 13 1511 14\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 13 15 1611 12 14\nIteration 1\nIteration 2\nIteration 3\nWhat is the time complexity of this recursive approach? On the first iteration, we merge 𝑘arrays of size 𝑛together, which takes time. OnΘ(𝑛𝑘)\nthe second iteration, we merge arrays of size 2𝑛together, which also takes time. In fact, each iteration of merging takes𝑘∕2 (𝑘∕2)×2𝑛=Θ(𝑛𝑘)\ntime, and since we are halving the number of arrays we have to merge at each iteration (from 𝑘down to 1), a total of iterationsΘ(𝑛𝑘) Θ(log(𝑘))\nare needed. The total time complexity is thus iterations work per iteration, or Θ(𝑛𝑘log(𝑘)), the same as the heap approach!Θ(log(𝑘)) Θ(𝑛𝑘)×\nFrom an asymptotic standpoint, both algorithms perform equally well.", "word_count": 399, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "babb7d25-7d21-5cfd-9a9a-0530bf6b24fd", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 321, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n309\nChapter 10 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. What is the worst-case time complexity of pushing an element into a priority queue of size 𝑛that is implemented using an unsorted and a\nsorted vector as the underlying container, respectively?\nA) Unsorted: Θ(1),\nSorted: Θ(1)\nB) Unsorted: Θ(1),\nSorted: Θ(𝑛)\nC) Unsorted: Θ(𝑛),\nSorted: Θ(1)\nD) Unsorted: Θ(𝑛),\nSorted: Θ(𝑛)\nE) Unsorted: Θ(log(𝑛)), Sorted: Θ(log(𝑛))\n2. What is the worst-case time complexity of removing the highest priority element from a priority queue of size 𝑛that is implemented using an\nunsorted and a sorted vector as the underlying container, respectively?\nA) Unsorted: Θ(1),\nSorted: Θ(1)\nB) Unsorted: Θ(1),\nSorted: Θ(𝑛)\nC) Unsorted: Θ(𝑛),\nSorted: Θ(1)\nD) Unsorted: Θ(𝑛),\nSorted: Θ(𝑛)\nE) Unsorted: Θ(log(𝑛)), Sorted: Θ(log(𝑛))\n3. For which of the following real-life situations would a priority queue be LEAST useful?\nA) A student who has limited time to study for several exams\nB) An email survey that offers a gift card to the first 100 respondents\nC) An emergency call center that takes calls for an entire city\nD) A professor who wants to assign seats to students in alphabetical order\nE) A restaurant that offers quicker service to loyal rewards members\n4. Which of the following statements is TRUE?\nA) The total time complexity of inserting an element into a priority queue of size 𝑛that is implemented with an unsorted sequence\nΘ(𝑛2)container and then taking it out is worst-case\nThe time complexities of insertion and removal are both worst-case if a priority queue is implemented using a binary heapB) Θ(log(𝑛))\nC) The time complexities of insertion and removal are both if a priority queue is implemented using a sorted sequence containerΘ(𝑛)\nImplementing a priority queue using an array of linked lists is always preferable to implementing one using a binary heap, as linkedD)\nlists allow for insertion and removalΘ(1)\nE) More that one of the above\n5. Your friend implemented a priority queue using an unknown container and sent this implementation to you. In order to identify the type of\ncontainer that was used, you timed how long it took for this implementation to push and pop 𝑛elements, for different values of 𝑛. The results\nare shown in the table below:\nNumber of Elements n\n10,000\n25,000\n50,000\n100,000\n250,000\n500,000\n1,000,000\nTotal Push Time for Elements (s)n\n0.00048\n0.00117\n0.00244\n0.00495\n0.00974\n0.01902\n0.04170\nTotal Pop Time for Elements (s)n\n0.21113\n0.96067\n3.71797\n13.1249\n62.7012\n232.871\n937.932\nWhich of the following container types could your friend have used to implement this priority queue?\nA) Unsorted sequence container\nB) Sorted sequence container\nC) Binary heap\nD) Pairing heap\nE) More than one of the above\n6. Consider the following snippet of code:\n1\nint main () {\n2\nstd::priority_queue<int32_t> pq;\n3\npq.push(22);\n4\npq.push(34);\n5\npq.push(15);\n6\npq.push(41);\n7\npq.push(26);\n8\nwhile (!pq.empty()) {\n9\nstd::cout << pq.top() << ' ';\n10\npq.pop();\n11\n} // while\n12\n} // main()\nWhat does this code output?\n15 22 26 34 41A)\n22 34 15 41 26B)\n41 34 26 22 15C)\n26 41 15 34 22D)\nE) None of the above", "word_count": 581, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "ac8e7e97-60aa-5658-8187-59f64d8e8ce8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 322, "real_page_number": null, "text": "310\nChapter 10. Priority Queues and Heaps\n7. Consider the following snippet of code:\n1\nstruct CustomComparator {\n2\nbool operator() (const int32_t const int32_t constlhs, rhs) {\n3\n// abs returns the absolute value of x\n4\nreturn std::abs(lhs - 25) > std::abs(rhs - 25);\n5\n} // operator()()\n6\n};\n7\n8\nint main () {\n9\nstd::priority_queue<int32_t, std::vector<int32_t>, CustomComparator> pq;\n10\npq.push(22);\n11\npq.push(34);\n12\npq.push(15);\n13\npq.push(41);\n14\npq.push(26);\n15\nwhile (!pq.empty()) {\n16\nstd::cout << pq.top() << ' ';\n17\npq.pop();\n18\n} // while\n19\n} // main()\nWhat does this code output?\n41 34 26 22 15A)\n41 15 34 22 26B)\n15 22 26 34 41C)\n26 22 34 15 41D)\nE) None of the above\nstd::priority_queue<>?8. Which of the following STL data structures can be used as the underlying data container for a\nstd::vector<>I.\nstd::list<>II.\nstd::deque<>III.\nA) I only\nB) III only\nC) I and II only\nD) I and III only\nE) I, II, and III\nfoo() bar(),9. Consider the following two functions, and which each take in a vector of integers and inserts its values into a priority queue.\nfoo() bar()The method iterates over the vector and pushes in the values one by one, while the method uses a range constructor to\nconstruct the resultant priority queue.\n1\nstd::priority_queue<int32_t> foo(const std::vector<int32_t>& input) {\n2\nstd::priority_queue<int32_t> pq;\n3\nfor (const int32_t val : input) {\n4\npq.push(val);\n5\n} // for val\n6\n7\nreturn pq;\n8\n} // foo()\n9\n10\nstd::priority_queue<int32_t> bar(const std::vector<int32_t>& input) {\n11\nstd::priority_queue<int32_t> pq(input.begin(), input.end());\n12\nreturn pq;\n13\n} // bar()\nfoo() bar()?If the input vector has a size of 𝑛, what are the time complexities of and\nfoo():A) Θ(𝑛),\nbar(): Θ(𝑛)\nfoo():B) Θ(𝑛),\nbar(): Θ(𝑛log(𝑛))\nfoo(): bar():C) Θ(𝑛log(𝑛)), Θ(𝑛)\nfoo(): bar():D) Θ(𝑛log(𝑛)), Θ(𝑛log(𝑛))\nE) None of the above\n10. Which of the following statements is FALSE regarding the pairing heap?\nA) A node in a pairing heap may have more than two children\nB) A pairing heap can be implemented using parent or previous pointers\nC) A queue can be effectively used to meld smaller pairing heaps into a larger pairing heap\nD) The worst-case time complexity of inserting an element into a pairing heap of size 𝑛cannot be better than Θ(log(𝑛))\nE) None of the above", "word_count": 401, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e7eaa6b9-1c03-5df4-b841-fbc03b15870e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 323, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n311\n11. Which one of the following statements is NOT necessarily true?\nA) In a max-heap, the key at each node must be larger than the keys of the node’s children\nB) No node in a max-heap has a key larger than the root’s key\nC) A max-heap gives easy access to the largest element in the heap\nD) A heap must have the completeness property\nE) Heaps can be implemented using both binary trees and arrays\n12. Which of the following are valid binary heaps?\nA)\n47\n39\n35\n30\n11\n12\n10\nB)\n22\n22\n22\n23\nC)\n56\n44\n18\n12\n28\n15\nD)\n43\n45\n46\n45\n44\n44\n43\nE) More than one of the above are valid binary heaps\n13. Which of the following represents a valid binary min-heap? The \"top\" of the heap is the element at the beginning of the array.\n[2, 13, 8, 16, 13, 10, 40, 25, 17]A)\n[47, 9, 12, 9, 2, 10, 10, 4, 3, 1]B)\n[3, 5, 6, 7, 12, 15, 14, 9, 10, 11]C)\n[59, 58, 60, 57, 85, 49, 32, 21, 5]D)\nE) None of the above\n14. Which of the following represents a valid binary max-heap? The \"top\" of the heap is the element at the beginning of the array.\n[14, 25, 27, 26, 28, 24, 30, 32, 34]A)\n[52, 11, 23, 9, 10, 12, 11, 10, 9, 8]B)\n[33, 24, 24, 7, 9, 24, 18, 7, 5, 9, 8]C)\n[76, 54, 33, 56, 32, 55, 33, 12, 14]D)\nE) None of the above\n15. What is the worst-case time complexity of fixing a max-heap of size 𝑛after an element in the heap is modified?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n16. What is the worst-case auxiliary space required to run heapify on an array of 𝑛integers, if you use the most efficient implementation?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n17. Four approaches to heapify are described below:\nfix_up()I. Proceeding from the bottom of the heap to the top, while repeatedly calling\nfix_down()II. Proceeding from the bottom of the heap to the top, while repeatedly calling\nfix_up()III. Proceeding from the top of the heap to the bottom, while repeatedly calling\nfix_down()IV. Proceeding from the top of the heap to the bottom, while repeatedly calling\nWhich of the above approaches successfully builds a valid heap?\nA) I and II only\nB) I and IV only\nC) II and III only\nD) III and IV only\nE) I, II, III, and IV\nfix_down() fix_up()18. Why is it more efficient to build a heap using the approach instead of the approach?\nfix_down(), fix_down()A) If you correctly heapify an array of size 𝑛using you would only need to call on elementsΘ(log(𝑛))\ninstead of elementsΘ(𝑛)\nfix_up(), fix_up()B) If you correctly heapify an array of size 𝑛using you would have to call on elements instead ofΘ(𝑛log(𝑛))\nelementsΘ(𝑛)\nfix_down(), fix_down()C) If you correctly heapify an array using you wouldn’t have to call on any internal nodes\nfix_down(), fix_down()D) If you correctly heapify an array using you wouldn’t have to call on any leaf nodes\nE) More than one of the above", "word_count": 555, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a981d856-9ca4-5a99-968f-69e71bdf45d8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 324, "real_page_number": null, "text": "312\nChapter 10. Priority Queues and Heaps\n19. Consider the following max-heap:\n86\n66\n25\n17\n52\n29\n34\n13\n21\nYou insert the value of 93 into this max-heap. After the invariant is fixed, what is the final array representation of this max-heap?\n[93, 86, 66, 34, 52, 17, 25, 21, 13, 29]A)\n[93, 86, 66, 52, 34, 17, 25, 21, 13, 29]B)\n[93, 52, 86, 34, 29, 66, 25, 21, 13, 17]C)\n[93, 86, 66, 52, 29, 17, 25, 34, 13, 21]D)\nE) None of the above\n20. Consider the following min-heap:\n19\n30\n57\n46\n35\n53\n48\nYou insert the value of 26 into this min-heap. After the invariant is fixed, what is the final array representation of this min-heap?\n[19, 26, 30, 35, 48, 46, 57, 53]A)\n[19, 26, 30, 35, 53, 46, 57, 48]B)\n[19, 35, 30, 26, 53, 46, 57, 48]C)\n[19, 35, 30, 48, 53, 46, 57, 26]D)\nE) None of the above\n21. Consider the following max-heap:\n62\n61\n45\n44\n56\n49\n51\n43\nYou delete the value of 62 from this max-heap. After the invariant is fixed, what is the final array representation of this max-heap?\n[61, 56, 43, 51, 49, 44, 45]A)\n[61, 56, 45, 51, 49, 44, 43]B)\n[61, 56, 51, 43, 49, 44, 45]C)\n[61, 56, 51, 49, 44, 45, 43]D)\nE) None of the above\n22. Consider the following min-heap:\n11\n33\n49\n37\n27\n42\n31\n58\n54\nYou delete the value of 11 from this min-heap. After the invariant is fixed, what is the final array representation of this min-heap?\n[33, 27, 37, 31, 42, 58, 49, 54]A)\n[27, 42, 33, 31, 58, 37, 49, 54]B)\n[58, 27, 33, 31, 42, 37, 49, 54]C)\n[27, 31, 33, 54, 42, 37, 49, 58]D)\nE) None of the above", "word_count": 320, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6f4c654f-5715-5278-9f96-51a179bc1a16", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 325, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n313\n23. Consider the following unsorted array:\n[13, 24, 1, 58, 69, 10, 32, 27, 71]\nPerform a heapify operation to turn this data into a valid max-heap. What are the contents of the array after the heapify operation?Θ(𝑛)\n[71, 13, 32, 24, 69, 10, 1, 27, 58]A)\n[71, 69, 32, 27, 13, 10, 1, 24, 58]B)\n[71, 69, 58, 32, 27, 13, 10, 24, 1]C)\n[71, 69, 32, 58, 13, 10, 1, 27, 24]D)\nE) None of the above\n24. Consider the following unsorted array:\n[63, 47, 50, 15, 58, 39, 26, 53, 12, 57]\nPerform a heapify operation to turn this data into a valid min-heap. What are the contents of the array after the heapify operation?Θ(𝑛)\n[12, 15, 26, 47, 53, 39, 50, 57, 63, 58]A)\n[12, 15, 26, 47, 58, 39, 50, 53, 57, 63]B)\n[12, 15, 26, 47, 57, 39, 50, 53, 63, 58]C)\n[12, 15, 26, 53, 47, 39, 50, 58, 63, 57]D)\nE) None of the above\n25. Given an empty min-heap priority queue, you push the following values in this order:\n34, 29, 22, 25, 19, 12, 15, 37\nWhat would the contents of the underlying array look like if the top value were popped off the heap?\n[12, 22, 15, 34, 25, 29, 19]A)\n[15, 22, 19, 34, 25, 29, 37]B)\n[15, 22, 37, 34, 25, 29, 19]C)\n[15, 19, 22, 34, 25, 29, 37]D)\nE) None of the above\n26. Consider a min-heap represented by the following array:\n[63, 74, 67, 85, 91, 94, 72, 88]\nPerform the following operations using the algorithms for binary heaps discussed in this chapter. Ensure that the heap property is restored\nat the end of every individual operation.\n1. Push the value of 60 into this min-heap.\n2. Push the value of 79 into this min-heap.\n3. Update element 85 to have a value of 58.\n4. Update element 67 to have a value of 96.\n5. Remove the min element from the heap.\nWhat does the array representation look like after all five operations are completed?\n[60, 72, 63, 94, 96, 74, 79, 88, 91]A)\n[60, 63, 72, 74, 79, 94, 96, 88, 91]B)\n[60, 74, 79, 63, 91, 94, 72, 88, 96]C)\n[60, 63, 72, 74, 79, 88, 91, 94, 96]D)\nE) None of the above\n27. Given an empty max-heap priority queue, push in the following letters in this order (priority is determined using alphabetical order, where\nlater letters have higher priority than earlier letters):\nH, E, L, L, O, W, O, R, L, D\nWhat would the contents of the underlying array look like after all the letters are inserted?\n[W, R, O, L, L, H, O, E, L, D]A)\n[W, O, R, L, L, H, O, E, L, D]B)\n[W, R, O, O, H, L, L, E, L, D]C)\n[W, R, O, L, O, L, H, E, L, D]D)\nE) None of the above\n28. You are given the following array, with the following contents:\n[H, E, L, L, O, W, O, R, L, D]\nPerforma heapifyoperationtoturnthisdataintoavalidmax-heap. Whatarethecontentsofthearrayaftertheheapifyoperation?bottom-up\nNote: for this problem, when fixing down, swap with the left child in the case of a tie.\n[W, R, O, L, L, H, O, E, L, D]A)\n[W, O, R, L, L, H, O, E, L, D]B)\n[W, R, O, O, H, L, L, E, L, D]C)\n[W, R, O, L, O, L, H, E, L, D]D)\nE) None of the above", "word_count": 625, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "22efe77d-9a59-5b77-9822-0e4fc9175e0c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 326, "real_page_number": null, "text": "314\nChapter 10. Priority Queues and Heaps\n29. You are given the following array, with the following contents:\n[H, E, L, L, O, W, O, R, L, D]\nPerform a heapify operation to turn this data into a valid max-heap. What are the contents of the array after the heapify operation?top-down\n[W, R, O, L, L, H, O, E, L, D]A)\n[W, O, R, L, L, H, O, E, L, D]B)\n[W, R, O, O, H, L, L, E, L, D]C)\n[W, R, O, L, O, L, H, E, L, D]D)\nE) None of the above\n30. Given an empty min-heap priority queue, push in the following letters in this order (priority is determined using alphabetical order, where\nearlier letters have higher priority than later letters):\nD, O, U, B, L, E, D, E, L, E, T, E\nWhat would the contents of the underlying array look like after all the letters are inserted?\n[B, D, D, E, E, E, E, U, L, L, O, T]A)\n[B, D, D, E, E, E, E, O, L, L, T, U]B)\n[B, D, D, E, E, E, U, O, L, L, T, E]C)\n[B, D, D, E, E, E, U, L, L, T, O, E]D)\nE) None of the above\n31. You are given the following array, with the following contents:\n[D, O, U, B, L, E, D, E, L, E, T, E]\nPerforma heapifyoperationtoturnthisdataintoavalidmin-heap. Whatarethecontentsofthearrayaftertheheapifyoperation?bottom-up\n[B, D, D, E, E, E, E, U, L, L, O, T]A)\n[B, D, D, E, E, E, E, O, L, L, T, U]B)\n[B, D, D, E, E, E, U, O, L, L, T, E]C)\n[B, D, D, E, E, E, U, L, L, T, O, E]D)\nE) None of the above\n32. You are given the following array, with the following contents:\n[D, O, U, B, L, E, D, E, L, E, T, E]\nPerform a heapify operation to turn this data into a valid min-heap. What are the contents of the array after the heapify operation?top-down\n[B, D, D, E, E, E, E, U, L, L, O, T]A)\n[B, D, D, E, E, E, E, O, L, L, T, U]B)\n[B, D, D, E, E, E, U, O, L, L, T, E]C)\n[B, D, D, E, E, E, U, L, L, T, O, E]D)\nE) None of the above\npop_random()33. Suppose you wanted to implement a method for a priority queue of size 𝑛that is implemented using an array-based\nbinary heap. When this method is invoked, a random element is swapped with the last element in the heap’s underlying array and then\nfix_up() fix_down()popped out from the back. What is the number of times you must call or on the resultant array tominimum\nguarantee that the binary heap property is preserved after this random element is swapped to the back and then popped out of the array?\nA) 1\nB) 2\n⌈log(𝑛)⌉C)\n⌈𝑛∕2⌉D)\nE) 𝑛\n34. What is the maximum possible difference in depth between any two leaf nodes of a binary heap containing 𝑛nodes with a height ℎ?\nA) 0\nB) 1\nC) ℎ\n⌈log(𝑛)⌉D)\n⌈𝑛∕ℎ⌉E)\n35. What is the worst-case time complexity of removing the highest priority element from a binary tree of size 𝑛that is NOTheap-ordered\ncomplete, provided that the tree must still remain heap-ordered after the removal?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)", "word_count": 604, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c30b9e7c-6d97-5673-b1c5-b2235b2e69ca", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 327, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n315\nfix_down()36. After removing the top element from a binary heap and then calling on the new root 𝑅(which was the value that was\nswapped from the back of the heap to the top, before the top element was removed), you notice that 𝑅ended up at the last position of the\nunderlying array. Which of the following claims is guaranteed to be TRUE?\nA) The priority of 𝑅is the highest of all remaining elements in the binary heap after the removal\nB) The priority of 𝑅is the lowest of all remaining elements in the binary heap after the removal\nC) The priority of 𝑅is at least as high as the priority of the element that was removed\nD) More than one of the above\nE) None of the above\nT37. Which of the following MUST be implemented for a type for the following code to compile?\nstd::priority_queue<T> pq;\noperator<A)\noperator==B)\noperator++C)\nD) More than one of the above\nE) None of the above\n𝑘thnums38. You are given an integer array and an integer 𝑘. Implement a function that returns the largest element in the array. For example,\n[5,2,3,1,6,4]given the array and 3, you would return 4, since that is the 3rd largest element in the array. You may assume that𝑘= 𝑘\nis a valid number between 1 and the length of the array.\nint32_t find_kth_largest(const std::vector<int32_t>& int32_tnums, k);\nYou solution should run in time and take up worst-case auxiliary space, where 𝑛is the size of the input array.Θ(𝑛log(𝑘)) Θ(𝑘)\n39. You are stuck in a underground tunnel, and a giant pile of rubble of size 𝑆is blocking your escape route. Luckily, you have 𝑁different\n𝑖thsticks of dynamite on hand, each with its own explosive power 𝐸. If you use the stick of dynamite, the amount of rubble blocking your\nway decreases by 𝐸𝑖. Your goal is to get rid of all the rubble so that you can escape, but there is a catch: you are given an additional number\n𝑘, such that you can only use at most 𝑘sticks of dynamite. Implement the following function, which returns the minimum number of sticks\n-1of dynamite necessary to remove all the rubble. Return if it is impossible to escape using at most 𝑘sticks of dynamite.\nint32_t min_tries_to_escape(const std::vector<int32_t>& int32_t int32_tdynamite, rubble_size, k);\ndynamite = [1,4,5,2,4], rubble_size = 12,For example, if you are given and 4, you would return a value of 3, since at𝑘=\nleast three sticks of dynamite are needed to fully remove the rubble (the ones with explosive power 4, 5, and 4). However, if 𝑘were 2 (and\n-1,everything else were the same), then you return since it would be impossible to remove all the rubble with just two sticks of dynamite.\ndynamiteYour solution should run in worst-case time and auxiliary space, where 𝑛is the length of the vector.Θ(𝑛log(𝑘)) Θ(𝑘)\npoints points[i] = [x_i, y_i]40. You are given an array of where represents a point on the Cartesian plane, as well as an integer\n𝑘. Implement a function that returns the 𝑘closest points to the origin (0, 0). Note that the distance between any two points (𝑥1,𝑦1)\nand on a Cartesian plane is equal to(𝑥2,𝑦2)\n√\n−𝑥1)2 −𝑦1)2. You may return the solution in any order, and the points in the(𝑥2 +(𝑦2\n[[101, 183],[-203,280],[281,-370]]solution are guaranteed to be unique. For example, given and 2, you would return𝑘=\n[[101,183],[-203,280]] (in any order), since these are the two points that are closest to the origin.\nstd::vector<std::vector<int32_t>> k_closest_to_origin(\nconst std::vector<std::vector<int32_t>>& int32_tpoints, k);\nYour solution should run in worst-case time and auxiliary space, where 𝑛is the number of points in the input vector.Θ(𝑛log(𝑘)) Θ(𝑘)\n41. It is the middle of summer, and the professors are looking to hire a new batch of EECS 281 student instructors to prepare for yet another\nget_qualification(),busy fall semester. You are given a function which is a secret, proprietary algorithm that returns a scoreΘ(1)\nfrom 0 to 100 indicating how qualified an applicant is to become an EECS 281 staff member (where 0 is the least qualified and 100 is the\nhire_staff(), Applicantmost qualified). Implement the function which takes in a vector of objects and the number of openings 𝑘,\nand returns a vector of the 𝑘applicants that should be hired (based on whoever has the highest qualification scores). If two applicants have\nthe same qualification score, choose the one with the lower application ID first.\nstruct Applicant {\nstd::string uniqname;\nint32_t application_id;\n};\n// This method is already implemented for you - it returns a number\n// in the range [0, 100] that identifies how qualified an applicant is\nint32_t get_qualification(const Applicant& applicant);\n// Implement this function\nhire_staff(const int32_tstd::vector<Applicant> std::vector<Applicant>& applicants, k);\nYour solution should run in worst-case time and auxiliary space, where 𝑛is the number of applicants in the input vector.Θ(𝑛log(𝑘)) Θ(𝑘)", "word_count": 832, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0688e0a3-2169-5aed-9438-1ba16c5dbee6", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 328, "real_page_number": null, "text": "316\nChapter 10. Priority Queues and Heaps\nChapter 10 Exercise Solutions\n1. The correct answer is (B). If the underlying priority queue container is unsorted, then inserting the element is trivial (since you can insert\nit anywhere). However, in a sorted container, you will have to find the position to insert in (and shift all subsequent elements after the point\nof insertion), which would take linear time.\n2. The correct answer is (C). If the underlying priority queue container is unsorted, then the item with the highest priority that you want\nto remove could be anywhere in the container, which would require a linear search before you can remove it. On the other hand, if the\ncontainer is sorted, you know where the highest priority element is and can remove it in constant time.\n3. Thecorrectansweris(B).Whilealloftheseoptionsinvolveachoicetobemadedependingonsomesortofpriority(suchastheimportance\nof an exam), the scenario in option (B) bases its priority on FIFO arrival order, which is more similar to the functionality of a standard\nqueue rather than a priority queue.\n4. The correct answer is (B). Option (A) is false because unsorted sequence containers provide insertion and removal. Option (C)Θ(𝑛)Θ(1)\nis false because sorted sequence containers provide removal. Option (D) is false because an array of linked lists is only viable forΘ(1)\npriorities of small integers and may involve other inefficiencies (such as memory overhead). Only option (B) is true: if you use a binary\nheap to implement a priority queue, then the time complexities of insertion and removal are both Θ(log(𝑛)).\n5. The correct answer is (A). Notice that the time rquired to pop elements grows much more rapidly than the time required to push elements\nas the number of elements you have to push or pop increases. This would point to a container where inserting is easy and removing is\ndifficult. Of the choices provided, an unsorted sequence container would make the most sense, as removing is a linear time operation due to\nthe unsorted nature of the container (you have to linear search for the element with the greatest priority to pop). An unsorted sequence\nΘ(𝑛2)container provides insertion and removal, which would be result in a and relationship in total if performed 𝑛times.Θ(𝑛) Θ(𝑛)Θ(1)\nThis fits with the data provided: the time for push grows linearly with 𝑛, while the time for pop grows quadratically with 𝑛. None of the\nother answer choices exhibit these same time complexities for push and pop.\nstd::priority_queue<> std::less(<)6. Thecorrectansweris(C).The defaultstothe comparatorifnocomparatorisexplicitly\nprovided, so larger elements have the greatest priority. Thus, the elements are popped out in descending order.\nstd::greater (>) abs(rhs - 25)7. The correct answer is (D). The comparator is in a format, so elements with a smaller value for\nhave greater priority and are popped out first. In other words, the elements are popped out in order of how far away they are from 25, with\nelements closer to 25 popped out before elements farther from 25.\nstd::priority_queue<>8. The correct answer is (D). The underlying container for a must be sequential and support constant time\nrandom access. This only applies for vectors and deques.\n.push() foo()9. The correct answer is (C). Each individual call to takes time, so the time complexity of isΘ(log(𝑛)) 𝑛×Θ(log(𝑛))=\nΘ(𝑛log(𝑛)). However, if you use the range constructor, the input range can be heapified in time instead. This is why it is better toΘ(𝑛)\ninitialize a priority queue with a range of data using the range constructor (if possible), instead of pushing each element in one by one.\n10. The correct answer is (D). Because priority queues can have more than two children, inserting an element can be done in constant time by\njust melding the lower priority root as a child of the higher priority one.\n11. The correct answer is (A). Option (A) is not necessarily true because a key at each node may be identical to one of its children due to the\npossibility of ties. It does not always have to be larger.\n12. The correct answer is (B). Option (A) is not a valid binary heap because 12 is larger than 11. Option (C) is not a valid binary heap because\nit is not complete (28 has one child but 44 has two). Option (D) is not a valid binary heap because 44’s parent and child both include 43.\n13. The correct answer is (A). A min-heap is valid if the children of each element (located at indices 2𝑖and using 1-indexing) are not2𝑖+1\nsmaller than it. Option (B) is not a valid binary min-heap because the children of 47, 9 and 12, are smaller than 47. Option (C) is not a valid\nbinary min-heap because the child of 12 (11) is smaller than 12. Option (D) is not a valid binary min-heap because a child of 59 (58) is\nsmaller than 59. Option (A) is valid: 13 and 8 are not smaller than 2, 16 and 13 are not smaller than 13, 10 and 40 are not smaller than 8,\nand 25 and 17 are not smaller than 16.\n14. The correct answer is (C). A max-heap is valid if the children of each element (located at indices 2𝑖and using 1-indexing) are not2𝑖+1\nlarger than it. Option (A) is not a valid binary max-heap because the children of 14 (25 and 27) are not larger than 14. Option (B) is not a\nvalid max-heap because a child of 9 (10) is larger than 9. Option (D) is not a valid binary max-heap because a child of 54 (56) is larger than\n54. Option (C) is valid: 24 and 24 are not larger than 33, 7 and 9 are not larger than 24, 24 and 18 are not larger than 24, 7 and 5 are not\nlarger than 7, and 9 and 8 are not larger than 9.\n15. The correct answer is (B). After an element in a max-heap is modified, it only needs to move at most levels to return to a validΘ(log(𝑛))\nlocation (since there are levels in a heap with 𝑛elements).Θ(log(𝑛))\nfix_up() fix_down().16. The correct answer is (A). You can perform a heapify in place, using the given array of elements and either or\n17. The correct answer is (C). To produce a valid heap, you must fix in the direction you are coming from. As an example, option I fails when\n[5,3,6,1,2,4,7]. [1,2,3,4,5,6,7].the array is Option IV fails when the array is\nfix_down(), fix_down()18. The correct answer is (D). If you heapify an array by calling you wouldn’t have to call on any of the leaf\n(fix_down()nodes since they have no children tries to fix in the direction of the children, but leaf nodes don’t have any).", "word_count": 1167, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "bf3fa981-cb51-55b7-ad9e-6acabd594848", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 329, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n317\n19. The correct answer is (A). Add 93 to the end of the heap and move it up until it is in the correct position.\n86\n66\n25\n17\n52\n29\n93\n34\n13\n21\n86\n66\n25\n17\n52\n93\n29\n34\n13\n21\n86\n66\n25\n17\n93\n52\n29\n34\n13\n21\n93\n66\n25\n17\n86\n52\n29\n34\n13\n21\n20. The correct answer is (B). Add 26 to the end of the heap and move it up until it is in the correct position.\n19\n30\n57\n46\n35\n53\n48\n26\n19\n30\n57\n46\n35\n53\n26\n48\n19\n30\n57\n46\n26\n53\n35\n48\n21. The correct answer is (B). Move 43 to the top of the heap after deleting 62, and then fix it down until it is in the correct position.\n43\n61\n45\n44\n56\n49\n51\n43\n43\n61\n45\n44\n56\n49\n51\n61\n43\n45\n44\n56\n49\n51\n61\n45\n43\n44\n56\n49\n51\n22. The correct answer is (D). Move 58 to the top of the heap after deleting 11, and then fix it down until it is in the correct position.\n58\n33\n49\n37\n27\n42\n31\n58\n54\n58\n33\n49\n37\n27\n42\n31\n54\n27\n33\n49\n37\n58\n42\n31\n54\n27\n33\n49\n37\n31\n42\n58\n54\n27\n33\n49\n37\n31\n42\n54\n58\nfix_down():23. Thecorrectansweris(D).Fora heapify,startfromthebottominternalnodesandmoveupwards,continuouslycallingΘ(𝑛)\n13\n1\n32\n10\n24\n69\n58\n71\n27\n13\n1\n32\n10\n24\n69\n71\n58\n27\n13\n1\n32\n10\n24\n69\n71\n58\n27\n13\n32\n1\n10\n24\n69\n71\n58\n27", "word_count": 307, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b8e91378-dbed-5a9e-8164-493938f0fed8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 330, "real_page_number": null, "text": "318\nChapter 10. Priority Queues and Heaps\n13\n32\n1\n10\n24\n69\n71\n58\n27\n13\n32\n1\n10\n71\n69\n24\n58\n27\n13\n32\n1\n10\n71\n69\n58\n24\n27\n13\n32\n1\n10\n71\n69\n58\n24\n27\n71\n32\n1\n10\n13\n69\n58\n24\n27\n71\n32\n1\n10\n69\n13\n58\n24\n27\nfix_down():24. Thecorrectansweris(C).Fora heapify,startfromthebottominternalnodesandmoveupwards,continuouslycallingΘ(𝑛)\n63\n50\n26\n39\n47\n58\n57\n15\n12\n53\n63\n50\n26\n39\n47\n57\n58\n15\n12\n53\n63\n50\n26\n39\n47\n57\n58\n15\n12\n53\n63\n50\n26\n39\n47\n57\n58\n12\n15\n53\n63\n50\n26\n39\n47\n57\n58\n12\n15\n53\n63\n26\n50\n39\n47\n57\n58\n12\n15\n53\n63\n26\n50\n39\n47\n57\n58\n12\n15\n53\n63\n26\n50\n39\n12\n57\n58\n47\n15\n53\n63\n26\n50\n39\n12\n57\n58\n15\n47\n53\n63\n26\n50\n39\n12\n57\n58\n15\n47\n53\n12\n26\n50\n39\n63\n57\n58\n15\n47\n53\n12\n26\n50\n39\n15\n57\n58\n63\n47\n53\n12\n26\n50\n39\n15\n57\n58\n47\n63\n53", "word_count": 213, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cab207bb-f553-5e52-9cc6-6e0c8aa16667", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 331, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n319\n25. The correct answer is (B). The steps are shown below:\n34\n34\n29\n29\n34\n29\n22\n34\n22\n29\n34\n22\n29\n34\n25\n22\n29\n25\n34\n22\n29\n25\n19\n34\n22\n29\n19\n25\n34\n19\n29\n22\n25\n34\n19\n29\n12\n22\n25\n34\n19\n12\n29\n22\n25\n34\n12\n19\n29\n22\n25\n34\n12\n19\n15\n29\n22\n25\n34\n12\n15\n19\n29\n22\n25\n34\n12\n15\n19\n29\n22\n25\n34\n37\n37\n15\n19\n29\n22\n25\n34\n37\n37\n15\n19\n29\n22\n25\n34\n15\n37\n19\n29\n22\n25\n34\n15\n19\n37\n29\n22\n25\n34\n26. The correct answer is (B). The steps are shown below:\n63\n67\n72\n94\n74\n91\n85\n60\n88\n63\n67\n72\n94\n74\n91\n60\n85\n88\n63\n67\n72\n94\n74\n91\n60\n85\n88\n63\n67\n72\n94\n74\n91\n60\n85\n88\n63\n67\n72\n94\n60\n91\n74\n85\n88\n63\n67\n72\n94\n60\n91\n74\n85\n88\n60\n67\n72\n94\n63\n91\n74\n85\n88\n60\n67\n72\n94\n63\n91\n79\n74\n85\n88\n60\n67\n72\n94\n63\n79\n91\n74\n85\n88\n60\n67\n72\n94\n63\n79\n91\n74\n58\n88\n60\n67\n72\n94\n63\n79\n91\n58\n74\n88\n60\n67\n72\n94\n58\n79\n91\n63\n74\n88\n58\n67\n72\n94\n60\n79\n91\n63\n74\n88\n58\n96\n72\n94\n60\n79\n91\n63\n74\n88\n58\n72\n96\n94\n60\n79\n91\n63\n74\n88\n91\n72\n96\n94\n60\n79\n91\n63\n74\n88\n91\n72\n96\n94\n60\n79\n63\n74\n88\n60\n72\n96\n94\n91\n79\n63\n74\n88\n60\n72\n96\n94\n63\n79\n91\n74\n88\n60\n72\n96\n94\n63\n79\n74\n91\n88", "word_count": 321, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "71af21dc-3181-598b-9814-62c1cf9c8bf9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 332, "real_page_number": null, "text": "320\nChapter 10. Priority Queues and Heaps\n27. The correct answer is (A). The steps are shown below:\nH\nH\nE\nH\nL\nE\nL\nH\nE\nL\nH\nE\nL\nL\nH\nL\nE\nL\nH\nL\nO\nE\nL\nH\nO\nL\nE\nO\nH\nL\nL\nE\nO\nH\nW\nL\nL\nE\nO\nW\nH\nL\nL\nE\nW\nO\nH\nL\nL\nE\nW\nO\nO\nH\nL\nL\nE\nW\nO\nO\nH\nL\nL\nE\nR\nW\nO\nO\nH\nL\nL\nR\nE\nW\nO\nO\nH\nR\nL\nL\nE\nW\nO\nO\nH\nR\nL\nL\nL\nE\nW\nO\nO\nH\nR\nL\nD\nL\nL\nE\nfix_down():28. The correct answer is (D). Start from the bottom internal nodes and move upwards, continuously calling\nH\nL\nO\nW\nE\nO\nD\nL\nL\nR\nH\nL\nO\nW\nE\nO\nD\nL\nL\nR\nH\nL\nO\nW\nE\nO\nD\nR\nL\nL\nH\nW\nO\nL\nE\nO\nD\nR\nL\nL\nH\nW\nO\nL\nR\nO\nD\nE\nL\nL\nH\nW\nO\nL\nR\nO\nD\nL\nL\nE\nH\nW\nO\nL\nR\nO\nD\nL\nL\nE\nW\nH\nO\nL\nR\nO\nD\nL\nL\nE\nW\nO\nH\nL\nR\nO\nD\nL\nL\nE", "word_count": 226, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "72f3fb82-80d7-52a7-8c81-4f13bfb68527", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 333, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n321\nfix_up():29. The correct answer is (A). Start from the top and move downwards, continuously calling\nH\nL\nO\nW\nE\nO\nD\nL\nL\nR\nH\nL\nO\nW\nE\nO\nD\nL\nL\nR\nH\nL\nO\nW\nE\nO\nD\nL\nL\nR\nL\nH\nO\nW\nE\nO\nD\nL\nL\nR\nL\nH\nO\nW\nE\nO\nD\nL\nL\nR\nL\nH\nO\nW\nL\nO\nD\nE\nL\nR\nL\nH\nO\nW\nL\nO\nD\nE\nL\nR\nL\nH\nO\nW\nO\nL\nD\nE\nL\nR\nO\nH\nO\nW\nL\nL\nD\nE\nL\nR\nO\nH\nO\nW\nL\nL\nD\nE\nL\nR\nO\nW\nO\nH\nL\nL\nD\nE\nL\nR\nW\nO\nO\nH\nL\nL\nD\nE\nL\nR\nW\nO\nO\nH\nL\nL\nD\nE\nL\nR\nW\nO\nO\nH\nL\nL\nD\nE\nL\nR\nW\nO\nO\nH\nL\nL\nD\nR\nL\nE\nW\nO\nO\nH\nR\nL\nD\nL\nL\nE\nW\nO\nO\nH\nR\nL\nD\nL\nL\nE\nW\nO\nO\nH\nR\nL\nD\nL\nL\nE\n30. The correct answer is (B). See question 27 for an example on how to approach this problem.\n31. The correct answer is (C). See question 28 for an example on how to approach this problem.\n32. The correct answer is (B). See question 29 for an example on how to approach this problem.\nfix_up()33. The correct answer is (A). After the swap and removal, there is only one value that could be out of place, so only one call to\nfix_down()or is needed to fix this value.\n34. The correct answer is (B). A binary heap must be complete, so there cannot be a gap of more than one level between two leaf nodes.\n35. The correct answer is (C). If a heap-ordered binary tree is not guaranteed to be complete, you no longer have a limit on theΘ(log(𝑛))\nheight of the tree. In the worst-case, the tree is in the form of a stick, which could cause you to iterate all 𝑛nodes while fixing an element.\n36. The correct answer is (E). Just because 𝑅moved to the last position of the underlying array after swapping and fixing down, it does not\nmean that 𝑅has a lower priority than all other elements. Any of the leaf nodes in the binary heap could be the value with the lowest priority,\nand not just the value at the last position in the underlying array.\noperator<37. The correct answer is (A). Only is needed for the priority queue initialization to compile.", "word_count": 456, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "86b18ead-b6bc-56de-9b6b-ebb1bda703a0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 334, "real_page_number": null, "text": "322\nChapter 10. Priority Queues and Heaps\n38. You can solve this problem trivially by sorting the array, or by pushing all the elements into a max-priority queue and popping 𝑘elements\nout, but these solutions would end up taking time. How do we do better than Θ(𝑛log(𝑛))? The idea is to use a min-heap of sizeΘ(𝑛log(𝑛))\n𝑘instead, and only push in a value if it is larger than the value at the top of the min-priority queue. By doing so, the min-priority queue\nessentially stores the 𝑘largest values you have encountered so far, and you can easily check if a new element is among the 𝑘largest you\nhave encountered by comparing its value with the smallest value at the top of the min-heap. This reduces the worst-case time complexity\nfrom to Θ(𝑛log(𝑘)), since the priority queue only has size 𝑘instead of 𝑛. One implementation of this solution is shown below:Θ(𝑛log(𝑛))\n1\nint32_t find_kth_largest(const std::vector<int32_t>& int32_tnums, k) {\n2\nstd::priority_queue<int32_t, std::vector<int32_t>, std::greater<int32_t>>\n3\npq(nums.begin(), nums.begin() + k);\n4\n5\nfor (int32_t i = k; i < nums.size(); ++i) {\n6\nif (nums[i] > pq.top()) {\n7\npq.pop();\n8\npq.push(nums[i]);\n9\n} // if\n10\n} // for i\n11\n12\nreturn pq.top();\n13\n} // find_kth_largest()\nTo minimize the number of dynamite sticks we need to remove all the rubble, we will want to use the most powerful dynamite first. This39.\ncan be done by placing the available dynamite into a priority queue. However, since we are only allowed to use at most 𝑘sticks of dynamite,\nwe can use the same strategy as in the previous problem and instead use a min-priority queue to keep track of the 𝑘most powerful dynamite\nsticks we have encountered so far. An implementation of this solution is shown below:\n1\nint32_t min_tries_to_escape(const std::vector<int32_t>& int32_t int32_tdynamite, rubble_size, k) {\n2\nstd::priority_queue<int32_t, std::vector<int32_t>, std::greater<int32_t>>\n3\npq(dynamite.begin(), dynamite.begin() + k);\n4\n5\nfor (int32_t i = k; i < dynamite.size(); ++i) {\n6\nif (dynamite[i] > pq.top()) {\n7\npq.pop();\n8\npq.push(dynamite[i]);\n9\n} // if\n10\n} // for i\n11\n12\nstd::vector<int32_t> strongest_dynamite;\n13\nstrongest_dynamite.reserve(k);\n14\nwhile (!pq.empty()) {\n15\nstrongest_dynamite.push_back(pq.top());\n16\npq.pop();\n17\n} // while\n18\n19\nint32_t num_sticks_used = 0;\n20\nint32_t idx = strongest_dynamite.size() - 1;\n21\nwhile (rubble_size > 0 && idx >= 0) {\n22\nrubble_size -= strongest_dynamite[idx];\n23\n--idx;\n24\n++num_sticks_used;\n25\n} // while\n26\n27\nreturn rubble_size > 0 ? -1 : num_sticks_used;\n28\n} // min_tries_to_escape()", "word_count": 415, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ce132662-c749-59ad-a6f1-2b7f2fb96402", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 335, "real_page_number": null, "text": "10.7 Solving Problems Using Priority Queues\n323\n40. Thisisanothervariationofthe\"𝑘closest\"problemthatwasdemonstratedintheprevioustwoproblems. Thesimplestsolutionistocalculate\nthe distance between every point and the origin and sort all the points by this distance, to retrieve the 𝑘closest points. However, this runs\nin time if we are given 𝑛points. We can improve on this solution by keeping track of a max-heap of size 𝑘. For each pointΘ(𝑛log(𝑛))\nwe encounter, we add it to the max-heap if its distance is lower than the distance of the point at the top of the heap (extracting the point\noriginally at the top of the heap in the process) — this ensures that our max-heap will always store the 𝑘closest points encountered so far.\nOnce we finish looking at all the input points, we can just return the contents of the max-heap as our solution. An implementation of this\nsolution is shown below:\n1\nstruct PointWithDistance {\n2\nint32_t x;\n3\nint32_t y;\n4\ndouble distance;\n5\n6\nPointWithDistance(int32_t int32_tx_in, y_in) : x{x_in}, y{y_in} {\n7\n// potential optimization: sqrt is not actually necessary since you are comparing relative\n8\n// distances, since a larger (x^2 + y^2) also implies a larger std::sqrt(x^2 + y^2)\n9\ndistance = std::sqrt(x x + y y);* *\n10\n} // PointWithDistance()\n11\n12\nbool operator<(const constPointWithDistance& other) {\n13\nreturn distance < other.distance;\n14\n} // operator<()\n15\n};\n16\n17\nstd::vector<std::vector<int32_t>> k_closest_to_origin(\n18\nconst std::vector<std::vector<int32_t>>& int32_tpoints, k) {\n19\nstd::priority_queue<PointWithDistance> pq;\n20\nfor (const auto& point : points) {\n21\npq.emplace(point[0], point[1]);\n22\nif (pq.size() > k) {\n23\npq.pop();\n24\n} // if\n25\n} // for point\n26\n27\nstd::vector<std::vector<int32_t>> solution;\n28\nsolution.reserve(k);\n29\n30\nwhile (!pq.empty()) {\n31\nconst auto& next = pq.top();\n32\nsolution.push_back({next.x, next.y});\n33\npq.pop();\n34\n} // while\n35\n36\nreturn solution;\n37\n} // k_closest_to_origin()", "word_count": 324, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1fb4a952-4eb8-51fe-a049-89e8c7753c5e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 336, "real_page_number": null, "text": "324\nChapter 10. Priority Queues and Heaps\n41. The idea behind this problem is the same as the previous problem. To avoid the time complexity of sorting all applicants byΘ(𝑛log(𝑛))\nqualification score, we will instead of use a min-heap of size 𝑘to keep track of the 𝑘most qualified applicants encountered so far. An\noperator<implementation of this solution is shown below (this example uses a custom comparator, but you can overload as well, as in\nthe previous problem):\n1\nstruct Applicant {\n2\nstd::string uniqname;\n3\nint32_t application_id;\n4\n};\n5\n6\n// This method is already implemented for you - it returns a number\n7\n// in the range [0, 100] that identifies how qualified an applicant is\n8\nint32_t get_qualification(const Applicant& applicant);\n9\n10\nstruct QualificationData {\n11\nApplicant applicant;\n12\nint32_t qualification;\n13\n14\nexplicit QualificationData(const Applicant& applicant_in) : applicant{applicant_in} {\n15\nqualification = get_qualification(applicant);\n16\n} // QualificationData()\n17\n};\n18\n19\nstruct ApplicantCompare {\n20\nbool operator() (const constQualificationData& lhs, QualificationData& rhs) {\n21\nif (lhs.qualification == rhs.qualification) {\n22\nreturn lhs.applicant.application_id < rhs.applicant.application_id;\n23\n} // if\n24\n25\nreturn lhs.qualification > rhs.qualification;\n26\n} // operator()()\n27\n};\n28\n29\nhire_staff(const int32_tstd::vector<Applicant> std::vector<Applicant>& applicants, k) {\n30\nstd::priority_queue<QualificationData, std::vector<QualificationData>, ApplicantCompare> pq;\n31\nfor (const auto& applicant : applicants) {\n32\npq.emplace(applicant);\n33\nif (pq.size() > k) {\n34\npq.pop();\n35\n} // if\n36\n} // for applicant\n37\n38\nstd::vector<Applicant> new_staff;\n39\nnew_staff.reserve(k);\n40\n41\nwhile (!pq.empty()) {\n42\nconst auto& next = pq.top();\n43\nnew_staff.push_back(next.applicant);\n44\npq.pop();\n45\n} // while\n46\n47\nreturn new_staff;\n48\n} // hire_staff()", "word_count": 271, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "33a30740-5589-5ad0-afd3-32b8ea3b0e6c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 337, "real_page_number": null, "text": "Chapter 11\nIterators and the Standard Template Library\n11.1\nThe Standard Template Library (STL)\nThe standard template library (STL) is a well-documented library that provides high-quality, fast implementations of data structures and\nalgorithms that can be used to operate on data. All of these implementations can be found in public headers that are available for anyone to see.\nThe STL is the reason why you do not have to implement everything from scratch when you write a program! If you want to store your\n#include <vector>data in a dynamic array that grows with the size of the data, you do not have to implement the container; you can just\nstd::vector<> Nodeand instantiate a in your program. If you want to use a doubly-linked list, you do not need to implement your own\n#include <list> std::list<>.class and list methods; you can just and instantiate a If you want to sort the contents of a container,\n#include <algorithm> std::sort().you do not have to implement your own sorting function; you can just and call Because the\nSTL is built for speed and was written by some of the best programmers around the world, you are guaranteed to have an efficient, bug-free\nimplementation available to you when you use an STL container or algorithm.\nWhat exactly does the STL provide? The STL consists of four primary components: containers, algorithms, functions, and iterators. The\nSTL provides implementations of different that can be used to store and retrieve data in different ways. For example, vectors, lists,containers\ndeques, stacks, queues, andpriorityqueuesareallcontainersthatareprovidedbytheSTL.TheSTLalsoprovidesimplementationsofalgorithms\nthatcanbeusedtooperateondataincontainers. Ifyouwanttouseanalgorithminyourprogram, thereisachancethatitisalreadyimplemented\nstd::sort() std::min_element()in the STL! As mentioned, can be called to sort a container of data, and other functions such as\nstd::max_element()and can be used to find the minimum and maximum value in a range of data. Many STL algorithms can be found in\n#include <algorithm>.the algorithm library, which can be accessed with We will cover this library in a later section of this chapter.\nstd::swap() <utility>Inadditiontoalgorithms,theSTLalsoprovidesimplementationsforutilities(e.g.,simpleoperationslike inthe\nstd::less<> std::greater<>library), function objects (e.g., comparators such as and that can be used to compare two values), and\nmemory allocators (which will not be covered in this class).", "word_count": 421, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8f11d4a4-0f56-5a8e-a319-566f2ccdfeed", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 338, "real_page_number": null, "text": "326\nChapter 11. Iterators and the Standard Template Library\nThe STL is very extensive, with entire books written on its contents. These notes will not go through every detail on all the different libraries\nthat C++ offers. Instead, this chapter will contain a sample of certain STL concepts that you might see again in future classes or in your career.\nYou will gain most of your exposure to the functionalities of the STL through programming experience and searching for resources online.\nRemark: You will not be required to memorize everything from the STL in this class! Even experienced programmers have to look things\nup every once in a while. However, there are certain things that you should be comfortable with, such as iterators, pairs, and function objects.\nThe other sections are just here as an optional reference for your coding assignments. If a concept was never mentioned in class, you will not\nneed to understand it for this class (but it still might be helpful to know!).\n11.2\nPairs\nA pair is a data type that groups together two values and treats them as a single unit. The two objects in a pair may be of different types. Many\nstd::map<> std::unordered_map<>different container classes rely on the pair construct to manage their elements (such as the and\nstd::pair<T1, T2>,containers, which will be covered in later chapters). To instantiate a pair, you should declare an object of type where\nT1 T2and are the types of the two objects in the pair.\nstd::pair<> first second.The two values that make up a can be accessed using its public members, and This is very similar to\nstruct std::pair<> struct firsthow a with two members would behave (in fact, a is implemented as a templated with two members\nsecond).and In addition to storing two values together, pairs can also be copied, assigned, swapped, and compared. Thus, if you want to bind\nstd::pair<>two data types together, a will allow you to do so without requiring you to redefine how simple operations work.\nThe following table lists some common pair operations. The list is not comprehensive, but it gives a sufficient overview of a few things you\ncan do with pair objects.\ntemplate <typename typenameT1, T2>\nstd::pair<T1, T2> p;\np T1 T2.Default constructor, initializes as a pair with the default constructed values of types and\ntemplate <typename typenameT1, T2>\np(const conststd::pair<T1, T2> T1& v1, T2& v2);\np T1 T2, v1 v2.Creates a pair with two values, one of type and one of type that are initialized to and\ntemplate <typename typenameT1, T2>\np2(conststd::pair<T1, T2> std::pair<T1, T2>& p1);\np2 p1.Copy constructor, initializes to be a copy of\np2 = p1;\np2 p1.Assignment operator, assigns a pair to the value of\np1 == p2;\nfirst second p1 p2.Checks if both and are identical for two pairs and\np1 < p2;\np1 p2, first second firstChecks if is less than first by comparing their values, and then by their values if the values are equal. This\n>, <=, >=.process works similarly for and\np.first;\np.The first element of the pair\np.second;\np.The second element of the pair\nstd::make_pair(v1, v2);\nfirst v1 second v2.Creates a pair with initialized to and initialized to\nstd::make_pair() std::make_pair(x, y)The functioncanbeusedtoconstructapair. Acalltothefunction constructsandreturns\nfirst x second y.a pair with set to and set to For instance, the statement\nint32_t>std::pair<std::string, myPair = std::make_pair(\"EECS\", 281);\nfirst \"EECS\" second 281.creates a pair with a value of and a value of Curly braces may also be used for the same result:\nint>std::pair<std::string, myPair = {\"EECS\", 281};\nstd::make_pair()The function can be used to pass a pair as an argument into a function. For example, consider the following function\nstd::pair<std::string, int>that takes in a as a parameter:\nint32_t int32_t>return_class_difficulty(std::pair<std::string, course);\nstd::make_pair()You can call in the function argument itself to build a pair and pass it in:\nreturn_class_difficulty(std::make_pair(\"EECS\", 281));\nCurly braces can also be used to pass a pair into a function:\nreturn_class_difficulty({\"EECS\", 281});\nstd::pair<> first secondThe also has built-in comparison methods. Two pairs are equal if and only if they have identical and values.\n< >, first firstWhen comparing pairs using and the value is compared first. If the values of two pairs differ, the comparison between\nfirst {12, 45} {10, 953}these values determines which pair is lesser or greater. For example, the pair is than the pair becausegreater\nfirst second12 is greater than 10. However, if the values are equal, the comparison of the values determines which pair is larger. For\n{10, 45} {10, 953}. first secondinstance, the pair is less than the pair Because these two pairs share identical values, their values\nare compared, and 953 is larger than 45.", "word_count": 819, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d067064d-d0e4-5057-8ee6-e77a2f9e9ae2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 339, "real_page_number": null, "text": "11.3 Tuples\n327\n11.3\nTuples\nA pair groups together two values and treats them as a single entity. A tuple does the exact same thing, but with number of elements. To useany\n#include <tuple>. std::tuple<>.tuples and tuple operations in your program, you must To instantiate a tuple, create an object of type\nA table of common tuple operations is shown below. This is not comprehensive, but it provides an overview of things you can do with tuple\nobjects. Note that much of this behavior is analogous to that of a pair.\ntemplate <typename typename typename typename...T1, T2, T3, OtherTypes>\nstd::tuple<T1, T2, T3, OtherTypes...> t;\ntDefault constructor, initializes a tuple with the default constructed values of all its component types.\ntemplate <typename typename typename typename...T1, T2, T3, OtherTypes>\nt(const const conststd::tuple<T1, T2, T3, OtherTypes...> T1& v1, T2& v2, T3& v3, OtherTypes...);\nt T1, T2, T3, v1, v2, v3Creates a tuple with values of types …, that are initialized to the values …\ntemplate <typename typenameT1, T2>\nt(conststd::tuple<T1, T2> std::pair<T1, T2>& p);\np, t p.Given a pair this initializes a two-element tuple with the contents of\nt2 = t1;\nt1 t2.Assignment operator, assigns the value of to\nt1 == t2;\nt1 t2Checks if is less than by comparing their first values, then their second values, then their third values, and so on.\nstd::make_tuple(v1, v2, v3, ...);\nCreates a tuple that is initialized with all the values that are passed in as arguments.\nstd::make_tuple() tSimilar to pairs, you can create tuples using the function or curly braces. For instance, the following creates a tuple\n\"EECS\", 281, \"PI\", 3.14:that is initialized with the contents and\nint32_t, double>std::tuple<std::string, std::string, t = std::make_tuple(\"EECS\", 281, \"PI\", 3.14);\nTuples also support copying, assignment, and comparison. The process of comparing tuples is similar to that of comparing pairs: the first\ncomponent is compared first, followed by the second, then the third, and so on.\nstd::get<> t, std::get<i>(t)To access data members of a tuple, you can use the method. Given a tuple can be used to retrieve\nithe component at position in the tuple, using zero indexing. For the above tuple:\nstd::get<0>(t) == \"EECS\"\nstd::get<1>(t) == 281\nstd::get<2>(t) == \"PI\"\nstd::get<3>(t) == 3.14\nstd::get<>, iA limitation with however, is that the value of the index must be known at compile time. That is, you have to know what the\nivalue of is you compile the program. Passing an index in at runtime is not possible; thus, the following code would compile:before not\nfor (int32_t i = 0; i < 4; ++i) {\nstd::cout << get<i>(t) << '\\n';\n// does not compile!\n} // for i\nstd::get<>Furthermore, checks if the index is valid before compiling. If you attempt to access a component of a tuple that does not exist,\nyou will get a compile error.\nstd::tuple_cat(). std::tuple_cat()Another interesting method is The function allows you to concatenate two tuples together.\nAs an example, suppose you had two tuples, each containing three components, as shown below:\nint32_t, double>std::tuple<std::string, t1 = std::make_tuple(\"EECS\", 281, 3.14);\nstd::tuple<double, int32_t, char>std::string, t2 = std::make_tuple(1.618, \"EECS\", 370, 'Q');\nstd::tuple_cat()If you ran on these two tuples:\nint32_t, double, double, int32_t, char>std::tuple<std::string, std::string, combined =\nstd::tuple_cat(t1, t2);\nYou would end up with a tuple that has seven components:\nstd::get<0>(combined) == \"EECS\"\nstd::get<1>(combined) == 281\nstd::get<2>(combined) == 3.14\nstd::get<3>(combined) == 1.618\nstd::get<4>(combined) == \"EECS\"\nstd::get<5>(combined) == 370\nstd::get<6>(combined) == 'Q'", "word_count": 584, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c43cf1a6-ae93-56cf-bab3-f4ff66a78370", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 340, "real_page_number": null, "text": "328\nChapter 11. Iterators and the Standard Template Library\nstd::tie()The function can be used to unpack the contents of a tuple. For example, suppose we wanted to unpack the contents of the\nfollowing tuple:\nint32_t, double>std::tuple<std::string, std::string, t = std::make_tuple(\"EECS\", 281, \"PI\", 3.14);\nstd::tie() tWe can use to retrieve the components of the tuple and store them into individual variables:\n1\nstd::string v1;\n2\nint32_t v2;\n3\nstd::string v3;\n4\ndouble v4;\n5\nstd::tie(v1, v2, v3, v4) = t;\n// stores tuple values in variables v1, v2, ...\n6\nstd::cout << v1 << '\\n';\n// prints \"EECS\"\n7\nstd::cout << v2 << '\\n';\n// prints 281\n8\nstd::cout << v3 << '\\n';\n// prints \"PI\"\n9\nstd::cout << v4 << '\\n';\n// prints 3.14\nstd::ignore std::tie()You can use within a call to to ignore certain components of the tuple:\n1\nstd::string v1;\n2\nint32_t v2;\n3\ndouble v4;\n4\nstd::tie(v1, v2, std::ignore, v4) = t;\n// ignores third item in tuple (\"PI\")\n5\nstd::cout << v1 << '\\n';\n// prints \"EECS\"\n6\nstd::cout << v2 << '\\n';\n// prints 281\n7\nstd::cout << v4 << '\\n';\n// prints 3.14\nstd::get<>,Lastly, because pairs are simply two-element tuples, tuple operations can also be used on pairs. Therefore, you can also use\nstd::tuple_cat(), std::tie()and when working with pairs.\nRemark: C++17 introduced the structured binding declaration, which allows you to bind variables to the subcomponents of another object.\nThis can be quite useful when working with pairs and tuples! Consider the following:\n1\nint>std::pair<std::string, p = {\"EECS\", 281};\n// p stores {\"EECS\", 281}\n2\nauto [str, num] = p;\n// structured binding\n3\nstd::cout << str << '\\n';\n// prints \"EECS\"\n4\nstd::cout << num << '\\n';\n// prints 281\nstr num,Here, line 2 is a structured binding that creates two variables, and whose values are derived from the contents of the pair. You can\nauto& [str, num].also declare a reference to the components of the pair using This is not required knowledge for the course, but it is a\nvery neat feature that is worthwhile to know. Structured bindings will be covered in a bit more detail in the last section of this chapter.\n11.4\nIterators\n¸ 11.4.1\nIterator Deﬁnitions\nIn the past few chapters, we discussed different approaches that can be used to implement different containers. If we used a pure object-oriented\napproach to implement our containers (e.g., like what we did in section 6.8), each container would be defined as a class, with algorithms defined\nas member functions.\nclass Vector {\n...\n// data\npublic:\n...\n// algorithms\n};\nclass List {\n...\n// data\npublic:\n...\n// algorithms\n};\nclass Deque {\n...\n// data\npublic:\n...\n// algorithms\n};\nHowever, there is an issue with this approach: each algorithm must be implemented multiple times, once for each container class! For instance,\nif we wanted to write an algorithm that can be used to find the smallest element in a container, we would have to implement it once for the vector,\nonce for the list, once for the deque, etc. In general, if we had 𝐴algorithms and 𝐶containers, we would have to write 𝐴×𝐶implementations to\nensure that all 𝐴algorithms are supported by all 𝐶containers.\nThis is not ideal, since many algorithms tend to have similar implementations. If we rewrite each algorithm implementation 𝐶times, we\ncan run into a lot of issues (e.g., code duplication, bugs, etc.). If possible, we would like to be able to write each algorithm once and generalize\nit to multiple containers.\nThis is where iterators come into play. In the STL, iterators serve as an interface between containers and algorithms. With iterators, an\nalgorithm’s implementation does not need to worry about the specific container type or the data layout in memory; it can operate independently\nof the container type! Any algorithm that we write will only have to deal with the actual data values in a container, which we can access using\nthe iterators that the container provides.\nAlgorithms\nContainers\nIterators", "word_count": 674, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0ff5614b-7b3e-5e7c-927d-41ccdc7184a6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 341, "real_page_number": null, "text": "11.4 Iterators\n329\nAn iterator is an object that can be used to traverse through elements of a container. Iterators are similar to pointers in that they can be\n(operator*), (operator++), (operator==), (operator=).dereferenced incremented compared andassigned However, theyarebetter\nthan pointers in that they provide an interface that is adapted to their underlying container. When you increment a pointer, you are incrementing\nthe address the pointer is pointing to. However, there is no guarantee that elements in an STL container are contiguous in memory. With an\niterator, you can move through the elements of a container without having to worry about how the data of the container is stored in memory.\nIncrementing an iterator, for instance, would point it to the next element in the container, regardless of whether memory is contiguous or not.\nBecause different containers store data differently, an iterator’s internal behavior depends on its underlying container. An iterator for a\nvector is implemented differently from an iterator for a list, for instance, because the elements in a vector are contiguous in memory while the\nelements in a list are not. However, even though the implementation may be different, the interface that an iterator provides is the same across\ncontainers. An algorithm that uses iterators can be used across many different containers to accomplish the same task. This removesdifferent\nthe need to write multiple versions of the same algorithm to support every type of STL container, since a single implementation of a function\ncan be used for multiple data structures.\nWith the exception of container adaptors (such as stacks and queues, which do not support iteration), STL container classes provide the\nfollowing member functions that return iterators to the container:\n.begin()• returns an iterator to the first element in the container.\n.end()• returns an iterator that points the last element in the container.one past\n.begin()\n.end()\n.begin() .end()The and iterators can be used to iterate through the contents of a container in a loop. This is done by incrementing an\niterator through the container until the end iterator is reached. An example is shown below:\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4};\n2\nfor (std::vector<int32_t>::iterator it = vec.begin(); it != vec.end(); ++it) {\n3\nstd::cout << *it << \" \";\n4\n} // for it\nwhileThe following code does the same thing, but with a loop:\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4};\n2\nstd::vector<int32_t>::iterator it = vec.begin();\n3\nwhile (it != vec.end()) {\n4\nstd::cout << *it++ << \" \";\n5\n} // while\n*it++Here, the term dereferences the iterator incrementing the iterator’s position (postfix incrementation).before\n.begin() .end(), .cbegin() .cend() constIn addition to and the member functions and can be used to return iterators to the\nconst constfirst and one past the last position of a container. Dereferencing a iterator returns a reference to a value that cannot be modified.\nstd::vector<int>::iterator)It should be noted that the type of the iterator (e.g., does need to be explicitly written out.not\nautoInstead, the keyword can be used to automatically deduce an iterator’s type.\nstd::begin() std::end()Remark: In C++11, the C++ standard library introduced the and functions. These two functions do the\n.begin() .end(),same thing as and but they make it easier to write generic code that can be applied to many different container types.\n.begin() .end()These two functions can also be used on C-style arrays, which do not have their own and member functions:\n1\nint32_t arr[5] = {1, 2, 3, 4, 5};\n2\nauto start = std::begin(arr);\n3\nwhile (start != std::end(arr)) {\n4\nstd::cout << *start++ << \" \";\n// prints 1 2 3 4 5\n5\n} // while\nc std::begin(c) c.begin(), std::end(c)Given any container that supports iterators, can be used instead of and can be used\nc.end(). std::begin(c) std::end(c) c.begin()insteadof Eventhough and providemoreflexibility,youaremorelikelytosee\nc.end() .begin()and just because the alternative is newer and not everyone knows about it (or just do not use out of habit, since and\n.end() have been the go-to standard for years).", "word_count": 692, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9e0649ca-20bd-5c66-bef7-ed183247d815", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 342, "real_page_number": null, "text": "330\nChapter 11. Iterators and the Standard Template Library\n¸ 11.4.2\nIterator Categories\nIterators in the STL can be placed into five different categories based on the operations they support. These five categories are:\n• read only, forward moving (single pass only)Input iterators:\n• write only, forward moving (single pass only)Output iterators:\n• forward moving (multiple passes allowed)Forward iterators:\n• forward and backward movingBidirectional iterators:\n• provides random accessRandom access iterators:\n--An input iterator values with forward movement (decrementation using is allowed). This type of iteratorreads not\n(++), (== !=).• can be dereferenced (*), incremented and compared and\n• does support multiple passes (once you increment the iterator, the previous position is no longer guaranteed to be dereferenceable,not\neven if you attempt to store an input iterator to the previous element).\n• canbedereferencedmultipletimes,butonlyforthecurrentelement—onceyouincrementaninputiterator,youcannolongerdereference\nvalues that it pointed to before.\nComparisons using input iterators are usually done to determine whether the iterator being incremented has reached the end of an iterator range.\n--An output iterator values with forward movement (decrementation using is allowed). This type of iteratorwrites not\n(++),• can be written to (using *) and incremented but be compared.cannot\n• does support multiple passes (once you increment the iterator, the previous position is no longer guaranteed to be dereferenceable).not\n• does supportmultiplewritestothesameelement(onceyoudereferenceandwrite, youcannotdereferenceagainwithoutincrementingnot\nthe iterator).\n--A forward iterator supports both reading and writing in the forward direction (decrementation using is allowed). This type of iteratornot\n(++), (== !=).• can be dereferenced (*), incremented and compared and\n• supports multiple passes.\n• supports multiple reads and writes to the same element.\nAbidirectionaliteratorcandoeverythingaforwarditeratorcan,butisalsosupportsbackwardmovement(i.e.,youcandecrementabidirectional\n--).iterator using\nA random access iterator can do everything a bidirectional iterator can, but it also supports pointer arithmetic and pointer comparisons. Given\n+, +=, -, -=a random access iterator, you are allowed to use mathematical expressions such as and to move the iterator’s position, as well as\n<, <=, >, >=comparison operators such as and to compare the relative positions of different random access iterators.\nInput\nOutput\nForward\nBidirectional\nRandom\nSupports dereference (*) and read\n✓\n✓\n✓\n✓\nSupports dereference (*) and write\n✓\n✓\n✓\n✓\n(++)Supports forward movement\n✓\n✓\n✓\n✓\n✓\n(--)Supports backward movement\n✓\n✓\nSupports multiple passes\n✓\n✓\n✓\n== !=Supports and\n✓\n✓\n✓\n✓\n(+, -,Supports pointer arithmetic etc.)\n✓\n(<, >,Supports pointer comparison etc.)\n✓\nOf the containers we have covered so far, vector and deque iterators are random access iterators, while list iterators are bidirectional iterators. A\nsummary of different STL containers and their iterator categories is shown below (we will cover many of these containers in later chapters):\nContainer Type\nIterator Category\nstd::vector<>\nRandom Access\nstd::deque<>\nRandom Access\nstd::string<>\nRandom Access\nstd::list<>\nBidirectional\nstd::set<>\nBidirectional\nstd::multiset<>\nBidirectional\nstd::map<>\nBidirectional\nstd::multimap<>\nBidirectional\nstd::unordered_set<>\nForward\nstd::unordered_multiset<>\nForward\nstd::unordered_map<>\nForward\nstd::unordered_multimap<>\nForward\nstd::forward_list<>\nForward\nThe type of iterator that a container supports is important for determining which algorithms can be used on that container. Some algorithms\nstd::sort()only support certain iterator types (for instance, in the algorithm library only accepts random access iterators, so this function\ncannot be used on a list — instead, lists have their own sort function that uses bidirectional iterators).", "word_count": 611, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1baeb890-2d5b-5d40-981c-9beafc832af4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 343, "real_page_number": null, "text": "11.5 Iterator Adaptors\n331\nAll of the iterator types can be organized into the following hierarchy:\nInput\nOutput\nForward\nBidirectional\nRandom Access\nThe random access iterator is highest in the hierarchy, while input and output iterators are lowest in the hierarchy. An iterator that is higher on\nthe hierarchy supports all of the features of iterators lower on the hierarchy. Because of this, if an iterator is expected in an algorithm, any iterator\nthat is higher on the hierarchy may be used in its place. For instance, a random access iterator supports all the functionalities of bidirectional,\nforward, input, and output iterators, and thus can be used anywhere one of these iterators is expected. Similarly, a bidirectional iterator supports\nall the functionalities of forward, input, and output iterators, but do not support all the functionalities of a random access iterator. Thus, a\nbidirectional iterator can be used in place of an input, output, or forward iterator, but it cannot be used in place of a random access iterator.\n11.5\nIterator Adaptors\n<iterator>The C++ library provides several iterator adaptors, which build upon the interface of the five categories of iterators to provide\nadditional functionality. There are several types of iterator adaptors, but we will focus on three: the reverse, stream, and insert iterators.\n¸ 11.5.1\nReverse Iterators\nA reverse iterator, much like its name implies, can be used to iterate through a container in reverse order. Incrementing a reverse iterator\nthe relative position of the iterator in the container.decrements\nAll containers that support bidirectional iterators or random access iterators also support reverse iterators. Reverse iterators for these\ncontainers can be retrieved using the following member functions:\n.rbegin() (std::rbegin()• returns an iterator pointing to the last element in the container can also be used).\n.rend() (std::rend()• returns an iterator pointing the first element in the container can also be used).one before\n.rend()\n.rbegin()\n--\n++\n.crbegin() .crend() const constThe member functions and can be used to return reverse iterators to a container. Much like standard\nconstiterators, dereferencing a reverse iterator returns a value that cannot be modified.\nReverse iterators are useful in that they allow algorithms to operate on a container’s data in reverse order without having to modify the\nordering of elements in the container itself. Passing reverse iterators into an algorithm is akin to passing in a range of data in backwards order.\nThe algorithm would run normally, but with the order of elements flipped — the algorithm would treat the last element as if it were the first\nelement in the container. AsTo show off an example, the following code uses reverse iterators:\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n2\nfor (auto it = vec.rbegin(); it != vec.rend(); ++it) {\n3\nstd::cout << *it << \" \";\n// prints 5 4 3 2 1\n4\n} // for it\n5\n6\n// range constructor uses reverse iterators, so vec_copy holds the\n7\n// contents of vec, but in reverse order: vec_copy == {5, 4, 3, 2, 1}\n8\nstd::vector<int32_t> vec_copy(vec.rbegin(), vec.rend());\n9\n// iterators can be used to copy data to a different container as well\n10\nstd::deque<int32_t> deq(vec.rbegin(), vec.rend());\n// deq == {5, 4, 3, 2, 1}\n11\nstd::list<int32_t> lst(vec.rbegin(), vec.rend());\n// lst == {5, 4, 3, 2, 1}\n¸ 11.5.2\n(✽)Stream Iterators\nA stream iterator is another iterator adaptor. Stream iterators can be used to traverse through the contents of a stream, as if the stream were\n(std::istream_iterator<>)a container. There are two classes of stream iterators, input stream iterators and output stream iterators\n(std::ostream_iterator<>). std::istream_iterator<>An is an input iterator that reads in the contents of a stream. Since\ninput stream iterators are input iterators, they only support a single pass of a stream.\nstd::istream_iterator<>There are two different forms that an can take on:\nstd::istream_iterator<>Initializing an with a stream object creates an iterator that reads in the first object in the stream.•\nstd::istream_iterator<T> it{std::cin} itFor example, creates an iterator that reads in the first object in the stream\nstd::cin. std::cin >> val, val T.This behavior is similar to calling where is of type\nstd::istream_iterator<>• Initializing an that is default constructed creates an iterator, or an iterator that indicatesend-of-stream\nendthat the stream has been entirely read (similar to the iterator of a standard container). This end-of-stream iterator can be compared\nstd::istream_iterator<>with another to determine if it has reached the end of the stream.", "word_count": 746, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bfac6d19-5947-5ed8-b3de-ee355d07425d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 344, "real_page_number": null, "text": "332\nChapter 11. Iterators and the Standard Template Library\nstd::istream_iterator<>Incrementing an reads in the next object in the stream. Note that an object is read in when the iterator is\nincremented, whenitisdereferenced! Dereferencingonlyreturnsacopyoftheobjectthatwasmostrecentlyreadin. Thefollowingexamplesnot\nstd::istream_iterator<>:make use of the\n1\n// 'it' is an iterator that can be used to\n2\n// traverse the contents of the cin stream\n3\nstd::istream_iterator<int32_t> it{std::cin};\n4\n// 'eof' is an iterator that identifies\n5\n// the end of the stream (default initialized)\n6\nstd::istream_iterator<int32_t> eof;\n7\n// iterate until 'it' reaches eof\n8\nwhile (it != eof) {\n9\nstd::cout << *it++ << \" \";\n10\n} // while\ninput.txt\n1 2\n3 4 5\n6 7 8 9\nOutput of program:\n1 2 3 4 5 6 7 8 9\n1\nstd::istream_iterator<std::string> it{std::cin};\n2\nstd::istream_iterator<std::string> eof;\n3\nwhile (it != eof) {\n4\nstd::cout << *it++ << \" \";\n5\n} // while\ninput.txt\nEECS 281\nis fun\nOutput of program:\nEECS 281 is fun\nstd::ostream_iterator<>An is an output iterator that writes objects to a stream. Since this type of iterator is an output iterator, it only\nstd::ostream_iterator<>supports a single pass in the forward direction. An can be quite useful for writing the contents of a container\nstd::cout):to an output stream (like\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n2\nstd::ostream_iterator<int32_t> out{std::cout};\n3\n// copy contents of vector to cout using std::copy()\n4\nstd::copy(vec.begin(), vec.end(), out);\n// prints 12345\nstd::ostream_iterator<>The can also be constructed with a delimiter argument. This delimiter is used to separate the elements that\nare written. The delimiter is optional, and without it, no separation is added between the data.\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n2\n// use a space to separate values in output\n3\nstd::ostream_iterator<int32_t> out{std::cout, \" \"};\n4\n// copy contents of vector to cout using std::copy()\n5\nstd::copy(vec.begin(), vec.end(), out);\n// prints 1 2 3 4 5\nstd::coutIn fact, you can condense the code above into the following (this writes the contents of an iterator range to in just one line of code,\nwhere elements are separated by spaces):\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n2\nstd::ostream_iterator<int32_t>{std::cout,std::copy(vec.begin(), vec.end(), \" \"});\nThis method can be quite useful for efficiently printing out the contents of any iterator range to an output stream.\n¸ 11.5.3\n(✽)Insert Iterators\nThe last iterator adaptor that we will discuss in this section is the insert iterator. By default, output iterators operate in overwrite mode. If you\ngive an output iterator to an algorithm that writes data, the algorithm will likely overwrite the data located at the position of the output iterator.\nstd::copy() <algorithm>Consider the function in the library, which takes in two input iterators and one output iterator and copies\n[first, last) result:the data in the iterator range to the range beginning at\nstd::copy(InputIterator first, InputIterator last, OutputIterator result);\norig dest.The following code copies the data from into However, since output iterators overwrite the data at their position, the final contents\ndest {5, 6, 7, 8}, {1, 2, 3, 4}of end up being since was overwritten.\n1\nstd::vector<int32_t> orig = {5, 6, 7, 8};\n2\nstd::vector<int32_t> dest = {1, 2, 3, 4};\n3\nstd::copy(orig.begin(), orig.end(), dest.begin());", "word_count": 564, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3714e949-a1be-5916-95f0-4b4946958bcc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 345, "real_page_number": null, "text": "11.5 Iterator Adaptors\n333\nAn insert iterator is a special type of output iterator that values at a given position instead of overwriting them. If you write a value to aninserts\ninsert iterator, the insert iterator inserts the value at that position of the container instead of overwriting the data that was originally there. There\nare three different types of insert iterators: the back-insert iterator, the front-insert iterator, and the generic insert iterator.\nThe back-insert iterator is an insert iterator that allows algorithms to insert new elements at the back of the underlying container (after the\n.push_back()last element). When a back-insert iterator is dereferenced and assigned to, a operation is called on the container. To construct\nstd::back_inserter()a back-insert iterator for a container, pass the container into the function. Back-insert iterators only work on\n.push_back() std::vector<>, std::deque<>, std::list<>).containers that support a member function (such as and\n1\nstd::vector<int32_t> vec = {1, 2, 3};\n2\nstd::back_insert_iterator<std::vector<int32_t>> it = std::back_inserter(vec);\n3\n*it = 4;\n// inserts 4 at the back of the container, final vec = {1, 2, 3, 4}\nBack-insert iterators can be used in conjunction with STL algorithms to insert values into a sequence rather than overwriting them. For example,\nstd::copy()the following code passes in a back-insert iterator as the output iterator of the function. By doing this, everything that the\nalgorithm copies is inserted to the back of the container:\n1\nstd::vector<int32_t> orig = {5, 6, 7, 8};\n2\nstd::vector<int32_t> dest = {1, 2, 3, 4};\n3\n// copy elements in orig and write to back_inserter(dest)\n4\nstd::copy(orig.begin(), orig.end(), std::back_inserter(dest));\n5\n// final contents of dest now {1, 2, 3, 4, 5, 6, 7, 8}\nThe front-insert iterator is an insert iterator that allows algorithms to insert new elements at the front of the underlying container (before\n.push_front()the first element). When a front-insert iterator is dereferenced and assigned to, a operation is called on the container. To\nstd::front_inserter()construct a front-insert iterator for a container, pass the container into the function. Front-insert iterators only\n.push_front() std::deque<> std::list<>).work on containers that support a member function (such as and\n1\nstd::deque<int32_t> deq = {2, 3, 4, 5};\n2\nstd::front_insert_iterator<std::deque<int32_t>> it = std::front_inserter(deq);\n3\n*it = 1;\n// inserts 1 at the front of the container, final deq = {1, 2, 3, 4, 5}\nWhen a front-insert iterator is used to insert a range of elements to the front of a container, the elements in the range are pushed to the front of\nthe container one by one. As a result, using the front-insert iterator to insert a range will reverse the order of the inserted elements (i.e., if you\n{1, 2, 3, 4}, 1 2, 3, 4; 4use a front-insert iterator to insert would be pushed to the front first, then then then at the end, ends up being at\nthe very front since it was pushed last).\n1\nstd::deque<int32_t> orig = {5, 6, 7, 8};\n2\nstd::deque<int32_t> dest = {1, 2, 3, 4};\n3\n// copy elements in orig and write to front_inserter(dest)\n4\nstd::copy(orig.begin(), orig.end(), std::front_inserter(dest));\n5\n// contents of dest now {8, 7, 6, 5, 1, 2, 3, 4}\nLastly, the generic insert iterator can be used to insert elements anywhere in a container. To construct a generic insert iterator, pass the\nstd::inserter()underlying container and an iterator to the position of insertion into the function. Generic insert iterators only work on\n.insert()containers that support an member function (this function is supported for most of the containers you will use in this class; the\nonly exceptions are container adaptors, arrays, and forward lists). Incrementing an insert iterator does nothing; if you want to insert at a new\nposition, you would have to redeclare it with the new desired position.\n1\nstd::vector<int32_t> vec = {1, 2, 3, 6, 7, 8};\n2\n// vec.begin() + 3 points to 6\n3\nstd::insert_iterator<std::vector<int32_t>> it = std::inserter(vec, vec.begin() + 3);\n4\n*it = 4;\n// inserts 4 before 6, vec now {1, 2, 3, 4, 6, 7, 8}\n5\n*it = 5;\n// inserts 5 before 6, vec now {1, 2, 3, 4, 5, 6, 7, 8}\nautoAs mentioned earlier, you can use instead of typing out the full name of the iterator type (which can be long and complicated). This is a\nautosituation where using can make your code a lot cleaner without losing valuable information.", "word_count": 734, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9de7d853-16d5-55cc-a587-7eadfd802ea1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 346, "real_page_number": null, "text": "334\nChapter 11. Iterators and the Standard Template Library\n11.6\nAuxiliary Iterator Functions (✽)\n¸ 11.6.1\n(✽)std::advance\n<iterator>The STL library provides several useful operations that can make iterator usage cleaner. As mentioned earlier, iterators can\nbe grouped into five categories, where the random access iterator has the most functionality. Unlike the other iterator types, a random access\niterator can support pointer arithmetic. This makes it easy to move a random access iterator around relative to its current position. For example,\nit,if we had a random access iterator and we wanted to increment the iterator by 10 positions, we could just do some simple math:\nit += 10;\nHowever, this is not something we can do with a non-random access iterator. In cases where we cannot increment an iterator by a large distance\nfunction:1std::advance()using math operations, we can instead use the\nvoid intstd::advance(InputIterator &it, n);\nadvance() it n nThe function takes the iterator and increments it by positions. If is negative, the iterator is moved backward instead of\nnforward. However, can only be negative if a random access or bidirectional iterator is passed in (since these are the only iterator types that\nsupport backward movement).\nit,For example, if we are given a non-random access iterator and we wanted to increment it by 10 positions, we could run the following\nit += 10code (which is equivalent to doing for a random access iterator):\nstd::advance(it, 10);\n¸ 11.6.2\n(✽)std::distance\nit_first it_lastIt is also tougher to find the distance between two iterators that do not support random access. If two iterators and\nsupport random access, we can find the distance between them by doing subtraction:\nint dist = it_last - it_first;\nHowever, subtraction is not allowed for non-random access iterators. To make distance calculations cleaner for these iterators, we can use the\nstd::distance() function:\nint std::distance(InputIterator first, InputIterator last);\nfirst last.This function calculates the number of elements between and For example, if we wanted to find the distance between two\nit_first it_last,non-random access iterators and we can run the following code:\nint dist = std::distance(it_first, it_last);\nThe iterator passed in as the first argument should come before the iterator passed in as the second argument.\n¸ 11.6.3\n(✽)std::next and std::prev\nn c, cLastly, if we wanted to instantiate a iterator that points to the element at index of a container we could do something like this ifseparate\naccess:2supports random\nRandomAccessIterator it = std::begin(c) + n;\nc std::next() std::prev()If does not provide random access, we can instead use the and functions:\nintForwardIterator std::next(ForwardIterator it, n);\nintBidirectionalIterator std::prev(BidirectionalIterator it, n);\nstd::next() it nThe function returns an iterator pointing to the element that would be pointing to if it advanced positions from its current\nstd::prev() it nposition. Similarly, the function returns an iterator to the element that would be pointing to if it were decremented\nstd::advance(), std::next() std::prev()positions. Unlike the and functions do move the iterators that are passed in. Annot\nexample of iterator function usage is shown below:\n1\nstd::list<int32_t> lst = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n2\nauto it = std::begin(lst);\n// 'it' points to 0\n3\nstd::advance(it, 3);\n// 'it' points to 3\n4\nstd::advance(it, 4);\n// 'it' points to 7\n5\nstd::advance(it, -2);\n// 'it' points to 5\n6\nauto foo = std::next(it, 4);\n// 'foo' points to 9, 'it' points to 5\n7\nauto bar = std::prev(it, 4);\n// 'bar' points to 1, 'it' points to 5\n8\nstd::cout << std::distance(bar, foo) << '\\n';\n// prints 8, the distance between 9 and 1\n1The n int, difference_type. difference_typetypeof isnotexplicitlydefinedasan butrathersomethingknownasa A isbasicallyanynumerical\ninttypethatisabletorepresentiteratordistance. Thisismainlyhereforportabilitysakeacrossdifferentcompilers,andyoucanjusttreatitlikean (whichis\nwhatwedohere). Thistypealsoappliesfortheotheriteratorfunctionsinthissection. Ifyouareconfusedaboutwhatthisinformationevenmeans,don’tworry\naboutit—it’snotsomethingyoureallyneedtoknow.\n2RandomAccessIterator std::vector<int>::iterator.isn’tanactualtype,butratheraplaceholderforanyrandomaccessiterator,suchasa", "word_count": 721, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b5d86340-b56f-5f3c-87f1-51e8101b927d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 347, "real_page_number": null, "text": "11.7 Predicates in the STL\n335\n11.7\nPredicates in the STL\noperator()A function object, or functor, is an object that can be used with to behave like a function. A predicate is a special type of\nboolfunction object that returns a and is stateless (i.e., it produces the same result for the same input value). A is a predicateunary predicate\nthat takes in one argument and checks a property of that argument. A is a predicate that takes in two arguments and compares abinary predicate\nproperty of these two arguments.\nFunction objects can also be used to compare two objects. A comparator is a binary predicate that takes in two objects and returns whether\none object goes before or after the other. The C++ standard library provides several predefined comparators, the most common of which are\nstd::less<> std::greater<>.and\nstd::less<>The comparator is a binary predicate that returns whether its first argument is less than the second (as returned by\noperator<). An example is shown below:\n1\nstd::less<int32_t> comp;\n2\nstd::cout << comp(4, 5) << '\\n';\n// returns 1 (true) since 4 is less than 5\n3\nstd::cout << comp(5, 4) << '\\n';\n// returns 0 (false) since 5 is not less than 4\n4\nstd::cout << comp(5, 5) << '\\n';\n// returns 0 (false) since 5 is not less than 5\nstd::greater<>The comparator is a binary predicate that returns whether its first argument is greater than the second (as returned by\noperator>). An example is shown below:\n1\nstd::greater<int32_t> comp;\n2\nstd::cout << comp(4, 5) << '\\n';\n// returns 0 (false) since 4 is not greater than 5\n3\nstd::cout << comp(5, 4) << '\\n';\n// returns 1 (true) since 5 is not greater than 4\n4\nstd::cout << comp(5, 5) << '\\n';\n// returns 0 (false) since 5 is not greater than 5\nT std::less<T> std::greater<T>) operator< std::less<T>The type of the arguments (i.e., the in and must support for to\noperator> std::greater<T> std::less<T> struct class,work, and for to work. Thus, if you want to use on a custom or the\noperator< operator> std::greater<T>).object definition must implement (or for\nstd::less<> std::greater<>The and comparators aren’t the only two function objects that are provided by the STL. Other objects\nthat are predefined by the standard library include:\nstd::equal_to<>,• which checks for equality of two arguments\nstd::not_equal_to<>,• which checks for inequality of two arguments\nstd::less_equal<>,• which checks whether one argument is less than or equal to the other\nstd::greater_equal<>,• which checks whether one argument is greater than or equal to the other\nstd::less<> std::greater<>This list is not comprehensive. However, and are the two STL function objects that you are most likely\nto see and use in this course (and thus are the only two you need to know well). These comparators play an important role in STL algorithms\nstd::priority_queue<>.and container declarations, such as with the\n11.8\nStandard Math Functions and Numeric Limits\n¸ 11.8.1\nThe Math Library\nIn this section, we will explore several functions in the standard library that work with numbers and mathematics. First, we will look at the\n<cmath> library, which contains several common mathematical operations, some of which are reproduced below (this list is not comprehensive,\nbut it contains operations you are most likely to encounter):\nFunction\nBehavior\ndouble std::exp(double x);\n𝑒𝑥Returns\ndouble std::log(double x);\nReturns ln(𝑥)\ndouble std::log10(double x);\nReturns log10(𝑥)\ndouble std::log2(double x);\nReturns log2(𝑥)\ndouble std::pow(double doublebase, exponent);\nbase(exponent)Returns\ndouble std::sqrt(double x);\nReturns\n√\n𝑥\ndouble std::cbrt(double x);\nReturns\n3√\n𝑥\ndouble std::ceil(double x);\nRounds 𝑥upwards and returns smallest integer value that is not\nless than 𝑥\ndouble std::floor(double x);\nRounds 𝑥downwards and returns largest integer value that is not\ngreater than 𝑥\ndouble std::round(double x);\nReturns the integer value that is nearest to 𝑥, with halfway cases\nrounded away from zero\ndouble std::abs(double x);\nReturns the absolute value of 𝑥(may be unsafe for floating point\nnumbers)3\ndouble std::fabs(double x);\ndouble floatReturnstheabsolutevalueof𝑥,saferfor and (see\nfootnote on the next page)", "word_count": 680, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a8f0ce7f-0393-560a-a101-4cdbb650e312", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 348, "real_page_number": null, "text": "336\nChapter 11. Iterators and the Standard Template Library\nThese functions are general-purpose operations that are implemented to work for many different cases. As a result, their versatility makes them\n320. 32,pow(3, 20)slower for simpler operations. For example, is really efficient at calculating However, if you want to calculate it would\n3*3 pow(3, 2),be much faster to run instead of since a function that is implemented to handle huge powers is unnecessarily complex for\nsomething as simple as squaring a number.\n¸ 11.8.2\nSetting Precision for Floating Point Output\nstd::setprecision()Another useful operation is the function, which can be used to set decimal precision for floating point output. This\n<iomanip>method is provided in the library:\nFunction\nBehavior\nstd::setprecision(int n);\nSets the decimal precision of a floating point number to 𝑛digits\nbefore it is displayed on output\nstd::setprecision()The following examples use to format decimal output:\n1\ndouble pi = 3.14159265358979;\n2\nstd::cout << std::setprecision(1) << pi << '\\n';\n// prints 3\n3\nstd::cout << std::setprecision(2) << pi << '\\n';\n// prints 3.1\n4\nstd::cout << std::setprecision(3) << pi << '\\n';\n// prints 3.14\n5\nstd::cout << std::setprecision(4) << pi << '\\n';\n// prints 3.142\n6\nstd::cout << std::setprecision(5) << pi << '\\n';\n// prints 3.1416\n7\nstd::cout << std::setprecision(6) << pi << '\\n';\n// prints 3.14159\n8\nstd::cout << std::setprecision(7) << pi << '\\n';\n// prints 3.141593\n9\nstd::cout << std::setprecision(8) << pi << '\\n';\n// prints 3.1415927\n10\nstd::cout << std::setprecision(9) << pi << '\\n';\n// prints 3.14159265\n11\nstd::cout << std::setprecision(10) << pi << '\\n';\n// prints 3.141592654\n12\nstd::cout << std::setprecision(11) << pi << '\\n';\n// prints 3.1415926536\n13\nstd::cout << std::setprecision(12) << pi << '\\n';\n// prints 3.14159265359\n14\nstd::cout << std::setprecision(13) << pi << '\\n';\n// prints 3.14159265359\n15\nstd::cout << std::setprecision(14) << pi << '\\n';\n// prints 3.1415926535898\n16\nstd::cout << std::setprecision(15) << pi << '\\n';\n// prints 3.14159265358979\n¸ 11.8.3\nNumeric Limits\nstd::numeric_limits<>Another useful component of the standard library is the class template, which provides a generalized method\nbool, char,for retrieving properties of arithmetic types, such as integers and floating point numbers. Examples of arithmetic types include\nint, float, double, short int, long double, unsigned int, <limits>and variations such as etc. This template is defined in the\n#include <limits>header, so you must to use these methods.\nstd::numeric_limits<> min(), lowest(), max(),The class provides several member functions, four of which are and\ninfinity(). Details about these functions are provided below:\nFunction\nBehavior\nstd::numeric_limits<T>::min();\nReturns the minimum finite value representable by the numeric type\nT. For non-floating point types, this is the lowest possible value that\nthe type can hold (i.e., the value that has no values less than it).\nfloatFor floating point types with denormalization, such as and\ndouble, numeric_limits<T>::min() returns the smallest\nfinite value that the type can hold.positive\nstd::numeric_limits<T>::lowest();\nT.Returns the lowest finite value representable by the numeric type\nTNothing of type can have a value lower than it.\nstd::numeric_limits<T>::max();\nReturns the maximum finite value representable by the numeric type T.\nstd::numeric_limits<T>::infinity();\nTReturns a special \"infinity\" value if the type supports infinity. This\nfloat double.isonlyusefulforfloatingpointnumberssuchas and\nOtherwise, this returns 0.\nA few numeric limits values are shown below. You do not need to memorize the rules above!\nstd::numeric_limits<int>::min() == -2147483648 int)• (lowest possible\nstd::numeric_limits<double>::min() == 2.22507E-308• (note that this is positive, but small)\nstd::numeric_limits<int>::lowest() == -2147483648 int)• (lowest possible\nstd::numeric_limits<double>::lowest() == -1.79769E308 double)• (lowest possible\nstd::numeric_limits<int>::max() == 2147483647 int)• (largest possible\nstd::numeric_limits<double>::max() == 1.79769E308 double)• (largest possible\n3WARNING: std::abs() #include<cmath>Eventhoughthisfunctionworksforfloatingpointnumbers,youmustuse and inyourprogramforthisto\nabs()workproperly. Thedefaultversionof istheCversionthatonlytakesinintegers! Ifyouwanttofindtheabsolutevalueofafloatingpointnumber,itis\nfabs()safertouse tofindtheabsolutevaluesinceitclarifiesyourintentandpreventsundetectablebugs.", "word_count": 675, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "30cc2a6c-67aa-5e3e-b0e6-29b724a85fff", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 349, "real_page_number": null, "text": "11.8 Standard Math Functions and Numeric Limits\n337\nstd::numeric_limits<> double int.If you want to use infinity in your program, you should use with a rather than an This is because\n1.79769E308, doubleinfinity is defined for integers. The value of infinity is considered larger than the largest value a can take on.not\nstd::numeric_limits<double>::infinity();Infinity in C++:\nint, INT_MIN INT_MAXIf you want to use the largest or smallest possible you can either use numeric limits, or you can use the constants and\n<climits> INT_MIN numeric_limits<int>::min(), INT_MAXin the library. has the same value as and has the same value as\nnumeric_limits<int>::max(). In C++, however, the numeric limits style is preferred.\n¸ 11.8.4\nstd::iota\nstd::iota() <numeric>Another useful function that you might encounter is the function defined in the library. This function takes in an\n[first, last) last n, n, n + 1, n + 2,iterator range defined by (where is exclusive) and a value and it fills the range with the values …,\nTall the way to the end of the iterator range. The type of must be incrementable.\ntemplate <typename typenameForwardIterator, T>\nvoid std::iota(ForwardIterator first, ForwardIterator last, T val);\n[first, last) n.Fills the iterator range with sequentially increasing values, starting with\nAn example of this function’s usage is shown below:\n1\n// initialize vector of size 10\n2\nstd::vector<int32_t> vec(10);\n3\n// fill up vector with values 1, 2, 3, 4, ...\n4\nstd::iota(vec.begin(), vec.end(), 1);\n5\n// contents of vec: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n6\n7\n// fill up vector with values 11, 12, 13, 14 in the range\n8\n// [vec.begin() + 3, vec.begin() + 7)\n9\nstd::iota(vec.begin() + 3, vec.begin() + 7, 11);\n10\n// contents of vec: {1, 2, 3, 11, 12, 13, 14, 8, 9, 10}\nstd::iota()The function can be quite useful in certain situations, as we will see when we discuss the union-find algorithm in chapter 13.\n¸ 11.8.5\n(✽)std::accumulate\n<numeric> std::accumulate()Another useful function in the library is the function, which sums up all the values in an iterator range:\ntemplate <typename typenameInputIterator, T>\nT accumulate(InputIterator first, InputIterator last, T init);\n[first, last) init.Sums up all the elements in the range and adds the result to the value\nstd::accumulate()An example of the function is shown below:\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n2\nstd::cout << std::accumulate(vec.begin(), vec.end(), 0) << '\\n';\n// prints 15\n3\nstd::cout << std::accumulate(vec.begin(), vec.end(), 5) << '\\n';\n// prints 20\nstd::accumulate() vec, 5Both calls to sum up all the values in the vector but the second call starts with an initial value of (which is\n20 15).why it prints instead of\nstd::accumulate()The function can do more than summing up elements. Even though summation is the default behavior of the\nfunction, you can also pass in a separate binary operation if you want it to do something different.\ntemplate <typename typename typenameInputIterator, T, BinaryOperation>\nT accumulate(InputIterator first, InputIterator last, T init, BinaryOperation op);\nop [first, last) init.Applies to accumulate all the values in the range to\nvecFor instance, the following code multiplies all the values in the vector and prints the result:\n1\nint32_t prod(int32_t int32_taccumulator, element) {\n2\nreturn accumulator element;*\n3\n} // prod()\n4\n5\nint main() {\n6\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n7\nstd::cout << std::accumulate(vec.begin(), vec.end(), 1, prod) << '\\n';\n// 120\n8\n} // main()", "word_count": 584, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6afd379a-dab4-5e8f-ad58-fcacabb78c73", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 350, "real_page_number": null, "text": "338\nChapter 11. Iterators and the Standard Template Library\nThe binary operation that you pass in should take in two arguments: an accumulator and an element. The accumulator is a value that starts at\ninit element std::accumulate()and gets accumulated over every in the iterator range. The function iterates through the iterator\nrange and applies the binary operation to each element using the accumulator. Thus, the following:\nstd::cout << std::accumulate(vec.begin(), vec.end(), 1, prod) << '\\n';\nbehaves identically to this:\n1\nint32_t init = 1;\n// accumulator value initialized to 3rd argument of function call\n2\nfor (int32_t elt : vec) {\n3\ninit = prod(init, elt);\n4\n} // for elt\n5\nstd::cout << init << '\\n';\nstd::accumulate()The binary operation makes much more versatile. For instance, the following code can be used to accumulate a\noperator+container of custom objects, even though each object does not define on its own:\n1\nstruct Student {\n2\nstd::string name;\n3\ndouble wealth;\n4\n};\n5\n6\ndouble add_wealth(double constaccumulator, Student& element) {\n7\nreturn accumulator + element.wealth;\n8\n} // add_wealth()\n9\n10\nint main() {\n11\nstd::vector<Student> students = {{\"Alice\", 1.00}, {\"Bob\",\n1.50}, {\"Cathy\", 2.00},\n12\n{\"Drew\",\n2.50}, {\"Emily\", 3.00}, {\"Frank\", 3.50}};\n13\nstd::cout << std::accumulate(students.begin(), students.end(), 0.00, add_wealth) << '\\n';\n14\n} // main()\nstd::accumulate()Here,the functionsumsupthewealthofeachofthestudentsinthevectorandprintsthetotalsum(inthiscase,$13.50).\n0.00,Note that the accumulator is initialized to a value of as shown in the third argument of the function call. The decimal was included so\nstd::accumulate() double int 0that would return an object of type rather than an object of type (which would have happened if were\nstd::accumulate() init!passed in without decimals). Remember that the return type of is the same type as the third argument,\n11.9\nThe Algorithm Library\n¸ 11.9.1\nIntroduction to the Algorithm Library\n<algorithm>In this section, we will look at several of the algorithms that C++ provides in the library. As mentioned earlier, algorithms in\nthe algorithm library utilize iterators to perform operations on a container’s data:\nAlgorithms\nContainers\nIterators\nIteratorsprovideaninterfacethatallowSTLalgorithmstoaccessthedataofanunderlyingcontainer. However, iteratorsonlyprovidealgorithms\nwith access to the data; they do identify the specific container type the data is stored in (if there even is one). That is, if an algorithm receivesnot\nan iterator range, it can only see the data values within that range, how the data is organized in memory. As a result, algorithms in the STLnot\n<algorithm> library only work on the data values they are given and data.cannot modify the internal structure of the container that holds the\nImportant: STL algorithms can modify the data of a container, but they cannot modify the of the container that the datavalues structure\nis stored in. This is because iterators provide an interface that gives algorithms access to data values, but not the container type itself.\nstd::remove()This is especially important to remember for STL algorithms that are designed to mutate the container, such as and\nstd::remove_if(). These algorithms simply modify the ordering of data to make removing easier, but the actual removal needs to be\n.erase()).done separately (by calling a member function of the container itself, such as\n<algorithm> beginAlgorithms in the STL library typically accept two iterators that denote an iterator range: a iterator that denotes the\nendstart of the range, and an iterator that denotes the element the last element in the range. By doing this, the algorithm has access toone past\n[begin, end) begin endall data values in the range (i.e., the element pointed to by is included in the range, but the element pointed to by\nis not). Iterator ranges can be used to capture any section of a container to invoke an algorithm on.", "word_count": 656, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c7235519-1349-5808-b520-67f340bdcd48", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 351, "real_page_number": null, "text": "11.9 The Algorithm Library\n339\n<algorithm>The library is quite extensive, and this section will not be able to cover every function that is available. Instead, we will focus\non a few algorithms that you will most likely encounter at some point in your programming career. These algorithms are listed below:\n• minimum and maximum algorithms\n• sorting algorithms\n• find algorithms\n• removal algorithms\n• replacement algorithms\n• transformation algorithms\n• copy algorithms\nstd::reverse()•\nstd::nth_element()•\n• lower and upper bound algorithms\nOther algorithms in this library will be introduced in later chapters as well, in sections that are more related to their use cases (for example, we\nwill cover set functions in the chapter about sets).\n¸ 11.9.2\nMinimum and Maximum Algorithms\nC++ provides several different ways to find the minimum or maximum value from a collection of data. Of these functions, the simplest ones are\nstd::min() std::max().and These two functions can be used to find the smaller or larger of two values.\ntemplate <typename typenameT, Compare>\nstd::min(const constT T& a, T& b, Compare comp);\na b. operator<Returns the smaller of and The comparator is optional, and is used if it is not specified.\ntemplate <typename typenameT, Compare>\nstd::max(const constT T& a, T& b, Compare comp);\na b. operator<Returns the larger of and The comparator is optional, and is used if it is not specified.\nstd::minmax() a b,The function returns both the smaller and larger elements as a pair. Given two values of and the smaller element is\nfirst second a breturned as the element of the pair, while the larger element is returned as the element. If both and are equivalent, the\nstd::make_pair(a, b).function returns\ntemplate <typename typenameT, Compare>\nstd::pair<const const std::minmax(const constT&, T&> T& a, T& b, Compare comp);\na b std::make_pair(a,Returnsapairwiththesmallerof and asthefirstelement,andthelargerasthesecond. Ifbothareequal,returns\nb). operator<The comparator is optional, and is used if it is not specified.\nAn example is shown below:\n1\nstd::cout << std::min(14, 12) << '\\n';\n// prints 12\n2\nstd::cout << std::max(14, 12) << '\\n';\n// prints 14\n3\nstd::pair<int32_t, int32_t> p = std::minmax(14, 12);\n4\nstd::cout << p.first << '\\n';\n// prints 12\n5\nstd::cout << p.second << '\\n';\n// prints 14\nThese functions cannot take in multiple arguments beyond the two you want to compare. If you want to find the minimum or maximum of more\nthan two values, you would have to pass the values in as an (a comma-separated sequence of values enclosed by curly braces). Aninitializer list\nexample of this is shown below:\n1\nint32_t a = 12, b = 15, c = 11, d = 19, e = 10, f = 14;\n2\nstd::cout << std::min({a, b, c, d, e, f}) << '\\n';\n// prints 10\n3\nstd::cout << std::max({a, b, c, d, e, f}) << '\\n';\n// prints 19\n4\nstd::pair<int32_t, int32_t> p = std::minmax({a, b, c, d, e, f});\n5\nstd::cout << p.first << '\\n';\n// prints 10\n6\nstd::cout << p.second << '\\n';\n// prints 19\nstd::min_element(),std::max_element(),Ifyouwanttofindtheminormaxvalueofanentirecontainer,youshouldusetheSTL’s\nstd::minmax_element()or functions. These functions accept iterators to the sequence you want to identify the minimum or maximum\nfor. Their behaviors are similar to the previous three functions, with the exception that the data is passed in using an iterator range as input.\ntemplate <typename typenameForwardIterator, Compare>\nForwardIterator std::min_element(ForwardIterator first, ForwardIterator last, Compare comp);\n[first, last).Returns an iterator pointing to the smallest element in the range The comparator is optional, and comparisons are done\noperator<using if it is not specified.\ntemplate <typename typenameForwardIterator, Compare>\nForwardIterator std::max_element(ForwardIterator first, ForwardIterator last, Compare comp);\n[first, last).Returns an iterator pointing to the largest element in the range The comparator is optional, and comparisons are done\noperator<using if it is not specified.\ntemplate <typename typenameForwardIterator, Compare>\nstd::pair<ForwardIterator, ForwardIterator>\nstd::minmax_element(ForwardIterator first, ForwardIterator last, Compare comp);\n[first, last)Returns a pair with an iterator pointing the smallest value in the range as the first element, and an iterator pointer to the\noperator<largest as the second. The comparator is optional, with comparisons done using if not specified.", "word_count": 727, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "96ce3418-9b5b-5cc5-a722-b1158eed8d59", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 352, "real_page_number": null, "text": "340\nChapter 11. Iterators and the Standard Template Library\nExamples using these functions are shown below:\n1\nstd::vector<int32_t> vec = {6, 2, 4, 9, 8, 3, 7, 1, 5};\n2\nstd::cout << *std::min_element(vec.begin(), vec.end()) << '\\n';\n// prints 1\n3\nstd::cout << *std::max_element(vec.begin(), vec.end()) << '\\n';\n// prints 9\n4\nauto it_pair = std::minmax_element(vec.begin(), vec.end());\n5\nstd::cout << *it_pair.first << '\\n';\n// prints 1\n6\nstd::cout << *it_pair.second << '\\n';\n// prints 9\nstd::min()The time complexity of all these operations is linear on the number of elements that are compared (with the exception of and\nstd::max() with only two arguments, which both take constant time).\n¸ 11.9.3\nSorting Algorithms\nstd::sort() [first, last).The algorithm library provides a function, which can be used to sort elements within an iterator range An\noptional comparator may be passed in as a third argument; without the comparator specified, elements in the range are sorted in ascending order.\nEquivalent elements are not guaranteed to maintain their relative ordering with this function.\ntemplate <typename typenameRandomAccessIterator, Compare>\nvoid std::sort(RandomAccessIterator first, RandomAccessIterator last, Compare comp);\n[first, last) compSorts the elements in the range in ascending order, or using the comparator if specified.\noperator<Without the comparator, elements are sorted in ascending order using as the default. If a comparator is passed in, the order of\ntrueelements after sorting is determined using the following rule: given two elements that are passed into the comparator, a return value of\nindicates that the first element comes before the second in sorted order; otherwise, it comes after.\nFor example, if you wanted to sort the elements of a container in descending order, you would want to pass in a comparator that returns\ntrue comp(a, b) true a > b, aonly if the first element passed in is larger than the second (i.e., returns only if since that would indicate\nb std::greater<>comes before in sorted order). This can be done using as the comparator, as shown below:\n1\nstd::vector<int32_t> vec = {6, 2, 4, 9, 8, 3, 7, 1, 5};\n2\n3\n// sort in ascending order -> no comparator needed (since std::less<> is default)\n4\nstd::sort(vec.begin(), vec.end());\n5\n// ordering of elements after sort: 1 2 3 4 5 6 7 8 9\n6\n7\n// sort in descending order -> pass in std::greater<> comparator\n8\nstd::greater<int32_t>());std::sort(vec.begin(), vec.end(),\n9\n// ordering of elements after sort: 9 8 7 6 5 4 3 2 1\nstd::sort()Since takes in an iterator range, you can use it to partially sort a range of values within a container. In the following example,\nonly the elements in the provided iterator range (4, 9, 8, and 3) end up being sorted relative to each other (since the sorting algorithm does not\nknow about the existence of elements outside the iterator range):\n1\nstd::vector<int32_t> vec = {6, 2, 4, 9, 8, 3, 7, 1, 5};\n2\n3\n// only sort elements in range [vec.begin() + 2, vec.begin() + 6)\n4\nstd::sort(vec.begin() + 2, vec.begin() + 6);\n5\n// ordering of elements after sort: 6 2 3 4 8 9 7 1 5\nIt is important to note that the algorithm library’s sorting function requires random access iterators to work. This is because the underlying\nsorting implementation uses iterator arithmetic. As a result, if a container does not support random access iterators, the algorithm library’s\nstd::sort() function cannot be used to sort it.\nstd::sort()Sorting also works with reverse iterators, as long as they support random access. When you pass a range of data into using\nreverse iterators, the algorithm sees the data in reverse order. Thus, the final result you obtain will be the reverse of what the algorithm would\nhave done with normal non-reverse iterators. For example, if you try to sort a iterator range in ascending order, you will end up with thereverse\nelements in order!descending\n1\nstd::vector<int32_t> u = {4, 2, 7, 8, 3, 6};\n2\nstd::vector<int32_t> v = {4, 2, 7, 8, 3, 6};\n3\n4\nstd::sort(u.begin(), u.end());\n// order of elements is 2 3 4 6 7 8\n5\nstd::sort(v.rbegin(), v.rend());\n// order of elements is 8 7 6 4 3 2\n6\n7\nstd::vector<int32_t> a = {4, 2, 7, 8, 3, 6};\n8\nstd::vector<int32_t> b = {4, 2, 7, 8, 3, 6};\n9\n10\nstd::sort(a.begin() + 2, a.begin() + 5);\n// order of elements is 4 2 3 7 8 6\n11\nstd::sort(b.rbegin() + 1, b.rbegin() + 4);\n// order of elements is 4 2 8 7 3 6\nstd::sort()The time complexity of given 𝑛elements is (the reason why will be covered in chapter 14).Θ(𝑛log(𝑛))", "word_count": 774, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fdf6e846-9d31-588d-ab56-a08dbef82ebf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 353, "real_page_number": null, "text": "11.9 The Algorithm Library\n341\nstd::partial_sort()Another sorting algorithm that is provided by the algorithm library is the function:\ntemplate <typename typenameRandomAccessIterator, Compare>\nvoid std::partial_sort(RandomAccessIterator first, RandomAccessIterator middle,\nRandomAccessIterator last, Compare comp);\n[first, last) [first, middle)Modifies the order of elements in the range so that all elements in the range are correct had\n[first, last) comp,the entire range from been sorted. Without the optional comparator elements in the range are sorted using\noperator< (i.e., ascending order).\nstd::partial_sort()The operation can be helpful if the information you need does not require an entire container to be sorted (e.g., if\nyou wanted to identify the three smallest values in a vector of a million elements).\n1\nstd::vector<int32_t> vec = {4, 2, 7, 5, 3, 6};\n2\n3\n// ensures smallest three elements are in the correct position\n4\nstd::partial_sort(vec.begin(), vec.begin() + 3, vec.end());\n5\n// ordering of elements: 2 3 4 7 5 6 (only 2 3 4 are guaranteed to be in correct\n6\n// position, other elements can be in an arbitrary order)\nstd::stable_sort()An additional sorting algorithm is the algorithm, which ensures that the relative order of equivalent elements is\npreserved. We will discuss stable sorts in more detail in chapter 14.\ntemplate <typename typenameRandomAccessIterator, Compare>\nvoid std::stable_sort(RandomAccessIterator first, RandomAccessIterator last, Compare comp);\n[first, last) compSorts the elements in the range in ascending order, or using the comparator if specified. If two elements are\na==b, a b aequivalent, their relative ordering is preserved after the sort completes (i.e., if and came before prior to sorting, will still come\nbbefore after sorting).\n¸ 11.9.4\nFind Algorithms\nThe algorithm library also provides several useful functions that can be used to find a specific element within an iterator range. The first of these\nstd::find() [first, last) val.is the function, which returns an iterator to the first element in the range that has a value equal to If no\nlast. operator== val:such value is found, the function returns the iterator This function uses to compare elements in the range with\ntemplate <typename typenameInputIterator, T>\nconstInputIterator std::find(InputIterator first, InputIterator last, T& val);\n[first, last) val. lastReturns an iterator to the first element in the range equal to If no match is found, is returned.\nstd::find()Because this algorithm compares all elements in the given range for a match, the complexity of the operation is linear in the\nfirst last.distance between and\nstd::find(), std::find_if()In addition to you can use the function to find an element in an iterator range that satisfies some\nstd::find_if()predefined condition. The function takes in two iterators and a unary predicate, and it returns an iterator to the first element\n[first, last) true. last.in the range for which the unary predicate returns If no such element is found, the function returns the iterator\ntemplate <typename typenameInputIterator, UnaryPredicate>\nInputIterator std::find_if(InputIterator first, InputIterator last, UnaryPredicate pred);\n[first, last) pred.Returns an iterator to the first element in the range that satisfies If no such element is found, the function returns\nlast.the iterator\nfind()Examples using algorithms are shown below:\n1\nstruct IsOddPred {\n2\nbool operator() (int32_t i) {\n3\nreturn i % 2 == 1;\n4\n} // operator()()\n5\n};\n6\n7\nint main() {\n8\nstd::vector<int32_t> vec = {24, 66, 12, 74, 26, 43, 92, 95, 92, 71};\n9\nauto it1 = std::find(vec.begin(), vec.end(), 92);\n10\nauto it2 = std::find(vec.begin(), vec.end(), 93);\n11\nauto it3 = std::find_if(vec.begin(), vec.end(), IsOddPred());\n12\n} // main()\nit1 92 43 95), it2 vec.end() 93In this case, points to the first in the vector (the one between and points to since cannot be found in the\nit3 43 43 IsOddPredicate truevector, and points to because is the first element for which returns (since it is the first odd number).", "word_count": 643, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e49d0459-d578-5bc1-846d-184c041cb6c1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 354, "real_page_number": null, "text": "342\nChapter 11. Iterators and the Standard Template Library\n¸ 11.9.5\n(✽)Removal Algorithms\n<algorithm>The library provides functions that can be used to remove elements in an iterator range that fit a certain condition. The\nstd::remove() [first, last) valfunction receives an iterator range and a target value to remove, and it \"removes\" all elements in the\nval.range that compare equal to\ntemplate <typename typenameForwardIterator, T>\nconstForwardIterator std::remove(ForwardIterator first, ForwardIterator last, T& val);\n[first, last) valTransforms the range into a range with all the elements that compare equal to removed, and returns an iterator to\nthe new end of the range.\nstd::remove()However, there is a catch: despite the function’s name, the function does not actually physically remove the elements from\nthe range. As mentioned in the beginning of the section, STL algorithms only receive iterators to data, and they have no power to modify\nstd::remove()the properties of the underlying container itself. As a result, works around this by moving all the elements that should be\nremoved to the back of the iterator range. After doing this, it returns an iterator to the position the last element that should not beone past\nremoved. It is your responsibility to physically remove these elements from the container — this can be done by erasing all contents from the\nThis is known as the erase-remove idiom.iterator that is returned to the end of the iterator range.\nstd::remove() .erase()Thus, every time you call (or any variant of this operation), you must also call to erase all elements starting\nstd::remove()from the iterator that returns to the end of the container:\nauto it = std::remove(vec.begin(), vec.end(), 1);\nvec.erase(it, vec.end());\nThis can be condensed to one line of code without creating an additional variable for the iterator:\nvec.erase(std::remove(vec.begin(), vec.end(), 1), vec.end());\nvec\nstd::remove(vec.begin(), vec.end(), 1);\n1\n3\n2\n1\n4\n1\n1\n3\nAlgorithms in the algorithm library cannot modify the\nstructure of the vector, so the 1’s in the container are not\nphysically removed. Instead, all positions that follow\nthe last unremoved element contain junk data; it is the\nprogrammer’s responsibility to clean up this part of the\nstd::remove()container since the function cannot.\nvec\n3\n2\n4\n3\n4\n1\n1\n3\nThis is what the container should\nlook like after the 1’s are removed.\nstd::remove() cannot modify the size\nof the vector on its own, so it returns\nan iterator to this element; it is your\nresponsibility to erase all elements\nin the range from here to the end!\nstd::remove() std::remove_if(),Another variant of is which removes elements that satisfy a given unary predicate.\ntemplate <typename typenameForwardIterator, UnaryPredicate>\nForwardIterator std::remove_if(ForwardIterator first, ForwardIterator last, UnaryPredicate pred);\n[first, last) pred trueTransforms the range into a range with all the elements for which returns removed, and returns an iterator\nto the new end of the range.\nstd::remove(), std::remove_if()Like with does not change the size of the underlying container that stores the iterator range. To\nstd::remove_if() .erase().physically eliminate the values that should be removed, should be coupled with The following code uses\nIsOddPred()the predicate defined earlier.\nauto it = std::remove_if(vec.begin(), vec.end(), IsOddPred());\nvec.erase(it, vec.end());\nThis can be condensed to one line of code without creating an additional variable for the iterator:\nvec.erase(std::remove_if(vec.begin(), vec.end(), IsOddPred()), vec.end());\nExamples of these two functions in use are shown below:\n1\nstd::vector<int32_t> v1 = {1, 3, 2, 1, 4, 1, 1, 3};\n2\n// remove all 1's from the above vector\n3\nv1.erase(std::remove(v1.begin(), v1.end(), 1), v1.end());\n4\n// contents of v1 are {3, 2, 4, 3}\n5\n6\nstd::vector<int32_t> v2 = {1, 3, 2, 1, 4, 1, 1, 3};\n7\n// remove all odd numbers from the vector using IsOddPred functor\n8\nv2.erase(std::remove_if(v2.begin(), v2.end(), IsOddPred()), v2.end());\n9\n// contents of v2 are {2, 4}", "word_count": 636, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "853f6177-d82e-5ca2-8210-f9ce971a23b6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 355, "real_page_number": null, "text": "11.9 The Algorithm Library\n343\nIf you instead wanted to make a copy of the iterator range with specific values removed (instead of modifying the original container), you could\nstd::remove_copy() std::remove_copy_if()use the and functions to write non-removed values to a new location.\ntemplate <typename typename typenameForwardIterator, OutputIterator, T>\nOutputIterator std::remove_copy(ForwardIterator first, ForwardIterator last,\nconstOutputIterator dest, T& val);\n[first, last) dest, val.Copies the elements in the range to the range beginning at except elements that compare equal to\ntemplate <typename typename typenameForwardIterator, OutputIterator, UnaryPredicate>\nOutputIterator std::remove_copy_if(ForwardIterator first, ForwardIterator last,\nOutputIterator dest, UnaryPredicate pred);\n[first, last) dest, pred true.Copies the elements in the range to the range beginning at except elements for which returns\nstd::remove_copy() std::remove(), std::remove()The operation behaves similarly to except that the result of is written to\ndest std::remove_copy_if().(leaving the original container unchanged). The same applies with Examples are shown below:\n1\nstd::vector<int32_t> v1 = {1, 3, 2, 1, 4, 1, 1, 3};\n2\n// init v2 to the same size as v1\n3\nstd::vector<int32_t> v2(v1.size());\n4\n// remove all 1's from v1 and store the result in v2; v1 unchanged\n5\nstd::remove_copy(v1.begin(), v1.end(), v2.begin(), 1);\n6\n// v2 now stores {3, 2, 4, 3, 0, 0, 0, 0}\n7\n// remove excess zeros from v2 using the standard remove function\n8\nauto it = std::remove(v2.begin(), v2.end(), 0);\n9\nv2.erase(it, v2.end());\n10\n// v2 now stores {3, 2, 4, 3}\nfirst last,All of these removal algorithms take linear time in the distance between and since each element in the given range is compared\nwith the value to remove.\nstd::erase_if(),Remark: BeginninginC++20,certaincontainersgainaccessto whichphysicallyeraseselementsfromthecontainer.\nstd::remove_if():For instance, the following removes all odd numbers from a vector, similar to the example covered earlier with\n1\nstruct IsOddPred {\n2\nbool operator() (int32_t i) {\n3\nreturn i % 2 == 1;\n4\n} // operator()()\n5\n};\n6\n7\nint main() {\n8\nstd::vector<int32_t> v = {1, 3, 2, 1, 4, 1, 1, 3};\n9\nauto num_values_erased = std::erase_if(v, IsOddPred());\n// contents of v are {2, 4}\n10\nstd::cout << num_values_erased << '\\n';\n// prints 6\n11\n} // main()\nThis is not something you will need to know, however.\n¸ 11.9.6\n(✽)Replacement Algorithms\n<algorithm>Along with these removal algorithms, the STL also provides algorithms that can be used to replace elements instead of\nremoving them. The functions are shown below:\ntemplate <typename typenameForwardIterator, T>\nvoid const conststd::replace(ForwardIterator first, ForwardIterator last, T& old_val, T& new_val);\nold_val [first, last) new_val.Replaces all instances of in the range with\ntemplate <typename typename typenameForwardIterator, UnaryPredicate, T>\nvoid conststd::replace_if(ForwardIterator first, ForwardIterator last, UnaryPredicate pred, T& new_val);\npred true [first, last) new_val.Replaces all elements for which returns in the range with\ntemplate <typename typename typenameInputIterator, OutputIterator, T>\nOutputIterator std::replace_copy(InputIterator first, InputIterator last, OutputIterator dest,\nconst constT& old_val, T& new_val);\n[first, last) dest, old_val new_val.Copies elements in the range to the range beginning at replacing all instances of with\ntemplate <typename typename typename typenameInputIterator, OutputIterator, UnaryPredicate, T>\nOutputIterator std::replace_copy_if(InputIterator first, InputIterator last, OutputIterator dest,\nconstUnaryPredicate pred, T& new_val);\n[first, last) dest, pred trueCopies the elements in the range to the range beginning at replacing all values for which returns\nnew_val.with", "word_count": 556, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b716a5b2-ac5f-5957-9a76-06f9b3af6bab", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 356, "real_page_number": null, "text": "344\nChapter 11. Iterators and the Standard Template Library\nstd::remove(), first last.Like with these replacement algorithms take linear time in the distance between and Examples using these\nfunctions are shown below:\n1\nstd::vector<int32_t> vec = {1, 3, 2, 1, 1, 7, 3, 6, 4, 5};\n2\n3\n// replace all 1's with 0's\n4\nstd::replace(vec.begin(), vec.end(), 1, 0);\n5\n// contents of vec now {0, 3, 2, 0, 0, 7, 3, 6, 4, 5}\n6\n7\n// replace all odd numbers with 8\n8\nstd::replace_if(vec.begin(), vec.end(), IsOddPred(), 8);\n9\n// contents of vec now {0, 8, 2, 0, 0, 8, 8, 6, 4, 8}\n10\n11\n// replace all 8's with 9's and store result in foo\n12\nstd::vector<int32_t> foo(vec.size());\n13\nstd::replace_copy(vec.begin(), vec.end(), foo.begin(), 8, 9);\n14\n// contents of vec now {0, 8, 2, 0, 0, 8, 8, 6, 4, 8}\n15\n// contents of foo now {0, 9, 2, 0, 0, 9, 9, 6, 4, 9}\n16\n17\n// replace all odd numbers with 4 and store result in bar\n18\nstd::vector<int32_t> bar(foo.size());\n19\nstd::replace_copy_if(foo.begin(), foo.end(), bar.begin(), IsOddPred(), 4);\n20\n// contents of vec now {0, 8, 2, 0, 0, 8, 8, 6, 4, 8}\n21\n// contents of foo now {0, 9, 2, 0, 0, 9, 9, 6, 4, 9}\n22\n// contents of bar now {0, 4, 2, 0, 0, 4, 4, 6, 4, 4}\n¸ 11.9.7\n(✽)Transformation Algorithms\nstd::transform() <algorithm>The function in the library can be used to apply an operation to every element in an iterator range.\nThe function can be invoked in two forms, which are shown below:\ntemplate <typename typename typenameInputIterator, OutputIterator, UnaryOperation>\nOutputIterator std::transform(InputIterator first1, InputIterator last1,\nOutputIterator result, UnaryOperation op);\nop [first1, last1)Calls the operation on each element in the range and stores the value returned by each operation in the range\nresult.beginning at An iterator pointing one past the last element written is returned.\ntemplate <typename typename typename typenameInputIterator1, InputIterator2, OutputIterator, BinaryOperation>\nOutputIterator std::transform(InputIterator1 first1, InputIterator1 last1, InputIterator2 first2,\nOutputIterator result, BinaryOperation binary_op);\nbinary_op [first1, last1)Calls with each element in the range as the first argument, and the corresponding element in the range\nfirst2 result.beginning at as the second argument. The value returned by each call is stored at the range beginning at An iterator\npointing one past the last element written is returned.\nstd::transform()The first variant takes in an unary operation, or an operation that takes in a single argument. The function applies this\nresult.unary operation to every element in the given range and writes the result to the range beginning at The caller of the function must\nresultmake sure that the range beginning at is large enough to hold the transformed elements (by either resizing the output range or using\nstd::transform()insert iterators). To physically overwrite the elements in the original range with the transformed data, can be called\nfirst1 resultwith and as the exact same iterator.\nA\nB\nC\nD\nOld Data\nop\nop(A) op(B) op(C) op(D)\nTransformed Data\nstd::transform(old_data.begin(), old_data.end(), new_data.begin(), op);\n::toupper()Asanexample,considerthe function,whichcanbeusedtoturnalowercaseletterintoanuppercaseone. However,thisfunction\nstd::transform()onlyoperatesononecharacteratatime. Ifyouwanttoconvertanentirestringfromlowercasetouppercase,youcanuse\nstring.4::toupper()to apply the function to every letter in the\n1\nstd::string s = \"eecs281\";\n2\nstd::transform(s.begin(), s.end(), s.begin(), ::toupper);\n3\nstd::cout << s << '\\n';\n// prints \"EECS281\"\n4If usingnamespacestd, toupper std::toupper(),youare youneedthetwocolonsbefore totelltheprogram touse whichisaseparatefunctionnot\n<locale> non-stddefinedinthe library. Instead,youwanttousethe versionprovidedintheClibrary.", "word_count": 631, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "147bd9d9-5e3e-5aa8-a024-3cd4dd06a863", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 357, "real_page_number": null, "text": "11.9 The Algorithm Library\n345\nstd::transform()The second variant of takes in a binary operation, which takes two arguments rather than one. This binary operation is\nstd::transform()applied to the contents of iterator ranges. The goes through both iterator ranges and passes in each element into thetwo\nfirst range as the first argument of the binary operation, and each element into the second range as the second argument of the binary operation.\nA\nB\nC\nD\nE\nF\nG\nH\nOld Data 1\nOld Data 2\nop\nop(A,E)\nop(B,F)\nop(C,G)\nop(D,H)\nTransformed Data\nbinary_op() v1 aFor example, the following code applies the function to both iterator ranges, where elements in are passed in as and\n52v2 b. v3 {26, 38, 52, 68, 86},elements in are passed in as The final contents of the vector (the destination vector) are since + 1 = 26,\n62 72 82 92+ 2 = 38, + 3 = 52, + 4 = 68, and + 5 = 86.\n1\nint32_t binary_op(int32_t int32_ta, b) {\n2\nreturn a a + b;*\n3\n} // binary_op()\n4\n5\nint main() {\n6\nstd::vector<int32_t> v1 = {5, 6, 7, 8, 9};\n7\nstd::vector<int32_t> v2 = {1, 2, 3, 4, 5};\n8\nstd::vector<int32_t> v3; // stores v1 v1 + v2*\n9\nstd::transform(v1.begin(), v1.end(), v2.begin(), std::back_inserter(v3), binary_op);\n10\n} // main()\n¸ 11.9.8\n(✽)Copy Algorithms\n<algorithm> std::copy()The library provides several methods that can be used to copy data between containers. The function can be\nfirst, last, dest,used to copy elements in an iterator range to another location. Given three iterators, and the function copies the elements\n[first, last) dest.in the range into the range that begins at the position of An iterator to the end of the destination range, or the element\nstd::copy()that follows the last element copied, is returned by the function.\ntemplate <typename typenameInputIterator, OutputIterator>\nOutputIterator std::copy(InputIterator first, InputIterator last, OutputIterator dest);\n[first, last) dest,Copies the elements in the range into the range beginning at returns an iterator one past the last element copied.\ndest [first,last), std::copy()Theiterator shouldnotpointtoanelementintherange sincethiscausesoverlapproblems. Becausethe\nfirst last.operation performs an assignment for each element in the range, this function is linear in the distance between and\nstd::copy() std::copy()The following code uses to copy a range of data from one container to another. Since the algorithm\naccepts iterators and operates independently of the underlying container that the data is stored in, it can be used to copy data even if the input\nand output containers are different.\n1\nstd::vector<int32_t> vec = {0, 1, 2, 3, 4, 5, 6, 7};\n2\nstd::deque<int32_t> deq(4); // deque of size 4\n3\nstd::copy(vec.begin() + 2, vec.begin() + 6, deq.begin());\n4\n// elements 2, 3, 4, and 5 are copied to the deque\n5\n// contents of deque now {2, 3, 4, 5}\nstd::copy_if(),If you only want to copy elements that fit a certain criteria, you can use which takes in a unary predicate. This function\npred true:only copies over elements for which returns\ntemplate <typename typename typenameInputIterator, OutputIterator, UnaryPredicate>\nOutputIterator std::copy_if(InputIterator first, InputIterator last,\nOutputIterator dest, UnaryPredicate pred);\n[first, last) pred true destCopies the elements in the range for which returns into the range beginning at and returns an iterator\none past the last element copied.\nstd::copy_if() dest.The function returns an iterator pointing to the position that follows the last element written to This is useful since\nit allows you to shrink a container to the correct size after a call to the function, since you may not know how many elements will be copied over.\nSince the iterator returned is an output iterator, it also allows you to continue writing data from where you left off.\n1\nstd::vector<int32_t> vec = {4, 2, 7, 5, 3, 6};\n2\n// copy over only odd numbers, init to size of vec to ensure space\n3\nstd::vector<int32_t> odds(vec.size());\n4\nauto end = std::copy_if(vec.begin(), vec.end(), odds.begin(), IsOddPred());\n5\n// contents of odds are {7, 5, 3, 0, 0, 0}, resize to correct size\n6\nodds.resize(std::distance(odds.begin(), end));\n7\n// contents of odds are {7, 5, 3}", "word_count": 710, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "00c7b4a8-a16c-5eca-b437-e99528ad657c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 358, "real_page_number": null, "text": "346\nChapter 11. Iterators and the Standard Template Library\n¸ 11.9.9\n(✽)Reversal Algorithms\nstd::reverse() [first, last)The function takes in a range and reverses the order of the elements in this range. Similarly, the\nstd::reverse_copy() destfunction does the same thing, but it copies the result to a new location beginning at instead of modifying the\ndata in the original container.\ntemplate <typename BidirectionalIterator>\nvoid std::reverse(BidirectionalIterator first, BidirectionalIterator last);\n[first, last).Reverses the order of elements in the range\ntemplate <typename typenameBidirectionalIterator, OutputIterator>\nOutputIterator std::reverse_copy(BidirectionalIterator first, BidirectionalIterator last,\nOutputIterator dest);\n[first, last) destWrites the elements in the range to the range beginning at in reverse order and returns an iterator one past the last\nelement copied.\nExample code that uses these reverse functions is shown below:\n1\nstd::vector<int32_t> vec = {1, 2, 6, 3, 8, 9, 4, 5, 7};\n2\nstd::reverse(vec.begin(), vec.end());\n3\n// contents of vec now {7, 5, 4, 9, 8, 3, 6, 2, 1}\n4\nstd::reverse(vec.begin() + 3, vec.begin() + 6);\n5\n// contents of vec now {7, 5, 4, 3, 8, 9, 6, 2, 1}\n¸ 11.9.10\nnth (✽)Element\n281stSuppose you wanted to find the smallest integer in a container of 1,000,000 integers. One way to find this number is to sort the container\n281st std::sort()and retrieve the element. However, takes time, and a lot of time is wasted on sorting the remaining 999,999Θ(𝑛log(𝑛))\nelements, which you may not even care about. Is there a way to find this element without sorting the entire container?\nstd::nth_element()The can help you with this. If you have a container that supports random access iterators, you can use\n𝑛thstd::nth_element() to find the element that should go in the position if the container were sorted.\ntemplate <typename typenameRandomAccessIterator, Compare>\nvoid std::nth_element(RandomAccessIterator first, RandomAccessIterator nth,\nRandomAccessIterator last, Compare comp);\n[first, last) nthRearranges the elements in the range so that the element pointed to by is the element that should be at that position\nif the range had been completely sorted. The comparator is optional; if you do not pass in a comparator, elements in the container are\noperator<compared using by default.\nstd::nth_element() std::sort() std::nth_element()The benefit of using instead of is that a call to takes time, whereΘ(𝑛)\nfirst last. std::nth_element()𝑛is the distance between and This function has many practical uses. For instance, you can use to\nfind the median of a collection of data values without having to sort the entire collection:\n1\nstd::vector<int32_t> vec = {14, 33, 74, 11, 25, 75, 24, 47, 85, 17, 59};\n2\nsize_t middle_idx = vec.size() / 2;\n3\nstd::nth_element(vec.begin(), vec.begin() + middle_idx, vec.end());\n4\n// now the median is guaranteed to be in the correct position of\n5\n// the vector if the vector had been fully sorted\n6\nstd::cout << vec[middle_idx] << '\\n';\n// prints 33\nThe remaining elements in the vector are left in an arbitrary order, and they are not guaranteed to be sorted. The only guarantee that you can\n𝑛thstd::nth_element()make after a call to for the element is that none of the elements preceding the position of this element are greater,\n33 33and that none of the elements following this position are smaller. In the above example, is guaranteed to be in the correct position, since\nvec.begin() + middle_idxwould have been at position if the vector were fully sorted. However, the other elements can follow any order,\nvec.begin() + middle_idx 33, vec.begin() +as long as elements to the left of are less than or equal to and elements to the right of\nmiddle_idx 33.are greater than or equal to\n¸ 11.9.11\nLower and Upper Bound Algorithms\nThe STL also provides several algorithms that can be used if the data in a container is in order. The following algorithms can besorted\nused to identify if a value exists in a sorted range of data, and if it doesn’t, it identifies where it should go to keep the data sorted. The\nstd::lower_bound() [first, last)function takes in an iterator range and returns an iterator to the first element in the range that\nval. std::upper_bound()does not compare less than The function takes in an iterator range and returns an iterator to the first element in\nval. last.the range that compares greater than If no such element fits these criteria, both functions return\ntemplate <typename typename typenameForwardIterator, T, Compare>\nForwardIterator std::lower_bound(ForwardIterator first, ForwardIterator last,\nconst T& val, Compare comp);\n[first, last) val.Returns an iterator pointing to the first element in the sorted range that does not compare less than The comparator\noperator<.is optional; without it, elements are compared using The value pointed to by the return iterator may also be equivalent to\nval, and not just greater.", "word_count": 799, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a9f594e0-0603-5c0f-af83-582450057841", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 359, "real_page_number": null, "text": "11.9 The Algorithm Library\n347\ntemplate <typename typename typenameForwardIterator, T, Compare>\nForwardIterator std::upper_bound(ForwardIterator first, ForwardIterator last,\nconst T& val, Compare comp);\n[first, last) val.Returns an iterator pointing to the first element in the sorted range that compares greater than The comparator is\noperator<. val,optional; without it, elements are compared using The value pointed to by the return iterator cannot be equivalent to\nonly greater.\nstd::lower_bound() std::upper_bound()The data passed into and must be sorted for these functions to work. If the data is not\nval [lower_bound,sorted, then the behaviors of these two functions are undefined. Also, notice that, if did exist in a container, then\nupper_bound) valwould represent an iterator range whose elements are all equal to (assuming no custom comparator is used).\nstd::lower_bound() 3 3As an example, consider the vector below. Using on the value returns an iterator to the first in the container.\nstd::upper_bound() 3 3Using onthevalue returnsaniteratortotheposition thelast inthecontainer. Thiseffectivelyconstructsonepast\n3an iterator range that provides a bound for where the element can be found.\nvec\n1\n2\n3\n3\n5\n7\n9\n9\nstd::lower_bound(vec.begin(), vec.end(), 3);\n(returns first element greater than or equal to 3)\nvec\n1\n2\n3\n3\n5\n7\n9\n9\nstd::upper_bound(vec.begin(), vec.end(), 3);\n(returns first element strictly greater than 3)\nstd::equal_range() std::lower_bound() std::upper_bound()The function combines the behavior of and to return an\nval. val std::equal_range()iterator range whose values are all equal to If is not in the iterator range, returns two iterators that point\nval, last valto the closest value greater than or if compares greater to all elements in the range.\ntemplate <typename typename typenameForwardIterator, T, Compare>\nstd::pair<ForwardIterator, ForwardIterator>\nconststd::equal_range(ForwardIterator first, ForwardIterator last, T& val, Compare comp);\n[first, last) val.Returns the bounds of the subrange that includes all the elements of the sorted range with values equivalent to The\noperator< valcomparator is optional, and values are compared with if it is not specified. If is not equivalent to any value in the range,\nthe subrange returned has a length of zero.\nstd::equal_range() val, first valWhen is called on a value it returns a pair of iterators, where represents the lower bound on and\nsecond val.represents the upper bound on Examples using these functions are shown below:\n1\nstd::vector<int32_t> vec = {1, 2, 5, 5, 6, 9};\n2\n3\n// it1 points to first 5\n4\nauto it1 = std::lower_bound(vec.begin(), vec.end(), 5);\n5\n6\n// it2 points to 6\n7\nauto it2 = std::upper_bound(vec.begin(), vec.end(), 5);\n8\n9\n// it3 points to 9\n10\nauto it3 = std::lower_bound(vec.begin(), vec.end(), 7);\n11\n12\n// it4 points to 9\n13\nauto it4 = std::upper_bound(vec.begin(), vec.end(), 7);\n14\n15\n// it5 points to vec.end()\n16\nauto it5 = std::lower_bound(vec.begin(), vec.end(), 10);\n17\n18\n// it6 points to vec.end()\n19\nauto it6 = std::upper_bound(vec.begin(), vec.end(), 10);\n20\n21\n// it_pair1.first points to first 5\n22\n// it_pair1.second points to 6\n23\nauto it_pair1 = std::equal_range(vec.begin(), vec.end(), 5);\n24\n25\n// it_pair2.first points to 1\n26\n// it_pair2.second points to 1\n27\nauto it_pair2 = std::equal_range(vec.begin(), vec.end(), 0);", "word_count": 531, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6740a68d-7692-581a-bea8-1daa38a081c8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 360, "real_page_number": null, "text": "348\nChapter 11. Iterators and the Standard Template Library\n¸ 11.9.12\nOther Algorithms in the STL\nThere are a few important algorithms that were not covered in this section, but will be covered in a later chapter. These include the set algorithms\nset_union(), set_intersection(), set_difference(), set_symmetric_difference(),of and which will be covered\nequal() unique(),in chapter 13, as well as the sequence operations of and which will be covered in chapter 16. To learn more about these\nmethods, you should consult their corresponding sections in the chapters listed above.\nAdditionally, there are a few algorithms that will not be covered in these notes at all. This does not mean that they are not important,\n<algorithm>however. You are encouraged to familiarize yourself with everything that the library provides (as well as other libraries) —\nthere are a lot of resources for this, such as cppreference.com, that provide the most up-to-date features of the language.\n11.10\nLambdas (✽)\n¸ 11.10.1\n(✽)Anatomy of a Lambda\nIsOddPred()Recall the function object we created to find the first odd number in a container:\n1\nstruct IsOddPred {\n2\nbool operator() (int32_t i) {\n3\nreturn i % 2 == 1;\n4\n} // operator()()\n5\n};\n6\n7\nint main() {\n8\nstd::vector<int32_t> vec = {24, 66, 12, 74, 26, 43, 92, 95, 92, 71};\n9\nauto it = std::find_if(vec.begin(), vec.end(), IsOddPred());\n10\n} // main()\nIsOddPred.Here, we defined a function object and gave it the name However, if this function object is only used once, it is a bit excessive to\nstructdefine a separate and implement a special-case function object with its own name just to use it once and never touch it again. It would\nstd::find_if()be much better if we could implement the function object \"on the fly\" when it is actually used, within the function.\nThe lambda is a construct introduced in C++11 to address this issue. Lambdas can be used to construct a short-lived, anonymous function\nIsOddPred()object place,directlyinthecodewhereitiscalled. Forexample,thecodebelowreimplementsthecodeabove,butdefinesthein\npredicate in place using a lambda, directly where we want to use it:\n1\nint main() {\n2\nstd::vector<int32_t> vec = {24, 66, 12, 74, 26, 43, 92, 95, 92, 71};\n3\nauto [](int32_tit = std::find_if(vec.begin(), vec.end(), i) {\n4\nreturn i % 2 == 1;\n5\n});\n6\n} // main()\nHow do you declare a lambda? A lambda expression consists of the following:\n[capture clause] (parameter list) {function body}\nA lambda always begins with a pair of square brackets, known as the clause. Following the square brackets is a pair of parentheses thatcapture\nholds the list. This component stores the arguments that would normally be passed into the function.parameter\nint n)For instance, the following function checks if a number is odd. Here, the arguments of the function (i.e., would go in the parameter\nlist of the lambda.\n1\nbool is_odd(int32_t i) {\n2\nreturn i % 2 == 1;\n3\n} // is_odd()\nFinally, the behavior of the function object should be defined in the function body, between the curly braces. Putting it all together, we can\nconvert the above function into a lambda by replacing the function name and return type with square brackets:\n1\n[](int32_t i) {\n2\nreturn i % 2 == 1;\n3\n}\nstd::find_if()We can now pass this lambda as a function object into STL algorithms. This is exactly what we did with above; instead of\nstd::find_if().defining a separate function object elsewhere, we just implemented the function we wanted \"on demand\" when we called\nThis makes using STL algorithms a lot easier.", "word_count": 619, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bbe6fe22-02b2-5759-9950-1ca71ce8651b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 361, "real_page_number": null, "text": "11.10 Lambdas\n349\nRemark: Starting in C++20, lambdas can be templated, with the template parameter coming between the capture clause and parameter list.\n[capture clause] <template\nparameter> (parameter list) {function body}\nAs an example, the following templated function adds two values of the same type.\n1\ntemplate <typename T>\n2\nT add(T first, T second) {\n3\nreturn first + second;\n4\n} // add()\nConverting this to a templated lambda gives us the following:\n1\n[]<typename returnT>(T first, T second) { first + second; }\nBecause a lambda behaves similarly to a function object, you can pass it an argument as if the expression\n[](int returnn){ i % 2 == 1; }\nwere the name of the function object itself:\n1\n[](int32_t returnstd::cout << i){ i % 2 == 1; }(5) << '\\n'; // prints 1 (true), 5 is odd\n2\n[](int32_t returnstd::cout << i){ i % 2 == 1; }(6) << '\\n'; // prints 0 (false), 6 is even\nYou can also store the lambda as a variable and use it as a function object:\n1\nauto [](int32_t returnis_odd_pred = i){ i % 2 == 1; };\n2\nstd::cout << is_odd_pred(5) << '\\n';\n// prints 1 (true), 5 is odd\n3\nstd::cout << is_odd_pred(6) << '\\n';\n// prints 0 (false), 6 is even\nRemark: What is the type of a lambda? It turns out that the type of a lambda is unspecified, and each instance of a lambda introduces its\nautoown unique type. This type is unnamed and cannot be explicitly written out, so you should use to deduce the type of a lambda for you.\nstd::function<>If you want to pass a lambda into a function, you can either use a templated function or convert the lambda to a object\nstd::function<>(which we will discuss later). The templated function, however, is preferred, since is a wrapper that adds a level of\nindirection and has a bit more overhead, and its type will also need to be changed if the return value or arguments of a lambda are changed.\nMuch like a normal function, a lambda does not need to accept any parameters. Consider the following function:\n1\nvoid print_hello_world() {\n2\nstd::cout << \"hello world\" << '\\n';\n3\n} // print_hello_world()\nBy removing the function name and adding square brackets, we can convert this into a lambda as follows. This lambda emulates a function\n\"hello world\"object that prints out whenever it is invoked.\n1\n[]() {\n2\nstd::cout << \"hello world\" << '\\n';\n3\n}\nLambdas can automatically deduce the return type of its function body in most cases, so there is no need to define the return type of a lambda\ndefinition in most cases. However, if you want to specify a return type that is different from the type that would normally be returned (e.g., using\na conversion), you can use the arrow operator as follows:\n[capture clause] (parameter list) -> return_type {function body}\ndouble:For example, the following lambda returns a\n1\n[](double doublex, y) {\n2\nreturn x + y;\n3\n}\nIf you want this lambda to return an integer instead, you would have to specify it as the return type:\n1\n[](double double int32_tx, y) -> {\n2\nreturn x + y;\n3\n}\nx = 1.23 y = 2.46, 3.69:If we called our original lambda (without the return type) with and we would get a return value of\n[](double double returnstd::cout << x, y){ x + y; }(1.23, 2.46) << '\\n';\n3, 3.69On the other hand, if we specify the return type as an integer, the return value would be since would get truncated during conversion:\n[](double double int32_t returnstd::cout << x, y) -> { x + y; }(1.23, 2.46) << '\\n';", "word_count": 635, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dcd95433-7625-5c36-b0ca-2975da80cdfd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 362, "real_page_number": null, "text": "350\nChapter 11. Iterators and the Standard Template Library\nAnother feature of lambdas is capture. This feature gives lambdas the ability to capture and use variables that are in the surroundingvariable\nscope at the time of the lambda’s creation. To capture a variable, place it within the square brackets (the clause), as shown in thecapture\nexample below. A variable that is captured by the lambda may be used within its function body.\n1\nint main() {\n2\nstd::vector<int32_t> vec = {24, 66, 12, 74, 26, 43, 92, 95, 92, 71};\n3\nint32_t factor;\n4\nstd::cin >> factor;\n5\nstd::cout << \"The first number that is a multiple of \" << factor << \" is \"\n6\n[factor](int32_t return<< *std::find_if(vec.begin(), vec.end(), i) { i % factor == 0; });\n7\n} // main()\nvariable.5factor factorIn this code, the lambda captures and stores a read-only copy of the Even though the variable was not declared in\nthe body of the lambda, we are still allowed to use it if we capture the variable in our capture clause.\nIt is important to note that the lambda grabs and saves a of a variable when it is captured. If the original variable is modified outside thecopy\nvar var 281.lambda, the copy stored inside the lambda does not change. In the following example, the lambda grabs a copy of when equals\nvar var 281.Because the lambda saved an internal copy, even if is modified outside the lambda, the value of within the lambda would still be\n1\nint32_t var = 281;\n2\nauto func = [var]() { std::cout << var << '\\n'; };\n// function object that prints var\n3\n4\n// the lambda grabbed var and made a copy; changing var now does NOT\n5\n// change the value of var for the lambda\n6\n++var;\n// var is now 282\n7\nfunc();\n// this prints out 281 since the lambda's copy of var is still equal to 281\nampersand:6If you want the lambda to capture a to a variable rather than a copy, you must specify it in the capture clause by using anreference\n1\nint32_t var = 281;\n2\nauto func = [&var]() { std::cout << var << '\\n'; };\n// function object that prints var\n3\n4\n// the lambda grabbed var as a reference, so changing var outside the\n5\n// lambda also changes the value of var inside the lambda\n6\n++var;\n// var is now 282\n7\nfunc(); // this prints out 282 since the lambda's stores a reference to var\nThis also works the other way around: changes made to a reference in the lambda are reflected in the outside code:\n1\nint32_t var = 281;\n2\nauto func = [&var]() { var += 10; };\n// adds 10 to var\n3\nfunc();\n// lambda runs, var becomes 291\n4\nstd::cout << var << '\\n';\n// prints 291\nThere are several ways to capture variables in a capture clause. Several of these options are shown below:\n[]• captures no variables\n[=]• captures all local variables used in the lambda by value (i.e., a copy is made)\n[&]• captures all local variables used in the lambda by reference\n[foo] foo• captures only the variable by making a copy\n[&foo] foo,• captures only the variable but by reference\n[foo, bar] foo bar,• captures only the variables and both by value\n[=, &foo] foo,• captures all local variables used in the lambda by value, except which is captured by reference\n[&, foo] foo,• captures all local variables used in the lambda by reference, except which is captured by value\n[this]• captures a reference to the current object — this is useful if you are trying to declare a lambda within an object’s member\nfunction, and the lambda needs access to that object’s member variables\n[=] [&])Capturing everything by value or reference (i.e., and is generally bad practice and should be avoided, unless you actually want to\ncapture everything in the surrounding scope. It is better style to explicitly select the variables that you want to capture when defining a lambda.\n5To mutable []()mutable{}).modifyvariablescapturedbyvalue,addthe keyworddirectlyaftertheparameterlist(i.e.,\n6Ifyoucapturevariables by reference, youmustmakesurethatthevariablesyoucapturedonot gooutofscopebeforeyouuse thelambda. Otherwise,youmay\ngetundefinedbehavior!", "word_count": 752, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3031224f-a93b-538d-b171-a5617dbe71d9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 363, "real_page_number": null, "text": "11.10 Lambdas\n351\n¸ 11.10.2\n(✽)Lambdas and the STL\nstd::for_each()LambdascanbequitepowerfulwhencombinedwiththeSTL.Oneparticularalgorithmthatworkswellwithlambdasisthe\n<algorithm> for_each()function in the library, which applies a function to every element in an iterator range. Unlike a range-for loop,\nmakes it easy to apply an operation over a range of data rather than an entire container.\ntemplate <typename typenameInputIterator, UnaryFunctor>\nUnaryFunctor std::for_each(InputIterator first, InputIterator last, UnaryFunctor fn);\nfn [first, last) fnApplies the function to each element in the range and returns after completion.\nFor example, the following code adds 281 to every element in an iterator range:\n1\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5, 6, 7, 8};\n2\n[](int32_t&std::for_each(vec.begin(), vec.end(), i) { i += 281; });\n3\n// vec now {282, 283, 284, 285, 286, 287, 288, 289}\nstd::for_each(),When you run every element in the provided iterator range is passed in as an argument to the lambda. Thus, if you want\nto modify the elements in the range, make sure to pass the variable in the parameter list by reference:\n[](int32_t& i) { i += 281; }\nA few more examples are shown below. The following code sorts a vector of students by age and prints out their names in sorted order. Using\nstd::sort()lambdas, we can define the comparator in the function call instead of in a separate comparator object.\n1\nstruct Student {\n2\nstd::string name;\n3\nint32_t age;\n4\n};\n5\n6\nstd::vector<Student> vec = { {\"Alice\", 21}, {\"Bob\",\n20}, {\"Cathy\", 16},\n7\n{\"Drew\",\n19}, {\"Erin\", 22}, {\"Frank\", 17} };\n8\n9\n[](const conststd::sort(vec.begin(), vec.end(), Student& lhs, Student& rhs) {\n10\nreturn lhs.age < rhs.age;\n11\n});\n12\n13\n[](conststd::for_each(vec.begin(), vec.end(), Student& s) {\n14\nstd::cout << s.name << ' ';\n15\n});\nThe output for the above code is:\nCathy Frank Drew Bob Alice Erin\nnums n numsThe following code function takes in a vector and an integer and returns a separate container with all the numbers in that are\nn. resultmultiples of Because we passed a back-insert iterator for (which pushes elements to the back of the container), we do not have to\nresultexplicitly allocate space for the vector on line 2.\n1\nstd::vector<int32_t> multiples_of_n(std::vector<int32_t>& int32_tnums, n) {\n2\nstd::vector<int32_t> result;\n3\n[n](int32_tstd::copy_if(nums.begin(), nums.end(), std::back_inserter(result), i) {\n4\nreturn i % n == 0;\n5\n});\n6\nreturn result;\n7\n} // multiples_of_n()\n8\n9\nint main() {\n10\nstd::vector<int32_t> nums = {6489, 2374, 3822, 1238, 7684, 5999, 6548, 2947};\n11\nstd::vector<int32_t> mult_7 = multiples_of_n(nums, 7);\n12\n[](int32_tstd::for_each(mult_7.begin(), mult_7.end(), i) {\n13\nstd::cout << i << \" = 7 \" << (i / 7) << '\\n';*\n14\n});\n15\n} // main()\nThe output for the above code is:\n6489 = 7 927*\n3822 = 7 546*\n5999 = 7 857*\n2947 = 7 421*", "word_count": 492, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0cf96e3d-5b1c-5149-b309-5c4757fa5ca6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 364, "real_page_number": null, "text": "352\nChapter 11. Iterators and the Standard Template Library\nLastly, we can rewrite the following example using lambdas. This example was first introduced in section 11.9.7:\n1\nint32_t binary_op(int32_t int32_ta, b) {\n2\nreturn a a + b;*\n3\n} // binary_op()\n4\n5\nint main() {\n6\nstd::vector<int32_t> v1 = {5, 6, 7, 8, 9};\n7\nstd::vector<int32_t> v2 = {1, 2, 3, 4, 5};\n8\nstd::vector<int32_t> v3; // stores v1 v1 + v2*\n9\nstd::transform(v1.begin(), v1.end(), v2.begin(), std::back_inserter(v3), binary_op);\n10\n} // main()\nbinary_op() std::transform()Redefining the function as a lambda within the function gives us the following:\n1\nmain() {\n2\nstd::vector<int32_t> v1 = {5, 6, 7, 8, 9};\n3\nstd::vector<int32_t> v2 = {1, 2, 3, 4, 5};\n4\nstd::vector<int32_t> v3; // stores v1 v1 + v2*\n5\nstd::transform(v1.begin(), v1.end(), v2.begin(), std::back_inserter(v3),\n6\n[](int int returna, b) { a a + b; });*\n7\n} // main()\nThis code does the exact same thing as before.\n¸ 11.10.3\n(✽)Generic Lambdas\nautoC++14 introduced generic lambdas, which can take in arguments of arbitrary types. Generic lambdas can be specified by using within\nthe parameter list, as shown by the example below:\nauto [](auto auto returnadd_two_values = x, y) { x + y; };\nBehind the scenes, the compiler would essentially create an anonymous templated function object with the following behavior:\n1\nstruct /* _anonymous_lambda_type_ */ {\n2\n/* _anonymous_lambda_type_ constructor only callable by compiler */\n3\ntemplate <typename typenameT, U>\n4\nauto operator()(T& const returnx, U& y) { x + y; }\n5\n};\nadd_two_valuesSome examples using the lambda are shown below:\n1\nauto [](auto auto returnadd_two_values = x, y) { x + y; };\n2\n3\nint32_t v1 = 1;\n4\nint32_t v2 = 2;\n5\ndouble v3 = 1.2;\n6\ndouble v4 = 2.5;\n7\nstd::string v5 = \"eecs\";\n8\nstd::string v6 = \"281\";\n9\n10\nstd::cout << add_two_values(v1, v2) << '\\n';\n// prints 3\n11\nstd::cout << add_two_values(v1, v4) << '\\n';\n// prints 3.5\n12\nstd::cout << add_two_values(v3, v4) << '\\n';\n// prints 3.7\n13\nstd::cout << add_two_values(v5, v6) << '\\n';\n// prints \"eecs281\"\nNot all the parameters in a generic lambda need to be generic: it is perfectly fine to define a generic lambda with certain parameters defined with\nauto int, double,and other parameters defined with a fixed type (such as etc.).\n11.11\nThe Functional Library (✽)\n¸ 11.11.1\n(✽)std::function\n<functional> std::function<>In C++11, the library introduced the class, which serves as a general-purpose polymorphic function\nwrapper that can be used to store any callable object, ranging from explicitly defined functions and function objects to anonymous lambdas. The\nstd::function<>declaration for a is defined as follows:\ntemplate <class class...R, Args>\nclass function<R(Args...)>\nR ArgsHere represents the type of the return value, while represents the types of the given parameters. In other words, the declaration syntax\nstd::function<> std::function<return_type(parameter_types)>for a is for the given return and function parameter\ntypes. For example, consider the following function definition, which takes in a constant string reference and an integer and returns a Boolean:\nbool func(const int32_tstd::string& str, val);\nstd::function<bool(const int32_t)>.std::function<> std::string&,The corresponding type would therefore be\nstd::function<bool(const int32_t)>std::string&, my_func_object = func;", "word_count": 542, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "52cdb5a8-a89a-5562-afee-0d5e375d5731", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 365, "real_page_number": null, "text": "11.11 The Functional Library\n353\nstd::function<>As long as a function object can be called with the specified return type and parameter types, it can be assigned to a that\nis defined using those types. For instance, we can also assign a lambda that returns a Boolean and takes in a constant string reference and an\nstd::function<>integer into the type we introduced above:\nstd::function<bool(const int32_t)>std::string&, my_func_object =\n(const int32_t return[] std::string& str, val) { str == \"eecs\" && val == 281; };\nstd::function<> std::string constTechnically, we can also assign this lambda to a that accepts its first argument as instead of\nstd::string&. The following assignment also compiles.\nstd::function<bool(std::string, int32_t)> my_func_object =\n(const int32_t return[] std::string& str, val) { str == \"eecs\" && val == 281; };\nstd::stringThis is okay because the lambda can still be called with a as its first parameter. If we instead tried to pass a function object\nstd::string& std::function<> const std::string&that accepts a into a that is declared with a as its first parameter, then the\nassignment would no longer compile:\n// DOES NOT COMPILE\nstd::function<bool(const int32_t)>std::string&, my_func_object =\nint32_t return[] (std::string& str, val) { str == \"eecs\" && val == 281; };\nstd::string&, const std::string& std::string&This is because the lambda expects a but you cannot pass in a into a parameter.\nstd::function<>Because can encapsulate any callable that matches its return and parameter types, it gives you flexibility to work\n\"EECS 281 is fun!\"):with different callable objects even if they have different types. An example is shown in the code below (this prints out\n1\n// regular function\n2\nvoid foo() {\n3\nstd::cout << \"EECS \";\n4\n} // foo()\n5\n6\n// regular function that will be bound (we will cover this later)\n7\nvoid bar(int32_t best_class) {\n8\nstd::cout << best_class << \" \";\n9\n} // bar()\n10\n11\n// functor\n12\nstruct Baz {\n13\n/* can store internal state */\n14\nvoid operator()() {\n15\nstd::cout << \"is \";\n16\n} // operator()()\n17\n};\n18\n19\nint main() {\n20\n// put all callable objects into a container of std::function<void()>\n21\nstd::vector<std::function<void()>> callables;\n22\ncallables.push_back(foo);\n23\ncallables.push_back(std::bind(bar, 281));\n// binds 'best_class' to a value of 281\n24\ncallables.push_back(Baz());\n25\ncallables.push_back([]() { std::cout << \"fun!\\n\"; });\n// pushes back a lambda\n26\n27\n// iterate over all the std::function<void()> objects and invoke each of them\n28\nfor (auto& func : callables) {\n29\nfunc();\n30\n} // for func\n31\n} // main()\nstd::function<> Foocan also be used to store a callable object as internal state within a custom object. On line 9, an instance of is\nfunc,constructed using a lambda is passed into its constructor. This lambda is stored internally in the class as the member variable and it is\nFoo::invoke()invoked via the function call to on line 10.\n1\nclass Foo {\n2\nstd::function<int32_t(int32_t, int32_t)> func;\n3\npublic:\n4\nexplicit Foo(const std::function<int32_t(int32_t, int32_t)>& func_in) : func{func_in} {}\n5\nint32_t invoke(int32_t int32_t returnx, y) { func(x, y); }\n6\n};\n7\n8\nint32_t subtract(int32_t int32_tx, y) {\n9\nreturn x - y;\n10\n} // subtract()\n11\n12\nint main() {\n13\nf1{[](int32_t int32_t returnFoo x, y) { x + y; }};\n14\nstd::cout << f1.invoke(280, 1) << '\\n';\n// prints 281\n15\nFoo f2{subtract};\n16\nstd::cout << f2.invoke(280, 1) << '\\n';\n// prints 279\n17\n} // main()", "word_count": 578, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "103b6fc0-7e5b-5221-9bc1-8efabedf6fee", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 366, "real_page_number": null, "text": "354\nChapter 11. Iterators and the Standard Template Library\nstd::function<>With all that being said, should only be used in cases where you need to use it (such as to encapsulate multiple\ncallable objects that may each have different types on their own, such as in the examples above). This is because the inner workings of a\nstd::function<> is quite complicated, involving inheritance, virtual functions, and dynamic memory allocation. Thus, if there is a simpler\nstd:function<>,way to represent a callable object, and you do not need the functionality of it is likely a more efficient choice.\n¸ 11.11.2\n(✽)Binding Function Arguments\nstd::bind() <functional>The method in the library can be used to \"bind\" parameters of a function to certain argument values. This\nmethod creates a function object that stores the original callable object with its bound arguments and provides a function call operator (i.e.,\noperator())overloaded that takes in any remaining arguments (if there are any that have not been bound) and calls the original callable with\nthese additional values, along with the values of the other parameters that have been previously bound.\nstd::bind(callable object, arguments to bind);\nFor instance, consider the following function, which prints out a given number.\n1\nvoid print(int32_t num) {\n2\nstd::cout << num << '\\n';\n3\n} // print()\nstd::bind() print() numWe can use to construct another function object that behaves just like but with the value of bound to 281:\n1\nvoid print(int32_t num) {\n2\nstd::cout << num << '\\n';\n3\n} // print()\n4\n5\nint main() {\n6\n// func is a function object that runs print() with its first argument bound to a value of 281\n7\nauto func = std::bind(print, 281);\n8\nfunc();\n// prints 281\n9\n} // main()\nstd::bind()std::bind() makes a copy of the arguments that are passed in, so if you want the function object returned by to track\nstd::ref().changes to a bound parameter, you would need to pass that parameter within a standard reference wrapper using By doing so,\nstd::bind()we allow the function object returned by to internally store a reference to the value that is bound.\n1\nvoid print(int32_t num) {\n2\nstd::cout << num << '\\n';\n3\n} // print()\n4\n5\nint main() {\n6\nint32_t val = 280;\n7\nauto func1 = std::bind(print, val);\n8\nauto func2 = std::bind(print, std::ref(val));\n// reference wrapper\n9\nfunc1();\n// prints 280\n10\nfunc2();\n// prints 280\n11\n12\n++val;\n// val becomes 281\n13\nfunc1();\n// still prints 280, since func1 stored a copy of val\n14\nfunc2();\n// prints 281, since func2 took in val as a reference wrapper\n15\n} // main()\nYou can also bind certain arguments in a function while also allowing other arguments to be provided by the user. This can be done using\nstd::placeholders. _1, _2, _3,special placeholders defined in These placeholders have the special symbols and so on. The number of\nthe placeholder identifies which argument of a bound function should be filled with each value in a function call. An example is shown below:\n1\nvoid print(const int32_tstd::string& department, num) {\n2\nstd::cout << department << \" \" << num << '\\n';\n3\n} // print()\n4\n5\nint main() {\n6\nstd::string department = \"EECS\";\n7\n// bind print()'s department argument to the value \"EECS\"\n8\nauto func = std::bind(print, department, std::placeholders::_1);\n9\n// first argument to function call (281) is passed in place of std::placeholders::_1\n10\nfunc(281);\n// prints \"EECS 281\"\n11\n} // main()\nThese placeholders also allow you to do other things like reorder the parameters of a function. The following creates a function object that\nprint(),behaves like the original but takes in the first parameter as if it were the second, and the second parameter as if it were the first.\n1\nvoid print(const int32_tstd::string& department, num) {\n2\nstd::cout << department << \" \" << num << '\\n';\n3\n} // print()\n4\n5\nint main() {\n6\n// swap the order of the arguments, so func takes in the num first, then the department string\n7\nauto func = std::bind(print, std::placeholders::_2, std::placeholders::_1);\n8\nfunc(281, \"EECS\");\n// prints \"EECS 281\"\n9\n} // main()", "word_count": 705, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "69cc70ad-7a71-5c20-997b-11441f8295c7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 367, "real_page_number": null, "text": "11.11 The Functional Library\n355\nstd::bind()Because returns a function object, it can also be passed into standard library algorithms that accept them. The following prints\nprint() \"EECS\":out all the classes in the vector, with the first parameter of the function bound to a value of\n1\nvoid print(const int32_tstd::string& department, num) {\n2\nstd::cout << department << \" \" << num << '\\n';\n3\n} // print()\n4\n5\nint main() {\n6\nstd::vector<int32_t> classes = {183, 203, 280, 281, 370, 376};\n7\nstd::for_each(classes.begin(), classes.end(), std::bind(print, \"EECS\", std::placeholders::_1));\n8\n} // main()\nThe output of this code is:\nEECS 183\nEECS 203\nEECS 280\nEECS 281\nEECS 370\nEECS 376\nstd::bind()The method is a useful tool for partial function application: you can use it to fix arguments to a function to have certain values.\nstd::bind()However, in C++14 and beyond, there are essentially no use cases where should be the method of choice — this is thanks\nstd::bind()to the power of lambdas. Instead of using to create a function object with fixed arguments and/or placeholders, we can use\nstd::bind()lambdas to do the same thing in a cleaner, more expressive manner. While was able to do things that lambdas could not in\nversions before C++14, these deficiencies have been addressed in later C++ versions.\nstd::bind() print() \"EECS\".Consider our example that fixed the first argument of the method to have a value of We can use a\nlambda to accomplish the same thing:\n1\nvoid print(const int32_tstd::string& department, num) {\n2\nstd::cout << department << \" \" << num << '\\n';\n3\n} // print()\n4\n5\nint main() {\n6\nstd::string department = \"EECS\";\n7\nauto [&department](int32_tfunc = num) { print(department, num); };\n8\nfunc(281);\n// prints \"EECS 281\"\n9\n} // main()\nThe same goes with the example where we swapped the order of parameters using placeholders:\n1\nvoid print(const int32_tstd::string& department, num) {\n2\nstd::cout << department << \" \" << num << '\\n';\n3\n} // print()\n4\n5\nint main() {\n6\nauto [](int32_t constfunc = num, std::string& department) { print(department, num); };\n7\nfunc(281, \"EECS\");\n// prints \"EECS 281\"\n8\n} // main()\nstd::bind()So, why do we need to talk about at all? Even though its functionality can fully emulated with lambdas in newer versions of\nC++, it is still something that you will likely see if you have to work with a large, older codebase. However, if you ever find yourself in the\nstd::bind().position where you need to bind arguments in a function, you should consider using a lambda as your go-to method instead of\n¸ 11.11.3\n(✽)Callback Functions\nAcallbackfunctionisafunctionthatispassedintoanotherfunctionasanargument,withtheintentionthatitwillbeexecutedeitherimmediately\nor after the other function has completed a task. Callback functions are typically used in event-driven systems, where program execution and\nbehavior is determined by events (such as a user clicking a button, data being received from a service, etc.). In C++11 and beyond, callbacks\nstd::function<>can be implemented and passed around using and lambdas.", "word_count": 528, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4c84ed20-e0f1-5678-b028-1c228cdf9f5f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 368, "real_page_number": null, "text": "356\nChapter 11. Iterators and the Standard Template Library\nTo demonstrate, we will consider a high-level example that uses a callback function. Suppose you are implementing a component that processes\ndata updates that are streamed from a service. When you get a message back from the service, you want to immediately execute a function that\nprocesses and publishes this data. To do this, you can pass the function as a callback to the client that is subscribed to the service (in this case,\nDataRetrievalClient),the and then execute the function as soon as an update is received. This is shown in the example code below:\n1\n// this class is in charge of processing the data; it stores a DataRetrievalClient\n2\n// that directly subscribes to the service that the data is streamed from\n3\nclass DataProcessor {\n4\nprivate:\n5\nDataRetrievalClient* client;\n6\nLogger* logger;\n7\n/* ... other member variables ... */\n8\npublic:\n9\n// this method should be executed whenever the client receives a data update\n10\nvoid process_and_publish_update(const DataUpdate& update) {\n11\n// does work to process the data update message\n12\n} // process_and_publish_update()\n13\n14\n// this method makes a subscription request to the DataRetrievalClient\n15\nsubscribe(constSubscriptionToken DataRequest& request) {\n16\nauto [this](consttoken = client->subscribe_for_updates(request, DataUpdate& update) {\n17\nlogger->log_message(\"Received data update to process and publish.\", update);\n18\nthis->process_and_publish_update(update);\n19\n});\n20\nreturn token;\n21\n} // subscribe()\n22\n};\n23\n24\n// this class directly subscribes to the service and gets its data\n25\nclass DataRetrievalClient {\n26\nprivate:\n27\nTcpCommunicator* comm;\n// sends and receives data via the network\n28\n/* ... other member variables ... */\n29\npublic:\n30\nsubscribe_for_updates(constSubscriptionToken DataRequest& request,\n31\nconst std::function<void(const DataUpdate&)>& callback) {\n32\n/* ... do work ... */\n33\n// subscribes to another service and gets data that needs to be processed\n34\nDataUpdate update = get_and_parse_update(...);\n35\n// invoke callback, this executes the process_data() method from the DataProcessor\n36\ncallback(update);\n37\n/* ... do work ... */\n38\n} // subscribe_for_updates()\n39\n};\nDataProcessor DataRetrievalClient. DataRetrievalClientIn this code, we have two classes, a and a The is responsible for\nDataProcessormaking a network connection to a service that streams back data, and the is responsible for processing the data whenever an\nprocess_and_publish_update()update is received from the server. To ensure that the method on line 10 is executed immediately\nstd::function<> DataRetrievalClient.whenever a message is received back from the server, it is passed in a callback to the Then,\nDataRetrievalClientwhenever the successfully gets a data update (line 34), it immediately invokes the callback on line 36 — this would\nDataProcessor::process_and_publish_update() DataUpdateexecute the method on the response that was received.\n11.12\nRandom Number Generators (✽)\n¸ 11.12.1\n(✽)rand\nThe C++ standard library provides an assortment of tools that can be used to generate random values. Randomization is a fantastic approach for\ngenerating test cases to test your code (in fact, many of the autograder test cases for projects were generated using these resources).\nrand()You may have heard about the operation, which can be used to generate a random integer. If you want to limit random numbers to\na specific range (e.g., if you only wanted numbers between 0 and 99), you can use the modulo operation to mod the random number with the\nsize of the range. If you want a non-zero initial value to the range, simply add it to the result of the modulus. Examples are shown below:\n1\nint32_t r1 = rand() % 100;\n// r1 stores random number between 0-99\n2\nint32_t r2 = rand() % 100 + 1\n// r2 stores random number between 1-100\n3\nint32_t r3 = rand() % 50 + 281;\n// r3 stores random number between 281-330\nrand()However, if you were to run over and over again, you will always get the same sequence of numbers:\n1804289383, 846930886, 1681692777, 1714636915, 1957747793, 424238335, ...\nrand()It turns out that, as long as is fed with the same seed, the sequence of numbers produced will always be the same. The seed is a number\nused to initialize a random number generator. If you do not explicitly set the seed of the random number generator, the seed is default set to 1.\nrand()Behind the scenes, takes a starting number (the seed) and performs mathematical operations on it to create another number that\nappears unrelated to the starting number. For the next number in the sequence, it takes the previously generated number and performs the same\nmathematical operations on it to get a new number. This process continues until the seed is reset.", "word_count": 775, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cdb9681e-cf5f-537e-8af1-af7c9bab0a48", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 369, "real_page_number": null, "text": "11.12 Random Number Generators\n357\nsrand(). srand(21) 21, rand()The seed can be set using For instance, would set the seed to and all subsequent calls to would produce\nthe sequence of numbers associated with a starting seed value of 21. An example is shown below:\n1\n// no seed, so it is default set to 1\n2\nfor (int32_t i = 0; i < 10; ++i) {\n3\nstd::cout << rand() << \" \";\n// prints sequence 1804289383 846930886 1714636915 ...\n4\n} // for i\n5\n6\n// set seed of rand to 281\n7\nsrand(281);\n8\nfor (int32_t j = 0; j < 10; ++j) {\n9\nstd::cout << rand() << \" \";\n// prints new sequence: 1462582710 1177062221 1109108190 ...\n10\n} // for j\n11\n12\n// reset seed back to 1\n13\nsrand(1);\n14\nfor (int32_t k = 0; k < 10; ++k) {\n15\nstd::cout << rand() << \" \";\n// prints original sequence 1804289383 846930886 1714636915 ...\n16\n} // for k\nrand() rand()If you want a quick and simple random number generator, using and modulo is sufficient. However, does not produce the\nbest results in terms of statistical randomness, and using modulo to set a range for random output actually biases the output toward certain\nrand()values. This is because remainders are not random! Some remainders will appear more than others; for instance, if you restrict a\nrandom number generator to the range 0-1999, the number 500 may be more likely to appear than the number 1500 due to the nature of modular\n<random> rand().arithmetic. For better random-number generation, the C++ library is preferred over\n¸ 11.12.2\n(✽)The Random Library\n#include <random>To use the C++ random number library, you will need to at the top of your code. Random number generation using\n<random>the library relies on two types of objects:\n• generators: function objects that generate uniformly distributed numbers\n• distributions: objects that transform sequences of numbers from a generator into sequences that follow a specific random variable\ndistribution (e.g., Uniform, Normal, Binomial, Poisson, etc.)\nIf you use a generator, it should be paired with a distribution. The C++ standard library provides several different generators that you can use. In\nstd::mt19937 std::mt19937this section, we will focus on the generator as our primary random number generator. The generator uses\ntheMersenneTwisteralgorithmtogeneraterandomnumbers,anditisbyfaroneofthemostwidelyusedgeneral-purposepseudorandomnumber\ngenerators.7 rand() std::mt19937This generator is much better than at producing values that are statistically random. To construct a\ngenerator, you can use the following constructor:\nstd::mt19937 gen(seed);\ngen seedwhere is the name of the generator (you can give it any variable name you want), and is the seed value that the generator is initialized\nto. For the best randomness, the value of the seed should be determined at runtime. One example is to use the current time to initialize the\nseed of the generator, as shown below. This method ensures that the seed is different every time the program is run. For example, when the\n1562341651852838259,program was run on July 5, 2019 at 11:47 AM EST (when this section was first written), the value of seed was\n3563885013, 1545485723, 4272454046, 3127047453,which produced the random number sequence …. When the program was run\n1562342251099698619,ten minutes later, the value of seed became which produced an entirely different sequence of random numbers.\n1\n#include <iostream>\n2\n#include <chrono>\n// used to retrieve system clock\n3\n#include <random>\n// std::mt19937 generator\n4\n5\nint main() {\n6\n// the seed depends on the system clock\n7\nauto seed = std::chrono::high_resolution_clock::now().time_since_epoch().count();\n8\n// construct generator using seed\n9\nstd::mt19937 gen(seed);\n10\n// generate and print out a random number using generator\n11\nstd::cout << gen() << '\\n';\n12\n} // main()\n7Apseudorandomnumbergenerator(PRNG)generatesrandomnumbersusingarithmeticmethods,andtheyoftenrequireastartingseed. Thisisdifferentfrom\na true random number generator (TRNG), which uses unpredictable physical means to generate random numbers (e.g., atmospheric noise, physical state of\nhardwaredevice,etc.).", "word_count": 692, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bc4ac23f-84dd-521f-a858-a1c23a158612", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 370, "real_page_number": null, "text": "358\nChapter 11. Iterators and the Standard Template Library\nstd::mt19937 std::random_device, <random>Another way to seed the generator is to feed it a which is also provided in the library.\nstd::random_deviceA can be used to generate \"true\" random numbers based on stochastic processes that may involve physical hardware\nstd::random_devicedevices on your machine. Because generating random numbers using a can be expensive, it is often only used to\nseed the generator. An example is shown below:\n1\nint main() {\n2\n// create a random_device to generate random number for seed\n3\nstd::random_device rd;\n4\n// sample the random_device and use result to seed the mt19937 generator\n5\nstd::mt19937 gen(rd());\n6\n// print out a random number using generator\n7\nstd::cout << gen() << '\\n';\n8\n} // main()\nAs shown by the examples above, the generator can be used to generate a giant range of numbers. How can we limit the random numbers\ngenerated into a specific range? For instance, suppose we only wanted random numbers between 0 and 1,000. We cannot use modulus if we\nwant a good distribution, since remainders may not be evenly distributed. Instead, we can use a distribution. The C++ standard library provides\naround 20 different distributions that can be used to specify the frequency of random numbers within a given range.\nstd::uniform_int_distribution<>For random number generation, two important distributions in the standard library are the\nstd::uniform_real_distribution<> a b,and the distributions. Both distributions take in two numbers, and that set the lower and\nupper bounds of the output range. When a distribution is applied to a generator, it ensures that the generator produces a random value in\n[a, b],the range and that all possible values within the range have an equal likelihood of being produced. This is known as a uniform\nstd::uniform_int_distribution<> int, long, unsigned),distribution. The should be used for integer types (such as while\nstd::uniform_real_distribution<> double float).should be used for floating points (such as and\nstd::mt19937The following code creates a generator and a uniform distribution. The distribution is then applied to the generator to\nspecify the range and frequency of values that are generated. To apply a distribution to a generator, simply pass in the generator object into the\noperator().distribution using\n1\nint main() {\n2\n// create a mt19937 generator\n3\nstd::random_device rd;\n4\nstd::mt19937 gen(rd());\n5\n// limit the generator so that it only generates numbers between\n6\n// 1 and 100, inclusive; this can be done by creating a distribution\n7\nstd::uniform_int_distribution<int32_t> dist(1, 100);\n8\n// apply the distribution to the generator every time a random\n9\n// number is generated (10 random numbers are generated below)\n10\nfor (int32_t i = 0; i < 10; ++i) {\n11\nstd::cout << dist(gen) << \" \";\n12\n} // for i\n13\n} // main()\nstd::mt19937Runningtheabovecodefivetimesinarowyieldsthefollowingoutput(thiswillbedifferentondifferentmachinesbecausethe\nstd::random_device):generator was seeded with a\nRun #1: 60 93 40 96 91 22 35 90 60 21\nRun #2: 8 45 57 51 48 8 81 26 3 36\nRun #3: 96 16 22 67 100 63 57 25 19 55\nRun #4: 65 80 40 70 83 93 86 8 85 16\nRun #5: 87 31 60 28 46 78 68 32 62 39\nThe following code can be used to generate random values between 1 and 100:floating point\n1\nint main() {\n2\nstd::random_device rd;\n3\nstd::mt19937 gen(rd());\n4\nstd::uniform_real_distribution<double> dist(1, 100);\n5\nfor (int32_t i = 0; i < 10; ++i) {\n6\nstd::cout << dist(gen) << \" \";\n7\n} // for i\n8\n} // main()\nRunning this code yields the following results (your results may differ since this is random):\nRun #1: 14.6965 39.4479 81.9015 37.1515 27.7026 82.0444 31.7411 47.3614 7.16891 98.6985\nRun #2: 33.4811 49.7041 93.0337 40.8993 54.661 8.63654 79.6556 11.052 69.1065 75.305\nRun #3: 86.6864 12.0326 72.6361 93.8472 53.819 78.9021 64.1423 88.8143 7.22277 63.2723\nRun #4: 68.8516 78.4234 56.8878 86.7274 64.0702 89.8117 9.73159 68.9019 4.01285 52.5718\nRun #5: 2.55291 65.394 2.9479 88.4693 1.73457 65.3447 42.9447 80.437 76.5555 21.75\nRandom number generators can be used in more ways than just to return a random number. For instance, generators can be used to shuffle the\nstd::shuffle() <algorithm>contents of a container using in the library:\ntemplate <typename typenameRandomAccessIterator, URNG>\nvoid std::shuffle(RandomAccessIterator first, RandomAccessIterator last, URNG&& g);\n[first, last) gRearranges the elements in the range randomly, using as a uniform random number generator. This function swaps\ng().each element with another random element picked using", "word_count": 768, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1c661eca-4148-56f4-9cce-d49044eab993", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 371, "real_page_number": null, "text": "11.12 Random Number Generators\n359\nAn example is shown below:\n1\nint main() {\n2\nstd::vector<int32_t> v = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};\n3\n// randomly shuffle contents of v\n4\nstd::random_device rd;\n5\nstd::mt19937 gen(rd());\n6\nstd::shuffle(v.begin(), v.end(), gen);\n7\nstd::cout << \"Shuffled elements:\";\n8\nfor (int32_t i : v) {\n9\nstd::cout << \" \" << i;\n10\n} // for i\n11\n} // main()\nOne possible output of this program is (each run produces a different shuffled order):\nShuffled elements: 3 7 10 9 12 2 14 8 13 1 15 5 11 4 6\nWhen generating test cases for projects, you may need to generate objects of non-numeric types. For instance, you may want to randomly\ngenerate words for projects that require strings as input. If you have a dictionary file that contains the words you want to select from, you can\nsimply add them all to a vector and use a generator to randomly select the indices that the words are chosen from.\n1\nstd::ifstream fin;\n2\nfin.open(\"dictionary.txt\");\n3\nstd::vector<std::string> words;\n// vector of strings that store the words\n4\nstd::string word;\n5\nwhile (fin >> word) {\n6\nwords.push_back(word);\n7\n} // while\n8\nfin.close();\n9\n10\n// create a generator for random number generation\n11\nstd::random_device rd;\n12\nstd::mt19937 gen(rd());\n13\n14\n// create a distribution to select a word at random from the vector\n15\nstd::uniform_int_distribution<int32_t> random_word_dist(0, words.size() - 1);\n16\n17\n// apply the distribution to the generator to select a random word\n18\nstd::string random_word = words[random_word_dist(gen)];\n19\nstd::cout << random_word << '\\n';\n20\n// random_word_dist(gen) generates a random index between 0 and words.size() - 1\nThe above program can be modified to fit the specifications of any project that requires random word generation. Note that you can use input\nstd::cin std::ifstreamredirection and instead of to read in the contents of a file.\nThe uniform distribution is not the only distribution you can use to generate random numbers. You can also generate random numbers to fit\nother distributions, such as Binomial, Geometric, Poisson, Exponential, and Normal. However, you likely will not be using these distributions\n<random>in this class (unless you want to have fun simulating exam scores for EECS 281). The contents of the library are a lot more\ncomprehensive than what was covered here, with many more generators and distributions available. However, the concepts detailed in this\nsection should be more than enough for this class.\nrand() <random>Remark: The random number generator and distributions in the library are not guaranteed to be portable. This\nrand()means that the random numbers generated may not be consistent across platforms and compilers. Someone who runs or a uniform\ndistribution on CAEN may not obtain the same numbers as someone who runs the code locally on Visual Studio, even if the seed and\neverything else is the same! Thus, you will need to be careful before sending out files that rely on randomization, as the person you are\nsending the code to may not get the same results!", "word_count": 519, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "856a7118-1224-5931-96c5-203682d809d3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 372, "real_page_number": null, "text": "360\nChapter 11. Iterators and the Standard Template Library\n11.13\nAdditional Features in C++17 (✽)\nEven though the language has existed for decades, C++ is still being actively maintained, with continuous updates being made every few years.\nThe arrival of C++11 in 2011 made drastic improvements to the existing C++03, with features that have become core components of the\nauto nullptr,language (such as type deduction, move semantics, lambdas, and so much more). Three years later, C++14 made some minor\nimprovements to C++11, such as function return type deduction and generic lambdas.\nIn this section, we will go over a few important features that were introduced in C++17. While these features may not be required material,\nthey are still valuable tools that are useful to know, especially as you gain experience developing in C++. At the least, these features can help\nmake your code more expressive and readable, if not more efficient.\n¸ 11.13.1\n(✽)Structured Bindings\nStructured bindings are a new feature introduced in C++17 that allows you to easily bind names to the subobjects of another object. The\nanatomy of a structured binding is as follows:\n<type> [<name>, <name>, ...] = <tuple-like object>\n<type> auto <name>where the is typically (either by value, reference, or const reference), is a variable name to be assigned to the unpacked\n<tuple-like object>values, and is an object like a tuple, struct, or array that can be unpacked.\nWe introduced an example previously involving pairs: before C++17, to declare variables that represent the first and second values of a pair,\n.first .second.we would have to define them explicitly using the member variables and\n1\nint>std::pair<std::string, p = {\"EECS\", 281};\n// p stores {\"EECS\", 281}\n2\nauto str = p.first;\n3\nauto num = p.second;\n4\nstd::cout << \"The value of str is: \" << str << '\\n';\n// \"The value of str is: EECS\"\n5\nstd::cout << \"The value of num is: \" << num << '\\n';\n// \"The value of num is: 281\"\nWith a structured binding, we can accomplish the same thing using the syntax on line 2:\n1\nint>std::pair<std::string, p = {\"EECS\", 281};\n// p stores {\"EECS\", 281}\n2\nauto [str, num] = p;\n// structured binding\n3\nstd::cout << \"The value of str is: \" << str << '\\n';\n// \"The value of str is: EECS\"\n4\nstd::cout << \"The value of num is: \" << num << '\\n';\n// \"The value of num is: 281\"\nconstThe identifiers in a structured binding can also be qualified and defined as a reference:\n1\nint>std::pair<std::string, p = {\"EECS\", 281};\n// p stores {\"EECS\", 281}\n2\nconst auto& [str, num] = p;\n// structured binding (const ref)\n3\nstd::cout << \"The value of str is: \" << str << '\\n';\n// \"The value of str is: EECS\"\n4\nstd::cout << \"The value of num is: \" << num << '\\n';\n// \"The value of num is: 281\"\nThis process is similar for a tuple, which is essentially a pair that is generalized to multiple elements:\n1\nint, char>std::tuple<std::string, t = {\"EECS\", 281, 'A'};\n2\nauto [str, num, grade] = t;\n3\nstd::cout << \"The value of str is: \" << str << '\\n';\n// \"The value of str is: EECS\"\n4\nstd::cout << \"The value of num is: \" << num << '\\n';\n// \"The value of num is: 281\"\n5\nstd::cout << \"The value of grade is: \" << grade << '\\n';\n// \"The value of grade is: A\"\nStructured bindings can also be used to unpack values in an array or compound object. The following example uses a structured binding to bind\nnames to the contents of an array (note that we cannot do this with a vector, since the structure of a vector is not known at compile time):\n1\nint32_t arr[3] = {203, 280, 281};\n// also works with std::array<int32_t, 3> arr = {203, 280, 281};\n2\nauto [x, y, z] = arr;\n3\nstd::cout << x << '\\n';\n// 203\n4\nstd::cout << y << '\\n';\n// 280\n5\nstd::cout << z << '\\n';\n// 281\nThe following uses a structured binding to identify values in a compound object:\n1\nstruct ClassName {\n2\nstd::string department = \"EECS\";\n3\nint32_t number = 281;\n4\n};\n5\n6\nint main() {\n7\nClassName cn;\n8\nauto [a, b] = cn;\n9\nstd::cout << a << '\\n';\n// \"EECS\"\n10\nstd::cout << b << '\\n';\n// 281\n11\n} // main()", "word_count": 749, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dde20b5e-3b1e-5152-99c3-453a8cd45861", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 373, "real_page_number": null, "text": "11.13 Additional Features in C++17\n361\nStructured bindings provide a elegant mechanism for emulating a function with multiple return values. Prior to the introduction of structured\nbindings, there were three primary ways to have a function \"return\" multiple values:\n1. Pass in pointers or references to the return values as parameters.\n1\nvoid int32_t& double&get_student_data(std::string& name, id, gpa) {\n2\nname = get_student_name();\n3\nid = get_student_id();\n4\ngpa = get_student_gpa();\n5\n} // get_student_data()\nstruct2. Return a that houses all of the values.\n1\nstruct StudentData {\n2\nstd::string name;\n3\nint32_t id;\n4\ndouble gpa;\n5\n};\n6\n7\nStudentData get_student_data() {\n8\nStudentData sd;\n9\nsd.name = get_student_name();\n10\nsd.id = get_student_id();\n11\nsd.gpa = get_student_gpa();\n12\nreturn sd;\n13\n} // get_student_data()\nstd::tuple<>3. Return a that houses all of the values.\n1\nint32_t, double>std::tuple<std::string, get_student_data() {\n2\nreturn std::make_tuple(get_student_name(), get_student_id(), get_student_gpa());\n3\n} // get_student_data()\nWith structured bindings, we have a cleaner way to unpack all the values for the latter two methods. An example is shown below.\n1\nint32_t, double>std::tuple<std::string, get_student_data() {\n2\nreturn std::make_tuple(get_student_name(), get_student_id(), get_student_gpa());\n3\n} // get_student_data()\n4\n5\nint main() {\n6\nauto [name, id, gpa] = get_student_data();\n7\n// ... do work ...\n8\n} // main()\nAlthough we will not discuss it here, structured bindings can be quite useful when working with map containers in the STL, since these\ncontainers support insertion methods that return both an iterator to the element with a given key and a Boolean indicating whether the insertion\nwas successful. We will go over these containers in more detail when we get to hash tables and trees in chapters 17 and 18.\n¸ 11.13.2\n(✽)std::optional\nPrior to C++17, one notable omission of the language was that there was no standardized way to represent the absence of a meaningful value,\ndespite the fact that there are situations where being able to return \"no solution\" or represent something as having \"no value\" is desired behavior.\nn:Consider the following function definition, which is intended to return the value of the first element in the vector that is a multiple of\nint32_t find_first_multiple(const std::vector<int32_t>& int32_tvec, n);\nn?What would you return if there are no elements that are a multiple of Prior to C++17, a common workaround is to return a dummy value,\n-1, -1such as that can be used to identify the absence of a value. However, it is clear there are pitfalls to this approach. What if is a valid return\n-1 -1?value? How can we differentiate between a dummy return value of and an actual solution of We could avoid this issue by returning a\nnullptrpointer to the integer we want and return if there is no solution, or by passing in the desired integer as a parameter reference and\nhaving the function return a Boolean, but these approaches are messy and not as expressive.\nstd::optional<>,This was fixed with the introduction of which allows you to represent an object that may or may not have a value.\nstd::optional<>We can rewrite our initial example to use the construct, as shown:\nstd::optional<int32_t> find_first_multiple(const std::vector<int32_t>& int32_tvec, n);\nstd::nulloptHere, our function can return the first multiple if it exists, or if it does not (this value indicates an optional has no value). A\nstd::optional<> #include <optional>.list of supported operations for an are summarized below. To use an optional, you should\ntemplate <typename T>\nstd::optional<T>();\nstd::optional<>Constructs a that does not contain a value.\ntemplate <typename T>\nstd::optional<T>(const std::optional<T>& other);\nstd::optional<> other std::optional<>Constructs a that does not contain a value if has no value, otherwise initializes the\nother.with the value of\ntemplate <typename T>\nstd::optional<T>(T&& value);\nstd::optional<> value.Constructs a with the value of", "word_count": 632, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6815dca6-7224-56fd-9701-47b80533daa0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 374, "real_page_number": null, "text": "362\nChapter 11. Iterators and the Standard Template Library\ntemplate <typename T>\nstd::optional<T>::make_optional(T&& value);\nstd::optional<> value.Constructs a with the value of\ntemplate <typename typename...T, Args>\nstd::optional<T>::make_optional(Args&&... args);\nstd::optional<> T’sConstructs a that is constructed in-place using the arguments of constructor.\n1\nstd::optional<int32_t> opt1;\n// has no value\n2\nstd::optional<int32_t> opt2{42};\n// has a value of 42\n3\nstd::optional<int32_t> opt3 = 281;\n// has a value of 281\n4\nstd::optional<int32_t> opt4 = std::nullopt;\n// has no value\n5\n6\n// has a value of 370\n7\nstd::optional<int32_t> std::make_optional<int32_t>(370);opt5 =\n8\n9\n// has a value of {9, 9, 9, 9, 9}\n10\nstd::optional<std::vector<int32_t>> std::make_optional<std::vector<int32_t>>(5,opt6 = 9);\n11\n12\n// has a value of {\"EECS\", 281}\n13\nint32_t>>std::optional<std::pair<std::string, opt7 =\n14\nint32_t>>(\"EECS\",std::make_optional<std::pair<std::string, 281);\nstd::optional<>:The following methods can be used to access the value of an\ntemplate <typename T>\nbool std::optional<T>::has_value();\nstd::optional<>Returns whether the has a value.\ntemplate <typename T>\nT& std::optional<T>::value();\nstd::optional<> std::bad_optional_accessReturns the value of the if it exists, otherwise throws a exception.\ntemplate <typename typenameT, U>\nconst&;T std::optional<T>::value_or(U&& default_value)\nstd::optional<> default_value.Returns the value of the if it exists, otherwise returns\n.value(), std::optional<>’s operator* operator->.Inadditionto youcanalsoaccessan valueusing and Theonlydifferenceis\nstd::optional<>thatthesemethodsdo checkifavalueexistsbeforehand, whichcausesundefinedbehaviorifusedonan withnovalue.not\noperator* operator-> std::optional<>, .has_value().Before you use or on an make sure to verify it has a value first using\n1\nstd::optional<int32_t> opt1;\n2\nstd::cout << opt1.has_value() << '\\n';\n// prints 0 for false\n3\nstd::cout << opt1.value_or(281) << '\\n';\n// prints 281\n4\n5\nstd::optional<int32_t> opt2{280};\n6\nstd::cout << opt2.has_value() << '\\n';\n// prints 1 for true\n7\nstd::cout << opt2.value_or(281) << '\\n';\n// prints 280\n8\nif (opt2.has_value()) {\n// always check .has_value() before using or ->*\n9\nstd::cout << *opt2 << '\\n';\n// prints 280\n10\n} // if\n11\n12\nstd::optional<std::vector<int32_t>> std::make_optional<std::vector<int32_t>>(5,opt3 = 9);\n13\nif (opt3.has_value()) {\n14\nstd::cout << opt3->size() << '\\n';\n// prints vector size, or 5\n15\n} // if\noperator*() std::optional<>Remark: A call to does not check if an contains a value, so it may be more efficient than calling\nstd::optional<T>::value() std::optional<T>::has_value()if has already been checked before.\n¸ 11.13.3\n(✽)std::variant\nstd::variant<>, <variant>,A definedintheheader makesitpossibletostoremultipletypesofdatainasinglevariable. Asanexample,\nint32_t std::string:the following is a variant that can either store an or\n1\nstd::variant<int32_t, std::string> var;\n2\nvar = 281;\n// variant holds the integer 281\n3\nvar = \"EECS\";\n// variant holds the string \"EECS\"\nA variant makes it easier to defer knowledge of a data’s type to when you actually need to use that data. For instance, suppose you wanted to\nparse a file or command line arguments, but you are not sure what types are you are going to get. With a variant, you can simply list out all the\npossible types using a single variable and then assign your data to that variable.", "word_count": 519, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3933206c-8b9f-549d-a5f2-055cc7cea481", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 375, "real_page_number": null, "text": "11.13 Additional Features in C++17\n363\nstd::get<>One way to get the value of a variant is to use with the correct type as its template, as shown below:\n1\nstd::variant<int32_t, std::string> var;\n2\nvar = 281;\n// variant holds the integer 281\n3\nstd::get<int32_t>(var)std::cout << << '\\n';\n// prints 281\n4\nvar = \"EECS\";\n// variant holds the string \"EECS\"\n5\nstd::cout << std::get<std::string>(var) << '\\n';\n// prints \"EECS\"\nstd::get<> std::bad_variant_accessHowever, if you attempt use with the wrong type, you will get a exception:\n1\nstd::variant<int32_t, std::string> var;\n2\nvar = \"EECS\";\n// variant holds the string \"EECS\"\n3\nstd::get<int32_t>(var)std::cout << << '\\n';\n// exception thrown, var is not an integer\nstd::get_if<>To check the type of a variant before getting its data, you can use the method. This method takes in the memory address of\nstd::variant<> nullptra and returns either a pointer to its data if the given type is correct, or if the given type is not correct.\n1\nstd::variant<int32_t, std::string> var;\n2\nvar = \"EECS\";\n3\nauto* str_value = std::get_if<std::string>(&var);\n// pointer to string value \"EECS\"\n4\nauto* std::get_if<int32_t>(&var);int_value =\n// nullptr since var is not an integer\nstd::get_if<> nullptr ifBecause returns a if the given type is not correct, we can use it handle variant logic within statements (since\nif (nullptr) false). if ifevaluates to This is shown below: the first statement runs if the variant’s data is a string, and the second runs\nif the variant’s data is an integer:\n1\nstd::variant<int32_t, std::string> var;\n2\nvar = get_variant_data();\n3\n// runs if var's data is a std::string\n4\nif (auto* value_ptr = std::get_if<std::string>(&var)) {\n5\nstd::string& value = *value_ptr;\n6\nstd::cout << \"var is a string with value \" << value << '\\n';\n7\n} // if\n8\n// runs if var's data is an int32_t\n9\nif (auto* std::get_if<int32_t>(&var))value_ptr = {\n10\nint32_t value = *value_ptr;\n11\nstd::cout << \"var is an integer with value \" << value << '\\n';\n12\n} // if\nOne particularly notable restriction of variants is that they are not allowed to initialize additional dynamic memory beyond the data they are\ndesigned to store. This feature can be quite useful, since dynamic memory allocation is typically more expensive than memory allocation in\nstd::vector<>);automatic storage. Note that the objects in a variant can allocate dynamic memory on their own (e.g., a variant that holds a\nstd::variant<>it’s just that the itself cannot allocate any additional dynamic memory to store its data.\n¸ 11.13.4\n(✽)std::visit and the Visitor Pattern\nstd::visit()Another useful feature provided by C++17 is the method, which can be used to apply a callable method (known as a visitor\nstd::visit()function) depending on the type that is held in a variant. When is called on a variant, it dispatches the appropriate visitor\nstd::visit()function based on the type that is currently held in that variant. The visitor object passed into must be callable with all\npossible types that are held in the variant. An example is shown below:\n1\nstruct Visitor {\n2\nvoid operator()(bool value) {\n3\nstd::cout << \"Variant is a bool!\\n\";\n4\n} // operator()(bool)\n5\nvoid operator()(const std::string& value) {\n6\nstd::cout << \"Variant is a string!\\n\";\n7\n} // operator()(string)\n8\n};\n9\n10\nint main() {\n11\nstd::variant<bool, std::string> var1;\n12\nvar1 = std::string{\"EECS 281\"};\n13\nstd::visit(Visitor{}, var1);\n// prints \"Variant is a string!\"\n14\n15\nstd::variant<bool, std::string> var2;\n16\nfalse;var2 =\n17\nstd::visit(Visitor{}, var2);\n// prints \"Variant is a bool!\"\n18\n} // main()", "word_count": 595, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6d288f65-3c1e-5ebd-b1fd-56ce1c13e992", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 376, "real_page_number": null, "text": "364\nChapter 11. Iterators and the Standard Template Library\nstd::visit()The method makes it cleaner to implement objects using the pattern, which is a design strategy of separating anvisitor\nalgorithm from the object it operates on. This pattern allows us to add new virtual methods to a collection of objects without having to modify\nShapeall the objects themselves. As an example, consider the following collection of shape objects that inherit from a base class:\n1\nstruct Shape {\n2\n};\n3\n4\nstruct publicCircle : Shape {\n5\ndouble radius;\n6\nCircle(double radius_in) : radius{radius_in} {}\n7\n};\n8\n9\nstruct publicRectangle : Shape {\n10\ndouble length;\n11\ndouble width;\n12\nRectangle(double doublelength_in, width_in) : length{length_in}, width{width_in} {}\n13\n};\n14\n15\nstruct publicTriangle : Shape {\n16\ndouble base;\n17\ndouble height;\n18\nTriangle(double doublebase_in, height_in) : base{base_in}, height{height_in} {}\n19\n};\ncalculate_area() Shape.Suppose we wanted to add a new method that can be used to calculate the area of any How do we go about\ndoing this? One strategy is to use plain old inheritance and polymorphism, as shown.\n1\nstruct Shape {\n2\nvirtual double constcalculate_area() = 0;\n3\n};\n4\n5\nstruct publicCircle : Shape {\n6\ndouble radius;\n7\nCircle(double radius_in) : radius{radius_in} {}\n8\ndouble const overridecalculate_area() {\n9\nreturn std::numbers::pi radius radius;* *\n10\n} // print_area()\n11\n};\n12\n13\nstruct publicRectangle : Shape {\n14\ndouble length;\n15\ndouble width;\n16\nRectangle(double doublelength_in, width_in) : length{length_in}, width{width_in} {}\n17\ndouble const overridecalculate_area() {\n18\nreturn length width;*\n19\n} // print_area()\n20\n};\n21\n22\nstruct publicTriangle : Shape {\n23\ndouble base;\n24\ndouble height;\n25\nTriangle(double doublebase_in, height_in) : base{base_in}, height{height_in} {}\n26\ndouble const overridecalculate_area() {\n27\nreturn 0.5 base height;* *\n28\n} // print_area()\n29\n};\n30\n31\nint main() {\n32\nCircle c = Circle{5};\n33\nRectangle r = Rectangle{3, 7};\n34\nTriangle t = Triangle{4, 6};\n35\nstd::vector<Shape*> shapes = { &c, &r, &t };\n36\nstd::vector<double> areas;\n37\nfor (const auto& shape : shapes) {\n38\ndouble area = shape->calculate_area();\n39\nareas.push_back(area);\n40\n} // for shape\n41\n} // main()", "word_count": 379, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c3618e08-139c-5154-b18b-d416a68c5d28", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 377, "real_page_number": null, "text": "11.13 Additional Features in C++17\n365\nAnother option is to implement a separate class that implements the behavior of this new method. This allows each of the shape objects to be\nsimpler and more flexible, as they would only need to define their own characteristics (and not any actions that may be performed on them).\nstd::visit(),This can be easily done using variants and as shown. The following code exhibits the same behavior as the code above:\n1\nstruct Shape {\n2\n};\n3\n4\nstruct publicCircle : Shape {\n5\ndouble radius;\n6\nCircle(double radius_in) : radius{radius_in} {}\n7\n};\n8\n9\nstruct publicRectangle : Shape {\n10\ndouble length;\n11\ndouble width;\n12\nRectangle(double doublelength_in, width_in) : length{length_in}, width{width_in} {}\n13\n};\n14\n15\nstruct publicTriangle : Shape {\n16\ndouble base;\n17\ndouble height;\n18\nTriangle(double doublebase_in, height_in) : base{base_in}, height{height_in} {}\n19\n};\n20\n21\nstruct ShapesAreaCalculator {\n22\ndouble operator()(const Circle& c) {\n23\nreturn std::numbers::pi c.radius c.radius;* *\n24\n} // operator()(Circle)\n25\ndouble operator()(const Rectangle& r) {\n26\nreturn r.length r.width;*\n27\n} // operator()(Rectangle)\n28\ndouble operator()(const Triangle& t) {\n29\nreturn 0.5 t.base t.height;* *\n30\n} // operator()(Triangle)\n31\n};\n32\n/* ...continued from previous page... */\n33\n34\nint main() {\n35\nCircle c = Circle{5};\n36\nRectangle r = Rectangle{3, 7};\n37\nTriangle t = Triangle{4, 6};\n38\nstd::vector<std::variant<Circle, Rectangle, Triangle>> shapes = { c, r, t };\n39\nstd::vector<double> areas;\n40\nfor (const auto& shape : shapes) {\n41\ndouble area = std::visit(ShapesAreaCalculator{}, shape);\n42\nareas.push_back(area);\n43\n} // for shape\n44\n} // main()\nstd::monostate std::variant<>.Remark: The type can be used to represent an empty state in a If you want a variant where\nstd::monostatehaving no value is a valid outcome, you can specify as one of its types. An example of this usage is shown below:\n1\nint32_t,std::variant<std::monostate, std::string> response;\n2\nresponse = \"EECS\";\n// variant stores string\n3\nresponse = 281;\n// variant stores integer\n4\nresponse = std::monostate{};\n// variant stores \"empty\" state\n¸ 11.13.5\n(✽)std::any\nstd::any <any>C++17 also introduced in the header, which can be used to store copy-constructible type:any\n1\nstd::any a = 281;\n2\nnullptr;std::any b =\n3\nstd::any c = \"potato\";\n4\nstd::vector<int32_t>{1,std::any d = 2, 3};\nstd::variant<>That being said, if you want the ability to store multiple data types in the same variable, is almost always preferable to\nstd::any. std::any,To get the data stored in an you would still need to know its underlying type, and the absence of type restrictions\n\"potato\" const char* std::string,makes it easier to run into bugs at runtime (for example, in the above example is a and not a so\nstd::string std::any_cast<> std::anytrying to convert to a using would cause an exception to be thrown). Furthermore, does\nstd::variant<>not have the same dynamic memory restrictions that a has, so it may be less efficient for storing larger objects.\nstd::anyA still has its use cases, however, and it was introduced to provide a safer means for storing data of any type. Prior to C++17,\nvoid* void std::anywas often used as an alternative, since a pointer was allowed to hold the address of any data type. An object of type\nvoid*, std::any void*is significantly safer than a as requires you to know the type of its underlying data before you can access it, while\ndoes not provide this safety guarantee.", "word_count": 588, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f1080c26-6cbd-56bb-8a2a-e7c637642c92", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 378, "real_page_number": null, "text": "366\nChapter 11. Iterators and the Standard Template Library\nChapter 11 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Algorithms in the standard template library (STL) often work on iterator ranges. In the STL, what is the standard convention for determining\nfirst last?which elements belong in an iterator range, given two iterators and\nfirst last [first, last]A) is inclusive, is inclusive:\nfirst last [first, last)B) is inclusive, is exclusive:\nfirst last (first, last]C) is exclusive, is inclusive:\nfirst last (first, last)D) is exclusive, is exclusive:\nE) None of the above\n2. Which of the following statements is FALSE about the STL?\nA) Using the STL can make your code more concise and readable\nB) The STL allows reuse of algorithms with different data structures\nC) The STL can help minimize explicit dynamic memory usage\nD) The implementation of STL algorithms is proprietary and never visible to the public\nE) None of the above\nstd::sort().3. Consider the following statements regarding the STL’s sorting algorithm,\ntemplate <typename typenameRandomAccessIterator, Compare>\nvoid std::sort(RandomAccessIterator first, RandomAccessIterator last, Compare comp);\n[first, last) compSorts the elements in the range in ascending order, or using the comparator if specified.\nstd::sort()I. The worst-case time complexity of is Θ(𝑛log(𝑛)), given input size 𝑛\nstd::sort()II. can be used to sort any container that supports iterators\nstd::sort()III. can be used to sort a vector of strings\nWhich of the above statements are TRUE?\nA) I only\nB) III only\nC) I and II only\nD) I and III only\nE) I, II, and III\n4. Which of the following statements is FALSE?\nA) Input iterators and output iterators can only read and write in the forward direction\nB) Bidirectional iterators share the same functionalities as forward iterators, but they also decrement\nC) Forward iterators can access the same value more than once\n.rend()D) The reverse iterator returned by points to the first element in a container\nE) None of the above\n5. Consider the following snippet of code:\n1\nint main() {\n2\nint32_t my_ints[] = {15, 21, 43, 40, 28, 50, 54, 35};\n3\nfor (auto& val : my_ints) {\n4\n++val;\n5\n} // for val\n6\n7\nstd::vector<int32_t> vec(my_ints, my_ints + 7);\n8\nstd::greater<int32_t>());std::sort(vec.begin(), vec.begin() + 4,\n9\nstd::reverse(vec.begin(), vec.end());\n10\nstd::less<int32_t>());std::sort(vec.begin(), vec.begin() + 3,\n11\n12\nauto it = vec.insert(vec.begin() + 5, 3, 19);\n13\nvec.erase(vec.begin() + 3, it);\n14\nstd::swap(vec[1], vec[4]);\n15\nfor (auto val : vec) {\n16\nstd::cout << val << \" \";\n17\n} // for val\n18\n} // main()\nWhat does this code output?\n29 19 55 19 51 19 41 44A)\n36 19 55 19 51 19 22 41 44B)\n16 19 51 19 36 19 29 41 44C)\n29 19 55 22 51 19 19 41 44D)\n29 19 55 19 51 41 44E)", "word_count": 524, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "95d03a83-46c6-5ddd-9c46-629ab3ea38f3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 379, "real_page_number": null, "text": "11.13 Additional Features in C++17\n367\n6. Consider the following snippet of code, which stores the total number of messages sent by EECS 281 staff members on the class Discord\nserver (as of August 27, 2023):\n1\nint main() {\n2\nint32_t>>std::vector<std::pair<std::string, total_messages;\n3\ntotal_messages.emplace_back(\"slime\", 70885);\n4\ntotal_messages.emplace_back(\"doubledelete\", 34197);\n5\ntotal_messages.emplace_back(\"toafu\", 30666);\n6\ntotal_messages.emplace_back(\"denalz\", 15391);\n7\ntotal_messages.emplace_back(\"khuldraeseth\", 25628);\n8\ntotal_messages.emplace_back(\"a-32\", 24450);\n9\nstd::sort(total_messages.begin(), total_messages.end());\n10\n11\nauto rit = total_messages.rbegin();\n12\nfor (int32_t i = 0; i < 4; ++i) {\n13\n++rit;\n14\n} // for i\n15\n16\nstd::cout << rit->first << '\\n';\n17\n} // main()\nWhat does this code output?\na-32A)\ndenalzB)\ndoubledeleteC)\nslimeD)\ntoafuE)\ncomp, comp(A, B) false,You have a vector of size 3, with three values A, B, and C. Given a comparator you know that returns7.\ncomp(B, C) true, comp(A, C) true. std::sort(), compreturns and returns If you sort this vector using where is passed in as\nan argument, what are the final contents of the vector after the sorting is complete?\nA) A, B, C\nB) B, A, C\nC) B, C, A\nD) C, A, B\nE) C, B, A\n8. Which of the following segments of code produces a outcome than all the others?different\nA) std::vector<int32_t> a = {1, 2, 3, 4, 5};\nstd::vector<int32_t> b(a.size());\nstd::reverse_copy(a.begin(), a.end(), b.begin());\nB) std::vector<int32_t> a = {1, 2, 3, 4, 5};\nstd::vector<int32_t> b(a.size());\nstd::reverse_copy(a.rbegin(), a.rend(), b.begin());\nstd::reverse(b.begin(), b.end());\nC) std::vector<int32_t> a = {1, 2, 3, 4, 5};\nstd::vector<int32_t> b(a.size());\nstd::reverse_copy(a.rbegin(), a.rend(), b.rbegin());\nD) std::vector<int32_t> a = {1, 2, 3, 4, 5};\nstd::vector<int32_t> b(a);\nstd::reverse(b.begin(), b.end());\nE) std::vector<int32_t> a = {1, 2, 3, 4, 5};\nstd::vector<int32_t> b(a.rbegin(), a.rend());\nstd::reverse(b.rbegin(), b.rend());\n9. Consider the following code, which attempts to multiply all the elements in a vector by two:\n1\nint main() {\n2\nstd::vector<int32_t> vec = {1, 2, 3, 4, 5};\n3\nauto it = vec.begin();\n4\nwhile (it != vec.end()) {\n5\nit = it *= 2;\n6\n} // while\n7\n} // main()\nvec [2, 4, 6, 8, 10].The expected contents of after this code runs to completion is However, this code has a bug. Which of the\nfollowing changes would fix this bug?\nit *= 2;A) Change line 5 to\n*it = *it 2;B) Change line 5 to *\n*it = *it++ 2;C) Change line 5 to *\n*it++ = *it 2;D) Change line 5 to *\nE) More than one of the above", "word_count": 417, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "04318a03-0796-58ca-bb03-03894043d2f6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 380, "real_page_number": null, "text": "368\nChapter 11. Iterators and the Standard Template Library\nInputIterators, it1 it2.10. Suppose you had two and Which of the following expressions are valid?\n*it1++I.\n--it1II.\nit1 != it2III.\nA) I only\nB) III only\nC) I and II only\nD) I and III only\nE) II and III only\nit. *(it + 3) it?11. You are given an iterator If the expression is valid, what category of iterator is\nit InputIteratorA) is an\nit OutputIteratorB) is an\nit BidirectionalIteratorC) is a\nit ForwardIteratorD) is a\nit RandomAccessIteratorE) is a\nBidirectionalIterators12. Suppose you had an algorithm that accepts as arguments:\ntemplate <typename BidirectionalIterator>\nvoid foo(BidirectionalIterator first, BidirectionalIterator last);\nfoo()Using the rules for bidirectional iterators covered in this chapter, which of the following types of iterators could you pass into the\nfunction without any issues? Select all that apply.\nForwardIteratorA)\nBidirectionalIteratorB)\nInputIteratorC)\nOutputIteratorD)\nRandomAccessIteratorE)\noperator--?13. Which of the following iterator types support(s) decrementation using Select all that apply.\nForwardIteratorA)\nBidirectionalIteratorB)\nInputIteratorC)\nOutputIteratorD)\nRandomAccessIteratorE)\noperator<?14. Which of the following iterator types support(s) comparison using Select all that apply.\nForwardIteratorA)\nBidirectionalIteratorB)\nInputIteratorC)\nOutputIteratorD)\nRandomAccessIteratorE)\n15. Which of the following best matches each of the specified containers with its iterator category?\nstd::vector<>: ForwardIteratorA)\nstd::list<>:\nForwardIterator\nstd::deque<>:\nForwardIterator\nstd::vector<>: ForwardIteratorB)\nstd::list<>:\nBidirectionalIterator\nstd::deque<>:\nForwardIterator\nstd::vector<>: BidirectionalIteratorC)\nstd::list<>:\nForwardIterator\nstd::deque<>:\nBidirectionalIterator\nstd::vector<>: RandomAccessIteratorD)\nstd::list<>:\nBidirectionalIterator\nstd::deque<>:\nBidirectionalIterator\nstd::vector<>: RandomAccessIteratorE)\nstd::list<>:\nBidirectionalIterator\nstd::deque<>:\nRandomAccessIterator", "word_count": 261, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fdc5e63a-8be0-58ec-b603-eef411d226b0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 381, "real_page_number": null, "text": "11.13 Additional Features in C++17\n369\nvec16. Consider the following code. What are the contents of after the code runs to completion?\n1\nint main() {\n2\nstd::vector<int32_t> vec = {19, 33, 52, 17, 37, 58, 39, 44};\n3\nstd::sort(vec.rbegin(), vec.rbegin() + 6);\n4\n} // main()\n[17, 19, 33, 37, 39, 52, 58, 44]A)\n[17, 19, 33, 37, 52, 58, 39, 44]B)\n[19, 58, 52, 44, 39, 37, 33, 17]C)\n[19, 33, 58, 52, 44, 39, 37, 17]D)\n[58, 52, 37, 33, 19, 17, 39, 44]E)\nvec17. Consider the following code. What are the contents of after the code runs to completion?\n1\nstruct Compare {\n2\nbool operator() (const int32_t const int32_t constlhs, rhs) {\n3\nreturn std::abs(lhs - 281) < std::abs(rhs - 281);\n4\n} // operator()()\n5\n};\n6\n7\nint main() {\n8\nstd::vector<int32_t> vec = {100, 150, 200, 250, 300, 350, 400, 450, 500};\n9\nstd::sort(vec.begin(), vec.end(), Compare());\n10\n} // main()\n[250, 200, 150, 100, 500, 450, 400, 350, 300]A)\n[300, 350, 400, 450, 500, 100, 150, 200, 250]B)\n[500, 100, 450, 150, 400, 200, 350, 250, 300]C)\n[300, 250, 350, 200, 450, 150, 500, 100, 400]D)\n[300, 250, 350, 200, 400, 150, 450, 100, 500]E)\n281thYouaregivenanunsortedvectorofamillionintegers,andyouwanttofindthe largestelement. Intermsofasymptotictimecomplexity,18.\nwhich of the following STL algorithms would be most efficient for this task?\nstd::find()A)\nstd::find_if()B)\nstd::sort()C)\nstd::partial_sort()D)\nstd::nth_element()E)\nstd::remove()?19. Which of the following statements is TRUE about\nstd::remove()A) A call to (and nothing else) can reduce the size of the container it is called on\nstd::remove()B) A call to (and nothing else) can reduce the capacity of the container it is called on\nstd::remove()C) A call to will always return an iterator that points to the last element not removed\nstd::remove() .erase()D) A call to should be followed with a call to the container’s to clean up the data that was removed\nE) More than one of the above\n20. Consider the following vector of integers:\n2\n3\n4\n4\n4\n5\n6\n6\n0\n1\n2\n3\n4\n5\n6\n7\nstd::lower_bound() [vec.begin(), vec.end())Suppose you called on the iterator range using a target value of 4. Which\nof the following iterators is returned?\nA) An iterator pointing to index 1\nB) An iterator pointing to index 2\nC) An iterator pointing to index 3\nD) An iterator pointing to index 4\nE) An iterator pointing to index 5\nstd::upper_bound() [vec.begin(), vec.end())21. Considerthesamevectorasinquestion20. Supposeyoucalled ontheiteratorrange\nusing a target value of 4. Which of the following iterators is returned?\nA) An iterator pointing to index 1\nB) An iterator pointing to index 2\nC) An iterator pointing to index 3\nD) An iterator pointing to index 4\nE) An iterator pointing to index 5\nstd::upper_bound() [vec.begin(), vec.end())22. Considerthesamevectorasinquestion20. Supposeyoucalled ontheiteratorrange\nusing a target value of 6. Which of the following iterators is returned?\nA) An iterator pointing to index 4\nB) An iterator pointing to index 5\nC) An iterator pointing to index 6\nD) An iterator pointing to index 7\nE) None of the above", "word_count": 574, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dcd22b73-0f3d-52c8-a964-a58691573cf2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 382, "real_page_number": null, "text": "370\nChapter 11. Iterators and the Standard Template Library\n23. Which one of the following statements is TRUE?\noperator++A) An iterator typically holds an address, and incrementing the iterator using always increments that address\nc c.end() - c.begin()B) For any valid STL container that supports iterators, you can use the expression to return the same\nc.size()value as\nC) An iterator that points to an element in an STL container is guaranteed to remain valid throughout the entire lifetime of that container,\nuntil the container goes out of scope\nc, c.end() cD) For any valid STL container the iterator returned by refers to the last valid element in\nE) None of the above\nstd::lower_bound(),24. What is the worst-case time complexity of given that the container is sorted, supports random access iterators,\nand contains 𝑛elements?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n25. Given an vector of integers of size 𝑛, what is the best attainable average-case time complexity of finding the three largest elementsunsorted\nin the vector?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n26. Which of the following STL containers is likely to invalidate existing iterators, pointers, and references when new elements are added?least\nstd::vector<>A)\nstd::deque<>B)\nstd::array<>C)\nstd::list<>D)\nE) All of the above containers are equally likely to invalidate existing iterators, pointers, and references\nunique_copy()27. Implement the STL’s function according to its official interface and description.\ntemplate <typename typenameForwardIterator, OutputIterator>\nOutputIterator unique_copy(ForwardIterator first, ForwardIterator last, OutputIterator result);\nunique_copy() [first, last) result,copies elements from the range to a range beginning with except that in a consecutive\ngroup of duplicate elements only the first one is copied. The function then returns an iterator to the position one past the end of the range to\nwhich the elements are copied. For example, if you execute the following code:\nint32_t data[6] = {1, 3, 3, 1, 1, 0};\nint32_t output[6];\nunique_copy(data, data + 6, output);\noutput:you would end up with the following contents of\n1\n3\n1\n0\nYou may NOT use any STL algorithms or functions, and the complexity of this function should be linear on the number of elements in\nlast - first operator==() last - firstthe input range. For elements in the range, exactly applications of and at most\nassignments are performed.\nset_difference()28. Implement the STL’s function according to its official interface and description.\ntemplate <typename typenameForwardIterator1, ForwardIterator2,\ntypename typenameOutputIterator, Compare>\nOutputIterator set_difference(ForwardIterator1 first1, ForwardIterator1 last1,\nForwardIterator2 first2, ForwardIterator2 last2,\nOutputIterator result,\nCompare comp);\nset_difference() resultThe function constructs a sorted range beginning in the location pointed to by with the set difference of\n[first1, last1) [first2, last2).the sorted range with respect to the sorted range The difference of two sets is formed by the\nelements that are present in the first set, but not in the second one. The elements copied by the function always come from the first range, in\nthe same order. For containers supporting multiple occurrences of a value, the difference includes as many occurrences of a given value as\nin the first range, minus the number of matching elements in the second, preserving order. The elements are compared using the comparator\ncomp. a b, !comp(a, b) && !comp(b, a).Two elements, and are considered equivalent if For example, given the vectors:\nfirst = [5, 10, 15, 20, 25]\nsecond = [10, 20, 30, 40, 50]\nresult [5, 15, 25]the set difference constructed at is (elements in the first range not in the second). You may NOT use any STL\nalgorithms or functions, and the complexity of this function should be linear on the number of elements in the input ranges.", "word_count": 621, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cd21e2ec-152e-5bba-9e57-37cb2efd5f89", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 383, "real_page_number": null, "text": "11.13 Additional Features in C++17\n371\nminmax_element()29. Implement the STL’s function according to its official interface and description.\ntemplate <typename typenameForwardIterator, Compare>\nstd::pair<ForwardIterator, ForwardIterator> minmax_element(ForwardIterator first, ForwardIterator last,\nCompare comp);\nminmax_element()The function returns a pair with an iterator pointing to the element with the smallest value in the provided iterator\n[first, last) first secondrange as the element, and an iterator pointing to the largest value in the range as the element. The\ncompcomparisons are performed using the comparator. If more than one equivalent element has the smallest value, the first iterator points\nto the of such elements. If more than one equivalent element has the largest value, the second iterator points to the of such elements.first last\nvec = [3, 7, 6, 9, 5, 8, 2, 4], vec.begin()Forexample,giventhevector runningthefunctionwith asthefirstargument,\nvec.end() operator< 2asthesecondargument, and asthecomparator, apairwouldbereturnedwithaniteratorto asthefirstelement\n9and an iterator to as the second element. You may NOT use any STL algorithms or functions, and your implementation must run in linear\ntime on the number of elements in the input range.\nreplace_if()30. Implement the STL’s function according to its official interface and description.\ntemplate <typename typenameForwardIterator, Predicate>\nvoid constreplace_if(ForwardIterator first, ForwardIterator last, Predicate pred, T& new_value);\nreplace_if() [first, last) pred true new_value.The function replaces all elements in the range for which returns with\nIsEven, trueFor example, suppose we have a functor, that returns for integers that are even:\nstruct IsEven {\nbool operator() (const int32_t constval) {\nreturn val % 2 == 0;\n} // operator()()\n};\n281:In this case, running the following code would replace all even numbers in the input iterator range with the value\nstd::vector<int32_t> nums = {100, 105, 110, 115, 120, 125, 130, 135, 140, 145};\nreplace_if(nums.begin(), nums.end(), IsEven(), 281);\n// contents of vector are now [281, 105, 281, 115, 281, 125, 281, 135, 281, 145]\nYou may NOT use any STL algorithms or functions, and your implementation must run in linear time on the number of elements in the\ninput range.\nChapter 11 Exercise Solutions\nfirst last1. The correct answer is (B). When working with STL iterator ranges, the standard convention is that is inclusive while is\nexclusive.\n2. The correct answer is (D). Many implementations of STL algorithms are public for anyone to view, so option (D) is false. Options (A),\n(B), and (C) are all true: the STL can make code more concise and readable, since you can just include and use a pre-implemented function\nthat satisfies your needs; the STL allows reuse of algorithms with different data structures through the use of iterators; and the STL can\nminimize explicit dynamic memory usage by handling memory allocation behind the scenes for you.\nstd::sort()3. The correct answer is (D). The STL algorithm library’s algorithm takes in two random access iterators. Containers\nstd::sort().that have iterators that do not support random access (such as lists) cannot be sorted using Statements I and III are true:\nstd::sort() runs in worst-case time, and it can be used to sort a vector of strings (in fact, if a container supports randomΘ(𝑛log(𝑛))\nstd::sort() operator<access iterators, can sort the container as long as the type of its elements supports or can be compared using\nthe passed in comparator argument).\n.rend()4. The correct answer is (D). The reverse iterator returned by points to the theoretical position the first element in abefore\n.end(),container, and not the last element itself (you can think of this as analogous to which points to the position one past the end, and\nnot the last element itself). All of the other options are true.\n5. The correct answer is (A). On line 2, we create the following array:\n[15, 21, 43, 40, 28, 50, 54, 35]\nOn line 4, we increment each value in the array by one.\n[16, 22, 44, 41, 29, 51, 55, 36]\nmy_ints my_ints + 7, my_ints + 7On line 7, we range construct a vector using all values from to not including the value of\nsince end is exclusive:\n[16, 22, 44, 41, 29, 51, 55]\nvec.begin() vec.begin() + 4, vec.begin() + 4On line 8, we sort the vector from to not including the value at since end is\nstd::greater<>exclusive. Since is used, the elements in this range are sorted in descending order:\n[44, 41, 22, 16, 29, 51, 55]\nvec.end()On line 9, we reverse the entire vector. All elements are included since points one past the end of the vector:\n[55, 51, 29, 16, 22, 41, 44]", "word_count": 789, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "6dc00d5f-a99d-5011-b853-916566f4bbea", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 384, "real_page_number": null, "text": "372\nChapter 11. Iterators and the Standard Template Library\nvec.begin() vec.begin() + 3, vec.begin() + 3On line 10, we sort the vector from to not including the value at since end\nstd::less<>is exclusive. Since is used, the elements in this range are sorted in ascending order:\n[29, 51, 55, 16, 22, 41, 44]\nvec.begin() + 5, it 19On line 12, we insert three values before each initialized to a value of 19. points to the first (in bold):\n19,[29, 51, 55, 16, 22, 19, 19, 41, 44]\nvec.begin() + 3 it, itOn line 13, we erase all values from to not including the value at itself:\n[29, 51, 55, 19, 19, 19, 41, 44]\n19, 51,[29, 55, 19, 19, 41, 44]On line 14, we swap the element at index 1 with the one at index 4 to give us our answer:\nfirst second6. The correct answer is (B). Pairs are sorted by their value, and then their value in the case of a tie. Thus, the call to\nstd::sort() on line 9 sorts the elements in the following order:\na-32 denalz doubledelete khuldraeseth slime toafu\n\"toafu\".On line 11, we initialize a reverse iterator, which points to the last element in the sorted vector, or the pair associated with\ntoafua-32 denalz doubledelete khuldraeseth slime\nWe then increment the reverse iterator four times, which moves it toward the front of the sorted vector by four positions:\nslimea-32 denalz doubledelete khuldraeseth toafu\nkhuldraesetha-32 denalz doubledelete slime toafu\ndoubledeletea-32 denalz khuldraeseth slime toafu\ndenalza-32 doubledelete khuldraeseth slime toafu\nfirst denalz.The value of this pair’s value is then printed out, which is\n7. The correct answer is (B). If a comparator is passed in, the order of elements after sorting is determined using the following rule: given\ntruetwo elements that are passed into the comparator, a return value of indicates that the first element comes before the second in sorted\ncomp(A, B) A B, comp(B, C) B C,order; otherwise, it comes after. In this case, indicates that comes after indicates that comes before\ncomp(A, C) A C.and indicates that comes before The final order would therefore be B, A, C.\na b [5, 4, 3, 2, 1]).8. The correct answer is (E). All of the options except for (E) inserts the elements of into in reverse order (i.e.,\nNote that the iterator range determines how the algorithm views the order of elements: if you pass in a reverse iterator range, the algorithm\nessentially sees and acts upon the data in reverse order.\nwhile9. The correct answer is (D). Options (A) and (B) do not work since the iterator is never incremented, so it can get stuck in the loop\nwithout ever exiting. For our desired result, we also want to multiply the value of the iterator by 2 first, before assigning it (which overwrites\nthe original value). This is not done in option (C), which increments before assigning, thereby assigning to the wrong value (which yields\n[1, 2, 4, 8, 16]the result instead).\n!=.10. The correct answer is (D). Input iterators support increments, dereferencing, and However, they do not support decrements. Thus,\nonly I and III are valid.\nit + 3.11. The correct answer is (E). Of the provided iterators, only random access iterators support pointer arithmetic, as performed with\n12. The correct answers are (B) and (E). Any iterator type that is higher in the hierarchy (e.g., supports at least the same operations) than\nthe iterator type required by the function argument can also be passed into the function. In this case, random access iterators are the only\ncategory that is higher that bidirectional iterators, so both of these iterator types can be passed into the function.\n13. The correct answers are (B) and (E). Bidirectional and random access iterators are the only two that support decrementation.\noperator<.14. The correct answer is (E). Only random access iterators support pointer comparison using\n15. The correct answer is (E). Vectors and deques support random access iterators, while lists only support bidirectional iterators.\nvec.rbegin() 44, vec.rbegin() + 6 33.16. The correct answer is (D). The value at is and the value of is The call to sort sorts all\n[52, 17, 37, 58, 39,the values between these two values, excluding the value pointed to by the end iterator (e.g., the values of\n44] std::sort()are sorted). However, since we passed in reverse iterators, the function call essentially views the elements in reverse\nvec.rbegin()order, so the first value in sorted order is placed at the position of and the last value in sorted order is placed at the\nvec.rbegin() + 5. 33position of The ends up rearranging all the values after in reverse sorted order (from highest to lowest), giving\n[19, 33, 58, 52, 44, 39, 37, 17].us a final vector of\nA B std::abs(A - 281) std::abs(B -17. The correct answer is (E). Using this comparator, a value comes before if is less than\n281). As a result, the vector ends up getting sorted in order of ascending distance from 281, which gives us the outcome in option (E).\nnthstd::nth_element()18. The correct answer is (E). As its name implies, can be used to find the value in sorted order without having\nto sort the entire container, which takes time. Options (A) and (B) can be used to find a target value, but it is not the best option if youΘ(𝑛)\nnthwant to find an element in an unsorted vector given its sorted position. Similarly, while (C) and (D) can make finding the easier by\nsorting the vector, these would require your algorithm to take time.Θ(𝑛log(𝑛))\nstd::remove()19. The correct answer is (D). Only option (D) is true, per the erase-remove idiom, since a call to does not physically\nerase elements from its container. Options (A) and (B) are false for this reason: the STL algorithm alone does not reduce the structure of\nstd::remove()the underlying container. Option (C) is false because returns an iterator the next available position after the removal,\nand not the last element not removed itself.", "word_count": 1037, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "49f06a52-f291-55d1-8430-0e1eaf7871de", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 385, "real_page_number": null, "text": "11.13 Additional Features in C++17\n373\nstd::lower_bound()20. The correct answer is (B). A call to on a sorted range returns an iterator to the first element that does not\nvalcompare less than the target argument of (or the end of the given iterator range if this does not exist). In this case, this would be the\n4first in the vector, which is located at index 2.\nstd::upper_bound()21. The correct answer is (E). A call to on a sorted range returns an iterator to the first element that compares\nvalgreater than the target argument of (or the end of the given iterator range if this does not exist). In this case, this would be the value of\n5, which is located at index 5.\nstd::upper_bound()22. The correct answer is (E). A call to on a sorted range returns an iterator to the first element that compares\nvalgreater than the target argument of (or the end of the given iterator range if this does not exist). In this case, there is no value that\n6, vec.end()compares greater than so is returned.\n23. The correct answer is (E). None of the statements are true. (A) is false because iterators provide an interface to the elements of a container\nand is not a perfect substitute for a memory address; incrementing an iterator does not always increment that element’s address (e.g., for a\nlist, the next element is not guaranteed to located in the next memory address). (B) is false because subtracting two iterators can only be\ndone if a container supports random access iterators, and not just any type of iterator. (C) is false due to the presence of iterator invalidation;\nfor containers like vectors, it is possible for the underlying data to be moved around in memory as the container grows in size. (D) is false\n.end()because returns the one past the end iterator, which points to the position directly after the last valid element.\nstd::lower_bound()24. The correct answer is (B). For a sorted container that supports random access iterators, can be done using a\nbinary search, which takes time on the number of elements 𝑛.Θ(log(𝑛))\nstd::nth_element()25. The correct answer is (C). This can be solved in linear time by using to identify the third largest element in the\nvector, and then doing another linear pass to identify the two other values that are larger.\nstd::list<>26. The correct answer is (D). The is the only container of the ones provided that guarantees that validity of iterators\nthroughout the lifetime of the element in the container. The other containers do not provide this guarantee (e.g., a vector’s elements may be\nreallocated in memory after new elements are added).\n27. One potential solution is shown below. In this implementation, we keep track of two adjacent iterators that iterate over the range in tandem.\nIf the front iterator has a value that is different from the back iterator, then we write the value of the front iterator to the output iterator that\nis passed in; otherwise, the value of the front iterator is ignored and the two iterators are incremented without writing it to the output.\n1\ntemplate <typename typenameForwardIterator, OutputIterator>\n2\nOutputIterator unique_copy(ForwardIterator first, ForwardIterator last, OutputIterator result) {\n3\n// check for empty range\n4\nif (first == last) {\n5\nreturn result;\n6\n} // if\n7\n8\n*result++ = *first;\n// write first value to output\n9\nForwardIterator prev = first++;\n10\n11\nwhile (first != last) {\n12\nif (!(*first == *prev)) {\n13\n*result++ = *first;\n14\n} // if\n15\nprev = first++;\n16\n} // while\n17\n18\nreturn result;\n19\n} // unique_copy()", "word_count": 620, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "72817852-9f70-545a-a582-e20ed26458ad", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 386, "real_page_number": null, "text": "374\nChapter 11. Iterators and the Standard Template Library\nA B,28. One potential solution is shown below. In this implementation, we iterate over the two iterator ranges (which we will denote as and\nA comp(A, B),where is the first range) until one reaches the end. When doing so, we compare the values at the two iterators. If we\nA compwrite the value of the iterator corresponding to to the result, and then increment this iterator (note that determines how the iterator\ncomp(B, A), Branges are sorted). If we write the value of the iterator corresponding to to the result, and then increment this iterator.\nOtherwise, the values are the same, so we increment both iterators. Lastly, if we reach the end of the second iterator range before the first\none, we will write all remaining elements in the first iterator range to the result.\n1\ntemplate <typename typenameForwardIterator1, ForwardIterator2,\n2\ntypename typenameOutputIterator, Compare>\n3\nOutputIterator set_difference(ForwardIterator1 first1, ForwardIterator1 last1,\n4\nForwardIterator2 first2, ForwardIterator2 last2,\n5\nOutputIterator result,\nCompare comp) {\n6\nwhile (first1 != last1 && first2 != last2) {\n7\nif (comp(*first1, *first2)) {\n8\n*result++ = *first1++;\n9\n} // if\n10\nelse if (comp(*first2, *first1)) {\n11\n++first2;\n12\n} // else if\n13\nelse {\n14\n++first1;\n15\n++first2;\n16\n} // else\n17\n} // while\n18\n19\n// write all remaining elements in the first iterator range to the result\n20\n// if there are still elements left after first2 reaches last2\n21\nwhile (first1 != last1) {\n22\n*result++ = *first1++;\n23\n} // while\n24\n25\nreturn result;\n26\n} // set_difference()\nOne potential solution is shown below. In this implementation, we iterate over the given iterator range and keep track of the largest and29.\nsmallest values we have encountered so far. At each element, we update the min value if the element is smaller than the best encountered,\nand we update the max value if the element is larger than the best encountered. Then, we return a pair containing the min and max.\n1\ntemplate <typename typenameForwardIterator, Compare>\n2\nstd::pair<ForwardIterator, ForwardIterator> minmax_element(ForwardIterator first,\n3\nForwardIterator last, Compare comp) {\n4\nForwardIterator min = first, max = first;\n5\nfor (; first != last; ++first) {\n6\nif (comp(*first, *min)) {\n7\nmin = first;\n8\n} // if\n9\nif (!comp(*first, *max)) {\n10\nmax = first;\n11\n} // if\n12\n} // for\n13\n14\nreturn {min, max};\n15\n} // minmax_element()\npred, new_value30. This can be implemented by iterating over the iterator range, checking if each element satisfies and replacing with if it\ndoes. An implementation of this solution is shown below:\n1\ntemplate <typename typenameForwardIterator, Predicate>\n2\nvoid constreplace_if(ForwardIterator first, ForwardIterator last, Predicate pred, T& new_value) {\n3\nfor (; first != last; ++first) {\n4\nif (pred(*first)) {\n5\n*first = new_value;\n6\n} // if\n7\n} // for\n8\n} // replace_if()", "word_count": 499, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9d8d7db0-86bb-5178-be3d-5c04aaf2cbe3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 387, "real_page_number": null, "text": "Chapter 12\nAmortization and Amortized Analysis\n12.1\nAmortized Complexity\nSuppose you are renting out an apartment with a rent of $1,500 a month, paid on the first of every month. On any given day, what is the\ncost that you would need to pay to be able to live in your apartment?worst-case\nThe answer to this question is $1,500, since that is the amount you would need to pay on the first of every month. However, it would be\nquite silly to claim that the daily cost of living in your apartment is $1,500, since that payment is not done in isolation. By paying that $1,500 up\nfront, not only do you earn the privilege to stay at your apartment for that day, but also for the remaining days in the month! It would be more\nmeaningful to distribute the $1,500 payment over all the days you are paying for to describe the daily cost of living in your apartment, which\ncomes out to $50 per day (assuming 30 days in the month).\nstd::vector::push_back(),Why is this analogy relevant? Consider which we claimed had a worst-case time complexity of Θ(𝑛).\nThis is because a single push may require you to reallocate the underlying array with double the capacity and copy all existing items over if it\ngoes beyond the vector’s existing capacity.\nbegin\nend_sz\nend_cap\n1\n2\n3\n4\nbegin\nend_sz\nend_cap\n1\n2\n3\n4\n1\n2\n3\n4\nbegin\nend_sz\nend_cap\n1\n2\n3\n4\n1\n2\n3\n4\n5", "word_count": 253, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f6e11fb7-bb8f-547f-8772-eaecb209210c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 388, "real_page_number": null, "text": "376\nChapter 12. Amortization and Amortized Analysis\nHowever, even though a single push could take time in the worst case, this worst-case push has the side effect of ensuring that the nextΘ(𝑛) 𝑛\npushes can be done in constant time (due to the reallocation). Because of this, the worst-case time complexity of does not give us the mostΘ(𝑛)\naccurate reflection of the operation’s performance. Similar to paying your rent upfront to avoid paying on the remaining days of the month,\nstd::vector::push_back() pays a cost up front during reallocation to allow subsequent pushes to be done in time.Θ(𝑛) Θ(1)\nThis is an issue that comes up in certain data structures, where an operation may be cheap in some scenarios, but expensive in others. When\nwe measure the cost of an operation, we often use its worst-case time complexity, since it gives us a reasonable upper bound for the resources\nstd::vector::push_back()that are needed for that operation. However, is a good example of a situation where the worst-case time\ncomplexity can be too pessimistic. For instance, what is the worst-case time complexity of pushing 𝑛elements to the back of a vector rather\nthan just one? If we strictly consider the worst-case scenario, we could assume that the worst-case time complexity of pushing 𝑛elements is\nΘ(𝑛2) if all 𝑛pushes take time. However, this produces an upper bound that is way too high, since it is impossible for all 𝑛pushes to takeΘ(𝑛)\ntime. How can we ensure that our complexity bound is as tight as possible for a sequence of operations, if a worst-case call to a singleΘ(𝑛)\noperation happens infrequently?\nTo answer this question, we will use a technique known as amortized analysis, which can be used to determine a tighter complexity bound\nfor a of operations. This bound can be used to derive an operation’s amortized complexity. The amortized complexity of an operationsequence\ngives us a convenient way to estimate an operation’s overall performance in the long run without having to worry about the details behind\nindividual calls (i.e., which calls exhibited worst-case behavior and which ones did not). To calculate an operation’s amortized complexity, we\nfirst consider a sequence of operations all at once, and then we distribute the maximum work required to complete this sequence of operations\nover the operations that are performed. This result gives us a better idea of how much an operation truly costs within a sequence of operations,all\neven if individual calls may be expensive on their own. Using the apartment example, for instance, we could say that the \"amortized cost\" of\nliving in the apartment is $50 per day ($1,500 distributed over 30 days), since the total cost during a 30-day period cannot exceed $1,500 —\ntreating the daily cost as $50 (rather than $1,500) gives us a more reasonable estimate for how much it costs to live in the apartment on each day.\n12.2\nAggregate Analysis\nThere are several ways we can compute the amortized complexity of an operation. One method to determine the amortized complexity of an\noperation is aggregate analysis. In an aggregate analysis, we first prove that a sequence of 𝑛operations requires at most work in the worst𝑇(𝑛)\ncase. Once we prove this, then the of each operation in the sequence is 𝑇(𝑛)∕𝑛.amortized cost\nstd::vector::push_back().As an example, let’s use aggregate analysis to determine the amortized complexity of Consider a\nstd::vector<>, which keeps track of its data using an underlying dynamic array. Whenever the underlying array is completely filled, adding\nan additional element forces the vector to allocate a larger array that is double the capacity of the original array. The original elements are then\ncopied over from the old array to the new array.\n.push_back()?How much work is required for a single call to This depends on whether reallocation is involved. If there is no\nreallocation involved, we can just append the element to the back of the vector in constant time. However, if reallocation is involved, we will\nhave to copy the existing elements in the vector to a new location before appending a new element at the end, which takes linear time. The\n.push_back()following table shows the work involved with different calls to on a vector:\nItem Number\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nCost\n1\n1+1\n1+2\n1\n1+4\n1\n1\n1\n1+8\n1\n1\n1\n1\n1\n1\n1\n1+16\n2nd,3rd,5th,9th, 17thSincethetabledoublesuponreallocation,wewouldneedtocopy1,2,4,8,and16elementsonthe and pushes,respectively.\n2ndThis is why pushing the element requires 1 + 1 = 2 units of work: 1 for appending the new element, and 1 for copying the old value over.\nWe can express the total work required to push 𝑛elements as\n𝑇(𝑛)=\n(1+1+1+1+…)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncostofappendingnewelements\n+\n(1+2+4+8+…+𝑛′)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncostofcopyingduringreallocation\n𝑛′where is done 𝑛times, and represents the largest power of two that is smaller than 𝑛. Since 1 is added 𝑛times, the total cost of1+1+…\nappending new elements is 𝑛. To determine the cost of copying during reallocation, we can use the following identity\n𝑛−1\n∑\n𝑘=0\n(𝑎𝑟𝑘) 𝑎=\n(1−𝑟𝑛\n1−𝑟\n)\n1+2+4+8+…+𝑛′where 𝑎is the first term, 𝑟is the common ratio, and 𝑛is the number of terms. We can express as\n𝑛+1)−1(log2\n∑\n𝑘=0\n𝑛+11−2log22𝑘=\n1−2\n𝑛×21−2log2=\n1−2\n1−2𝑛=\n2𝑛−1=1−2\n2𝑛−1=2−1\nPutting it all together, the total work required to push 𝑛elements in the worst case is bounded by\n𝑇(𝑛) 𝑛+2𝑛−1 Θ(𝑛)= =\nUsing aggregate analysis, we can calculate the amortized complexity as\n𝑇(𝑛)Amortized Complexity=\n𝑛\nΘ(𝑛)=\n𝑛\n=Θ(1)", "word_count": 970, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e9acb9eb-f6cb-5eda-8de6-0e1f0671a8a6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 389, "real_page_number": null, "text": "12.2 Aggregate Analysis\n377\nThisimpliestwothings. First,ifcapacityisdoubleduponreallocation,thetimecomplexityofcalling 𝑛timesis Θ(𝑛),𝑛×Θ(1).push_back() =\n.push_back()regardless of how many pushes are made. Second, since the time complexity of 𝑛calls to is bounded by Θ(𝑛), each call to\n.push_back() within a of 𝑛pushes can be treated as if it were a time operation.sequence Θ(1)\n.push_back()We can also see this visually. Consider the following figure, which shows the amount of work required for 17 calls to on\na vector. The black rectangles (pushes 2, 3, 5, 9, and 17) indicate elements that were copied due to reallocation. The dark gray rectangles at\nthe top of each column represent the new elements added with each call. The white rectangles indicate unused capacity, and the light gray\nrectangles represent positions in the vector that are already occupied.\n.push_back()Each individual call to involves a constant amount of work for the new element that was added in, but calls 2, 3, 5, 9, and 17\nhave additional work associated with reallocation. During reallocation, a new array with double the capacity is allocated, and the data in the old\n.push_back()array is copied over one-by-one. If we modify the figure to only include the elements that contribute to the work of each call,\nwe would get the following:\nThe number of rectangles in the above figure represents the total work needed to push 17 elements into a vector, assuming no other calls are\n9th.push_back()made. The height of each column indicates the amount of work required for that specific call. The push, for example, is\n8th 9thmuch costlier than the push, as adding the element would require the previous 8 to be copied due to reallocation.\n.push_back()We can see that the worst case of happens when elements 2, 3, 5, 9, and 17 are added, since reallocation is involved. The\namount of work required for these pushes is on the order of the number of elements that need to be copied over, or Θ(𝑛). On the other pushes,\nthe amount of work required is constant.\nHowever, there is a pattern when it comes to pushes. Even though copying elements during a reallocation takes extra work, this additional\n9thcopying reduces the work required for subsequent pushes. For instance, the reallocation done on the push ensures that pushes 10-16 can be\ndone in constant time. If we take the expensive copying work and distribute it evenly across all the pushes, we get a different picture:", "word_count": 425, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "135bdc56-7afa-5128-8ef8-d380f698f1c3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 390, "real_page_number": null, "text": "378\nChapter 12. Amortization and Amortized Analysis\nNote that the work done is still the same! However, by distributing the work across all pushes, each push is now associated with a constanttotal\n.push_back()amount of work. This obviously is not how works behind the scenes, since some calls may require more work than others.\n.push_back()However, from a complexity analysis standpoint, we can simply treat as an operation that always takes constant time, and\n.push_back()we would still obtain the same result for a sequence of pushes. This is what we mean when we say that has an amortized\n.push_back(),complexity of Θ(1): given a sequence of calls to the average cost per operation in the sequence is bounded above by Θ(1).\n.push_back()It should be mentioned that would have an amortized time complexity of if capacity were increased by anot fixedΘ(1)\nduring reallocation instead of doubled. To prove this, suppose we increased the capacity of a vector by an arbitrary fixed constantconstant\n𝑘during reallocation. In this scenario, we would have to copy over 𝑘elements during the first reallocation, 2𝑘elements during the second\nreallocation, 3𝑘elements during the third reallocation, and so on. The total work required to push 𝑛elements can therefore be expressed as\n𝑇(𝑛)=\n(1+1+1+1+…)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncostofappendingnewelements\n+(𝑘+2𝑘+3𝑘+4𝑘+…+\n⌊𝑛\n𝑘\n⌋\n𝑘)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncostofcopyingduringreallocation\nPulling out the 𝑘, we get\n𝑇(𝑛)=\n(1+1+1+1+…)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncostofappendingnewelements\n+𝑘\n(\n1+2+3+4+…+\n⌊𝑛\n𝑘\n⌋)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncostofcopyingduringreallocation\nThe sum of an arithmetic sequence can be solved using the following equation, where 𝑛is the number of terms, is the value of the first term,𝑎1\nand 𝑎𝑛is the value of the last term.\n𝑛(𝑎1+𝑎𝑛)𝑆𝑛=\n2\nIn this case, the sequence 1+2+3+…+\n⌊\n𝑛\n𝑘\n⌋\ncan be rewritten as\n1+2+3+…+\n⌊\n𝑛\n𝑘\n⌋\n=\n⌊\n𝑛\n𝑘\n⌋(\n1+\n⌊\n𝑛\n𝑘\n⌋)\n2\nΘ(𝑛2)=\nPutting this back into the original expression for total work, we get\n(1+1+1+1+…)+Θ(𝑛2) 𝑛+Θ(𝑛2) Θ(𝑛2)𝑇(𝑛)= = =\n.push_back()Using aggregate analysis, the amortized complexity of on a vector that increases capacity by a fixed constant during\nreallocation is thus\n𝑇(𝑛)Amortized Complexity=\n𝑛\nΘ(𝑛2)=\n𝑛\nΘ(𝑛)=\n12.3\nThe Accounting Method (✽)\nThe accounting method is another technique that can be used to determine the amortized complexity of an operation. In the accounting method,\nwe assign a monetary cost to each type of operation within a sequence of operations; this cost represents the operation’s cost. Unlikeamortized\naggregate analysis, each type of operation can have its own amortized cost. If the amortized cost of an operation exceeds its actual cost, the\ndifference is stored as that can be used for later operations. The goal of the accounting method is to assign an amortized cost for eachcredit\noperation so that the entire sequence of operations can be completed without ever running out of credit.\n.push_back()Toillustratethisprocess, let’sgobacktothevector example. Let’sdenotethe costofpush𝑖as𝑐𝑖andtheamortizedactual\ncost of push as ̂𝑐. For instance, to push 10 elements into a vector, the actual cost incurred to complete these pushes would be +…+𝑐10.𝑐1+𝑐2\nHowever, some of these pushes may be costlier than others. Our goal is to find a tight bound on ̂𝑐such that\n𝑛\n∑\n𝑖=1\n𝑐𝑖≤𝑛̂𝑐\nholds for any sequence of 𝑛operations. This inequality just means that the total credit (i.e., amortized cost) you accumulate from a sequence of\noperations (𝑛̂𝑐) must be greater than or equal to the actual cost incurred to perform these operations (the summation term). If this inequality\nholds for a sequence of 𝑛operations, the value of ̂𝑐would be the amortized cost of the operation.\nRemark: The above inequality was used under the assumption that all 𝑛operations are the same and exhibit the same amortized cost.\nHowever, the accounting method can also be used to assign amortized costs to different types of operations within a sequence ofdifferent\n𝑖thoperations. If this were the case, then the following expression should be used instead, where ̂𝑐𝑖is the amortized cost of the operation\nwithin a sequence of 𝑛operations.\n𝑛\n∑\n𝑖=1\n𝑐𝑖≤\n𝑛\n∑\n𝑖=1\n̂𝑐𝑖\nHere, we would want to find an amortized cost ̂𝑐for each type of operation within the sequence such that the summation of these amortized\ncosts does not exceed the actual cost required to complete all 𝑛operations.", "word_count": 746, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "33785b90-3d25-5f1a-b668-dd3aa1efeb4b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 391, "real_page_number": null, "text": "12.3 The Accounting Method\n379\nThe idea behind finding ̂𝑐is to overcharge for the cheaper operations so that enough credit can be built up for more expensive operations later.\n.push_back()Consider the vector example, where we consider the cost of a single push (with no reallocation) as $1. In this example, the\n8th 8th 9thpush does not require any copying, so the cost to push back the element is just $1. However, the push triggers a reallocation, which\ncosts $9 in total ($1 for inserting the new element, $8 for copying over the existing 8 elements). Note that we cannot charge $1 for each push,\nsince we would quickly run out of credit for the expensive calls.\nHowever, if we instead charge $3 for each push, our credit balance would never fall below zero. We start off with $0 and an empty vector.\nDuring the first push request, we charge $3 (the cost) and use up $1 (the cost) to conduct the push. The amount of excess creditamortized actual\nwe obtain from this push is the difference between amortized and actual cost, or $3 - $1 = $2.\n$2\nBalance\n1\nVector\n$2\nSurplus charged for this push\nthat we haven’t spent yet\nThe second push causes a reallocation, doubling the vector’s capacity to 2. The cost for the second reallocation is $2: $1 for appending the new\nelement, and $1 for copying the existing element over. We charged $3 for this operation, but spent $2, so the net credit from this second push is\n$1. Our credit balance is now $2 + $1 = $3. Note that we will always pay for copying costs using the surplus earned from previous pushes.\n$3\nBalance\n1\nVector\n$1\n2\n$2\nThe third push causes another reallocation, doubling the vector’s capacity to 4. The cost of this push is $3, since 1 new element was added and 2\nold elements were copied. Since we charged $3 for this push, we are able to handle the cost of this reallocation.\n$3\nBalance\n1\nVector\n$1\n2\n$0\n3\n$2\nThe fourth push costs $1, since no reallocation is done. We get $3 but only spend $1, so we add $2 to our surplus.\n$5\nBalance\n1\nVector\n$1\n2\n$0\n3\n$2\n4\n$2\nThe fifth push causes a reallocation, doubling the vector’s capacity to 8. The cost of this reallocation is $5: $1 for adding the new element, and\n$4 for copying the existing element over. The $4 cost of the copying will be paid for by the existing credit balance, while the $1 cost of adding\nthe new element will be paid from the $3 we earned on the push.\n$3\nBalance\n1\nVector\n$1\n2\n$0\n3\n$0\n4\n$0\n5\n$2\nThe sixth, seventh, and eighth pushes can be done without copying, since we have excess capacity:\n$9\nBalance\n1\nVector\n$1\n2\n$0\n3\n$0\n4\n$0\n5\n$2\n6\n$2\n7\n$2\n8\n$2\nThe ninth push requires a reallocation, so the total cost is $9: $1 for adding the new element, and $8 for copying the old elements over. We have\nenough credit in our balance to pay for the reallocation (using the surplus gained from the fifth to eighth pushes).\n$3\nBalance\n1\nVector\n$1\n2\n$0\n3\n$0\n4\n$0\n5\n$0\n6\n$0\n7\n$0\n8\n$0\n9\n$2", "word_count": 568, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f4625d27-45de-5fff-9751-cc98cc360e69", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 392, "real_page_number": null, "text": "380\nChapter 12. Amortization and Amortized Analysis\nIf we continue this process, our balance will never drop below zero (after each reallocation, the balance always resets to $3). The $3 charged for\neach element is spent as follows: $1 is spent on appending the element when it is newly inserted, $1 is spent on moving this element during\nits first reallocation, and $1 is spent on moving a previous element during reallocation. Thus, if we charge $3 for each push, we will be able\nto sustain the cost of reallocation in perpetuity. Since we now know that every push has a constant cost of $3, we can define the amortized\n.push_back()complexity of as Θ(1), since 3 is a constant.\nThe accounting method is best suited for proving a bound on an operation’s amortized time complexity. When analyzing an algorithmΘ(1)\nusing the accounting method, we typically set elementary operations (such as adding an element to a container) to a cost of $1. Then, we try to\nfind a price we can charge for each operation that will allow us to keep our balance non-negative. If we can assign an operation a constant, fixed\nprice (such as $3) and prove that our balance will always stay non-negative for any 𝑛calls to that operation, then that operation must have an\namortized complexity of Θ(1).\n12.4\nThe Potential Method (✽)\nThe potential method is another method that can be used to measure the amortized complexity of an operation. Similar to the accounting\nmethod, the potential method keeps track of prepaid costs that can be saved up and used for more expensive operations down the line. However,\ninstead of measuring a credit balance, this method measures the \"potential energy\" of a data structure using something known as a potential\nfunction. The potential energy of a data structure represents work that has already been \"paid for\" in the amortized analysis but not yet used;\nsurplus energy gained from cheap operations can be used to complete more expensive operations later on. By measuring potential energy using\na function rather than a numeric credit balance, the potential energy of a data structure can be easily derived at any point in time, after any\nsequence of operations on the data structure.\nSuppose we wanted to complete a sequence of 𝑛operations on a data structure, which starts off at an initial state we will denote as 𝐷0.\nUsing this notation, we will denote as the state of the data structure after the first operation, as the state of the data structure after the𝐷1 𝐷2\n𝑛thsecond operation, and (in general) 𝐷𝑛as the state of the data structure after the operation.\nThe goal of the potential method is to come up with a potential function that maps each state 𝐷𝑖to some real number thatΦ(𝐷𝑖)Φ\nrepresents the potential energy of the data structure at that state. We want to define such thatΦ\n• = 0, where is the initial state of the data structure (i.e., potential energy starts at 0).Φ(𝐷0) 𝐷0\n≥0 ≤𝑖≤𝑛(i.e.,• for all states 𝐷𝑖, where the potential energy at any state cannot be negative).Φ(𝐷𝑖) 1\n𝑖thOnce we define a potential function, we can measure the amortized cost of the operation as\n̂𝑐𝑖=𝑐𝑖+Φ(𝐷𝑖)−Φ(𝐷𝑖−1)\nAny potential function that satisfies the above two conditions can be used, but it is best to define so that the amortized cost ̂𝑐𝑖of each operationΦ\nis as small as possible. To do so, we ideally want the difference in potential energy to be positive if operation 𝑖has a relativelyΦ(𝐷𝑖)−Φ(𝐷𝑖−1)\nlow cost and negative if operation 𝑖has a relatively high cost (this is because we want potential energy to build up during cheap calls so that we\ncan use it for expensive calls later on). This strategy allows us to accumulate just enough potential energy with the low-cost operations to pay for\nfuture high-cost operations without the total potential energy of the data structure ever falling below 0.\nWhy does the potential method work? Suppose we have a sequence of 𝑛operations on a data structure, with costs 𝑐1, 𝑐2, …, 𝑐𝑛.actual\nSome of these costs are expensive, some of them are cheap. However, we can safely express the time complexities of these operations using\ntheir amortized costs instead of their actual costs because the sum of all amortized costs (̂𝑐1 ̂𝑐𝑛) is guaranteed to be no better than thê𝑐2+ +…+\nactual cost required to execute all 𝑛operations (𝑐1 +…+𝑐𝑛), as proven below.+𝑐2\n𝑛\n∑\n𝑖=1\n̂𝑐𝑖=̂𝑐1 ̂𝑐2 ̂𝑐𝑛+ +…+\n𝑐1+Φ(𝐷1)−Φ(𝐷0)+𝑐2+Φ(𝐷2)−Φ(𝐷1)+…+𝑐𝑛+Φ(𝐷𝑛)−Φ(𝐷𝑛−1)=\n𝑐1+𝑐2+…+𝑐𝑛+Φ(𝐷𝑛)= =\n𝑛\n∑\n𝑖=1\n≥𝑐𝑖+Φ(𝐷𝑛)\n𝑛\n∑\n𝑖=1\n𝑐𝑖(since must always be non-negative)Φ(𝐷𝑛)\nThus, the total amortized cost obtained by the potential method is an upper bound on the actual cost required to complete a sequence of\noperations, regardless of how expensive or cheap each individual operation is. Because of this, it is valid to express the cost of each operation in\nthe sequence using its amortized cost instead of its actual cost.\n.push_back()Let’s consider the example from before, where capacity is doubled during reallocation. To perform amortized analysis\nusing the potential method, we can use the following potential function\nΦ(𝐷𝑖) 2𝑛𝑖−𝑘𝑖=\n𝑖th 𝑖th.push_back(),where 𝐷𝑖represents the state of the vector after the call to 𝑛𝑖represents the size of the vector after the call, and 𝑘𝑖\n𝑖threpresents the capacity of the vector’s underlying data array after the call. This is a valid potential function, as will always be non-negativeΦ\n2𝑛𝑖≥𝑘𝑖).because the underlying array is guaranteed to be at least half-full after reallocation (i.e.,", "word_count": 940, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "28033708-7a63-58c7-8ea5-49ed3460ec21", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 393, "real_page_number": null, "text": "12.5 Amortized Analysis: Implementing a Queue with Two Stacks\n381\n.push_back() .push_back()Now, let’s measure the amortized cost of each call. Note that there are two scenarios that can happen: the\n𝑖thcall either triggers a reallocation, or it does not. If the push does trigger a reallocation, the actual cost of the push 𝑐𝑖is 1. Thus, thenot\namortized cost of a push with no reallocation is:\n̂𝑐𝑖=𝑐𝑖+Φ(𝐷𝑖)−Φ(𝐷𝑖−1)\n1+(2𝑛𝑖−𝑘𝑖)−(2(𝑛𝑖−1)−𝑘𝑖)=\n1+2𝑛𝑖−𝑘𝑖−2𝑛𝑖+2+𝑘𝑖=\n=1+2=3\n𝑖thIf there is a reallocation, the actual cost of the push is 𝑛𝑖, since the first elements have to be copied over, in addition to the insertion(𝑛𝑖−1)\n𝑖thcost of 1 for the new element that was added. Furthermore, if we know that the push triggered a reallocation, the capacity of the vector at\nmust have been (𝑛𝑖−1), and the capacity of the vector at 𝐷𝑖must be after doubling during reallocation. Plugging this into the𝐷𝑖−1 2(𝑛𝑖−1)\nformula for amortized cost, we get\n̂𝑐𝑖=𝑐𝑖+Φ(𝐷𝑖)−Φ(𝐷𝑖−1)\n𝑛𝑖+(2𝑛𝑖−2(𝑛𝑖−1))−(2(𝑛𝑖−1)−(𝑛𝑖−1))=\n𝑛𝑖+2𝑛𝑖−2𝑛𝑖+2−2𝑛𝑖+2+𝑛𝑖−1=\n=2+2−1=3\n.push_back() .push_back()In both cases, the amortized cost of has a constant value of 3. Thus, the amortized complexity of using the\npotential method also ends up being Θ(1), since 3 is a constant.\nRemark: For the potential function to work, does not actually need to be equal to 0. This is just a convention that makes it easier toΦ(𝐷0)\ncome up with a potential function.\n≥Φ(𝐷0)In fact, any function for which for all 𝑖can be used as a valid potential function. This is because we can construct anotherΦ(𝐷𝑖)Φ\n≥0,Φ′(𝐷𝑖) Φ′(𝐷0) Φ′(𝐷𝑖) Φ′potential function that satisfies and and the amortized costs of and would be exactlyΦ(𝐷𝑖)−Φ(𝐷0)= =0 Φ\nthe same. This is shown below:\n𝑐𝑖+Φ′(𝐷𝑖)−Φ′(𝐷𝑖−1)̂𝑐𝑖=\n𝑐𝑖+(Φ(𝐷𝑖)−Φ(𝐷0))−(Φ(𝐷𝑖−1)−Φ(𝐷0))=\n𝑐𝑖+Φ(𝐷𝑖)−Φ(𝐷𝑖−1)=\n≥Φ(𝐷0) Φ′In other words, it is possible to convert any potential function that satisfies into a potential function that satisfiesΦ(𝐷𝑖)Φ\n≥0Φ′(𝐷0) Φ′(𝐷𝑖)and without changing the result of the amortized analysis. Because of this, can still be a valid potential function as=0 Φ\n≥Φ(𝐷0)long as is satisfied for all 𝑖.Φ(𝐷𝑖)\n12.5\nAmortized Analysis: Implementing a Queue with Two Stacks\nIn this section, we will conduct an amortized analysis on the implementation of a queue with two stacks (as discussed in chapter 9). In this\nalgorithm, we treat one stack as the \"front\" of the queue and the other stack as the \"back\" of the queue.\nStack 1 (Front)\nStack 2 (Back)\nBack of Queue\nFront of Queue\nTop of\nStack 1\nBottom of\nStack 1\nTop of\nStack 2\nBottom of\nStack 2\n.push(), .pop(),Consider the following two functions in this implementation: which pushes an element to the back stack, and which\n.pop()removes an element from the front stack. If the front stack is empty when is called, all of the elements in the back stack are transferred\nto the front stack.\n1\nvoid push(T x) {\n2\nstackBack.push(x);\n3\n} // push()\n4\n5\nvoid pop() {\n6\nif (stackFront.empty()) {\n7\nwhile (!stackBack.empty()) {\n8\nstackFront.push(stackBack.top());\n9\nstackBack.pop();\n10\n} // while\n11\n} // if\n12\nstackFront.pop();\n13\n} // pop()", "word_count": 536, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8f97dd4c-a60a-5a81-813c-9a8ea105deba", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 394, "real_page_number": null, "text": "382\nChapter 12. Amortization and Amortized Analysis\nThe worst-case time complexity of removing an element from this queue is Θ(𝑛). This happens when we try to remove an element when the\nfront stack is empty, which forces us to copy the entirety of the back stack into the front stack. However, this situation occurs so rarely that it\nΘ(𝑛2)would be pessimistic to expect a sequence of 𝑛pops to take time, since it is impossible for every pop to take time. Is there a way weΘ(𝑛)\n.pop()can assign the operation a tighter bound on runtime? It turns out that we can, using amortized analysis.\n¸ 12.5.1\nAggregate Analysis\nLet’s first consider the aggregate analysis approach. If we look at the lifetime of any element that enters and leaves the queue, the total cost\nassociated with that element is at most 4 (1 to push into the back stack, 1 to pop out of the back stack, 1 to push into the front stack, 1 to pop out\nof the front stack). Thus, for any sequence of 𝑛operations on this data structure, the total cost incurred cannot be greater than 4𝑛. As a result,\nwe can divide this by 𝑛to obtain an amortized complexity of Θ(1).\n𝑇(𝑛)Amortized Complexity=\n𝑛\n4𝑛=\n𝑛=Θ(1)\n¸ 12.5.2\n(✽)The Accounting Method\n.push() .pop()We can also use the accounting method to prove that the amortized complexities of and are both Θ(1). If we consider the\nactual cost of pushing or popping an element from the underlying stacks as $1, we can charge an amortized cost of $3 per push and $1 per pop\nto ensure that our credit balance is always non-negative.\nHow does this work? When we first push an element into the back stack, we charge an amortized cost of $3 for an operation that actually\ncosts $1. This allows us to accumulate $2 worth of credit for each element that we initially push in. However, if we have to move an element\nfrom the back stack to the front stack, we will have to incur an additional $2 cost ($1 to pop out of the back stack, and $1 to push into the front\nstack). This is why we overcharged $2 for every push — by doing so, we are guaranteed to have enough credit to move each element from one\nstack to the other, since this process was paid for in advance when the element was first pushed into the back stack. Lastly, the $1 cost to pop an\n.pop(). .push() .pop()element from the front stack is handled by the $1 amortized cost of Since has an amortized cost of $3 and has an\namortized cost of $1, we can conclude via the accounting method that both operations have an amortized complexity of Θ(1).\n¸ 12.5.3\n(✽)The Potential Method\nWith the potential method, we can use the potential function 2𝑛𝑖, where 𝑛𝑖is the number of elements in the back stack. This is a validΦ(𝐷𝑖)=\n.push()potential function because and is always non-negative for any 𝐷𝑖. The amortized cost for isΦ(𝐷0) Φ(𝐷𝑖)=0\n̂𝑐𝑖=𝑐𝑖+Φ(𝐷𝑖)−Φ(𝐷𝑖−1)\n1+2𝑛𝑖−(2(𝑛𝑖−1))=\n1+2𝑛𝑖−2𝑛𝑖+2=\n=1+2=3\n.pop():There are two cases that can happen for the front stack can either be filled or empty. If the front stack is filled, the cost of the pop is 1,\n.pop()and the number of elements in the back stack remains unchanged. Here, the amortized cost of is\n̂𝑐𝑖=𝑐𝑖+Φ(𝐷𝑖)−Φ(𝐷𝑖−1)\n1+2𝑛𝑖−2𝑛𝑖=\n=1\nOn the other hand, if the front stack is empty, all the elements in the back stack must be copied to the front stack. Since there are 𝑛𝑖elements in\nthe back stack, the total cost of pop is (the 2𝑛𝑖comes from the cost to pop 𝑛𝑖elements from the back stack and then push them into the2𝑛𝑖+1\nfront stack, and the additional comes from popping off the top element after the copying is done). After all the elements are moved from the1\nback stack to the front stack, the potential energy is 0, since there are no more elements remaining in the back stack. Thus, the amortized cost of\n.pop() in this scenario is\n̂𝑐𝑖=𝑐𝑖+Φ(𝐷𝑖)−Φ(𝐷𝑖−1)\n(2𝑛𝑖+1)+0−2𝑛𝑖=\n=1\n.push() .pop()Inbothcases, theamortizedcostof is3, andtheamortizedcostof is1. Sincebothareconstants, theamortizedcomplexities\n.push() .pop()of and are both Θ(1).", "word_count": 741, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1d2e6dcf-f916-5072-92e9-0bc4f0905b84", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 395, "real_page_number": null, "text": "12.6 Amortized Analysis: Binary Counter\n383\n12.6\nAmortized Analysis: Binary Counter (✽)\nAIn this section, we will look at a counter, implemented using a 𝑘-element binary array that stores a base-2 number representedbinary k-bit\n.increment()using 0s and 1s. The binary counter is initialized to 0 and supports an function, which adds 1 to the binary number in the\n.increment():1array. For instance, the following are the contents of the array after eight calls to\nA[0]\nA[1]\n...\nA[k-4]\nA[k-3]\nA[k-2]\nA[k-1]\ncost\n0\n0\n...\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n...\n0\n0\n1\n0\n0\n0\n...\n0\n0\n1\n1\n0\n0\n...\n0\n1\n0\n0\n0\n0\n...\n0\n1\n0\n1\n0\n0\n...\n0\n1\n1\n0\n0\n0\n...\n0\n1\n1\n1\n0\n0\n...\n1\n0\n0\n0\n1\n2\n1\n3\n1\n2\n1\n4\n.increment()Every time we call on this binary counter, one unit of work is required to flip a single bit from 0 to 1 (or vice versa). Thus,\n.increment()the worst-case time complexity of a single call is Θ(𝑘), which happens if all 𝑘bits need to be flipped (this occurs when we\n2𝑘−1 2𝑘−1). .increment()increment from to However, what if we called 𝑛times, a worst-case time complexity class of is aΘ(𝑛𝑘) not−1\n.increment()tight bound on runtime. For the worst case to be Θ(𝑛𝑘), all 𝑛calls to must take time, which is impossible. To establishΘ(𝑘)\na tighter bound on runtime, we can use amortized analysis.\n¸ 12.6.1\n(✽)Aggregate Analysis\n.increment() .increment()First, let’s determine the amortized complexity of using aggregate analysis. If we call 𝑛times, what is\n.increment()the maximum amount of work required to perform these calls? To solve this, notice that not every bit is flipped with each\n(A[k-1]) (A[k-2])call. The last bit in the array is flipped during every increment, but the second to last bit is only flipped every other time.\n(A[k-3]) (A[k-4])Similarly, the third to last bit is flipped once every 4 increments, the fourth to last bit is flipped once every 8 increments,\n𝑛th 2𝑛−1and so on. In general, the to last bit in the array is only flipped once every increments.\n⌊𝑛.increment()As a result, if we call 𝑛times, the last bit is flipped 𝑛times, the second to last bit is flipped\n2⌋times, the third to last bit\n⌊𝑛is flipped\n⌊𝑛4⌋times, the fourth to last bit is flipped\n8⌋times, and so on. Thus, the total number of flips involved with 𝑛increment calls is\n𝑇(𝑛) 𝑛+=\n⌊\n𝑛\n2\n⌋\n+\n⌊\n𝑛\n4\n⌋\n+\n⌊\n𝑛\n8\n⌋\n+…+\n⌊\n𝑛\n2𝑘−1\n⌋\nPutting this as a summation and applying the sum of a geometric series, we get\n∑𝑘−1𝑇(𝑛)=\n𝑖=0\n⌊\n𝑛\n2𝑖\n⌋\n𝑛∑𝑘−1=\n𝑖=0\n(\n1\n2\n)𝑖\n2𝑛=\n(\n1−1\n2𝑘\n)\n< 2𝑛\nWe can therefore conclude that a sequence of 𝑛incrementations cannot involve more than 2𝑛bit flips, which is Θ(𝑛). Using aggregate analysis,\n.increment()each call ends up with an amortized cost of\n𝑇(𝑛)Amortized Complexity=\n𝑛\nΘ(𝑛)=\n𝑛\n=Θ(1)\n¸ 12.6.2\n(✽)The Accounting Method\nThe accounting method and the potential method lead us to the same conclusion. Using the accounting method, we can assign an amortized cost\n.increment().of $2 to every call to By doing so, $1 can be used to flip a bit from 0 to 1, and the remaining $1 can be used to flip the bit\nback from 1 to 0. We can satisfy all flips this way without ever running out of credit, as shown:\n$0\nBalance\n0\nVector\n0\n0\n0\n0\n0\n0\n0\n$1\nBalance\n0\nVector\n0\n0\n0\n0\n0\n0\n1\n$1\n1Ifyouarenotfamiliarwithbinaryandhavenoideawhatthetableisshowing,0inbinaryis0,1inbinaryis1,2inbinaryis10,3inbinaryis11,4inbinaryis\n100,5inbinaryis101,6inbinaryis110,7inbinaryis111,and8inbinaryis1000. Thisishowthebitsareflippedinthetable;noticethatthebitsinthearray\nareflippedfrom...0000to...0001to...0010to...0011to...0100to...0101,andsoon.", "word_count": 739, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "32dba7dd-c63a-5986-a434-5ebb074d4c79", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 396, "real_page_number": null, "text": "384\nChapter 12. Amortization and Amortized Analysis\n$1\nBalance\n0\nVector\n0\n0\n0\n0\n0\n1\n$1\n0\n$0\n$2\nBalance\n0\nVector\n0\n0\n0\n0\n0\n1\n$1\n1\n$1\n$1\nBalance\n0\nVector\n0\n0\n0\n0\n1\n$1\n0\n$0\n0\n$0\n$2\nBalance\n0\nVector\n0\n0\n0\n0\n1\n$1\n0\n$0\n1\n$1\n$2\nBalance\n0\nVector\n0\n0\n0\n0\n1\n$1\n1\n$1\n0\n$0\n$3\nBalance\n0\nVector\n0\n0\n0\n0\n1\n$1\n1\n$1\n1\n$1\n$1\nBalance\n0\nVector\n0\n0\n0\n1\n$1\n0\n$0\n0\n$0\n0\n$0\nHere, every time a bit is flipped from 0 to 1, we end up charging $2 and then using $1 to perform the flip (with $1 left over). This is why each\nbit gets assigned a $1 credit whenever it is flipped from 0 to 1. Then, when the bit needs to be flipped back to zero, we use this extra $1 credit to\n.increment()perform this flip, which is why a bit’s credit drops back to $0 when it is flipped from 1 to 0. By charging a constant $2 per\n.increment()call, we can continue flipping bits without ever ending with a negative balance; this proves that the amortized cost of is Θ(1).\n¸ 12.6.3\n(✽)The Potential Method\n.increment()The potential method can also be used to prove that runs in amortized time. To show this, we will use the potentialΘ(1)\n𝑖thfunction 𝑛𝑖, where 𝑛𝑖represents the number of 1s in the binary number after the increment. If we were to calculate the first fewΦ(𝐷𝑖)=\nvalues of ̂𝑐𝑖using this potential function, we would see a pattern:\n̂𝑐1 𝑐1+Φ(𝐷1)−Φ(𝐷0) 𝑐1+𝑛1−𝑛0= = =1+1−0=2\n̂𝑐2 𝑐2+Φ(𝐷2)−Φ(𝐷1) 𝑐2+𝑛2−𝑛1= = =2+1−1=2\n̂𝑐3 𝑐3+Φ(𝐷3)−Φ(𝐷2) 𝑐3+𝑛3−𝑛2= = =1+2−1=2\n̂𝑐4 𝑐4+Φ(𝐷4)−Φ(𝐷3) 𝑐4+𝑛4−𝑛3= = =3+1−2=2\n̂𝑐5 𝑐5+Φ(𝐷5)−Φ(𝐷4) 𝑐5+𝑛5−𝑛4= = =1+2−1=2\n̂𝑐6 𝑐6+Φ(𝐷6)−Φ(𝐷5) 𝑐6+𝑛6−𝑛5= = =2+2−2=2\n̂𝑐7 𝑐7+Φ(𝐷7)−Φ(𝐷6) 𝑐7+𝑛7−𝑛6= = =1+3−2=2\n̂𝑐8 𝑐8+Φ(𝐷8)−Φ(𝐷7) 𝑐8+𝑛8−𝑛7= = =4+1−3=2\n̂𝑐9 𝑐9+Φ(𝐷9)−Φ(𝐷8) 𝑐9+𝑛9−𝑛8= = =1+2−1=2\nIn fact, we can prove that ̂𝑐𝑖will always equal 2 for all values of 𝑖. To do so, we will first define 𝑚𝑖as the number of bits that are flipped from 1\n𝑖th 𝑖th.increment(),back to 0 on the increment. Since exactly one bit is always flipped from 0 to 1 with each call to the actual cost of the\nincrement must be (𝑚𝑖bits flipped from 1 to 0 by definition, and one bit flipped from 0 to 1, for a total of bits flipped). Furthermore,𝑚𝑖+1 𝑚𝑖+1\nthe change in potential, or Φ(𝐷𝑖)−Φ(𝐷𝑖−1), must be equal to the difference between the number of bits flipped to 1 and the number of bits\nflipped to 0, or 1−𝑚𝑖(since changes in potential are determined by changes in the number of 1s in the binary number). Thus, the amortized cost\n.increment()of any call to is\n̂𝑐𝑖=𝑐𝑖+Φ(𝐷𝑖)−Φ(𝐷𝑖−1)\n(𝑚𝑖+1)+(1−𝑚𝑖)=\n=2\n.increment()Since ̂𝑐𝑖is always equal to a constant 2, the amortized complexity of is Θ(1).", "word_count": 560, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "17a68dc6-4379-58f2-85e0-546a11795446", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 397, "real_page_number": null, "text": "12.7 Amortized Analysis: Pairing Heap\n385\n12.7\nAmortized Analysis: Pairing Heap (✽)\nWhen we discussed the time complexities of pairing heaps back in chapter 10, we claimed that the time complexity of popping the highest\npriority element was amortized 𝑂(log(𝑛)). Now that we have gone over the amortization process, we can justify why this is the case.\nFirst, let us consider the worst-case time complexity of a single pop using the pairing heap implementation discussed in chapter 10. In the\nworst case, every other node is a direct descendant of the root; this would require us to iterate over and meld subheaps to rebuild the heapΘ(𝑛)\nafter popping off the root. The worst-case time complexity of a single pop is therefore Θ(𝑛).\n9\n8\n7\n6\n5\n4\n3\n2\n1\n.pop()\n8\n7\n6\n5\n4\n3\n2\n1\nΘ(𝑛)Must iterate over children and meld,\nHowever, if we use a two-pass or multipass approach, notice what happens to the remelded tree after a worst-case pop:\n1\n2\n3\n4\n5\n6\n7\n8\n2\n1\n4\n3\n6\n5\n8\n7\n4\n3\n2\n1\n8\n7\n6\n5\n8\n7\n6\n5\n4\n3\n2\n1\nEven though our initial pop took time, the remaining nodes of the pairing heap ended up arranging themselves into a structure that allowsΘ(𝑛)\nfuture pops to be done in time. In fact, if we repeatedly insert 𝑛elements into an initially empty pairing heap, and then successively𝑂(log(𝑛))\npop out all 𝑛elements until the pairing heap is empty again, the total time required to clear the heap is bounded by due to the𝑂(𝑛log(𝑛))\nreorganization caused by melding the children using a two-pass or multipass approach. Using aggregate analysis, we can average out this\nwork over all 𝑛pops to get an amortized cost of for each individual pop operation:𝑂(𝑛log(𝑛)) 𝑂(log(𝑛))\n𝑇(𝑛)Amortized Complexity=\n𝑛\n𝑂(𝑛log(𝑛))=\n𝑛\n𝑂(log(𝑛))=\n12.8\nAmortized vs. Average-Case Analysis\nItisveryeasytomixuptheconceptofamortizedcomplexitywithaverage-casecomplexity,buttheyare thesame! Theamortizedcomplexitynot\nof an operation deals with its cost when operations. On the other hand, the average-case complexity of an operationevaluated over a sequence of\nrepresents the expected cost of a to that operation over all possible inputs.single call\nBecause an average-case analysis deals with expectation over all possible inputs, it relies on probabilistic assumptions about the data\nstructures and inputs involved in a given algorithm or operation. To compute the average-case time complexity of an operation, you will have to\nlook at all the possible ways an operation can be run, and then compute its expected performance over all these possible inputs.\nIn contrast, an amortized analysis does not need to deal with these factors. Instead of considering all the possible inputs an operation may\nbe run on, amortized complexity simply establishes a bound on a single operation’s average cost contribution when it is considered within a\nsequence of operations. You can think of amortized analysis as a worst-case analysis for a of operations rather than a operation:sequence single\nafter a worst-case bound on total work is calculated for a sequence of operations, this work is averaged across all the operations performed to\nobtain each operation’s amortized cost.\nSince an amortized analysis measures the average cost of an operation over a sequence of operations, a single call to an operation may\nbe more expensive than its amortized cost. However, if we were to run that operation enough times and average the total cost over the entire\nsequence of calls, we end up with that operation’s amortized cost. For example, if an operation had a worst-case time complexity of and anΘ(𝑛)\namortized complexity of Θ(1), running the operation may result in a performance, but running the operation multiple times ensuresΘ(𝑛)once\nthat the average performance of each operation in the sequence comes out to Θ(1).", "word_count": 658, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bfd9e4f9-fc71-5c0e-880f-5be7c511f4cd", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 398, "real_page_number": null, "text": "386\nChapter 12. Amortization and Amortized Analysis\nChapter 12 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Which of the following statements is/are TRUE regarding amortization?\nI. Amortized complexity analysis typically results in a time complexity class that is worse than the asymptotic worst-case complexity\nof a given operation.\nII. Amortized complexity analysis is most useful when the average-case and worst-case time complexities of an operation are identical.\nIII. Amortized complexity analysis provides a tighter upper bound on an operation for which individual calls may have different time\ncomplexities, regardless of input.\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) II and III only\n.shove_back().2. Suppose you are implementing a vector with an underlying dynamic array, but with a special insertion method known as\n.push_back()), .shove_back()This method works normally when the array is not full (emulating but when the array fills up, the\nmethod creates a new dynamic array that is double the size of the old array, copies the elements from the old array to the new array,\nstd::sort()performs a call to on the new array, and then deletes the old array. What is the amortized time complexity ofΘ(𝑛log(𝑛))\n.shove_back()the operation?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n.push_back() std::vector<>3. Suppose that we decide to modify the method of the STL such that, instead of doubling the capacity\n⌈1.5⌉(i.e.,during reallocation, we only increase the capacity by a factor of if a vector of size 𝑘reaches capacity, we move all the elements to\n⌈1.5⌉𝑘).a new vector of capacity Which of the following statements is/are TRUE?\nA) The amortized time complexity will be than when we doubled capacity during reallocationhigher (worse)\nB) The amortized time complexity will be than when we doubled capacity during reallocationlower (better)\nC) The amortized time complexity will be as when we doubled capacity during reallocationthe same\nD) More than one of the above\nE) None of the above\n.push_back() std::vector<>4. Suppose that we decide to modify the method of the STL such that, instead of doubling the capacity\nduring reallocation, we only increase the capacity by a constant value of 1000 with every reallocation (i.e., if a vector of size 𝑘reaches\ncapacity, we move all the elements to a new vector of capacity 𝑘+1000). Which of the following statements is/are TRUE?\nA) The amortized time complexity will be than when we doubled capacity during reallocationhigher (worse)\nB) The amortized time complexity will be than when we doubled capacity during reallocationlower (better)\nC) The amortized time complexity will be as when we doubled capacity during reallocationthe same\nD) More than one of the above\nE) None of the above\n𝑖th5. Suppose you are performing a sequence of 𝑛operations on a data structures in which the operation takes time if 𝑖is an exact powerΘ(𝑖)\nof 2 (𝑖=1,2,4,8,16,32,…), and time otherwise. What is the amortized time complexity of a single operation on this data structure?Θ(1)\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nE) Θ\n(\n2𝑛\n𝑛\n)\nstd::queue<T, std::list<T>>).6. Consider an STL queue that is defined with a doubly-linked list as its underlying container (i.e., a\nWhich of the following correctly summarizes the worst-case and amortized time complexities of a single call to the queue’s push and pop\noperations? Elements are appended to the back of the underlying linked list and removed from the front.\nA) Worst-case push: Θ(1), Worst-case pop: Θ(1), Amortized push: Θ(1), Amortized pop: Θ(1)\nB) Worst-case push: Θ(𝑛), Worst-case pop: Θ(𝑛), Amortized push: Θ(1), Amortized pop: Θ(1)\nC) Worst-case push: Θ(𝑛), Worst-case pop: Θ(1), Amortized push: Θ(1), Amortized pop: Θ(1)\nD) Worst-case push: Θ(1), Worst-case pop: Θ(𝑛), Amortized push: Θ(1), Amortized pop: Θ(1)\nE) Worst-case push: Θ(𝑛), Worst-case pop: Θ(1), Amortized push: Θ(𝑛), Amortized pop: Θ(1)", "word_count": 684, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "d2ee8d9d-1a48-51fb-9ab0-106df69c178b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 399, "real_page_number": null, "text": "12.8 Amortized vs. Average-Case Analysis\n387\n7. An is a stack where the elements always appear in increasing order. This special stack supports the following operations:ordered stack\n.pop():• removes and returns the top element from the ordered stack\n.push(x):• continually removes elements greater than 𝑥from the top of the stack until 𝑥is the largest element on the stack; after all\nremovals are complete, 𝑥is pushed to the top of the stack\n.push(6)The following example illustrates a call to on an ordered stack with elements 1, 3, 4, 5, 7, and 9. To reestablish order, elements\n9 and 7 are removed from the top of the stack before 6 is pushed in. The elements 7 and 9 are readded once the push is complete. Thenot\nstack is implemented using a doubly-linked list, with a pointer to the top element.\n1\n3\n4\n5\n7\n9\n.push(6)\n1\n3\n4\n5\n6\n.pop() .push()(a) What is the worst-case time complexities of and in terms of the stack size 𝑛?\n.pop() .push()(b) What is the amortized time complexity of and in terms of the stack size 𝑛?", "word_count": 191, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7190d9e6-fb01-52de-ae72-169447bb2468", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 400, "real_page_number": null, "text": "388\nChapter 12. Amortization and Amortized Analysis\nChapter 12 Exercise Solutions\n1. The correct answer is (C). Statement I is false because amortized complexity averages out the cost of a single operation within a sequence\nof operations, which is at least as good as the worst-case time complexity of the operation itself. Statement II is false because amortized\nanalysis is most useful when the worst-case time complexity is a lot worse than the average performance of the operation, but it happens\nrarely. Statement III is true, as amortized analysis gives you a better idea of what the true upper bound of a sequence of operations is if\nindividual calls to each operation may have different costs.\n2. The correct answer is (B). In amortized analysis, we average the time required to perform a sequence of operations over all the operations\n.shove_back() .push_back()performed. Since the special operation works similarly to when the array is not at capacity, we\n.shove_back()would expect such a call for to take time. However, when the array is at full capacity, we have to reallocate andΘ(1)\nsort the contents of the original array, which takes a dominating time. Using aggregate analysis, we therefore see that theΘ(𝑛log(𝑛))\n.shove_back()amortized complexity of a call to is Θ(log(𝑛)).Θ(𝑛log(𝑛))∕𝑛=\n3. The correct answer is (C). Increasing the capacity multiplier by a different constant (in this case, 1.5 instead of 2) does not change the fact\nthat the total work performed over 𝑛operations is (you can see this using the sum of a geometric series). As a result, the amortizedΘ(𝑛)\ntime complexity remains Θ(1).Θ(𝑛)∕𝑛=\nΘ(𝑛2)4. The correct answer is (A). Increasing the capacity by a fixed constant results in a total amount of work (as demonstrated using the\nΘ(𝑛2)∕𝑛=example at the end of section 12.2). As a result, the amortized time complexity becomes Θ(𝑛), which is worse than the Θ(1)\namortized complexity that comes with doubling capacity during reallocation.\n5. The correct answer is (A). The total amount of work required to perform 𝑛operations can be expressed as:\n𝑇(𝑛) <𝑛+Θ(𝑛) Θ(𝑛)=1+2+1+4+1+1+1+8+1+…=(1+1+1+…)+(1+2+4+8+…) =\nUsing aggregate analysis, we conclude that the amortized time complexity of a single operation is Θ(1). (Note: the scenarioΘ(𝑛)∕𝑛=\ndescribed in this problem is exactly what happens during vector reallocation.)\n6. The correct answer is (A). Unlike vectors, lists do not perform any reallocation when elements are inserted, and the doubly-linked nature\nstd::list<>of the ensures that the worst-case time complexity of pushing and popping is Θ(1). It follows from this that the amortized\ntime complexities of these operations must also be Θ(1), since running a sequence of 𝑛pushes and pops will take at most time in total,Θ(𝑛)\nresulting in an amortized complexity of Θ(1).Θ(𝑛)∕𝑛=\n.pop() .push()7. (a) In the worst case, takes time since it is guaranteed to remove only one element from the stack, but takes Θ(𝑛)Θ(1)\ntime, since you may have to remove all the other elements in the stack if you push in a value that is lower than everything in the stack.\n(b) If we look at the lifetime of any element that enters and leaves the stack, the total cost associated with that element is at most 2 (1 to\npush into the stack, and 1 when it is popped out). Thus, for any sequence of 𝑛pushes and pops, the total cost incurred cannot be greater\nthan 2𝑛. Thus, the amortized complexity of push and pop are (essentially, a more expensive push also gets rid of more2𝑛∕𝑛= Θ(1)\nelements from the stack, which makes future pushes less likely to be as costly, resulting in a constant amortized cost for each operation).\nYou can look at the queue with two stacks problem in section 12.5 for a similar example.", "word_count": 634, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "6134b571-88b2-5c46-aad7-96f9ae0b7602", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 401, "real_page_number": null, "text": "Chapter 13\nSets and Union-Find\n13.1\nSets and Set Operations\n¸ 13.1.1\nSet Terminology\nA set is a well-defined collection of elements. In practice, sets are often used to check if a specific value exists within a group ofdistinct\nelements. However, before we begin discussing use cases of sets, let us first introduce some set terminology:\n• The union (∪) of two sets consists of all elements that are in one set or the other. For example, if set A consists of the elements {1, 2, 3}\nand set B consists of the elements {1, 3, 5}, the union of sets A and B (denoted as A ∪B) is the set consisting of the elements {1, 2, 3, 5}.\n• The intersection (∩) of two sets consists of all elements that are members of both sets. Using the same example as above, the intersection\nof sets A and B (denoted as A ∩B) consists of the elements {1, 3}, since only 1 and 3 are members of both sets A and B.\n• The set difference (⧵or –) of two sets consists of all elements that are members of one set but not the other. In the provided example, the\nset difference A ⧵B (or A – B) consists of the element {2}, since 2 is the only element in set A that is not in set B. The set difference B ⧵\nA (or B – A) consists of the element {5}, since 5 is the only element in set B that is not in set A.\n• The symmetric difference (⊕) of two sets consists of all elements that are in either set but not both. In the provided example, the\nsymmetric difference A ⊕B consists of the elements {2, 5}, since 2 and 5 are the only elements that exist in only one of A or B.\n¸ 13.1.2\nSet Implementation\nHow would you implement a set? There are several ways to implement a set that efficiently supports the operations specified above. For instance,\n(std::unordered_set<>) (std::set<>),the STL implements its sets using hash tables and self-balancing trees both of which will be\ncovered in later chapters. In this chapter, we will consider a simple set implementation that uses a sorted vector as its underlying container.\nSorted vectors work well because binary search (see chapter 15) can be used to check an element’s membership in time.Θ(log(𝑛))", "word_count": 400, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "155412e5-1ba3-5b5b-81d9-81f404f1c2f3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 402, "real_page_number": null, "text": "390\nChapter 13. Sets and Union-Find\nTo find the union of two sets whose elements are stored in a sorted vector, you would first initialize iterators to the beginning of both vectors.\nThen, compare the values at each of these iterators and push the smaller element to a destination vector representing the set union. The iterator\npointing to the element that was pushed in should then be incremented. If the elements at both iterators are identical, only one copy of the\nelement is pushed to the destination vector and both iterators are incremented (this is done to avoid duplication, which is not allowed in a set).\nThis process is repeated until both iterators point to the end of their respective containers. If one iterator reaches the end before the other, iterate\nthrough the remaining elements of the other vector and push them into the destination vector.\nTo illustrate this, consider the following example. Here, set A contains the elements {3, 4, 6, 8, 9, 10} and set B contains the elements {2, 4,\n5, 8}. Both sets are implemented using a sorted vector. We want to build the set A ∪B, which stores the elements in the union of A and B.\nSet A\n3\n4\n6\n8\n9\n10\nSet B\n2\n4\n5\n8\nA ∪B\nFirst, we initialize two iterators at the beginning of the two underlying vectors. The two elements at these iterators are then compared, with the\nsmaller value being sent to the output set A ∪B. This process continues until one of the iterators reaches the end of its respective sorted vector.\nSet A\n3\n4\n6\n8\n9\n10\nSet B\n2\n4\n5\n8\nA ∪B\n2 is smaller than 3, so we push 2 into the output vector and increment the iterator for set B.\nSet A\n3\n4\n6\n8\n9\n10\nSet B\n2\n4\n5\n8\nA ∪B\n2\n3 is smaller than 4, so we push 3 into the output vector and increment the iterator for set A.\nSet A\n3\n4\n6\n8\n9\n10\nSet B\n2\n4\n5\n8\nA ∪B\n2\n3\nBoth iterators now point to the same value, so we push 4 into the output vector and increment both iterators.\nSet A\n3\n4\n6\n8\n9\n10\nSet B\n2\n4\n5\n8\nA ∪B\n2\n3\n4\nContinuing this process, we push 5 into the union, followed by 6. After these values are added, both iterators would point to 8.\nSet A\n3\n4\n6\n8\n9\n10\nSet B\n2\n4\n5\n8\nA ∪B\n2\n3\n4\n5\n6", "word_count": 442, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8d0df69d-e32e-591b-87e4-3b4d38a3c340", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 403, "real_page_number": null, "text": "13.1 Sets and Set Operations\n391\nSince both iterators point to the same value, we push 8 into the output vector and increment both iterators.\nSet A\n3\n4\n6\n8\n9\n10\nSet B\n2\n4\n5\n8\nA ∪B\n2\n3\n4\n5\n6\n8\nAfter 8 is added to the union, notice that the iterator for set B is pointing off the end of the underlying vector. When this happens, all the\nremaining elements in the other set (in this case, set A) are pushed into the union. This is okay since both sets are traversed in sorted order. If\none set reaches the end before another, the other set must contain elements that are larger than the contents of the set that reached the end. Thus,\nthe output vector would remain sorted and all values would remain unique.\nSet A\n3\n4\n6\n8\n9\n10\nSet B\n2\n4\n5\n8\nA ∪B\n2\n3\n4\n5\n6\n8\n9\n10\nA potential implementation of the set union process is shown below. The following code takes in iterators to the underlying containers of two\ncomp(a, b) true a b.sets and an iterator pointing to the output vector. The comparator returns if is less than\n1\ntemplate <class class class classInputIterator1, InputIterator2, OutputIterator, Compare>\n2\nOutputIterator set_union(InputIterator1 first1, InputIterator1 last1,\n3\nInputIterator2 first2, InputIterator2 last2,\n4\nOutputIterator result, Compare comp) {\n5\n// loop until one set reaches the end\n6\nwhile (first1 != last1 && first2 != last2) {\n7\n// if element at first1 is less than element at first2, add element\n8\n// at first1 to result and increment first1\n9\nif (comp(*first1, *first2)) {\n10\n*result++ = *first1++;\n11\n} // if\n12\n// else if element at first2 is less than element at first1, add\n13\n// element at first2 to result and increment first2\n14\nelse if (comp(*first2, *first1)) {\n15\n*result++ = *first2++;\n16\n} // else if\n17\n// else if both elements are equal, add one copy of the element to\n18\n// result and increment both iterators\n19\nelse {\n20\n*result++ = *first1++;\n21\n++first2;\n22\n} // else\n23\n} // while\n24\n// if first2 reaches last2 first, copy remaining elements in container 1\n25\n// to the result set\n26\nwhile (first1 != last1) {\n27\n*result++ = *first1++;\n28\n} // while\n29\n// if first1 reaches last1 first, copy remaining elements in container 2\n30\n// to the result set\n31\nwhile (first2 != last2) {\n32\n*result++ = *first2++;\n33\n} // while\n34\n// returns iterator that points one past the end of the sorted union\n35\n// this is the behavior expected for standard algorithm functions\n36\nreturn result;\n37\n} // set_union()\nset_union() first1 first2What is the time complexity of the function? With each iteration, either or is incremented. As a result, if 𝑚\nset_union()and 𝑛are the sizes of the two sets, the algorithm iterates through a total of 𝑚+𝑛elements. Thus, runs in time.Θ(𝑚+𝑛)", "word_count": 513, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9c14558a-1781-56e7-9e87-9af565531059", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 404, "real_page_number": null, "text": "392\nChapter 13. Sets and Union-Find\nset_union() set_intersection()How would we change the algorithm to a algorithm? When dealing with a set intersection, we\nelseonly add the element to the output vector if it belongs in both sets. Thus, only the block on line 19 of the previous code should write the\nelement to the output vector. We can also get rid of lines 24 to 33; once the first set reaches the end, we know that the elements remaining in the\nset_intersection().other set cannot be in the set intersection. This leads us to the following modified code for\n1\ntemplate <class class class classInputIterator1, InputIterator2, OutputIterator, Compare>\n2\nOutputIterator set_intersection(InputIterator1 first1, InputIterator1 last1,\n3\nInputIterator2 first2, InputIterator2 last2,\n4\nOutputIterator result, Compare comp) {\n5\n// loop until one set reaches the end\n6\nwhile (first1 != last1 && first2 != last2) {\n7\n// if element at first1 is less than element at first2, do not add\n8\n// to result and only increment first1\n9\nif (comp(*first1, *first2)) {\n10\n++first1;\n11\n} // if\n12\n// else if element at first2 is less than element at first1, do not\n13\n// add to result and only increment first2\n14\nelse if (comp(first2, first1)) {\n15\n++first2;\n16\n} // else if\n17\n// else if both elements are equal, add one copy of the element to\n18\n// result and increment both iterators\n19\nelse {\n20\n*result++ = *first1++;\n21\n++first2;\n22\n} // else\n23\n} // while\n24\n// return iterator that points one past the end of the sorted union\n25\nreturn result;\n26\n} // set_intersection()\nset_difference() set_symmetric_difference().The same template can be used to implement and The only difference is in\nset_difference(), if set_union()the values that are added to the result vector. In the case of only the block on line 9 of the code\nwhileshould write its value to the result vector, and the loop on lines 26-28 should be kept.\n1\ntemplate <class class class classInputIterator1, InputIterator2, OutputIterator, Compare>\n2\nOutputIterator set_difference(InputIterator1 first1, InputIterator1 last1,\n3\nInputIterator2 first2, InputIterator2 last2,\n4\nOutputIterator result, Compare comp) {\n5\n// loop until one set reaches the end\n6\nwhile (first1 != last1 && first2 != last2) {\n7\n// if element at first1 is less than element at first2, add element\n8\n// at first1 to result and increment first1\n9\nif (comp(*first1, *first2)) {\n10\n*result++ = *first1++;\n11\n} // if\n12\n// else if element at first2 is less than element at first1, increment first2\n13\nelse if (comp(*first2, *first1)) {\n14\n++first2;\n15\n} // else if\n16\n// else if both elements are equal, increment both iterators\n17\nelse {\n18\n++first1;\n19\n++first2;\n20\n} // else\n21\n} // while\n22\n// if first2 reaches last2 first, copy remaining elts in ctn 1 to result set\n23\nwhile (first1 != last1) {\n24\n*result++ = *first1++;\n25\n} // while\n26\n// returns iterator that points one past the end of the sorted union\n27\nreturn result;\n28\n} // set_difference()", "word_count": 524, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "74593703-9261-571c-9c00-48ab54137fc8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 405, "real_page_number": null, "text": "13.1 Sets and Set Operations\n393\nset_symmetric_difference()Theimplementationof (elementsthatexistineithersetbutnotboth)isalsosimilartotheimplementation\nset_union(), elseof except that the block on line 19 should write a value to the result vector. The code is shown below:not\n1\ntemplate <class class class classInputIterator1, InputIterator2, OutputIterator, Compare>\n2\nOutputIterator set_symmetric_difference(InputIterator1 first1, InputIterator1 last1,\n3\nInputIterator2 first2, InputIterator2 last2,\n4\nOutputIterator result, Compare comp) {\n5\nwhile (first1 != last1 && first2 != last2) {\n6\nif (comp(*first1, *first2)) {\n7\n*result++ = *first1++;\n8\n} // if\n9\nelse if (comp(*first2, *first1)) {\n10\n*result++ = *first2++;\n11\n} // else if\n12\nelse {\n13\n++first1;\n14\n++first2;\n15\n} // else\n16\n} // while\n17\nwhile (first1 != last1) {\n18\n*result++ = *first1++;\n19\n} // while\n20\nwhile (first2 != last2) {\n21\n*result++ = *first2++;\n22\n} // while\n23\nreturn result;\n24\n} // set_symmetric_difference()\n¸ 13.1.3\nSet Operations in the STL\nset_union(),set_intersection(),set_difference(), set_symmetric_Thesefoursetoperationsforsortedcontainers: and\ndifference(), <algorithm>are all implemented in the STL’s library and behave similarly to the examples above. All four functions\ntake in iterator ranges to two sequence containers, an output iterator that points to the destination container, and an optional comparator.sorted\nThese four functions all return an iterator one past the last element of the constructed range.\ntemplate <typename typename typename typenameInputIterator1, InputIterator2, OutputIterator, Compare>\nOutputIterator std::set_union(InputIterator1 first1, InputIterator1 last1,\nInputIterator2 first2, InputIterator2 last2,\nOutputIterator result, Compare comp);\nresult [first1,Constructs a sorted range beginning at with the contents of the set union of the elements in the two sorted ranges\nlast1) [first2, last2)and and returns an iterator one past the last element in the constructed range. The comparator is optional,\noperator<and is used by default if it is not specified.\ntemplate <typename typename typename typenameInputIterator1, InputIterator2, OutputIterator, Compare>\nOutputIterator std::set_intersection(InputIterator1 first1, InputIterator1 last1,\nInputIterator2 first2, InputIterator2 last2,\nOutputIterator result, Compare comp);\nresult [first1,Constructs a sorted range beginning at with the contents of the set intersection of the elements in the two sorted ranges\nlast1) [first2, last2)and and returns an iterator one past the last element in the constructed range. The comparator is optional,\noperator<and is used if it is not specified.\ntemplate <typename typename typename typenameInputIterator1, InputIterator2, OutputIterator, Compare>\nOutputIterator std::set_difference(InputIterator1 first1, InputIterator1 last1,\nInputIterator2 first2, InputIterator2 last2,\nOutputIterator result, Compare comp);\n[first1, last1)Constructs a sorted range beginning at with the contents of the set difference of the elements in the sorted rangeresult\n[first2, last2)with respect to the elements in the sorted range and returns an iterator one past the last element in the constructed\noperator<range. The comparator is optional, and is used by default if it is not specified.\ntemplate <typename typename typename typenameInputIterator1, InputIterator2, OutputIterator, Compare>\nOutputIterator std::set_symmetric_difference(InputIterator1 first1, InputIterator1 last1,\nInputIterator2 first2, InputIterator2 last2,\nOutputIterator result, Compare comp);\nresultConstructs a sorted range beginning at with the contents of the set symmetric difference of the elements in the two sorted ranges\n[first1, last1) [first2, last2)and and returns an iterator one past the last element in the constructed range. The comparator\noperator<is optional, and is used if it is not specified.\nstd::set_union() <algorithm>For example, the following code uses in the library.\n1\nstd::vector<int32_t> v1 = {1, 2, 3};\n2\nstd::vector<int32_t> v2 = {1, 3, 5};\n3\nstd::vector<int32_t> v_union;\n4\nstd::set_union(v1.begin(), v1.end(), v2.begin(), v2.end(), std::back_inserter(v_union));\n5\nfor (int32_t val : v_union) {\n6\nstd::cout << val << \" \";\n// prints 1 2 3 5\n7\n} // for val", "word_count": 611, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "aa969030-9ad7-5dc3-a1cd-a0434bc0d897", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 406, "real_page_number": null, "text": "394\nChapter 13. Sets and Union-Find\n13.2\nUnion-Find and Path Compression\n¸ 13.2.1\nDisjoint Sets and the Union-Find Data Structure\nIn the previous section, we introduced the concept of a set. One benefit of using a set is that we are able to quickly determine if any element is\npart of that set. However, certain problems go beyond simply querying if an element exists in a set.\nOne such problem involves something known as a disjoint set. We consider two sets to be disjoint if they do not share any elements. In\ndisjoint set problems, we are often required to organize a collection of objects into different groups and efficiently determine whether two\nobjects belong to the same group. As an example, suppose we wanted to implement a program that can determine whether two computers in\na network are connected to each other. Consider the following diagram, where each node represents a computer, and each edge represents a\nnetwork connection that exists between two computers.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nGiven this setup, we have two disjoint sets, one that includes computers A-E, and one that includes computers F-K.\nSuppose we were asked if data can be sent through the network from computer C to computer E. How would we answer this question? In\nthis case, the answer would be yes, since computers C and E belong to the same disjoint set; to send data from C to E, we could first send the\ndata from C to B, and then from B to E.\nWhat if we were instead asked if data could be sent through the network from computer C to computer I? The answer to this question would\nbe no, since computers and C and I belong to two different disjoint sets — there is no way to use the existing connections to travel from C to I.\nNow, let’s suppose we added a connection between computers A and K, as shown below.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nWhat happens to our answers to the previous two questions? In the case of computers C and E, the answer would still be yes, since C and E still\nbelong to the same disjoint set. However, the answer for C and I would change to yes, since the new connection ended up joining C and I into\nthe same disjoint set! Even though neither C nor E had their connections directly modified, an external connection caused their disjoint set\nmembership to be the same.\nIf we want to efficiently solve problems involving disjoint sets, we would need to store our data in a container that allows us to keep track of\nmultiple disjoint sets and membership within these sets. This is done using a container known as a union-find data structure (alternatively\nknown as a structure). The union-find data structure is a container that relies on two primary operations, union and find:disjoint-set data\nunion_set(x, y) x y.• is used to merge the set containing element with the set containing element Using the above example,\nK.1union_set(A, K) would add a connection between computers A and\nfind_set(x) x• is used to identify the disjoint set that element belongs to.\nHow should we implement a union-find data structure to support efficient union and find operations? The simplest approach would be to use the\nsame sorted vector approach to keep track of our disjoint sets. Each disjoint set would be stored in its own sorted vector, as shown:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nunion_set()How would union and find be implemented using this approach? If is called on two elements, we would first check if the two\nelements are in the same sorted vector. If they are not, we would have to shift all the elements in one vector to the other to merge the two sets.\nFor example, if we wanted to union computers A and K, we would have to merge the first two sorted vectors (A-E and F-K). This takes linear\nfind_set()time. If is called on an element, we would have to check every vector for the existence of that element. In the worst case, this\nalso takes linear time. Because both union and find are linear-time operations using this approach, this implementation is inefficient, especially\nas the number of disjoint sets in the container increases. It would be better to store all the elements in the container, rather than a separatesame\ncontainer for each disjoint set.\n1The union_set() union() unionreasonweareusing andnot isthatthekeyword isreservedinC++andcannotbeusedtonameacustomfunctionor\nset_union()variable. Similarly, isnotusedbecausethatisthenameofafunctionintheSTLalgorithmlibrary.", "word_count": 827, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "af2e08ac-6fd9-5fd5-82e4-b0c0a5b4d93f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 407, "real_page_number": null, "text": "13.2 Union-Find and Path Compression\n395\nHowever, if we do store all our data in a single container, we will need to store additional information to keep track of which disjoint set each\nelement belongs to. We can do this using a system, where each element in the union-find data structure remembers a uniquerepresentative\nrepresentative that identifies which set it belongs in.\nTo use an analogy, suppose you are in a room full of people from all over the world, and you want to organize the people into disjoint sets\nbased on citizenship. If everyone is mingled together, how would you be able to identify which country everyone is from? One way to solve this\nproblem is to ask each person who the leader of their country is. If they answer with the name of the United States president, they must be from\nthe United States. If they answer with the name of the Canadian Prime Minister, they must be from Canada. You can then use this information\nto organize the people into their correct groups.\nThe representative system works similarly for data in a union-find data structure. Each element in a disjoint set is assigned a representative\nthat acts as the \"leader\" of its set. Every member of the set then remembers who their representative is. An example is shown below, where\ncomputers B, G, and O are arbitrarily chosen as the representative of each of the disjoint sets.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nElement\nRepresentative\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nB\nB\nB\nB\nB\nG\nG\nG\nG\nG\nG\nO\nO\nO\nO\nHere, computer B serves as the representative of all computers in the first disjoint set, computer G serves as the representative of all computers\nin the second disjoint set, and computer O serves as the representative of all computers in the third disjoint set. Compared to the previous\nfind_set()implementation that kept a sorted vector for each disjoint set, the operation is much more efficient with this approach. To\nfind_set()determine which set an element belongs to, would just need to check that element’s representative. To determine if two elements\nare in the same disjoint set, simply compare their representatives. This procedure of forcing each element to remember its ultimate representative\nfind_set()is known as the implementation of union-find, since can be completed in time.quick-find Θ(1)\nHowever, the union operation is still inefficient here. If we wanted to union computers A and K together, we would have to change the\nrepresentatives of all the elements in one of the sets — this would require a linear traversal of the container.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nElement\nRepresentative\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nO\nO\nO\nO\nWe can make the union process faster by removing the need for each element to remember its ultimate representative. It is okay if each element\nremembers an element that may be the ultimate representative itself, as long as the element it remembers either (1) knows who the ultimatenot\nrepresentative is itself or (2) remembers another element that knows who the ultimate representative is. The ultimate representative serves as the\nleader of the entire disjoint set, and its representative is itself. This implementation is known as the implementation of union-find.quick-union\nReturning to the citizenship analogy, this process is similar to asking everyone to name a person who is a citizen of the same country rather\nthan a direct leader (or ultimate representative). The idea here is that we will eventually be able to figure out which country each person belongs\nto just by following the chain of representatives (e.g., if Alice says Bob is her representative, Bob says Cathy is his representative, and Cathy\nsays that the president of the United States is her representative, we would know that Alice, Bob, and Cathy are all Americans).\nunion_set()When using this implementation, a call to would only need to modify the ultimate representative of one of the disjoint sets\nthat was merged. In the example, the ultimate representative of the first disjoint set was computer B, and the ultimate representative of the\nsecond disjoint set was computer G. If a connection were added between computers A and K, we would first check if A and K are already part\nfind_set().the of the same disjoint set by comparing their ultimate representatives using two calls to If they are not part of the same set, we\nwould modify the ultimate representative of either A or K so that it refers to the ultimate representative of the other disjoint set.", "word_count": 814, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "aefa32b2-94d7-5dcd-9801-3a2364e8e07c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 408, "real_page_number": null, "text": "396\nChapter 13. Sets and Union-Find\nIn the following example, since K’s ultimate representative is G and A’s ultimate representative is B, we could change G’s representative to B.\nNote that we could have changed B’s representative to G instead; the decision to change G’s representative instead of B’s was made arbitrarily.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nElement\nRepresentative\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nB\nB\nB\nB\nB\nG\nB\nG\nG\nG\nG\nO\nO\nO\nO\nThis reduces the amount of work needed for union, since we no longer need to traverse the entire container and reset the representative of every\nelement whose representative was G. However, the find operation will likely be slower than before, since we may need to traverse an entire\nchain of elements to find the disjoint set that an element belongs to. For instance, suppose we called find on computer F. We know that F’s\nrepresentative is G. However, G’s representative was changed to B. Because of this, we know that computer F is a member of the disjoint set\nwhose ultimate representative is B (we know B is an ultimate representative because its representative is itself).\nHowever, this is a simple case. Since we removed the requirement for each element to directly remember its ultimate representative, we\ncould end up with something like this:\nA\nB\nC\nD\nE\nF\nG\nH\nElement\nRepresentative\nA\nB\nC\nD\nE\nF\nG\nH\nA\nA\nB\nC\nD\nE\nF\nG\nfind_set()Suppose we called on computer H using this setup. H’s representative is G, but G’s representative is F, and F’s representative is\nE, and E’s representative is D, we would have to traverse through every element in the container to figure out that H’s ultimate representative…\nis computer A. This is a worst-case traversal for a single find operation! In addition, since union itself calls find twice (to determine theΘ(𝑛)\nultimate representatives of the two elements to join), the union operation here also runs in time (even if it is slightly faster on average inΘ(𝑛)\nmost situations). How can we adjust our implementation so that both union and find can be done efficiently? The solution is to use a strategy\nknown as compression.path\n¸ 13.2.2\nPath Compression\nWhen we forced each element to directly remember its ultimate representative, find could be done quickly, but union was inefficient. However,\nif we removed this requirement and allowed elements to store any representative that belongs to its own disjoint set, union became faster but find\nbecame less efficient.\nA key insight to notice here is that, even though it took a traversal to learn that computer H’s ultimate representative was computer A,Θ(𝑛)\nfind_set()this work only needs to be done once. If we call on H again, we should not need to traverse the entire chain of elements again;\nfind_set()we already know that its ultimate representative is A! As a result, after calling on H the first time, we can speed up subsequent\nfind_set() find_set()calls to by changing H’s representative to A, so that further calls on H would no longer require a traversal.Θ(𝑛)\nA\nB\nC\nD\nE\nF\nG\nH\nElement\nRepresentative\nA\nB\nC\nD\nE\nF\nG\nH\nA\nA\nB\nC\nD\nE\nF\nA\nHowever, that is not all we can do! By traveling down the entire chain of elements, we learned more than the fact that H’s ultimate representative\nis A. Because H’s old representative was G, H and G must be part of the same disjoint set. Thus, if H’s ultimate representative is A, G’s ultimate\nrepresentation must also be A. And since G’s representative is F, G and F must be part of the same disjoint set, so F’s ultimate representative\nmust also be A. Not only can we change the representative of H to A, but we can change the representative of every element on the entire chain\nto A as well!\nA\nB\nC\nD\nE\nF\nG\nH\nElement\nRepresentative\nA\nB\nC\nD\nE\nF\nG\nH\nA\nA\nA\nA\nA\nA\nA\nA", "word_count": 713, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2272f3fd-13e1-5645-9911-7b9d379f6527", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 409, "real_page_number": null, "text": "13.3 Implementing a Union-Find Container\n397\nfind_set()Now, every subsequent call to can be done quicker since every node now stores its ultimate representative. This process of\nmoving elements closer to its ultimate representative — so that future calls to find will require less work — is known as path compression. The\nbenefit of path compression is further illustrated in the figures below.\nSuppose we have a large disjoint set of people in our union-find container. Person A is the ultimate representative of everyone in the set.\nPerson B has person A as their representative, person C has person B as their representative, and person D has person C as their representative.\nfind_set()If we call on person D to identify the disjoint set they belong to, we would move up the tree from D and discover that person A\nis the ultimate representative. However, after this call to find, we would also discover that persons B, C, and D all share the same ultimate\nfind_set()representative! If we ever call on B, C, or D, we would not want to repeat the work of walking up the tree to find the ultimate\nrepresentative. With path compression, we can bring B, C, and D closer to their ultimate representative so that future calls to find run faster.\nThis is done by updating each of their representatives to person A, as shown below:\n13.3\nImplementing a Union-Find Container\nIn this section, we will implement a union-find data structure using the concepts covered in the previous section. Suppose we are given a\ncollection of 𝑛elements that we want to organize into disjoint sets:\n0\n1\n2\n3\n4\n…\n𝑛−1\nAs mentioned before, the union-find container will need to keep track of each element’s representative, so we will initialize an underlying vector\nUnionFindof indices as a private member variable of the class. Each index of the vector stores the representative of the element at that index.\n1\nclass UnionFind {\n2\n// vector that keeps track of representatives\n3\n// the value at index i is the index of the ith element's representative\n4\nstd::vector<int32_t> reps;\n5\n...\n6\n};\nrepsHow should the vector be initialized? We do not want to default initialize all the representatives to 0, since that would imply that element\n0 is the representative of every element in the collection. Instead, we want each element to be in its own disjoint set (of size 1) when the\ni repscontainer is first initialized. This means that every element starts with as its representative (that is, the value at index of the vectoritself\ni).should be initialized to\n0\n1\n2\n3\n4\n5\n6\n7\n…\nn-1\n[0] [1] [2] [3] [4] [5] [6] [7]\n[n-1]", "word_count": 461, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "56ab587e-72cd-50fe-971c-65a70d4221a3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 410, "real_page_number": null, "text": "398\nChapter 13. Sets and Union-Find\nrepsThe following constructor takes in a size and initializes the representatives in the vector:\n1\nclass UnionFind {\n2\nstd::vector<int32_t> reps;\n3\npublic:\n4\n// the reps vector should have each index initialized to the\n5\n// value of the index itself\n6\nUnionFind(int32_t size) {\n7\nreps.reserve(size);\n8\nfor (int32_t i = 0; i < size; ++i) {\n9\n// at the beginning, every node represents itself\n10\nreps.push_back(i);\n11\n} // for i\n12\n} // UnionFind()\n13\n...\n14\n};\nfor std::iota()The loop can also be condensed into one line using the function:\nstd::iota(reps.begin(), reps.end(), 0);\nunion_set() find_set()Now, we have to implement the and methods for our union-find container. To implement these, let’s first look\nat how these operations should operate visually. Suppose we had five elements that all start off disjoint in their own set.\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\nunion_set()Suppose we called on elements 3 and 4. When this happens, 3 and 4 are merged into the same set. To reflect this change in our\nunion-find container, we would need to change 4’s representative to 3 to indicate that 3 and 4 are together in the same set (note that this choice\nto update 4 is arbitrary; we could have instead updated 3’s representative to 4 if we wanted to).\n0\n1\n2\n3\n3\n0\n1\n2\n3\n4\nNow, suppose we union elements 2 and 3. To update our union-find container, we will update 3’s representative to 2 to indicate that 2 and 3\nbelong to the same set.\n0\n1\n2\n2\n3\n0\n1\n2\n3\n4\nNotice that elements 2, 3, and 4 form a set in this container, with 2 as the ultimate representative of every element in this set. We know this\nbecause 2 is the only element in this set whose ultimate representative is itself.\nContinuing on, if we unioned together elements 1 and 2, 2’s representative would be updated to 1:\n0\n1\n1\n2\n3\n0\n1\n2\n3\n4\nfind_set()Now, let’s call on element 4. When we call find, we want to return the ultimate representative of 4, which serves as an identifier\nfor the disjoint set it belongs in. To do this, we will search up the chain of representatives from 4 until we encounter an element whose\nrepresentative is itself. Here, 4’s representative is 3, 3’s representative is 2, 2’s representative is 1, and 1’s representative is itself. Thus, 1 is the\nultimate representative is 4.\nHowever, that is not all we have to do. If we were to call find on 4 again, we would not want to look down its chain of representatives all\nover again. To address this, we can implement path compression and change the representative of every element along the path from 4 to 1 to\nthe ultimate representative of 1.\n0\n1\n1\n1\n1\n0\n1\n2\n3\n4", "word_count": 500, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "33e77ed5-3cc9-5132-8264-a8725721977a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 411, "real_page_number": null, "text": "13.3 Implementing a Union-Find Container\n399\nNow, let’s union together 0 and 4. This is a bit tricky, since we cannot just change 4’s representative to 0. Doing so would produce the following\nincorrect result:\n0\n1\n1\n1\n0\n0\n1\n2\n3\n4\nIf we just changed 4’s representative, 4 would not longer be in the same set as 1, 2, and 3 after the union call. This is invalid, since 4 was moved\nout of its original set, which should not happen. Instead, the intended behavior of unioning 0 and 4 is for all five elements to be in the same set.\nTo fix this issue, we should update the representative of 4’s representative, rather than 4 itself. In this case, 1 is the ultimateultimate\nrepresentative of 4, so we modify 1’s representative to 0. By doing so, 4 is able to join 0 without breaking its previous connections to 1, 2, and 3.\n0\n0\n1\n1\n1\n0\n1\n2\n3\n4\nfind_set() union_set() find_set(),Wenowhaveenoughinformationtoimplementthe and functionsoftheunion-findcontainer. With\nwe would first find the ultimate representative of a given element by looping through its representatives until we find an element that represents\nitself. Then, we would utilize path compression to set every element along the chain of representatives we visited to this ultimate representative.\n1\n// find_set(x) returns the ultimate representative of x\n2\nint32_t find_set(int32_t x) {\n3\nint32_t current = x;\n4\n// pass 1: find the ultimate representative\n5\nwhile (reps[x] != x) {\n6\nx = reps[x];\n7\n} // while\n8\n// x now stores the ultimate representative\n9\n// pass 2: path compression, set all reps along path to x\n10\nwhile (reps[current] != x) {\n11\nint32_t next = reps[current];\n12\nreps[current] = x;\n13\ncurrent = next;\n14\n} // while\n15\n// return the ultimate representative\n16\nreturn x;\n17\n} // find_set()\nWe can also use recursion to shorten the find process into the following lines of code:\n1\nint32_t find_set(int32_t x) {\n2\nif (x == reps[x]) {\n3\nreturn x;\n4\n} // if\n5\nreps[x] = find_set(reps[x]);\n6\nreturn reps[x];\n7\n} // find_set()\nunion_set()?What about To implement the union operation, we would want to find the ultimate representative of each element we want to\nunion and update one representative to point to the other representative:\n1\n// union_set() unions the elements x and y into the same set\n2\nvoid union_set(int32_t int32_tx, y) {\n3\nint32_t x_rep = find_set(x);\n4\nint32_t y_rep = find_set(y);\n5\nreps[y_rep] = x_rep;\n6\n} // union_set()\ny’sIn the above implementation, we arbitrarily decided that representative will always be the one that is updated. This works, but it is not\nalways ideal. In the next section, we will discuss methods to strategically select which representative gets updated in order to further improve\nthe efficiency of union-find.", "word_count": 495, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bc4ccd62-bb15-5271-8b62-48a5610e055e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 412, "real_page_number": null, "text": "400\nChapter 13. Sets and Union-Find\nExample 13.1 The EECS 281 staff is testing out a brand new social media site to promote social interaction among EECS students!\nSuppose there are a total of 𝑛students that are invited to the site, and each student is assigned a unique integer ID from 0 to 𝑛−1. You are\nnum_students,given the number of students in the network, and a vector of activity logs, where each log stores an integer timestamp\nand the IDs of two students on the site:\n1\nstruct Log {\n2\nint32_t timestamp;\n3\nint32_t id_A;\n4\nint32_t id_B;\n5\n};\nLog id_A id_BEach objectrepresentsthetimewhenstudents and becamefriends. ThetimestampisrepresentedinYYYYMMDDformat\n(e.g., February 26, 2020 is represented as the integer 20200226). This format allows you to compare timestamps by directly comparing the\nintegers themselves. Friendship is also symmetric: if A is friends with B, then B is friends with A.\nLet’s say that person A is with person B if A and B are in the same friend group; that is, if A is friends with B, or A is a friend ofacquainted\nearliest_acquaintance()someone acquainted with B. Implement the function, which returns the earliest timestamp for which\n-1every person in the network is acquainted with every other person. Return if there is no such earliest time.\nint32_t int32_tearliest_acquaintance(std::vector<Log>& friendships, num_students);\nnum_students=5 friendships [{20200229,2,3},{20200227,1,4},{20200303,0,3},Example1: Given and =\n{20200228, 0, 4}, {20200301, 1, 2}, {20200226, 0, 2}], 20200229,you would return since that is the first timestamp for\nwhich all five students are acquained (on the 29th, person 0 is direct friends with 2 and 4, acquainted to 1 via 4, and acquainted to 3 via 2).\nnum_students = 5 friendships [{20200304, 1, 4}, {20200307, 0, 2}, {20200306, 3,Example 2: Given and =\n4}], -1.you would return This is because person 0 and person 2 are in a separate friend group and are never acquainted with 1, 3, or 4.\nIn this example, we want to find the earliest timestamp during which every student on the network is acquainted. The key insight to notice is that\ntwo students are acquainted if they are part of the same disjoint set! This makes the problem a good candidate for the union-find data structure.\nLet us look at the examples visually. Consider the first example: at the very beginning, no one is friends with anyone else, and thus none of\nstudents start off as acquaintances.\n0\n1\n2\n3\n4\nCurrent timestamp: < 20200226\nOn the 26th, the earliest timestamp in the vector, 0 becomes friends with 2. As a result, these two students become part of the same friend group\n(i.e., the same disjoint set), and thus are acquainted.\n0\n1\n2\n3\n4\nCurrent timestamp: 20200226\nOn the 27th, 1 becomes friends with 4, forming another distinct friend group:\n0\n1\n2\n3\n4\nCurrent timestamp: 20200227\nOn the 28th, 0 becomes friends with 4. Notice that this combines the two exisiting friend groups; at this point, all but student 3 are acquainted.\n0\n1\n2\n3\n4\nCurrent timestamp: 20200228\nOn the 29th, 2 becomes friends with 3, which ends up joining 3 with the friend group containing all the other students. There is only one disjoint\nset remaining after this friendship, so everyone is acquainted, and we can return the timestamp of 20220229.\n0\n1\n2\n3\n4\nCurrent timestamp: 20200229", "word_count": 589, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7f4301ff-732f-5a76-b4e4-a5982e55a8da", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 413, "real_page_number": null, "text": "13.3 Implementing a Union-Find Container\n401\nLet’s consider the second example. Again, no one is friends with each other at the beginning, so each student starts off in their own disjoint set.\n0\n1\n2\n3\n4\nCurrent timestamp: < 20200304\nOn the 4th, 1 becomes friends with 4, which puts them in the same disjoint set.\n0\n1\n2\n3\n4\nCurrent timestamp: 20200304\nOn the 6th, 3 becomes friends with 4, which adds 3 to the disjoint set containing 1 and 4.\n0\n1\n2\n3\n4\nCurrent timestamp: 20200306\nOn the 7th, 0 becomes friends with 2, forming their own disjoint set.\n0\n1\n2\n3\n4\nCurrent timestamp: 20200307\nHowever, no additional friendships are made. There are multiple disjoint sets left over, so not every person in the network is acquainted with\n-1.every other person (e.g., 0 and 2 are not acquainted with 1, 3, or 4). Because of this, we will return\nFrom these examples, we can come up with the steps needed to solve this problem. Our algorithm is as follows:\n1. Sort the vector of logs by timestamp — this allows us to traverse the logs from the earliest timestamp to the latest timestamp.\n2. Initialize the students using a union-find container, with each student starting off as its own representative (i.e., each student initially\nbelongs in its own disjoint set).\n3. Loop over the vector of logs. Every time you encounter a new friendship, union them together.\n4. If only one disjoint set remains, return the timestamp at which this occurs. Otherwise, if all the logs are traversed and there are still\n-1.multiple disjoint sets, return\nThe final step is not as straightforward as it seems: how do we determine if there is only one disjoint set remaining? This can be done by\nidentifying the number of ultimate representatives that exist in our union-find container. Recall that the ultimate representative is the element\nthat serves as the \"leader\" of its disjoint set, and they have the unique property of being their own ultimate representative. However, if we want\nto count the number of elements whose ultimate representative is equal to itself, we would have to iterate over the entire union-find container —\nthis would take time given 𝑛students. Is there a way to do better?Θ(𝑛)\nInstead of iterating over all the students and counting the number of ultimate representatives after every new friendship, a more efficient\napproach would be to count the number of times two students in disjoint sets are unioned together.different This works because the total number\nSince we start off with 𝑛disjoint sets (as no oneof disjoint sets decreases by one every time two non-acquainted students are unioned together.\nis friends with each other), we are guaranteed to have one disjoint set remaining after making exactly connections between non-acquainted𝑛−1\nfind_set()students. Notice that we can easily determine whether two students are non-acquainted or not by calling on each of them and\nverifying that their ultimate representatives are different. By utilizing this strategy, we would only need to increment or decrement a counter\nafter every friendship, which takes time.Θ(1)\nThere are two ways to keep track of the counter: you can either start the counter at 0 and increment the counter every time you union two\nnon-acquainted students, stopping once the counter reaches 𝑛−1; or, you can start the counter at 𝑛and decrement the counter every time you\nunion two non-acquainted students, stopping the counter once you reach 1. The first approach counts the number of non-acquainted friendships\nthat are made, while the second approach counts the total number of disjoint sets that exist among the students. Both approaches would get you\nto the same answer. A full implementation of the problem solution is shown below:\n1\nstruct LogComp {\n2\nbool operator() (const const constLog& lhs, Log& rhs) {\n3\nreturn lhs.timestamp < rhs.timestamp;\n4\n} // operator()()\n5\n};\n6\n7\n// find the representative (using path compression)\n8\nint32_t find(std::vector<int32_t>& int32_treps, id) {\n9\nif (id == reps[id]) {\n10\nreturn id;\n11\n} // if\n12\nreps[id] = find(reps, reps[id]);\n13\nreturn reps[id];\n14\n} // find()\n15\n16\n// ... the rest of the solution is continued on the next page ...", "word_count": 716, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c2fd363a-26f4-51cf-8d7f-a8354c5f0a91", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 414, "real_page_number": null, "text": "402\nChapter 13. Sets and Union-Find\n17\nint32_t int32_tearliest_acquaintance(std::vector<Log>& friendships, num_students) {\n18\nstd::sort(friendships.begin(), friendships.end(), LogComp()); // can also use a lambda\n19\nstd::vector<int32_t> reps(num_students);\n20\nint32_t count = 0; // can also start at n and decrement\n21\nfor (int32_t i = 0; i < num_students; ++i) {\n22\n// set each student to its own representative (can also use std::iota() to save lines)\n23\nreps[i] = i;\n24\n} // for i\n25\nfor (const Log& f : friendships) {\n26\nint32_t rep_A = find(reps, f.id_A);\n27\nint32_t rep_B = find(reps, f.id_B);\n28\n// if rep_A and rep_B are different, then A and B are part of different disjoint sets when they\n29\n// are unioned together, so increment the counter (or decrement if you started the counter at n)\n30\nif (rep_A != rep_B) {\n31\nreps[rep_B] = rep_A;\n32\n++count;\n33\n} // if\n34\n// done when n - 1 connections are made (check count == 1 if decrementing from n)\n35\nif (count == num_students - 1) {\n36\nreturn f.timestamp;\n37\n} // if\n38\n} // for f\n39\n// return -1 if all logs are visited and fewer than n - 1 non-acquainted unions are made\n40\nreturn -1;\n41\n} // earliest_acquaintance()\n13.4\nUnion-by-Size and Union-by-Rank (✽)\nSo far, our union implementation blindly sets the ultimate representative of the second disjoint set to that of the first. However, this process\ncould still give a worst-case scenario where union and find both take time.Θ(𝑛)\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\nSuppose we called the following sequence of union operations:\nunion_set(3, 4)•\nunion_set(2, 3)•\nunion_set(1, 2)•\nunion_set(0, 1)•\nIf we did this, 4’s representative would become 3, 3’s representative would become 2, 2’s representative would become 1, and 1’s representative\nwould become 0. This is shown accordingly:\n0\n0\n1\n2\n3\n0\n1\n2\n3\n4\nWe have now run into a situation where the worst-case complexity of union and find both become Θ(𝑛). If you wanted to call find on element 4,\nyou would have to do a linear traversal of the entire vector to discover that 4’s ultimate representative is 0.\nAt this point, you may be wondering: shouldn’t path compression handle this case? After we do the traversal, we would change theΘ(𝑛)\nrepresentatives of all the elements to 0 so that future calls to union and find become cheaper — as such, wouldn’t we need to do the traversal\nonly once? Although this is true, the issue is that we may have to do a traversal in the first place! Even though path compression improvesΘ(𝑛)\nthe cost of union and find, it does not improve the cost of either operation. That is, path compression does not prevent youamortized worst-case\nfrom ending up with a worst-case \"stick\" formation of elements, such as in the example above. To address this, there are two optimization\nstrategies we can use: and union-by-rank.union-by-size\n¸ 13.4.1\n(✽)Union-by-Size\nIn a union-by-size approach, we additionally keep track of the sizes of each disjoint set in our union-find data structure. When two disjoint sets\nare unioned together, we always update the representative of the smaller set (ties are broken arbitrarily). Consider the same sequence of union\noperations from above, but this time using union-by-size to determine which representative is updated.\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4", "word_count": 578, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3c3ea359-0c0a-55d7-a01e-4f3efd71832d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 415, "real_page_number": null, "text": "13.4 Union-by-Size and Union-by-Rank\n403\nWhen we union 3 and 4, are are able to arbitrarily select which representative we want to update (since both sets are the same size). In this case,\nlet’s suppose we updated 4’s representative to be 3.\n0\n1\n2\n3\n3\n0\n1\n2\n3\n4\nNow, we want to union 2 and 3. The disjoint set that 2 belongs to has a size of 1, and the disjoint set that 3 belongs to has a size of 2. Since 2’s\ndisjoint set is smaller, we assign 2’s representative to 3’s ultimate representative, or 3.\n0\n1\n3\n3\n3\n0\n1\n2\n3\n4\nNow, we want to union 1 and 2. The disjoint set that 1 belongs to has a size of 1, and the disjoint set that 2 belongs to has a size of 3. Since 1’s\ndisjoint set is smaller, we assign 1’s representative to 2’s ultimate representative, or 3.\n0\n3\n3\n3\n3\n0\n1\n2\n3\n4\nNow, we want to union 0 and 1. The disjoint set that 0 belongs to has a size of 1, and the disjoint set that 2 belongs to has a size of 4. Since 0’s\ndisjoint set is smaller, we assign 0’s representative to 1’s ultimate representative, or 3.\n3\n3\n3\n3\n3\n0\n1\n2\n3\n4\nNotice here that we no longer have a stick-like chain of representatives. In fact, if union-by-size is used, the worst-case time complexity of\na single union or find call drops from to Θ(log(𝑛)), where 𝑛is the number of elements in the container. This is because union-by-sizeΘ(𝑛)\nensures that the longest possible chain of representatives has a length of at most Θ(log(𝑛)).\n¸ 13.4.2\n(✽)Union-by-Rank\nAnother optimization technique is union-by-rank, which keeps track of the approximate height (or rank) of each disjoint set in the data\nstructure instead of its size. When two disjoint sets are unioned together under the union-by-rank optimization approach, we always update the\nrepresentative of the set with the smaller rank. Some examples of rank are shown below.\nRank = 0\nA\nRank = 1\nB\nE\nD\nC\nRank = 2\nF\nI\nM\nH\nL\nK\nG\nJ\nRank = 3\nN\nQ\nP\nS\nT\nR\nO\nFor instance, if we wanted to union elements F and N, we would update F’s representative to N because F’s disjoint set has a smaller rank. The\nrank of N, however, would still be 3 after the sets are joined together.\nRank = 3\nN\nQ\nP\nS\nT\nR\nO\nF\nI\nM\nH\nL\nK\nG\nJ", "word_count": 443, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "68092a8e-e477-5256-a9e7-527457041be9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 416, "real_page_number": null, "text": "404\nChapter 13. Sets and Union-Find\nRank is incremented if two disjoint sets with the same rank are unioned together. When unioning two disjoint sets with the same rank, it does\nnot matter which representative is updated to the other.\nRank = 3\nN\nQ\nP\nS\nT\nR\nO\nRank = 3\nU\nW\nV\nY\nX\nZ\nRank = 4\nN\nQ\nP\nS\nT\nR\nO\nU\nW\nV\nY\nX\nZ\nNote that we consider rank as an for the longest representative chain that exists between an element and its ultimate representative.approximation\nfind_set(T)This is because rank is recomputed if path compression changes the actual maximum length. For example, if we call on thenot\nfollowing disjoint set using union-by-rank, both S and T’s representatives would change to N, but the rank of the entire set would still be 3.\nRank = 3\nN\nQ\nP\nS\nT\nR\nO\nRank = 3\nN\nQ\nT\nS\nP\nR\nO\nfind_set(T)\n¸ 13.4.3\n(✽)Union-Find Complexity Analysis\nSo far, we have discussed several union-find optimization strategies, including path compression, union-by-size, and union-by-rank. Path\ncompression can be used to move elements closer to their ultimate representatives, which speeds up future calls to union and find. Union-by-size\nor union-by-rank can be used to build the connections in a way that prevents the length of any representative chain from growing beyond\nΘ(log(𝑛)). However, how do these optimizations impact the overall time complexities of union and find?\nThe answer is not trivial, so we will not be going into the full details here. If we look at a union-find container that only implements path\ncompression but not union-by-rank or union-by-size, then the time complexity of union operations and 𝑚find operations is bounded by𝑛−1\n𝑚≥𝑛.for 𝑛, and for On a similar vein, if we have a union-find container that only implements union-by-rankΘ(𝑛+𝑚log(𝑛)) 𝑚< Θ(𝑚log(𝑛))\nor union-by-size, but not path compression, then the time complexity of union operations and 𝑚find operations is also bounded by a𝑛−1\nworst-case time complexity of Θ(𝑚log(𝑛)). This is because the costs of find and union are both bounded by the largest rank or size among the\ndisjoint sets, which is itself bounded by Θ(log(𝑛)).\nHowever, if we combine path compression with either union-by-size or union-by-rank, the time complexity of any 𝑚union-find operations\ndrops down to Θ(𝑚𝛼(𝑛)), which also means that the amortized costs of find and union both become Θ(𝛼(𝑛)). Here, represents the inverse𝛼(𝑛)\nAckermann function, which is a function that grows so slowly that its value can practically be treated as a constant for reasonable values of 𝑛.\nfind_set() union_set()In other words, if path compression is combined with union-by-size or union-by-rank, and can both be done in\ntime!2(essentially) amortized constant\nDisjoint sets and union-find have practical use cases in several important computer science problems, such as image processing or counting\nconnected components. We will revisit the concept of union-find in chapter 20 when we discuss Kruskal’s algorithm, which can be used to find\nthe minimum cost required to connect all nodes in a graph.\n2Theanalysesinthissectionwereobtainedfromtheresearchpaper\"Worst-CaseAnalysisofSetUnionAlgorithms\"byTarjanandvanLeeuwen(1984),and\nchapter21of\"IntroductiontoAlgorithms(Thirded.) byCormen,Leiserson,Rivest,andStein(2009).", "word_count": 555, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab2b798b-7eaf-55a9-83d1-95dab693cdcf", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 417, "real_page_number": null, "text": "13.4 Union-by-Size and Union-by-Rank\n405\nChapter 13 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Let 𝐹represent the set of all students enrolled in the Fall 2017 semester of EECS 281, let 𝑊represent the set of all students enrolled in the\nWinter 2018 semester of EECS 281, and let 𝑆represent the set of all students enrolled in the Spring 2018 semester of EECS 281. Which of\nthe following student would NOT be included in the set (𝐹∩𝑊)∪𝑆?\nA) A student who took EECS 281 only once and passed during the Winter 2018 semester\nB) A student who took EECS 281 only once and passed during the Spring 2018 semester\nC) A student who took but did not pass EECS 281 during the Fall 2017 semester and retook it during the Winter 2018 semester\nD) A student who took but did not pass EECS 281 during the Winter 2018 semester and retook it during the Spring 2018 semester\nE) More than one of the above\n2. You are given two sorted vectors of integers, both of size 𝑛, and you want to devise an algorithm that can identify all items in one vector but\nnot the other. Assuming that you are not allowed to create any additional containers, what is the worst-case time complexity of the most\nefficient algorithm that is attainable for solving this problem?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n3. You are given two unsorted vectors of integers, both of size 𝑛, and you want to devise an algorithm that can identify all items are either\nthe first or second vector, but not both. Assuming that you are not allowed to create any additional containers, what is the worst-case time\ncomplexity of the most efficient algorithm that is attainable for solving this problem?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\nstd::set_union(), std::set_intersection(),4. The STL’s set algorithms of etc., rely on which of the following assumptions\nabout their input to produce the desired behavior?\nI. The two input ranges passed in must already have their values sorted.\nII. The two input ranges passed in must be stored in containers that support random access.\nIII. The two input ranges passed in must contain the same number of elements.\nA) I only\nB) III only\nC) I and II only\nD) I and III only\nE) I, II, and III\n5. Which of the following could be a practical application of the union-find data structure?\nA) Implementing a image editing program that identifies and updates regions of an image with a similar color\nB) Implementing a networking program that identifies groups of servers that can communicate with each other\nC) Implementing an algorithm that can be used to identify people in a social network with similar interests\nD) Implementing an algorithm that counts the number of connected components in a graph\nE) More than one of the above\n6. Which of the following statements is TRUE about an implementation of the union-find container that forces every element in the data\nstructure to keep track of the representative of its disjoint set, if no path compression is used?ultimate\nA) The find operation may take up to time, given 𝑛elements in the union-find containerΘ(𝑛)\nB) The union operation may take up to time, given 𝑛elements in the union-find containerΘ(𝑛)\nC) The amortized time complexities of find and union are both Θ(𝑛), given 𝑛elements in the union-find container\nThe amortized time complexities of find and union are both Θ(𝛼(𝑛)), given 𝑛elements in the union-find container, where denotesD) 𝛼(𝑛)\nthe inverse Ackermann function\nE) More than one of the above\n7. Consider a union-find container with 𝑛elements, each with its representative set to itself. Assuming path compression is used, and only one\nunion_set()call to is invoked with two arbitrary elements in this container (with no other union or find operations completed before or\nafter), what is the largest possible number of elements that could have their representatives modified after this function call?\nA) 0\nB) 1\nC) 2\nD) 𝑛−1\nE) 𝑛", "word_count": 721, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "11df21b3-7d81-5d5a-a192-ca9728ae3a56", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 418, "real_page_number": null, "text": "406\nChapter 13. Sets and Union-Find\n8. Consider a union-find container where, in addition to remembering one of its representatives, each element also keeps track of the size\nof the disjoint set that it belongs to. When elements from two disjoint sets are unioned together, the representative of the smaller set is\nalways updated to the representative of the larger set (with ties broken arbitrarily). Under this implementation, what is the worst-case time\nfind_set()complexity of a single call to on this union-find container? Note that represents the inverse Ackermann function.𝛼(𝑛)\nA) Θ(1)\nB) Θ(𝛼(𝑛))\nC) Θ(log(𝑛))\nD) Θ(𝑛)\nE) Θ(𝑛𝛼(𝑛))\n9. The following represents the state of a union-find container:\nItem\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nRepresentative\n5\n6\n7\n2\n1\n3\n6\n9\n8\n8\nAmong these 10 elements, how many disjoint sets are there?\nA) 0\nB) 1\nC) 2\nD) 3\nE) 4\n10. Consider the same union-find container as in question 9. Suppose you wanted to merge all of the disjoint sets among these elements into a\nsingle set. What is the number of union operations necessary to accomplish this?minimum\nA) 0\nB) 1\nC) 2\nD) 3\nE) 4\n11. Consider the same union-find container as in question 9. Suppose you used the path compression approach when you implemented\nunion-find. If you were to call the find function on item 0, which of the following statements would be TRUE?\nA) 0’s representative would become 3\nB) 5’s representative would become 7\nC) 4’s representative would become 6\nD) 2’s representative would become 8\nE) More than one of the above\n12. Consider the same union-find container as in question 9. Suppose you used the path compression approach when you implemented\nunion-find. Which of the following operations would NOT change any of the representatives given in the above table?\nA) Calling find on item 7\nB) Calling find on item 4\nC) Calling find on item 3\nD) Calling find on item 1\nE) None of the above\n13. You are given a union-find data structure, which implements path compression. There are 281 elements in the container, and 203 of these\nelements have themselves as their ultimate representative. What is the largest possible size that a disjoint set within this container can have?\nA) 77\nB) 78\nC) 79\nD) 202\nE) 203\n14. You are given a union-find data structure, which implements path compression (but no other optimizations). There are 370 elements in\nfind_set()the container, and 280 of these elements have themselves as their ultimate representative. You perform one call to on this\ncontainer with an arbitrary element (with no other operations completed). What is the largest possible number of elements that could\nfind_set()?potentially have its representative changed with this call to\nA) 90\nB) 91\nC) 279\nD) 280\nE) 370", "word_count": 478, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6697fc18-1960-5291-94db-aa5440d68443", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 419, "real_page_number": null, "text": "13.4 Union-by-Size and Union-by-Rank\n407\n15. You are given a union-find data structure, which implements path compression. There are five elements in this container: A, B, C, D, and E.\nItem A has a representative of E, item B has a representative of A, item C has a representative of B, and item D has a representative of D.\nunion_set()After calling on items C and D, you notice that the ultimate representative of item D was changed from D to E. Which of\nthe following statements is TRUE?\nunion_set(),A) Before the call to there were three disjoint sets in this union-find container\nunion_set(),B) Before the call to items D and E were part of the same disjoint set\nunion_set(),C) Before the call to items C and E were part of different disjoint sets\nunion_set(),D) Before the call to the ultimate representative of item E was C\nunion_set(),E) Before the call to the ultimate representative of item E was E\nequations equations[i]16. You are given an array of strings that represent relationships between variables, where is a string of length\n\"x==y\" \"x!=y\" x y4 that takes on one of two different forms: or (where and can be interchanged with any other character from a to z,\nwhich you may assume will always be lowercase). Implement a function that returns whether it is possible to satisfy all the given equations.\n\"a==b\", \"b==c\" \"a!=c\", false,For example, if you are given the equations and you would return since there is no way to satisfy all\ntrue.three of these equations. However, if you were just given the first two equations, you would return\nbool are_equations_satisfiable(const std::vector<std::string>& equations);\nfirewall,17. You are given a list of 𝑛computers, labeled from 0 to 𝑛−1. You are also given a two-dimensional vector of integers where\nfirewall[i] = [x, y] x yindicates that computers and cannot be connected at all, either directly or indirectly through other\ncomputers within the same network.\nInitially, none of the computers are connected to each other. You are given a list of connections that you are requested to make in the\nrequests, requests[i] = [u, v] u v.form of a vector where is a request to add a network connection between computers and A\nu v firewallconnection request is successful if and can be connected without breaking any of the restrictions specified in the vector.\nEach request is processed in the order specified, and upon a successful request, the two computers involved retain their direct connection for\nall future requests.\nithresult, result[i]Return a boolean array where is true if the request is successful, and false if it is not. You may assume that\nrequests to connect two computers that are directly connected are trivially successful.\n[[0, 1]], [[1, 2], [0, 2]],For example, given computers, the following firewall restrictions: and the following requests:𝑛=3\n[true, false].you would return This is because the first request is successful, since connecting computers 1 and 2 does not break any\nfirewall rules. However, the second request is not successful, since computer 2 is now connected to computer 1 via the first request, and\nconnecting 2 with 0 would break the restriction that computers 0 and 1 cannot be connected in any fashion, both directly and indirectly.\nstd::vector<bool> connection_requests(int32_t const std::vector<std::vector<int32_t>>&n, firewall,\nconst std::vector<std::vector<int32_t>>& requests);\n18. You are given a list of 𝑛computers, labeled from 0 to 𝑛−1. There are two ways you can link these computers with each other:\n• a link that connects two computers directly.Physical link:\n• allows a computer to communicate directly with other computer that also has a wireless link. For example, ifWireless link: any\ncomputers 0, 1, and 2 are given wireless links, then they would all be able to communicate with each other.\nTwo computers do not need to be directly connected to be able to receive messages from each other. For example, if computer 0 has a\nphysical link with computer 1, and computer 1 has a physical link with computer 2, then we would still be able to send a message from\ncomputer 0 to 2. Similarly, if computer 0 has a physical link with computer 1, and both computers 1 and 2 support wireless links, then\ncomputer 0 would also be able to send a message to computer 2 using a direct connection to 1 and a wireless connection to 2.\nImplement the following function, which takes in the number of computers 𝑛, two computers 𝑖and 𝑗(which are indices from 0 to 𝑛−1),\nlinks[d] d,and a 2-D vector of links, where includes all the links that will be installed on day and returns the day number on which\n-1 Linkcomputer 𝑖will be able to communicate with computer 𝑗(or if not possible). Each entity is defined as follows, for two computers\nsrc dest. dest -1, src src destand If is equal to then will be given a wireless link; otherwise a physical link between and will be\nUnionFindadded. You may also use the pre-implemented data structure shown below in your solution.\n// Pre-implemented data structure that you can use in your solution (no need to implement this)\nclass UnionFind {\npublic:\nUnionFind(int32_t n);\n// creates a union-find container with n elements\n// Returns whether i and j are in the same disjoint set\nbool connected(int32_t int32_ti, j);\n// Merges i and j into the same disjoint set if they are not in one already\nvoid union_set(int32_t int32_ti, j);\n};\n// Link entity that indicates a request to link src with dest\n// If dest is -1, a wireless link is added to src instead\nstruct int32_t int32_tLink { src; dest; };\n// Implement this function: returns earliest day computers i and j can communicate with each other\nint32_t earliest_connection_day(int32_t int32_t int32_tn, i, j,\nconst std::vector<std::vector<Link>>& links);", "word_count": 983, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "79166ca1-0071-5b3f-904c-086652f1ec14", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 420, "real_page_number": null, "text": "408\nChapter 13. Sets and Union-Find\nChapter 13 Exercise Solutions\n1. The correct answer is (A). A student who took EECS 281 once and passed in the spring semester would be included in 𝑆, a student who\ntook EECS 281 in both the fall and winter semesters would be included in 𝐹∩𝑊, and a student who took EECS 281 in both winter and\nspring would be included in 𝑆. Only a student who took EECS 281 once in the winter would be included in neither 𝐹∩𝑊nor 𝑆.\n2. Thecorrectansweris(C).Sincethevectorsaresorted, youcanfindthesetdifferencebysimplyperformingalinearpassofthetwovectors,\nset_difference()using the sorted order to determine if an element exists in one vector but not the other. See the implementation of\nin section 13.1.2 for an example of how this can be done.\n3. The correct answer is (D). If you are not allowed to create any additional containers, the best time complexity you can do is Θ(𝑛log(𝑛)),\nsince you would need to presort the vectors before you can solve the problem using a linear pass (similar to the solution for question 2).\nΘ(𝑛2)Note that the best solution if both vectors remain unsorted takes time, since for each value in one vector, you would need to perform\na linear pass over the other vector to identify if it also exists there. This is inferior to the cost of sorting the vector beforehand.Θ(𝑛log(𝑛))\n4. The correct answer is (A). Only statement I is required for the STL’s set algorithms, as they rely on the assumption that the provided values\nare in sorted order. Statement II is false since these methods take in input iterators, and statement III is false because the two provided\niterator ranges can be of different sizes (what happens to any extra values in the larger iterator range depends on the method called).\n5. The correct answer is (E). The union find data structure is most useful for working with disjoint sets. All of the options are examples\nwhere this may be applicable, since they all deal with identifying or merging elements into disjoint sets.\n6. The correct answer is (B). If every element in the data structure is forced to keep track of its ultimate representative, a single union call\nmay require elements to have their representatives updated. This is the \"quick-find\" implementation of union find, which allowsΘ(𝑛)\nfind_set() union_set()to be done in constant time at the expense of a more expensive call.\n7. The correct answer is (B). An element has a representative equal to itself if it is the ultimate representative of its disjoint set. Since every\nunion_set()element has its representative equal to itself, each element must be in its own disjoint set — because of this, a call to would\nmerge two sets of size one, and thus will only be able to update one representative.\nfind_set()8. The correct answer is (C). The worst-case performance of depends on length of the representative chain from an element\nto its ultimate representative. Normally, this would be since you could have a representative chain in the form of a stick (where A is anΘ(𝑛)\nultimate representative, B’s representative is A, C’s representative is B, etc.). However, if you enforce that the smaller set is always updated\nto be part of the larger set after a union, then the maximum length of a representative chain drops down from to Θ(log(𝑛)), which alsoΘ(𝑛)\nfind_set()reduces the worst-case time complexity of a single call.\n{0, 2, 3, 5, 7, 8, 9} {1, 4, 6}.9. The correct answer is (C). There are two disjoint sets: and One strategy is to count the\nnumber of elements with a representative equal to itself, since this is the number of ultimate representatives that exist in the container.\n10. The correct answer is (B). Since there are two disjoint sets, only one union operation is needed to bring the total number of disjoint sets\ndown to one (by calling union on two elements in different sets). In general, if there are 𝑛disjoint sets, at least calls to union are𝑛−1\nneeded to merge everything into a single set.\nfind_set()11. The correct answer is (D). Using the path-compression approach, if we were to call on an element 𝑗, we would traverse\nall elements on the way to its ultimate representative 𝑘and change all these elements to have a representative of 𝑘. Since we called\nfind_set() on element 0, we follow the chain all the way to 0’s ultimate representative of 8:\n→8.→5→3→2→7→90\nHere, all of these elements would have their representatives changed to 8.\nfind_set()12. The correct answer is (D). Calling on an element would not change its representative if there are no intermediaries between\nsuch an element and its ultimate representative. This is only true for element 1, as 1’s representative is 6, which is its own representative.\nCalling find on element 7 would change its representative fo 8, calling find on element 4 would change its representative to 6, and calling\nfind on element 3 would change its representative to 8 (as well as others down the chain to 8).\n13. The correct answer is (C). If there are 281 elements in the container, and 203 have themselves as their ultimate representative, then there\nmust be 203 disjoint sets. If 202 of these sets have only one element, then there are 281 - 202 = 79 elements remaining to form the last\ndisjoint set. The size cannot be larger than 79, or it would be impossible for there to be 203 disjoint sets in total.\n14. Thecorrectansweris(A).Sincepathcompressionissupported,thelargestpossiblenumberofelementsthatcouldhavetheirrepresentatives\nupdated is equal to the size of the largest disjoint set, minus 1 (since the ultimate representative of the set does not get updated), which could\nfind_set()happen if you have a representative chain in the form of a stick, and you call on the ultimate representative and farthest\nelement along the stick. The largest disjoint set can have a total of 370 - 279 = 91 elements, so the most number of representatives that\ncould potentially be updated is 90.\n15. The correct answer is (E). Option (A) is false because, from the existing representatives, there were only two disjoint sets before the union:\n{A, B, C, E} and {D}. This also means (B) and (C) are false as well. Of the remaining options, (E) is correct because the disjoint set\ncontaining {A, B, C, E} would not have an ultimate representative otherwise, as none of A, B, or C have its representative equal to itself.", "word_count": 1132, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "cf7c97e2-8a5a-53a5-aea9-f9d4b4ec6bba", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 421, "real_page_number": null, "text": "13.4 Union-by-Size and Union-by-Rank\n409\n(==)16. This problem can be solved using a union-find container. To do so, we would first iterate over all the equality equations and merge\n(!=)themintothesamedisjointset. Then, weiterateoverallthenon-equality equationsandcallfindtodetermineiftheybelongtodifferent\n!=disjoint set. If not, then we have a contradiction, so we return false. Otherwise, if all the equations involve variables that belong in\ndifferent disjoint sets, we return true. One implementation of this solution is shown below:\n1\nint32_t find(std::vector<int32_t>& int32_treps, id) {\n2\nif (id == reps[id]) {\n3\nreturn id;\n4\n} // if\n5\nreps[id] = find(reps, reps[id]);\n6\nreturn reps[id];\n7\n} // find()\n8\n9\nbool are_equations_satisfiable(const std::vector<std::string>& equations) {\n10\nstd::vector<int32_t> reps(26);\n// 'a' to 'z'\n11\nfor (int32_t i = 0; i < 26; ++i) {\n12\nreps[i] = i;\n13\n} // for i\n14\nfor (const std::string& equation : equations) {\n15\nchar var1 = equation[0] - 'a';\n16\nchar var2 = equation[3] - 'a';\n17\nif (equation[1] == '=') {\n18\nreps[find(reps, var1)] = find(reps, var2);\n19\n} // if\n20\n} // for equation\n21\nfor (const std::string& equation : equations) {\n22\nchar var1 = equation[0] - 'a';\n23\nchar var2 = equation[3] - 'a';\n24\nif (equation[1] == '!' && find(reps, var1) == find(reps, var2)) {\n25\nreturn false;\n26\n} // if\n27\n} // for equation\n28\nreturn true;\n29\n} // are_equations_satisfiable()\nThis problem can also be solved using a union-find container, since we need to be able to quickly identify if two computers are on the same17.\nnetwork (i.e., disjoint set). The solution is pretty similar to the previous problem: we iterate over the requests and check if we can make\na connection without breaking any restrictions (using a call to find). If we can, then a connection is made (using a call to union). One\nimplementation of this solution is shown below:\n1\nint32_t find_set(std::vector<int32_t>& int32_treps, id) {\n2\nif return(id == reps[id]) id;\n3\nreps[id] = find_set(reps, reps[id]);\n4\nreturn reps[id];\n5\n} // find_set()\n6\n7\nvoid union_set(std::vector<int32_t>& int32_t int32_treps, x, y) {\n8\nint32_t x_rep = find_set(reps, x);\n9\nint32_t y_rep = find_set(reps, y);\n10\nreps[y_rep] = x_rep;\n11\n} // union_set()\n12\n13\nstd::vector<bool> connection_requests(int32_t const std::vector<std::vector<int32_t>>&n, firewall,\n14\nconst std::vector<std::vector<int32_t>>& requests) {\n15\nstd::vector<int32_t> reps(n);\n16\nfor (int32_t i = 0; i < n; ++i) {\n17\nreps[i] = i;\n18\n} // for i\n19\nstd::vector<bool> result;\n20\nfor (const std::vector<int32_t>& request : requests) {\n21\nbool true;can_connect =\n22\nint32_t comp1 = find_set(reps, request[0]);\n23\nint32_t comp2 = find_set(reps, request[1]);\n24\nfor (const std::vector<int32_t>& f : firewall) {\n25\nint32_t fw_rep1 = find_set(reps, f[0]);\n26\nint32_t fw_rep2 = find_set(reps, f[1]);\n27\nif ((comp1 == fw_rep1 && comp2 == fw_rep2) || (comp2 == fw_rep1 && comp1 == fw_rep2)) {\n28\nfalse;can_connect =\n29\nbreak;\n30\n} // if\n31\n} // for f\n32\nresult.push_back(can_connect);\n33\nif (can_connect) {\n34\nunion_set(reps, comp1, comp2);\n35\n} // if\n36\n} // for request\n37\nreturn result;\n38\n} // connection_requests()", "word_count": 537, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e2bacc03-1414-5279-95c2-c1a10d8f4f2b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 422, "real_page_number": null, "text": "410\nChapter 13. Sets and Union-Find\n18. One possible solution is shown below. We will use the union-find container to initialize a container of elements, where indices 0 to𝑛+1\nrepresent the 𝑛computers, and index 𝑛represents a wireless link (e.g., any computer in the same disjoint set as index 𝑛is part of the𝑛−1\nsrc destwireless network, and thus can communicate with each other). For each day, we iterate over the links and union indices and if a\nsrcphysical link is specified, or index and index 𝑛if a wireless link is specified. Then, if the input computers 𝑖and 𝑗are connected at the\nend of the day, we return that day number from our function.\n1\nint32_t earliest_connection_day(int32_t int32_t int32_tn, i, j,\n2\nconst std::vector<std::vector<Link>>& links) {\n3\nUnionFind network(n + 1);\n4\nfor (int32_t day_num = 0; day_num < links.size(); ++day_num) {\n5\nfor (const Link& link : links[day_num]) {\n6\nif (link.dest == -1) {\n7\nnetwork.union_set(link.src, n);\n8\n} // if\n9\nelse {\n10\nnetwork.union_set(link.src, link.dest);\n11\n} // else\n12\n} // for link\n13\n14\nif (network.connected(i, j)) {\n15\nreturn day_num;\n16\n} // if\n17\n} // for day_num\n18\n19\nreturn -1;\n20\n} // earliest_connection_day()", "word_count": 207, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f70d4035-cffa-5895-b22c-8182fc743ad1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 423, "real_page_number": null, "text": "Chapter 14\nSorting Algorithms\n14.1\nTypes of Sorting Algorithms\nIn this chapter, we will explore several different algorithms that can be used to sort a sequence of elements in some predefined order. To start\noff, we will first look at elementary sorting algorithms. Elementary sorting algorithms tend to be simple and straightforward, and they can be\nused to illustrate different approaches that can be used to sort a collection of objects. From a complexity standpoint, elementary sorts are often\nnot as good as their high-performance counterparts. However, despite their simplicity, they can be good enough if the data set you want to sort\nis small. Elementary sorts also serve as a building block for advanced sorting algorithms that may have better performance.\nBefore we begin, let’s introduce some terminology. An inversion is defined as a pair of numbers in a sequence not in sorted order. That is,\narr i j, (arr[i], arr[j]) arr i < j arr[i] >if you are given an array and two indices and the pair is an inversion of the array if and\narr[j] (assuming the array should be sorted in ascending order). For instance, the elements 4 and 5 form an inversion in the array below.\n0\n1\n2\n3\n5\n4\n6\n7\nA sorting algorithm is considered to be stable if the relative order of identical elements is not modified after the completion of the sort. If two\nelements A and B are identical, and A comes before B before sorting, then A should still be before B after the sequence is sorted using a stable\n{14, \"Banana\"} {14, \"Apple\"}sorting algorithm. In the following example, was before in the initial container, so if the container were\n{14, \"Banana\"} {14, \"Apple\"}only sorted by number using a stable sort, would still be before upon the sort’s completion.\n{14, \"Banana\"}\n{12, \"Carrot\"}\n{14, \"Apple\"}\n{15, \"Eggplant\"}\n{13, \"Durian\"}\n12,{ \"Carrot\"}\n13,{ \"Durian\"}\n14,{ \"Banana\"}\n14,{ \"Apple\"}\n15,{ \"Eggplant\"}", "word_count": 330, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c49f5f99-f782-52af-b35e-522606b1b14f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 424, "real_page_number": null, "text": "412\nChapter 14. Sorting Algorithms\nA sorting algorithm is considered to be adaptive if the sequence of operations performed by the algorithm differs based on the order of the data\nthat needs to be sorted. An adaptive sort often takes advantage of the existing order of elements in its input: for instance, an adaptive sort may\nrun more efficiently if it is given an array that is nearly sorted. Because of this, adaptive sorts often have different best-case and worst-case time\ncomplexities, since their performance can depend on the ordering of data that is passed in.\nOn the other hand, a non-adaptive sort always completes the same sequence of operations on the data it needs to sort, regardless of how the\ndata is ordered. A non-adaptive sorting algorithm would run the same operations on an array that is nearly sorted versus an array that is not close\nto being sorted at all. Non-adaptive sorts are typically simpler to implement as they do not have to check the order of elements before sorting.\nWhat dictates the efficiency of a sorting algorithm? Like with most other algorithms, the efficiency of a sorting algorithm can be measured\nusing its asymptotic time complexity. This can be done by analyzing the number of comparisons and swaps that are completed by the algorithm.\nA sorting algorithm’s efficiency can also be measured using its auxiliary space usage — some sorts can be done in-place, while others may\nrequire additional memory usage that depends on the input size (either through an additional container or stack frames via recursive calls).\nIn the next few sections, we will discuss different sorting algorithms that fall into different categories based on their implementation and\nbehavior. As mentioned before, we will first cover elementary sorts (bubble sort, selection sort, insertion sort) before moving on to more\nadvanced comparison sorts (heapsort, quicksort, mergesort). Lastly, we will discuss linear-time sorting algorithms that are asymptotically faster\nthan comparison sorts, but require special assumptions about the data that needs to be sorted.\n14.2\nBubble Sort\nBubble sort is an elementary sorting algorithm that sorts elements by repeatedly comparing two adjacent elements at a time and swapping them\nif they are in the wrong order. The following implements a standard bubble sort algorithm for a vector of integers:\n1\nvoid bubble_sort(std::vector<int32_t>& vec) {\n2\nfor (size_t i = 0; i < vec.size() - 1; ++i) {\n3\nfor (size_t j = 0; j < vec.size() - i - 1; ++j) {\n4\n// if out of order, swap the elements\n5\nif (vec[j] > vec[j + 1]) {\n6\nstd::swap(vec[j], vec[j + 1]);\n7\n} // if\n8\n} // for j\n9\n} // for i\n10\n} // bubble_sort()\n[5, 3, 9, 1, 7, 8, 2, 4, 6].Let’s look at how bubble sort works using the following unsorted array:\nThe bubble sort algorithm looks at two adjacent elements at a time, swapping them if they are out of order. At the end of each pass, the\nlargest element \"bubbles\" to the end of the vector, as shown below:\nFirst pass:\n[5, 3, 9, 1, 7, 8, 2, 4, 6]\n5, 9, 1, 7, 8, 2, 4, 6][3,\n[3, 5, 9,1, 7, 8, 2, 4, 6]\n[3, 5, 1, 9,7, 8, 2, 4, 6]\n[3, 5, 1, 7, 9, 8, 2, 4, 6]\n[3, 5, 1, 7, 8, 9, 2, 4, 6]\n[3, 5, 1, 7, 8, 2, 9,4, 6]\n[3, 5, 1, 7, 8, 2, 4, 9, 6]\n9][3, 5, 1, 7, 8, 2, 4, 6,\n5 and 3 are out of order, so swap!\n5 and 9 are in the correct order, so no swap needed.\n9 and 1 are out of order, so swap!\n9 and 7 are out of order, so swap!\n9 and 8 are out of order, so swap!\n9 and 2 are out of order, so swap!\n9 and 4 are out of order, so swap!\n9 and 6 are out of order, so swap!\nFirst pass complete! 9 is fixed in position.\nBecause 9 is the largest element in the array, it ends up bubbling to the end. After the first pass, we will keep 9 fixed (since we know that 9 is\nnow in the correct position) and complete another pass of bubble sort. During this second iteration, we will bubble the second largest element\nup to its correct position.\nSecond pass:\n9][3, 5, 1, 7, 8, 2, 4, 6,\n9][3, 5,1, 7, 8, 2, 4, 6,\n9][3, 1, 5,7, 8, 2, 4, 6,\n9][3, 1, 5,7, 8, 2, 4, 6,\n9][3, 1, 5, 7, 8, 2, 4, 6,\n9][3, 1, 5, 7, 2, 8,4, 6,\n9][3, 1, 5, 7, 2, 4, 8, 6,\n8, 9][3, 1, 5, 7, 2, 4, 6,\n3 and 5 are in the correct order, so no swap needed.\n5 and 1 are out of order, so swap!\n5 and 7 are in the correct order, so no swap needed.\n7 and 8 are in the correct order, so no swap needed.\n8 and 2 are out of order, so swap!\n8 and 4 are out of order, so swap!\n8 and 6 are out of order, so swap!\nSecond pass complete! 8 is fixed in position.\nWe can repeat this process by conducting multiple passes until the array is fully sorted. What is the time complexity of bubble sort? If we define\nthe size of the array as 𝑛, then we complete comparisons on the first pass, comparisons on the second pass, comparisons on the𝑛−1 𝑛−2 𝑛−3\nΘ(𝑛2).third pass, and so on. The total number of comparisons we complete is equal to the sum Thus,(𝑛−1)+(𝑛−2)+…+1 𝑛(𝑛−1)∕2= =\nΘ(𝑛2)our current bubble sort implementation runs in time.\nRemark: This specific approach is not the only way to complete a bubble sort: you can also iterate from the back to the front, instead of\nfrom the front to the back. In this alternative method, the values get bubbled to the of the array.smallest front", "word_count": 1023, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "585e32d0-5795-5ab4-a0b7-5a890a3a0beb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 425, "real_page_number": null, "text": "14.3 Selection Sort\n413\nΘ(𝑛2)What if the array were already sorted? The bubble sort algorithm we currently have would do work regardless of the contents of the array.\nHowever, we can prevent the algorithm from doing extra work by adding an additional check in the inner loop: if no swaps were made during a\npass, the array must be fully sorted! With this check, we ensure that our bubble sort algorithm immediately terminates once the array is sorted.\n1\n// adaptive bubble sort\n2\nvoid bubble_sort(std::vector<int32_t>& vec) {\n3\nfor (size_t i = 0; i < vec.size() - 1; ++i) {\n4\nbool false;swap_made =\n5\nfor (size_t j = 0; j < vec.size() - i - 1; ++j) {\n6\nif (vec[j] > vec[j + 1]) {\n7\nstd::swap(vec[j], vec[j + 1]);\n8\ntrue;swap_made =\n9\n} // if\n10\n} // for j\n11\n// if swapped is still false, no swaps were made during pass\n12\nif false)(swap_made = {\n13\nbreak;\n14\n} // if\n15\n} // for i\n16\n} // bubble_sort()\n[1, 2, 3, 4, 5]Let’s see what happens when we pass the sorted array into the adaptive bubble sort:\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n[1, 2, 3,4, 5]\n[1, 2, 3,4, 5]\n1 and 2 are in the correct order, so no swap needed.\n2 and 3 are in the correct order, so no swap needed.\n3 and 4 are in the correct order, so no swap needed.\n4 and 5 are in the correct order, so no swap needed.\nswap_made true.Since we completed an entire pass of the array without making a single swap, the variable is never set to Thus, we know\nthat the array must be fully sorted, so we can terminate the algorithm without doing any more passes. With this implementation of bubble sort,\nthe best-case time complexity becomes Θ(𝑛), which occurs when the array is fully sorted.\nIn addition to being adaptive, bubble sort is also stable. By ensuring that equality does not lead to a swap, we can guarantee that duplicate\nelements end up in the same relative order after the sorting process is done. Furthermore, bubble sort can be done in-place and thus only uses\nauxiliary space. A summary of bubble sort is shown in the table below:Θ(1)\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nΘ(𝑛)\nΘ(𝑛2)\nΘ(𝑛2)\nΘ(1)\nYes\nYes\n14.3\nSelection Sort\nSelection sort is an elementary sorting algorithm that sorts a container by repeatedly selecting the smallest element that has not been sorted and\nplacing it in its correct sorted position. The code is shown below:\n1\nvoid selection_sort(std::vector<int32_t>& vec) {\n2\nfor (size_t i = 0; i < vec.size() - 1; ++i) {\n3\n// find the smallest element that is not in correct sorted position\n4\nsize_t min_index = i;\n5\nfor (size_t j = i + 1; j < vec.size(); ++j) {\n6\nif (vec[j] < vec[min_index]) {\n7\nmin_index = j;\n8\n} // if\n9\n} // for j\n10\n// swap the element at min_index to its correct position\n11\nstd::swap(vec[i], vec[min_index]);\n12\n} // for i\n13\n} // selection_sort()\n[5, 3, 9, 1, 7, 8, 2, 4, 6].Let’s look at selection sort in action, using the same array we used earlier: With each pass, we find the next\nsmallest value and place it in its correct position. In the process below, the two underlined elements are swapped, and bolded elements are fixed\nin their correct sorted position.\n[5, 3, 9,1, 7, 8, 2, 4, 6]\n[1, 3, 9, 5, 7, 8, 2, 4, 6]\n[1, 2, 9, 5, 7, 8, 3, 4, 6]\n[1, 2, 3, 5, 7, 8, 9,4, 6]\n[1, 2, 3, 4,7, 8, 9, 5, 6]\n[1, 2, 3, 4, 5, 6, 9,7, 8]\n[1, 2, 3, 4, 5, 6, 7, 9, 8]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n1 is the smallest element, so swap it to index 0.\n2 is the next smallest element, so swap it to index 1.\n3 is the next smallest element, so swap it to index 2.\n4 is the next smallest element, so swap it to index 3.\n5 is the next smallest element, so swap it to index 4.\n6 is the next smallest element, so swap it to index 5.\n7 is the next smallest element, so swap it to index 6.\n8 is the next smallest element, so swap it to index 7.\n9 is the last element, so we are done!\nSimilar to bubble sort, we need to make comparisons on the first pass, comparisons on the second pass, and so on. This results in a𝑛−1 𝑛−2\nΘ(𝑛2) time algorithm. Can we make this better?", "word_count": 821, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "59ca693b-a581-5309-b2bf-0b38b3951e1e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 426, "real_page_number": null, "text": "414\nChapter 14. Sorting Algorithms\nIt turns out that we cannot do much to improve the complexity of selection sort. We can add a check to prevent our algorithm from swapping an\nΘ(𝑛2)element with itself, but this does not prevent us from having to do comparisons. Because the execution of selection sort is independent of\nthe input, selection sort is not adaptive.\nSelection sort is also not stable. This is because you cannot guarantee where an element will end up after it is swapped. Consider the\n[4, 3, 4', 2, 1],following unsorted array with two 4’s: where 4’ is the second 4 in the array. The first two passes of selection sort are:\n[4, 3, 4', 2,1]\n[1, 3, 4', 2, 4]\n...\n[1, 2, 3, 4', 4]\n1 is the smallest element, so swap it to index 0\n2 is the next smallest element, so swap it to index 1\n...\nAfter sorting, 4’ is now before 4, so the sort is not stable!\nNotice that we have a problem when we swapped 1 to its correct position: after swapping 1, we ended up swapping 4 to a position 4’. Thus,after\n4’ will end up before 4 in the sorted array, even though 4 was positioned before 4’ prior to sorting. Because you cannot predict where duplicate\nelements will end up (the first 4 could have been sent before after 4’ depending on where 1 was), selection sort is not stable. (Note that it isor\ntechnically possible to make selection sort stable by modifying its behavior or by using additional memory, but for the purposes of the class, the\nconcept of a selection sort will typically refer to the standard, unstable implementation, unless otherwise specified.)\nΘ(𝑛2)So, not only does selection sort always run in time, it is also neither stable nor adaptive. Based on this, it might appear that selection\nsort is inferior to bubble sort in every way! However, selection sort does have its redeeming qualities. The most notable advantage of selection\nsort is that it minimizes the total number of swaps needed during the sorting process. Unlike bubble sort, which could require a swap for every\npair of elements in an array, selection sort only requires swaps in the worst-case (given an input size of 𝑛). Do not confuse this with the𝑛−1\nΘ(𝑛2)runtime of selection sort, however; even though selection sort could perform zero swaps, it still needs to complete comparisons, which is\nΘ(𝑛2).why the best-case time complexity of selection sort is still A summary of selection sort is shown in the table below:\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nΘ(𝑛2)\nΘ(𝑛2)\nΘ(𝑛2)\nΘ(1)\nNo\nNo\n14.4\nInsertion Sort\nInsertion sort is an elementary sorting algorithm that looks at each element sequentially and builds the sorted result one at a time. The\nimplementation of insertion sort is shown below:\n1\nvoid insertion_sort(std::vector<int32_t>& vec) {\n2\nfor (size_t i = 1; i < vec.size(); ++i) {\n3\nint32_t current = vec[i];\n4\nint32_t j = i - 1;\n5\nwhile (j >= 0 && vec[j] > current) {\n6\nvec[j + 1] = vec[j];\n7\n--j;\n8\n} // while()\n9\nvec[j + 1] = current;\n10\n} // for i\n11\n} // insertion_sort()\nAs we iterate through the array, we look at each element and compare it with the elements that come before it. If the element before the current\nelement is out of position compared to the current element (i.e., there is an inversion), we shift that element one to the right. This allows us to\nidentify where the current element should be placed relative to the elements that come before it.\n[5, 3, 9, 1, 7, 8, 2, 4, 6].Let’s analyze insertion sort using the same unsorted array we used earlier: We will start from the left\nend of the array and consider each element one at a time.\n[5, 3, 9, 1, 7, 8, 2, 4, 6]\n[5, 3, 9, 1, 7, 8, 2, 4, 6]\n5 is in the correct position relative to the elements before it (trivially).\nLooking only at the elements before 3, where should 3 go? Before 5.\nAt this step, we shift 5 one index to the right and insert 3 into the correct relative position, before 5.\n[3, 5, 9, 1, 7, 8, 2, 4, 6]\nLooking only at the elements before 9, where should 9 go?\n9 is in the correct position relative to 3 and 5, so no shifting is needed.\n[3, 5, 9,1, 7, 8, 2, 4, 6]\nLooking only at the elements before 1, where should 1 go? Before 3.\nCompared to the elements that have been visited before it, 1 should go before 3. Thus, we would shift 9 one position to the right (to index 3), 5\none position to the right (to index 2), 3 one position to the right (to index 1), and insert 1 in its correct relative position at index 0.\n[1, 3, 5, 9,7, 8, 2, 4, 6]\nLooking only at the elements before 7, where should 7 go? Before 9.\nThus, we would shift 9 one position to the right (to index 4) and insert 7 in its correct relative position at index 3.\n[1, 3, 5, 7, 9, 8, 2, 4, 6]\nLooking only at the elements before 8, where should 8 go? Before 9.", "word_count": 907, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2c5d24a6-8d03-5671-afe2-57302f4a9683", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 427, "real_page_number": null, "text": "14.5 Heapsort\n415\nThus, we would shift 9 one position to the right (to index 5) and insert 8 in its correct relative position at index 4.\n[1, 3, 5, 7, 8, 9, 2, 4, 6]\nLooking only at the elements before 2, where should 2 go? Before 3.\nThus, we would shift 9 one position to the right (to index 6), 8 one position to the right (to index 5), 7 one position to the right (to index 4), 5\none position to the right (to index 3), 3 one position to the right (to index 2), and insert 2 in its correct relative position at index 1.\n[1, 2, 3, 5, 7, 8, 9,4, 6]\nLooking only at the elements before 4, where should 4 go? Before 5.\nThus, we would shift 9 one position to the right (to index 7), 8 one position to the right (to index 6), 7 one position to the right (to index 5), 5\none position to the right (to index 4), and insert 4 in its correct relative position at index 3.\n[1, 2, 3, 4, 5, 7, 8, 9, 6]\nLooking only at the elements before 6, where should 6 go? Before 7.\nThus, we would shift 9 one position to the right (to index 8), 8 one position to the right (to index 7), 7 one position to the right (to index 6), and\ninsert 6 in its correct relative position at index 5.\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nThe array is now fully sorted.\nThere is one additional optimization that we can complete to make our insertion sort even more efficient. Notice that we have the following\nwhile loop in our current insertion sort implementation:\nwhile (j >= 0 && vec[j] > current) { ... }\nΘ(𝑛2)j >= 0 vec[j] > current j >= 0 false!Here, we check the expressions and a total of times. However, the check seldom returns\nfalseThis comparison only returns if the element we are considering is the smallest element we have encountered so far (which would require\nvec[j] > currentthe element to be placed all the way to index 0). However, as long as we are not looking at the smallest element, the\njcheck is enough to ensure that we don’t index out of bounds (as would never reach 0). Thus, we can save some time by moving the smallest\nelement to the very beginning of the array before we begin our insertion sort (this element is known as a value). By doing this, wesentinel\nwhilewould only need one check in our loop:\nwhile (vec[j] > current) { ... }\nThe behavior of insertion sort naturally allows it to be adaptive. Since each element is only compared to the values that come before it, insertion\nsort would only need to do 𝑛comparisons if the array were already sorted. This is because each value in a sorted array is guaranteed to be\nlarger than the elements before it, and only a single comparison is needed per element to realize that it is already in its correct relative position.\nFurthermore, insertion sort is also stable, since elements are always considered in order from front to back. By shifting only when the current\nelement is strictly lesser than the element it is being compared to, duplicates will always maintain their original relative order.\nΘ(𝑛2)Despite being an elementary sort with an average-case time complexity, insertion sort can be fast in certain situations. In fact, the\nstd::sort()GCC implementation of uses insertion sort when the number of elements to sort is fewer than 16 — this is because, for small\narray sizes, insertion sort is actually faster than many of the high-performance sorts that we will cover in the next few sections. A summary of\ninsertion sort is provided in the table below:\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nΘ(𝑛)\nΘ(𝑛2)\nΘ(𝑛2)\nΘ(1)\nYes\nYes\nThe improved version of our insertion sort algorithm is shown below:\n1\nvoid insertion_sort(std::vector<int32_t>& vec) {\n2\n// find minimum element and place at index 0 (sentinel)\n3\nint32_t min_index = 0;\n4\nfor (size_t i = 1; i < vec.size(); ++i) {\n5\nif (vec[i] < vec[min_index]) {\n6\nmin_index = i;\n7\n} // if\n8\n} // for i\n9\nstd::swap(vec[0], vec[min_index]);\n10\n// run insertion sort on remaining elements\n11\nfor (size_t i = 2; i < vec.size(); ++i) {\n12\nint32_t current = vec[i];\n13\nint32_t j = i - 1;\n14\nwhile (vec[j] > current) {\n15\nvec[j + 1] = vec[j];\n16\n--j;\n17\n} // while\n18\nvec[j + 1] = current;\n19\n} // for i\n20\n} // insertion_sort()\n14.5\nHeapsort\nSo far, we have covered elementary sorts that tend to be simpler but have less efficient time complexities. Starting in this section, we will cover\nseveral high-performance sorts. These sorts tend to be more complex, but they are also more efficient for large input sizes. The first of these\nsorts is heapsort, which relies on the heap structure to sort elements.\nTo sort an array of elements using heapsort, the contents of the array are first heapified into a binary max-heap. By doing this, we ensure\nthat the largest element ends up at the top of the heap. After this largest element is identified, we swap it with the last element in the array —\nthis places the largest element at the end, in its correct sorted position. Then, we call fix down on the element that was swapped to the top of the\nheap. This process is repeated several times until the array becomes fully sorted.", "word_count": 952, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9d980a26-ee80-5260-81c1-f62a1f7291c4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 428, "real_page_number": null, "text": "416\nChapter 14. Sorting Algorithms\n[53, 17, 89, 46, 34].To illustrate the heapsort process, let’s consider the following unsorted array:\n53\n89\n17\n34\n46\nArray Representation\n53 17\n89 46\n34\nFirst, we will heapify this array into a binary max-heap so that the largest element is at the top of the heap. The bottom-up heapify approach is\nused since it can be done in worst-case time. During this process, we first fix down on 17 by swapping it with 46, and then we fix down onΘ(𝑛)\n53 by swapping it with 89. After heapify is complete, we have the following valid binary max-heap:\n53\n89\n17\n34\n46\nArray Representation\n53 17\n89 46\n34\n89\n53\n46\n34\n17\nArray Representation\n89 46\n53 17\n34\nBecause the heap is now a max-heap, we know that the element at the top (89) must be the largest element in the array, so 89’s sorted position\nmust be at the back of the array. Thus, we will move 89 to the very end of the array by swapping it with 34. Since we know 89 must now be in\nthe correct position, we will ignore 89 for the remainder of the algorithm.\n89\n53\n46\n34\n17\nArray Representation\n89 46\n53 17\n34\n34\n53\n46\n89\n17\nArray Representation\n34 46\n53 17\n89\nSince 34 was moved to the top of the heap, the heap is no longer a valid binary max-heap. However, because 34 is the only element that is out of\nplace, we can fix it by calling fix down on 34. As a result of this fix down, 34 ends up swapping with 53.\n34\n53\n46\n89\n17\nArray Representation\n34 46\n53 17\n89\n53\n34\n46\n89\n17\nArray Representation\n53 46\n34 17\n89", "word_count": 307, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4f6e6c7c-f8ed-58bc-8cec-d4c5453ba5f8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 429, "real_page_number": null, "text": "14.5 Heapsort\n417\nNow, 53 is at the top of the heap. Since we know that our heap is a valid max-heap (ignoring 89), 53 must be the second-largest element in the\narray. Thus, we can move 53 to its correct position by swapping it with the second-to-last element, 17.\n53\n34\n46\n89\n17\nArray Representation\n53 46\n34 17\n89\n17\n34\n46\n89\n53\nArray Representation\n4617\n34\n53\n89\nWe will repeat this process until the array is fully sorted. Now, we will fix down on 17:\n17\n34\n46\n89\n53\nArray Representation\n4617\n34\n53\n89\n46 is now at the top of the heap, so it is the next largest element that has not yet been placed in the correct position. 46 is thus moved to the\ncorrect position by swapping it with 34:\n46\n34\n17\n89\n53\nArray Representation\n46 17\n34\n53\n89\n34\n46\n17\n89\n53\nArray Representation\n34 17\n46\n53\n89\nOnce again, we fix down on the top element, 34. Nothing ends up changing since 34 is already larger than 17. We now know that 34 is the next\nlargest element, so it is put in place by swapping with 17. The array is now sorted!\n34\n46\n17\n89\n53\nArray Representation\n34 17\n46\n53\n89\n17\n46\n34\n89\n53\nArray Representation\n17\n34 46\n53\n89", "word_count": 237, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "78f75597-bc1b-5db8-8a53-9d315e7da07e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 430, "real_page_number": null, "text": "418\nChapter 14. Sorting Algorithms\nThe heapsort algorithm is outlined in the code below (using 1-indexing with a dummy element at index 0, for simplicity).\n1\n// assume this function calls fix down on vec[idx] using only\n2\n// a valid idx in the index range [1, numElts)\n3\nvoid fix_down(std::vector<int32_t>& size_t size_tvec, idx, numElts);\n4\n5\nvoid heapsort(std::vector<int32_t>& vec) {\n6\n// build heap using bottom-up heapify\n7\nfor (size_t i = vec.size() / 2; i > 0; --i) {\n8\nfix_down(vec, i, vec.size());\n9\n} // for i\n10\n// loop through all elements from back to front, assumes dummy at idx 0\n11\nfor (size_t j = vec.size(); j > 0; --j) {\n12\n// move largest element to the back\n13\nstd::swap(vec[1], vec[j]);\n14\n// fix top element, ignoring elements that are already fixed (index > j - 1)\n15\nfix_down(vec, 1, j - 1);\n16\n} // for j\n17\n} // heapsort()\nWhat is the time complexity of the heapsort algorithm? Heapsort consists of the following two steps:\n1. Heapify the entire array at the very beginning (using bottom-up heapify).\nSwap the element at the top of the heapified array to the back (ignoring elements that have already been fixed in position) and fix it in2.\nposition. Then, call fix down on the element that was just swapped to the front. This is repeatedly done until the array is fully sorted.\nThe bottom-up heapify completed during the first step takes time. Then, for the second step, fix down is called 𝑛times, once for eachΘ(𝑛)\nelement. Because each fix down call takes time, the overall time complexity of 𝑛fix down calls is Θ(𝑛log(𝑛)). SinceΘ(log(𝑛)) 𝑛×Θ(log(𝑛))=\nthese two steps are completed consecutively, the overall time complexity of heapsort is = Θ(𝑛log(𝑛)).Θ(𝑛+𝑛log(𝑛))\nBecause there is no guarantee that duplicate elements will maintain their ordering during heap operations such as fix down, heapsort is not\nstable. For similar reasons, it is difficult for heapsort to be made adaptive. A summary of heapsort is provided in the table below:\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(1)\nNo\nNo\n14.6\nQuicksort\nQuicksort is a divide-and-conquer algorithm that relies on a concept known as to sort an array. During partitioning, elements inpartitioning\nthe array are reorganized based on how large they are relative to a \"pivot\" element. The quicksort algorithm consists of the following two steps,\nwhich are repeated until the array is fully sorted:\n1. A pivot element is selected (the method of selecting the pivot depends on the implementation, but a common strategy is to select the last\nelement as the pivot — this is the method that will be used in these notes).\n2. The remaining elements in the array are partitioned so that all elements to the left of the pivot are lesser than the pivot, and all elements\nto the right of the pivot are greater than the pivot.\nAn outline of the quicksort algorithm is shown below:\n1\nint32_t partition(std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\nint32_t pivot = --right;\n3\nwhile (true) {\n4\nwhile (vec[left] < vec[pivot]) {\n5\n++left;\n6\n} // while\n7\nwhile (left < right && vec[right - 1] >= vec[pivot]) {\n8\n--right;\n9\n} // while\n10\nif (left >= right) {\n11\nbreak;\n12\n} // if\n13\nstd::swap(vec[left], vec[right - 1]);\n14\n} // while\n15\nstd::swap(vec[left], vec[pivot]);\n16\nreturn left;\n17\n} // partition()\n18\n19\nvoid quicksort(std::vector<int32_t>& int32_t int32_tvec, left, right) {\n20\nif (left + 1 >= right) {\n21\nreturn;\n22\n} // if\n23\nint32_t pivot = partition(vec, left, right);\n24\nquicksort(vec, left, pivot);\n25\nquicksort(vec, pivot + 1, right);\n26\n} // quicksort()", "word_count": 630, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b80b5d72-d5c4-5d7d-9ca0-fcc49222cb64", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 431, "real_page_number": null, "text": "14.6 Quicksort\n419\npartition() [left, right)The function takes in an index range and selects the last element as the pivot. The remaining elements are\npartitioned so that elements to the left of the pivot are lesser, and elements to the right are greater. Let’s look at what this code does using an\nexample. Consider the following unsorted array:\n5\n3\n9\n1\n7\n8\n2\n4\n6\nFirst, we will choose an arbitrary pivot in this array. In our examples, we will select the last element as our pivot:\n5\n3\n9\n1\n7\n8\n2\n4\n6\nNow, we will need to partition the array so that all elements less than 6 end up to the left of 6, and all elements greater than 6 end up to the right\nleft right.of 6. To do so, we will keep track of two indices, and The left index starts at the first element (5), while the right index starts at\nthe last non-pivot element (4). We will refer to these two indices as L and R in the figures below.\n5\n3\n9\n1\n7\n8\n2\n4\n6\nL\nR\nFirst, we increment L until the element it points to is greater than the pivot element, 6. In this case, L is incremented until it points to 9.\n5\n3\n9\n1\n7\n8\n2\n4\n6\nL\nR\nNext, we decrement R until the element it points to is less than the pivot, 6. Since the element that R is currently pointing to is already less than\n6, we do not need to decrement R. Once L and R are pointing to elements that are out of place relative to the pivot, the two elements are swapped.\n5\n3\n4\n1\n7\n8\n2\n9\n6\nL\nR\nWe repeat the above process until L and R meet. Increment L until it encounters a value larger than the pivot, and decrement R until it encounters\na value smaller than the pivot. This happens when L is at 7 and R is at 2.\n5\n3\n4\n1\n7\n8\n2\n9\n6\nL\nR\nThese two elements are then swapped.\n5\n3\n4\n1\n2\n8\n7\n9\n6\nL\nR\nIncrement L until it encounters an element greater than the pivot, 6. This happens when L is at 8:\n5\n3\n4\n1\n2\n8\n7\n9\n6\nL\nR\nDecrement R until it encounters an element less than the pivot, 6. However, notice that the first decrement causes R to point to L.\n5\n3\n4\n1\n2\n8\n7\n9\n6\nL\nR", "word_count": 440, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5cc6f03d-c9b3-5903-9a40-7548347fd41b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 432, "real_page_number": null, "text": "420\nChapter 14. Sorting Algorithms\nWhen L and R meet, swap the element at this position (in this case, 8) with the pivot value, 6.\n5\n3\n4\n1\n2\n6\n7\n9\n8\nL\nR\nThis ensures that the pivot value of 6 is now in its correct sorted position. Furthermore, all the elements to the left of 6 are smaller than 6, and\nall the elements to the right of 6 are larger. Since our initial array has been partitioned into two subarrays, one with elements less than 6, and one\nwith elements greater than 6, we can recursively call quicksort on both the left and right subarrays to sort the entire array. To continue our\nexample, let’s make a recursive call and quicksort the elements to the left of 6. Note that this recursive call must run to completion before we\ncan quicksort elements to the right of 6.\n5\n3\n4\n1\n2\n6\n7\n9\n8\nLike before, we will select the last element, 2, as our pivot, and initialize L and R:\n5\n3\n4\n1\n2\n6\n7\n9\n8\nL\nR\nIncrement L until it points to a value greater than 2, and decrement R until it points to a value less than 2 (in this case, neither L nor R needs to\nbe moved). Then, swap the two elements.\n1\n3\n4\n5\n2\n6\n7\n9\n8\nL\nR\nIncrement L until it points to a value greater than 2, and decrement R until it points to a value less than 2. Both L and R end up at 3.\n1\n3\n4\n5\n2\n6\n7\n9\n8\nL\nR\nSince L and R are equal, we swap 3 with the pivot. The pivot element of 2 is now in its correct sorted position.\n1\n2\n4\n5\n3\n6\n7\n9\n8\nL\nR\nOnce again, we will make recursive calls and quicksort elements to the left and right of 2. Since there is only one element to the left of 2, it must\ntrivially be in the correct sorted position (we immediately exit in the base case). To recursively quicksort the elements to the right of 2, we\nselect the last element (3) as the pivot.\n1\n2\n4\n5\n3\n6\n7\n9\n8\nL\nR\nIf we repeat the process of incrementing L until we reach an element greater than 3 and decrementing R until we reach an element less than 3,\nboth L and R end up pointing to 4. We would then swap 4 with the pivot, 3:\n1\n2\n3\n5\n4\n6\n7\n9\n8\nWe would then recursively quicksort elements to the left and right of our pivot, 3. There are no elements to the left of 3 (since 3 is the leftmost\nelement of the subarray we are considering), so no work is done here. When we recursively quicksort elements to the right of 3, the elements 4\nand 5 end up getting swapped. After both recursive calls complete, the array looks like this:\n1\n2\n3\n4\n5\n6\n7\n9\n8", "word_count": 524, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "70e66203-7961-52b4-bdbf-e43ec844cb2e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 433, "real_page_number": null, "text": "14.6 Quicksort\n421\nAt this point, we have finished quicksorting elements to the left of our original pivot, 6. Now that all elements to the left of 6 are sorted, we can\nmake a recursive call to quicksort the elements to the right of 6. The process is similar to before: we select the last element, 8, as the pivot, and\ninitialize L and R.\n1\n2\n3\n4\n5\n6\n7\n9\n8\nL\nR\nIncrement L until it points to a value greater than the pivot value of 8. In this case, L ends up pointing to 9 since 9 is the first value greater than\n8, but R is also pointing to 9.\n1\n2\n3\n4\n5\n6\n7\n9\n8\nL\nR\nSince L and R are equal, we swap 9 with the pivot.\n1\n2\n3\n4\n5\n6\n7\n8\n9\nL\nR\nThere is only one element to the left and right of 8, which are both trivially sorted. Thus, we have finished quicksorting the elements to the right\nof our initial pivot of 6. The entire quicksort algorithm is complete, and our array is fully sorted.\n1\n2\n3\n4\n5\n6\n7\n8\n9\nWhat is the runtime complexity of quicksort? Let’s look at the code for quicksort again:\n1\nvoid quicksort(std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\nif (left + 1 >= right) {\n3\nreturn;\n4\n} // if\n5\nint32_t pivot = partition(vec, left, right);\n6\nquicksort(vec, left, pivot);\n7\nquicksort(vec, pivot + 1, right);\n8\n} // quicksort()\npartition()Quicksort is a recursive algorithm. On line 5, we call the function, which ensures that the pivot value is placed in its correct\nsorted position, and that all elements to the left are smaller and all elements to the right are larger. This partitioning step takes time, since aΘ(𝑛)\nsingle pass of the array is conducted when traversing using the left and right indices. Then, on lines 6 and 7, we recursively call quicksort on the\nelements to the left and right of the pivot.\nWhat is the input size of these recursive calls? This actually depends on the value of the pivot! If the pivot were the median, the number of\nelements to the left and right of the pivot would be equal, and the input size would be 𝑛∕2. For example, there are an equal number of elements\nto the left and right of the median value, 5:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n𝑛∕2\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n𝑛∕2\nIn this case, both recursive calls (on lines 6 and 7) would be called on an input size of 𝑛∕2. If the median were chosen as the pivot every time,\nthe recurrence relation for quicksort would be\n𝑇(𝑛)=\n{\n1,\nif or𝑛=0 1\n2𝑇(𝑛∕2)+𝑛,\nif 𝑛>1\nHere, the term comes from the two recursive quicksort calls, and the 𝑛term comes from the partitioning step. Using Master’s Theorem,2𝑇(𝑛∕2)\nwe can see that the time complexity of quicksort is if the median were selected as the pivot. However, this is the best-case scenario,Θ(𝑛log(𝑛))\nas it assumes that the median is chosen as the pivot every time! What would the worst-case time complexity of quicksort be?", "word_count": 547, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ebb2af11-ffa6-559b-9059-5b10735be1fa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 434, "real_page_number": null, "text": "422\nChapter 14. Sorting Algorithms\nIn the worst case, either the smallest or largest element is chosen as the pivot at every step. When this happens, the pivot always leaves one side\nempty and the other side with all the remaining elements. For instance, if 1 were chosen as the pivot, every other element would be partitioned\nto the right of the pivot:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n𝑛−1\nWhen this happens, one of the recursive calls would have an input size of 0, and the other would have an input size of 𝑛−1. If either the smallest\nor largest element is chosen as the pivot every time, the recurrence relation for quicksort becomes\n𝑇(𝑛)=\n{\n1,\nif or𝑛=0 1\n𝑇(𝑛−1)+𝑛,\nif 𝑛>1\nHere, the comes from the recursive quicksort call with input size 𝑛−1, and the 𝑛comes from the partitioning step. Using substitution,𝑇(𝑛−1)\nΘ(𝑛2).we can see that the worst-case time complexity of quicksort is\nWhatabouttheaverage-casetimecomplexityofquicksort? Itturnsoutthattheaverage-casetimecomplexityofquicksortisalsoΘ(𝑛log(𝑛)).\nThe following provides an intuitive explanation for why this is the case: if every element in an array has an equal chance of being selected as the\npivot, then there is a 50% chance the pivot ends up in the \"middle half\" of the sorted values, close to the median. Note that we are talking about\nthe elements whose values are closest to the median value, and the elements that are physically located in the middle of the initial array,not\nwhich may be unsorted.\n1\n2\n3\n4\n5\n6\n7\n8\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n50%chancepivotendsuphere\nIf the pivot does land in this range, the worst possible partition that could happen is a 3-to-1 split, as shown below.\n1\n2\n3\n4\n5\n6\n7\n8\n⏟⏞⏟⏞⏟\n𝑛∕4\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n3𝑛∕4\n1\n2\n3\n4\n5\n6\n7\n8\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n3𝑛∕4\n⏟⏞⏟⏞⏟\n𝑛∕4\nThus, in the average-case, the recurrence relation for half of the quicksort calls is at worst\n𝑇(𝑛)=\n{\n1,\nif or𝑛=0 1\n𝑇(3𝑛∕4)+𝑇(𝑛∕4)+𝑛,\nif 𝑛>1\nWhat is the time complexity of this recurrence relation? We can use a recursion tree to gain some intuition.\n𝑛\n𝑛\n4\n𝑛\n16\n⋮\n⋮\n3𝑛\n16\n⋮\n⋮\n3𝑛\n4\n3𝑛\n16\n⋮\n⋮\n9𝑛\n16\n⋮\n⋮\nRecursion Tree for 𝑇(𝑛) 𝑇(3𝑛∕4)+𝑇(𝑛∕4)+𝑛=", "word_count": 408, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5b36d498-2375-5dde-9021-d02cda00b135", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 435, "real_page_number": null, "text": "14.6 Quicksort\n423\nIf you are not familiar with recursion trees, you will not have to worry about them for this class (however, they were briefly covered in chapter\n5). Essentially, the sum of all values at each level of the recursion tree represents the total amount of work that is done at that recursion depth.\nTo determine the total amount of work done throughout all the recursive calls, calculate the sum of all values in the tree. In this case, we can see\nthat work is done for the first few recursion depths.Θ(𝑛)\n𝑛\n𝑛\n4\n𝑛\n16\n⋮\n⋮\n3𝑛\n16\n⋮\n⋮\n3𝑛\n4\n3𝑛\n16\n⋮\n⋮\n9𝑛\n16\n⋮\n⋮\n𝑛\n3𝑛\n𝑛+4\n𝑛=4\n9𝑛\n3𝑛+16\n3𝑛+16\n𝑛+16\n𝑛=16\nSincetheamountofworkdoneateachlevelofthetreeis𝑛,thetotalamountofworkthatisdoneshouldbeequalto𝑛×the number of levels in the tree.\nHowever, there is a catch: since the right branch represents a subproblem that is 1/3 the size of the left branch, the input size of the right branch\ndecreases faster, and thus hits the base case first. The number of levels required for the recursive call to hit the base case is approximately𝑇(𝑛∕4)\nlog4(𝑛), since the input size is reduced by a factor of 4 with each iteration. On the other hand, the number of levels required for the call𝑇(3𝑛∕4)\nto hit the base case is approximately log4∕3(𝑛), since the input size is reduced by a factor of 4/3 with each iteration.\n𝑛\n𝑛\n4\n𝑛\n16\n⋮\n1\n1\n⋮\n1\n1\n3𝑛\n16\n⋮\n1\n1\n⋮\n1\n1\n3𝑛\n4\n3𝑛\n16\n⋮\n⋮\n1\n1\n⋮\n1\n1\n⋮\n⋮\n1\n1\n⋮\n1\n1\n9𝑛\n16\n⋮\n⋮\n1\n1\n⋮\n1\n1\n⋮\n⋮\n1\n1\n⋮\n1\n1\nlog4(𝑛)\nlog4∕3(𝑛)\nBecause of this imbalance, there exist layers at the bottom of the tree where only the recursive call has any work to do (the left side of𝑇(3𝑛∕4)\nthe tree in the above illustration). These layers therefore complete less than 𝑛work (since 𝑛is the amount of work needed if recursive callsboth\nstill have work to do). As a result, the total amount of work required for the entirety of quicksort must be less than if every pivot𝑛×log4∕3(𝑛)\nresults in a 3-1 split. This is Θ(𝑛log(𝑛)).\nHowever, in the average-case, only 50% of pivots end up creating a 3-1 split or better. Let’s consider what would happen if we ended up\nwith a worst-case pivot (i.e., either the largest or smallest value) the other 50% of the time. To make our analysis easier, suppose the 3-1 and\nworst-case splits alternate between pivots (i.e., the first pivot creates a worst-case split, the second pivot creates a 3-1 split, the third pivot creates\na worst-case split, etc.). The recursion tree would look like this:\n𝑛\n0\n𝑛−1\n𝑛−1\n4\n0\n𝑛−1\n−14\n⋮\n⋮\n3(𝑛−1)\n4\n0\n3(𝑛−1)\n4\n−1\n⋮\n⋮\n𝑛\n(𝑛−1)+0 𝑛−1=\n3(𝑛−1)\n4\n𝑛−1+\n4\n𝑛−1=\n3(𝑛−1)\n4\n𝑛−1−1+\n𝑛−3−1=4\nHow much work is done in this scenario? While this may seem tricky to solve, there is a key insight that can make this problem easier. Notice\nthat this tree is almost exactly the same as the one introduced right before it, where every pivot resulted in a 3-1 split. The only difference here is\nthat the 3-1 split is done at every level. Therefore, the number of levels in this tree must be double what it was before.other", "word_count": 620, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "77f4c649-f5ce-5fd7-be3d-58635c975601", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 436, "real_page_number": null, "text": "424\nChapter 14. Sorting Algorithms\nFor example, our first recursion tree reduced the input size of the larger recursive call by a factor of 4/3 at every level, so a total of log4∕3(𝑛)\nlevels were needed before we reached the base case. When we introduced the worst-case pivot, however, the input size was reduced by a factor\nof 4/3 every levels instead. It now takes twice as many levels (i.e., 2log4∕3(𝑛)) to reach the base case! Since each level still takes time,Θ(𝑛)two\nthe total work involved with the alternating 3-1 and worst-case split is Θ(𝑛)×2log4∕3(𝑛), which simplifies to Θ(𝑛log(𝑛)).\nWith average-case behavior, we would expect that half of the pivots would fall within the middle half of sorted values. The example above\nwas the worst-case scenario that could happen with this setup: the pivots that were within the middle half were as far away from the median as\npossible (3-1 split), and the pivots outside the middle half always ended up being the largest or smallest value in the array. Yet, even with this\nworst-case scenario, the time complexity of quicksort still ended up being Θ(𝑛log(𝑛)). Since the average-case time complexity of quicksort\ncannot be worse than (as shown above), but it also cannot be better than (which is the best-case time complexity), itΘ(𝑛log(𝑛)) Θ(𝑛log(𝑛))\nmust be precisely equal to Θ(𝑛log(𝑛)).\nFor another explanation as to why the average-case of quicksort is Θ(𝑛log(𝑛)), suppose you got super unlucky and every pivot you chose\nended up creating a 9-1 split (i.e., 90% of elements on one side of the pivot and 10% on the other). If this were to happen, the recursion tree for\nquicksort would look like this:\n𝑛\n𝑛\n10\n𝑛\n100\n⋮\n⋮\n9𝑛\n100\n⋮\n⋮\n9𝑛\n10\n9𝑛\n100\n⋮\n⋮\n81𝑛\n100\n⋮\n⋮\n𝑛\n9𝑛\n𝑛+10\n𝑛=10\n81𝑛\n9𝑛+100\n9𝑛+100\n+100\n𝑛\n𝑛=100\nUsing the same logic as before, the number of levels needed for the longest branch to hit the base case is log10∕9(𝑛). Since each level of the tree\ncompletes at most work, the total amount of work needed if you ended up with a 9-1 split every time is Θ(𝑛log(𝑛)).Θ(𝑛) Θ(𝑛)×log10∕9(𝑛)=\nThus, even if you got super unlucky and every pivot resulted in a 9-1 split, the time complexity of quicksort would still be Θ(𝑛log(𝑛)). On\naverage, you would expect to do better than a 9-1 split every time, so the average-case time complexity must intuitively also be Θ(𝑛log(𝑛)).\nNote that these two explanations are not mathematically rigorous, and that formal proofs are significantly more complex. However, they are\nincluded to provide an intuitive description as to why quicksort runs in average-case time.Θ(𝑛log(𝑛))\nIn summary, quicksort has a best-case time complexity of Θ(𝑛log(𝑛)), which occurs if the median is selected as the pivot every time (and\nΘ(𝑛2),thus partitions the array into two subarrays of roughly the same size). Quicksort has a worst-case time complexity of which occurs if\neither the largest or smallest element gets selected as the pivot at every step (which results in the most imbalanced partition possible, where all\nelements end up on one side of the pivot). In the average-case, however, quicksort runs in time, which we showed above.Θ(𝑛log(𝑛))\nHow can we improve the performance of quicksort? If we always select the last element as the pivot, we may end up selecting a bad pivot\nevery once in a while. Is there a way to reduce the likelihood of selecting a bad pivot?\nstd::nth_element()As mentioned before, the best pivot you can choose for quicksort is the median. Thus, one method is to use the\nfunction at every iteration of quicksort to identify the median in time. By doing this, the worst-case time complexity of quicksort becomesΘ(𝑛)\nΘ(𝑛log(𝑛)), since the median would always be chosen as the pivot. However, this approach is never used in practice. Even though the worst-case\ntime complexity is theoretically better, the algorithm itself runs slower in most cases. This is because the coefficient term in front of the 𝑛log(𝑛)\nis large enough that the change does not translate to increased performance for reasonable input sizes.\nSince it is expensive to find the median at every step, we can instead estimate the median by taking a sample of the elements we want to sort.\nA simple approach would be to randomly select three elements in the array and select the median of these three elements as the pivot. Randomly\nselecting three elements is a constant time operation, so it is not as costly as finding the exact median. The worst-case time complexity of\nΘ(𝑛2)this approach would still be since you could still get extremely unlucky with your random values, but the likelihood of this worst case\nhappening is extremely low. In the example below, 1, 42, and 28 are randomly chosen, and the median of these three numbers, 28, is used as the\npivot. To make partitioning easier, the chosen pivot, 28, can be swapped with the last element — by doing so, we can run quicksort as if the last\nelement were always chosen as the pivot.\n15 3417\n2\n1\n4\n5734 1842 14\n9\n28 84\n5\nIt should also be noted that quicksort uses auxiliary space in the worst case. Consider the code for quicksort:Θ(log(𝑛))\n1\nvoid quicksort(std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\nif (left + 1 >= right) {\n3\nreturn;\n4\n} // if\n5\nint32_t pivot = partition(vec, left, right);\n6\nquicksort(vec, left, pivot);\n7\nquicksort(vec, pivot + 1, right);\n8\n} // quicksort()", "word_count": 945, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "73a56d76-03fd-5fa4-9092-1da1b6f4f7aa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 437, "real_page_number": null, "text": "14.7 Mergesort\n425\nBecause quicksort is not tail-recursive (there are two recursive calls on lines 6 and 7), the algorithm requires additional memory in the form\nof stack frames. By implementing quicksort such that the partition with the smaller input size is recursively sorted first (on line 6), we can\nguarantee that the partition with the larger input size is sorted as the last step of the algorithm (on line 7). This ensures that memory usage does\nnot approach Θ(𝑛), since the deeper recursive call is always done last and is tail recursive. The smaller recursive call will only require up to\nstack frames (since if it uses any more, it cannot be the smaller recursive call). The additional stack frames required by quicksort isΘ(log(𝑛))\ntherefore bounded by Θ(log(𝑛)).\n1\nvoid quicksort(std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\nif (left + 1 >= right) {\n3\nreturn;\n4\n} // if\n5\nint pivot = partition(vec, left, right);\n6\nif (pivot - left < right - pivot) {\n7\nquicksort(vec, left, pivot);\n8\nquicksort(vec, pivot + 1, right);\n9\n} // if\n10\nelse {\n11\nquicksort(vec, pivot + 1, right);\n12\nquicksort(vec, left, pivot);\n13\n} // else\n14\n} // quicksort()\nOverall, the efficiency of quicksort depends on which pivot is selected. If the pivot value always ends up as the largest or smallest value to be\nΘ(𝑛2).sorted, the runtime of quicksort ends up being However, this is rather uncommon, and tuning methods like random sampling can be used\nto prevent this worst case from happening frequently. Additionally, most implementations of quicksort are not stable due to the nature of the\npartitioning step, and the complex structure of the algorithm makes it non-adaptive. A summary of the quicksort algorithm is shown below:\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(𝑛2)\nΘ(log(𝑛))\nNo\nNo\n14.7\nMergesort\nMergesort is another divide-and-conquer sorting algorithm. During the mergesort process, the input array is divided into two halves. The two\nmerge()halves are then sorted recursively and merged together using a function. An implementation of mergesort is shown in the code below:\n1\nvoid mergesort(std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\n// base case: if there are fewer than two items, return\n3\nif (right - left < 2) {\n4\nreturn;\n5\n} // if\n6\n// split the array in half\n7\nint32_t mid = left + (right - left) / 2;\n8\n// recursive merge the left and right halves\n9\nmergesort(vec, left, mid);\n10\nmergesort(vec, mid, right);\n11\n// merge the two sorted halves together\n12\nmerge(vec, left, mid, right);\n13\n} // mergesort()\nOn line 12, we merge two sorted arrays together. This merging step can be done in time using the following process:Θ(𝑛)\ni j,1. Keep track of two indices, and that refer to the first element of each of the sorted halves.\nvec.size()2. Initialize a separate vector of size to store the merged output.\nvec[i] <= vec[j], vec[i] i. vec[j]3. If copy to the output vector and increment Otherwise, copy to the output vector and\nj.increment\n4. Repeat until the entire output vector is filled. If one sorted half reaches the end before the other, simply append all of the remaining\nelements in the unfinished half into the output vector.\nmerge() [5, 6, 3, 1, 8, 2, 4, 7].Consider the function on the following unsorted array: During the mergesort process, we first\n[5, 6, 3, 1] [8, 2, 4, 7]recursively mergesort the two halves of the array, and (as shown on lines 9 and 10 of the above code). This sorts\nthe two halves of the array. After this step, the contents of the array are as follows:\n1\n3\n5\n6\n2\n4\n7\n8\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nsorted\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nsorted", "word_count": 640, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "61fa4189-d5b8-597a-a070-3e6f6893cd4c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 438, "real_page_number": null, "text": "426\nChapter 14. Sorting Algorithms\ni jTo merge the two sorted halves, we will first initialize an empty vector to store the sorted output and initialize indices and to refer to the first\nkelements of the two halves. The variable represents the next open slot in the output vector.\n1\n3\n5\n6\n2\n4\n7\n8\ni\nj\nOutput\nk\ni j k i jWe then compare and and copy the smaller of the two values to the output vector. The index is then incremented, along with either or\ni.depending on which element was copied. In this case, 1 < 2, so we move 1 to the output vector and increment\n1\n3\n5\n6\n2\n4\n7\n8\ni\nj\nOutput\n1\nk\ni j j.Now, points to 3 and points to 2. Since 2 < 3, we copy 2 to the output vector and increment\n1\n3\n5\n6\n2\n4\n7\n8\ni\nj\nOutput\n1\n2\nk\ni.Since 3 < 4, we copy 3 to the output vector and increment\n1\n3\n5\n6\n2\n4\n7\n8\ni\nj\nOutput\n1\n2\n3\nk\nj.Since 4 < 5, we copy 4 to the output vector and increment\n1\n3\n5\n6\n2\n4\n7\n8\ni\nj\nOutput\n1\n2\n3\n4\nk\ni.Since 5 < 7, we copy 5 to the output vector and increment\n1\n3\n5\n6\n2\n4\n7\n8\ni\nj\nOutput\n1\n2\n3\n4\n5\nk\ni.Since 6 < 7, we copy 6 to the output vector and increment\n1\n3\n5\n6\n2\n4\n7\n8\ni\nj\nOutput\n1\n2\n3\n4\n5\n6\nk\niNotice that has gone off the edge of its portion of the array! As a result, we know that every element in the first half has been added to the\noutput vector already. This means that the remaining elements in the second half are guaranteed to be larger than the elements added so far, so\nthey can be copied to the output vector one by one.\n1\n3\n5\n6\n2\n4\n7\n8\nOutput\n1\n2\n3\n4\n5\n6\n7\n8", "word_count": 373, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0112f922-cb1b-5479-82fc-61d5bf7aa5ea", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 439, "real_page_number": null, "text": "14.7 Mergesort\n427\nThis merge process is implemented in the code below:\n1\nvoid merge(std::vector<int32_t>& int32_t int32_t int32_tvec, left, mid, right) {\n2\nint32_t size = right - left;\n3\nstd::vector<int32_t> output(size);\n// init output vector with 'size' elements\n4\nfor (int32_t i = left, j = mid, k = 0; k < size; ++k) {\n5\nif (i == mid) {\n// if first half is complete\n6\noutput[k] = vec[j++];\n// copy all elements in second half over\n7\n} // if\n8\nelse if (j == right) {\n// if second half is complete\n9\noutput[k] = vec[i++];\n// copy all elements in first half over\n10\n} // else if\n11\nelse {\n12\nif (vec[i] <= vec[j]) {\n13\noutput[k] = vec[i++];\n// add vec[i] to output\n14\n} // if\n15\nelse {\n16\noutput[k] = vec[j++];\n// add vec[j] to output\n17\n} // else\n18\n} // else\n19\n} // for i\n20\n// copy contents of output to original vector\n21\nstd::copy(output.begin(), output.end(), vec.begin() + left);\n22\n} // merge()\nmerge() vec[i] vec[j]Notice that duplicates are handled by this function because is always added before if the two values are equal.\nThis ensures that the correct number of duplicates are added to the output vector, and duplicates in the first half are copied to the output vector\nbefore duplicates in the second half. Since merging preserves the relative ordering of duplicate values, the mergesort algorithm is also stable!\nmerge()What is the time complexity of mergesort? If we look at the function implemented above, we can see that the algorithm does a\nsingle pass through all the elements in its input vector range. Thus, the merging operation takes time. We can now express the mergesortΘ(𝑛)\nfunction as a recurrence relation (the code is reproduced below).\n1\nvoid mergesort(std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\n// base case: if there are fewer than two items, return\n3\nif (right - left < 2) {\n4\nreturn;\n5\n} // if\n6\n// split the array in half\n7\nint32_t mid = left + (right - left) / 2;\n8\n// recursive merge the left and right halves\n9\nmergesort(vec, left, mid);\n10\nmergesort(vec, mid, right);\n11\n// merge the two sorted halves together\n12\nmerge(vec, left, mid, right);\n13\n} // mergesort()\nmerge()Since mergesort makes two recursive calls with input size and calls (which takes linear time), the mergesort process can be𝑛∕2\nexpressed using the following recurrence relation:\n𝑇(𝑛)=\n{\n1,\nif 𝑛=1\n2𝑇(𝑛∕2)+𝑛,\nif 𝑛>1\nUsing Master’s Theorem gives us a complexity of Θ(𝑛log(𝑛)). Since mergesort always splits the input into two halves and merges them together,\nthe best-case, average-case, and worst-case time complexities all involve the same recurrence relation, and thus are all Θ(𝑛log(𝑛)).\nAdditionally, since mergesort initializes an output vector to store the contents of the sorted output, the auxiliary space required by the\nsorted.1algorithm is Θ(𝑛), as the size of the output vector is linear on the number of elements that need to be\nUnlike many of the other sorting algorithms covered, mergesort does not require random access. As a result, mergesort is great for linked\nlist sorting, external memory sorting, and parallel sorting. External memory sorting is useful if the data that needs to be sorted is so large that it\ncannot fit into the main memory of a computer. Mergesort can also be done concurrently; if you have a large collection of data, you can split the\ndata into segments and send each part to a different server. Each server is then able to sort its input separately and return the output, which can\nthen be combined together.\n1Itisactuallypossibletorunmergesortona using auxiliaryspace,butwewillnotcoverthisimplementationhere.linkedlist Θ(1)", "word_count": 646, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4cdfddb2-166e-5fe8-8529-f657d3784b2c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 440, "real_page_number": null, "text": "428\nChapter 14. Sorting Algorithms\nThe following diagram illustrates the entire mergesort process on our initial array.\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n1\n3\n2\n8\n4\n7\n1\n3\n5\n6\n2\n4\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\nThe implementation of mergesort we have introduced is a implementation, since it uses recursion to break the input array into halvestop-down\nthat can be sorted individually. However, it is also possible to implement mergesort iteratively, without any recursive calls. The following is an\nimplementation of this mergesort:bottom-up\n1\nvoid mergesort(std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\nfor (int32_t size = 1; size <= right - left; size *= 2) {\n3\nfor (int32_t i = left; i <= right - size; i += 2 size) {*\n4\nmerge(vec, i, i + size, min(i + 2 size, right));*\n5\n} // for i\n6\n} // for size\n7\n} // mergesort()\nforThis variation of mergesort uses nested loops to simulate the behavior of the recursive calls. First, it merges groups of two elements, then it\nmerges groups of four elements, then it merges groups of eight elements, and so on until the entire array is sorted. To illustrate this, consider the\n[5, 6, 3, 1, 8, 2, 4, 7].unsorted array On the first pass, the algorithm considers elements in pairs of two and merges these pairs together:\n[5, 6, 3, 1, 8, 2, 4, 7]\n1, 3,[5, 6, 8, 2, 4, 7]\n2, 8,[5, 6, 1, 3, 4, 7]\n4, 7][5, 6, 1, 3, 2, 8,\nMerge 5 and 6 in sorted order.\nMerge 3 and 1 in sorted order.\nMerge 8 and 2 in sorted order.\nMerge 4 and 7 in sorted order.\nsizeOn the second pass, increases to two, and the algorithm merges elements in groups of four:\n[1, 3, 5, 6, 2, 8, 4, 7]\n2, 4, 7, 8][1, 3, 5, 6,\nMerge 5, 6, 1, and 3 in sorted order.\nMerge 2, 8, 4, and 7 in sorted order.\nOn the third and final pass, size gets increased to three, and the algorithm merges elements in groups of eight:\n[1, 2, 3, 4, 5, 6, 7, 8]\nMerge 1, 3, 5, 6, 2, 4, 7, and 8 in sorted order.\nA summary of mergesort is shown in the table below:\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(𝑛)\nYes\nNo", "word_count": 445, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cfbeb5f2-79be-5785-b9f0-2d08148cd5b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 441, "real_page_number": null, "text": "14.8 Analysis of Comparison Sorts\n429\n14.8\nAnalysis of Comparison Sorts\nAll of the sorting algorithms we have discussed so far are sorting algorithms. These sorting algorithms sort a container bycomparison-based\ncomparing elements with each other using a comparison operator.\nSort\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nBubble\nΘ(𝑛)\nΘ(𝑛2)\nΘ(𝑛2)\nΘ(1)\nYes\nYes\nSelection\nΘ(𝑛2)\nΘ(𝑛2)\nΘ(𝑛2)\nΘ(1)\nNo\nNo\nInsertion\nΘ(𝑛)\nΘ(𝑛2)\nΘ(𝑛2)\nΘ(1)\nYes\nYes\nHeap\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(1)\nNo\nNo\nQuick\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(𝑛2)\nΘ(log(𝑛))\nNo\nNo\nMerge\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(𝑛log(𝑛))\nΘ(𝑛)\nYes\nNo\nJust by looking at the table, one may assume that heapsort, quicksort, and mergesort should always be used over the elementary sorts, since their\ntime complexities are the best. However, this is not actually true. If the array you want to sort is small, insertion sort actually performs the best!\nHow can this be? In this section, we will answer three relevant questions regarding the analysis of comparison-based sorting algorithms.\n1. How is it possible for an elementary sort to perform better than an advanced sort?\n2. Is it possible for a comparison sort to have a worst-case time complexity that is better than Θ(𝑛log(𝑛))?\nstd::sort()?3. Which sorting implementation does the STL use in its implementation of\n¸ 14.8.1\nPerformance of Elementary and Advanced Sorts\nFor the first question, it is important to remember that time complexity is simply a measurement of how runtime scales, not the actual runtime\nitself. For instance, an algorithm that takes 2𝑛steps is part of the same complexity class as an algorithm that takes 200𝑛steps — however,\nthe 2𝑛algorithm would run much faster than the 200𝑛algorithm. This can be extended to algorithms in different complexity classes. For\n2𝑛2instance, if one algorithm takes steps and the other takes steps, the second algorithm has a better time complexity than the first200𝑛log(𝑛)\none. However, this only tells us that the runtime of the second algorithm scales slower than the runtime of the first algorithm as the input size\n2𝑛2grows. This does not mean that the second algorithm is downright faster; if 𝑛is small, the algorithm would run faster than the 200𝑛log(𝑛)\nΘ(𝑛2)algorithm, despite the fact that a algorithm has a better time complexity than a algorithm. This reasoning is precisely whyΘ(𝑛log(𝑛))\ninsertion sort is faster than many sorts for small input sizes, despite its inferior complexity class. Because insertion sort is simple, itΘ(𝑛log(𝑛))\ndoes not need to do as much work per element compared to the more advanced sorts, giving it an advantage for small input sizes. The benefit of\ndoing more work per element only becomes advantageous as the input size becomes large.\n¸ 14.8.2\nCan Comparison Sorts Do Better Than Θ(n log n) Time?\nThe second question is bit tougher to answer, but the answer is no: it is not possible for a comparison sort to have a worst-case time complexity\nthat is better than Θ(𝑛log(𝑛)). If you are only allowed to use comparisons to determine the sorted order of elements in an arbitrary container,\nyou make comparisons in the worst case if you want to ensure that your container is sorted. Let’s look at why.Ω(𝑛log(𝑛))must\ncomp x y, comp(x, y)Suppose you have a comparator that can be used to compare two elements. Given two values and would return\n≤ytrue x false x > y. a, b, c,if and if You are then given three values, and and you are told to determine the sorted order of these three\ncomp.values using only the comparator The comparisons you need to make to determine the sorted order of these three elements are shown in\nthe following decision tree:\ncomp(a, b)\ncomp(a, c)\ncomp(b, c)\n[c, b, a]\n[b, c, a]\n[b, a, c]\ncomp(b, c)\ncomp(a, c)\n[c, a, b]\n[a, c, b]\n[a, b, c]\nT\nF\nT\nF\nT\nF\nT\nF\nT\nF\nSince you are given three values, there are a total of 3! = 6 permutations that could potentially be the true sorted order. These six permutations\nare bolded in the tree above. To determine which of the six permutations is the correct sorted order, you would have to compare the elements\ncomp(a, b) true, awith each other and walk down the correct branch of the tree. For instance, if returned that would mean is less than or\nb, a b).equal to and you would walk down the left branch of the tree (these are all the permutations where is before\ncomp(a, b)\ncomp(a, c)\ncomp(b, c)\n[c, b, a]\n[b, c, a]\n[b, a, c]\ncomp(b, c)\ncomp(a, c)\n[c, a, b]\n[a, c, b]\n[a, b, c]\nT\nF\nT\nF\nT\nF\nT\nF\nT\nF", "word_count": 795, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "51c58a3e-624d-57eb-b7fe-ad6a6a480d46", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 442, "real_page_number": null, "text": "430\nChapter 14. Sorting Algorithms\nb c. comp(b, c) true, [a, b, c], aThen, you would compare and If returns you would know that the correct order is since is less than or\nb, b c.equal to and is less than or equal to\ncomp(a, b)\ncomp(a, c)\ncomp(b, c)\n[c, b, a]\n[b, c, a]\n[b, a, c]\ncomp(b, c)\ncomp(a, c)\n[c, a, b]\n[a, c, b]\n[a, b, c]\nT\nF\nT\nF\nT\nF\nT\nF\nT\nF\ncomp(b, c) false, a c c a,However, if returns you would have to compare and before you can determine the correct order. If is less than\n[c, a, b]. [a, c, b].the correct order would be Otherwise, it would be\ncomp(a, b)\ncomp(a, c)\ncomp(b, c)\n[c, b, a]\n[b, c, a]\n[b, a, c]\ncomp(b, c)\ncomp(a, c)\n[c, a, b]\n[a, c, b]\n[a, b, c]\nT\nF\nT\nF\nT\nF\nT\nF\nT\nF\nIn the worst case, how many comparisons do you need before you can determine with certainty what the correct sorted order is? Notice that\ncomp(a, b) true,every comparison you make gets rid of roughly half the possible permutations. For instance, if returns you would be able\n[b, a, c], [b, c, a], [c, b, a]to eliminate and as possible orders.\ncomp(a, b)\ncomp(a, c)\ncomp(b, c)\n[c, b, a]\n[b, c, a]\n[b, a, c]\ncomp(b, c)\ncomp(a, c)\n[c, a, b]\n[a, c, b]\n[a, b, c]\nT\nF\nT\nF\nT\nF\nT\nF\nT\nF\nIf you given a container of 𝑛elements, there would be a total of possible permutations that could potentially be the sorted order. Every𝑛!\ncomparison that you make is able to get rid of half of the permutations from the set of possible orders. As a result, the number of comparisons\nyou need in the worst case is equal to the number of times you can divide in half before it reaches 1 (i.e., halving permutations until a𝑛! 𝑛!\nsingle permutation is left). We can express the number of comparisons that are needed as the variable 𝑥in the following equation:\n𝑛!\n2𝑥=1\nSolving for 𝑥, we get:\n𝑥=log2(𝑛!)\nThus, comparisonsareneededataminimumtosort𝑛elementsintheworstcase. Sinceweprovedinchapter4that Θ(𝑛log(𝑛)),log2(𝑛!) log(𝑛!)=\nthe number of comparisons needed to sort a container must also be at least Θ(𝑛log(𝑛)). Because of this, the worst-case time complexity of any\ncomparison-based sorting algorithm cannot be better than Θ(𝑛log(𝑛)).\n¸ 14.8.3\nIntrosort and STL Sorting Implementations\nstd::sort()Lastly, the STL relies on a hybrid sorting algorithm known as introsort (short for sort) to implement the function.introspective\nIntrosort starts off with quicksort, but it switches to heapsort if the recursion depth is too high. This prevents introsort from ending up with the\nΘ(𝑛2) scenario of quicksort. Furthermore, if a quicksort partition is very small (fewer than 16 elements on GCC), introsort switches to insertion\nsort, which is fastest on small arrays.\nIntrosort is able to achieve average-case and worst-case performance by following this process. Thus, introsort is the perfectΘ(𝑛log(𝑛))\nembodiment of an adaptive sort, as it uses many factors to determine which sorting algorithm it should use at any point in time. Since introsort\nstd::stable_sort()relies on non-stable sorts, it itself is not stable. However, the function can be used to maintain stability while sorting.\nstd::stable_sort(),The STL relies on mergesort to implement as mergesort is the only advanced comparison sort that maintains the\nrelative order of duplicate elements.", "word_count": 603, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7b39bf03-3129-5bc2-b341-0ed2795ccdb8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 443, "real_page_number": null, "text": "14.9 Counting Sort\n431\n14.9\nCounting Sort\nIn this section, we will look at a sorting algorithm. Unlike comparison-based sorting algorithms, linear-time sorting algorithms maylinear-time\nrun faster than Θ(𝑛log(𝑛)), but they require specific conditions on the input that must be met. As a result, these sorting algorithms may not be\napplicable to all situations.\nCounting sort is a linear-time sorting algorithm that can be used to sort data whose values that fall within a limited set of keys. This\nalgorithm works by first counting the number of times each key appears, and then using arithmetic to determine where each key should be\nplaced in the output sequence. To illustrate this process, consider the following vector of grades. Assume that a grade can only take on six\npossible values: A, B, C, D, E, and F.\nD\nB\nA\nC\nB\nF\nA\nB\nDuring counting sort, we first do an initial pass of the data and count the number of times each key appears. The results from this initial pass are\nshown below (in this example, there are 2 A’s, 3 B’s, 1 C, 1 D, 0 E’s, and 1 F):\n2\n3\n1\n1\n0\n1\nA\nB\nC\nD\nE\nF\nThen, we iterate over the counters to compute offsets that determine where each key should begin in the sorted container. This is done by adding\neach counter with the counter values before it. Using the example, the counter for B is updated to 2 + 3 = 5, the counter for C is updated to 2 +\n3 + 1 = 6, the counter for D is updated to 2 + 3 + 1 + 1 = 7, and so on.\n2\n5\n6\n7\n7\n8\nA\nB\nC\nD\nE\nF\nAfter this, we do another pass of the original data, but this time in reverse order (this allows counting sort to be stable). For each value we\nencounter during this pass, we\n1. look up the value’s offset in the offset vector (we’ll call this value 𝑘)\n2. place the value at index of an output vector𝑘−1\n3. decrement 𝑘in the offset vector\nIn our example, the first data value we encounter is B. We see that B has an offset value of 5. As a result, we would write B to index 5 - 1 = 4 of\nan output vector and decrement the offset of B.\nB\n0\n1\n2\n3\n4\n5\n6\n7\n2\n4\n6\n7\n7\n8\nA\nB\nC\nD\nE\nF\nThe next value is A. Since A has an offset value of 2, we write A to index 2 - 1 = 1 of the output vector and decrement the offset of A.\nA\nB\n0\n1\n2\n3\n4\n5\n6\n7\n1\n4\n6\n7\n7\n8\nA\nB\nC\nD\nE\nF\nThe next value is F. Since F has an offset value of 8, we write F to index 8 - 1 = 7 of the output vector and decrement the offset of F.\nA\nB\nF\n0\n1\n2\n3\n4\n5\n6\n7\n1\n4\n6\n7\n7\n7\nA\nB\nC\nD\nE\nF\nThe next value is B. Since B has an offset value of 4, we write B to index 4 - 1 = 3 of the output vector and decrement the offset of B.\nA\nB\nB\n0\n1\n2\n3\n4\n5\n6\n7\n1\n3\n6\n7\n7\n7\nA\nB\nC\nD\nE\nF\nThe next value is C. Since C has an offset value of 6, we write C to index 6 - 1 = 5 of the output vector and decrement the offset of C.\nA\nB\nB\nC\n0\n1\n2\n3\n4\n5\n6\n7\n1\n3\n5\n7\n7\n7\nA\nB\nC\nD\nE\nF\nThe next value is A. Since A has an offset value of 1, we write A to index 1 - 1 = 0 of the output vector and decrement the offset of A.\nA\nA\nB\nB\nC\n0\n1\n2\n3\n4\n5\n6\n7\n0\n3\n5\n7\n7\n7\nA\nB\nC\nD\nE\nF\nThe next value is B. Since B has an offset value of 3, we write B to index 3 - 1 = 2 of the output vector and decrement the offset of B.\nA\nA\nB\nB\nB\nC\nF\n0\n1\n2\n3\n4\n5\n6\n7\n0\n2\n5\n7\n7\n7\nA\nB\nC\nD\nE\nF", "word_count": 769, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c5533765-ea01-5111-aaf6-887bc28ca79c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 444, "real_page_number": null, "text": "432\nChapter 14. Sorting Algorithms\nThe next value is D. Since D has an offset value of 7, we write D to index 7 - 1 = 6 of the output vector and decrement the offset of D. After\nwriting D, we are done with the traversal, and everything is sorted.\nA\nA\nB\nB\nB\nC\nD\nF\n0\n1\n2\n3\n4\n5\n6\n7\n0\n2\n5\n6\n7\n7\nA\nB\nC\nD\nE\nF\nThe code for this example is provided below:\n1\nvoid counting_sort(std::vector<char>& grades) {\n2\nsize_t num_keys = 6;\n// six possible grade values\n3\nstd::vector<char> output(grades.size());\n4\nstd::vector<size_t> offsets(num_keys);\n5\n// first pass: iterate through data and add to counter\n6\nfor (auto it = grades.begin(); it != grades.end(); ++it) {\n7\n++offsets[static_cast<size_t>(*it - 'A')];\n// A = 0, B = 1, ...\n8\n} // for it\n9\n// second pass: calculate cumulative sum for offset\n10\nfor (size_t i = 1; i < num_keys; ++i) {\n11\noffsets[i] += offsets[i - 1];\n12\n} // for i\n13\n// third pass: write to output vector\n14\nfor (auto it = grades.rbegin(); it != grades.rend(); ++it) {\n15\noutput[--offsets[static_cast<size_t>(*it - 'A')]] = *it;\n16\n} // for it\n17\nstd::swap(grades, output);\n18\n} // counting_sort()\nWhat is the time complexity of counting sort? Notice that this algorithm does two traversals of the original data vector and one traversal of the\noffset vector. If we denote the size of the data vector as 𝑛and the size of the offset vector as 𝑘, the total work completed by the algorithm is\n2𝑛+𝑘, or Θ(𝑛+𝑘). Similarly, since the algorithm allocates an additional output vector of size 𝑛and an additional offset vector of size 𝑘, the\nauxiliary space used is Θ(𝑛+𝑘).\nThere is another variation of counting sort that can be used if the objects that are being counted are also the objects that need to be sorted.\nIn this variation, the values to be sorted are written directly into the original vector, removing the need for a separate output vector. A sample\nimplementation is shown below (using the grade example):\n1\nvoid in_place_counting_sort(std::vector<char>& grades) {\n2\nsize_t num_keys = 6;\n// six possible grade values\n3\nstd::vector<size_t> counters(num_keys);\n4\n// count the number of times each grade appears\n5\nfor (char grade : grades) {\n6\n++counters[static_cast<size_t>(grade - 'A')];\n7\n} // for grade\n8\n// use the counters to determine how many of each grade to write to output\n9\nsize_t current = 0;\n10\nfor (size_t i = 0; i < num_keys; ++i) {\n11\nfor (size_t j = 0; j < counters[i]; ++j) {\n12\nstatic_cast<char>('A'grades[current++] = + i);\n13\n} // for j\n14\n} // for i\n15\n} // in_place_counting_sort()\nThis implementation essentially counts the number of A’s, B’s, C’s, D’s etc. that exist in the data. After counting, the algorithm directly writes\nthe correct number of each key to the output vector (in the example above, it would iterate through the original vector and overwrite the first two\nvalues with A’s, the next three values with B’s, and so on). This in-place variation of counting sort uses less auxiliary space, but it may not\nwork in all situations depending on the type of object being sorted. It is also stable, which could cause issues depending on the type ofnot\nproblem you are trying to solve. For example, the implementation of — another linear-time sorting algorithm that can be used to sortradix sort\na collection of numeric values — relies on a stable implementation of counting sort to work.\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nΘ(𝑛+𝑘)\nΘ(𝑛+𝑘)\nΘ(𝑛+𝑘)\nΘ(𝑛+𝑘)\nYes\nNo\n14.10\nRadix Sort (✽)\nRadix sort is another linear-time sorting algorithm that can be used to sort a collection of numeric values or fixed-size strings. During the radix\nsort process, the individual digits or characters of the data values are first grouped together based on their significance position. Then, counting\nsort\nthe following container of numbers, which will be sorted using radix sort:\n659\n424\n953\n139\n380\n811\n187\n354\n769\n415\n286\n331", "word_count": 696, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4f58b554-9636-5389-ac21-04f7a78790c7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 445, "real_page_number": null, "text": "14.11 Index Sorting\n433\nFirst, counting sort is used to sort the numbers based on only the least significant digit (the ones digit). Since counting sort is stable, the relative\nordering of numbers with the same ones digit does not change after the sort is completed (e.g., since 659 comes before 139 in the original data,\n659 would still be before 139 after this initial sort).\n659\n424\n953\n139\n380\n811\n187\n354\n769\n415\n286\n331\n380\n811\n331\n953\n424\n354\n415\n286\n187\n659\n139\n769\nThen, counting sort is used to sort the numbers based on only the second-least significant digit (the tens digit).\n380\n811\n331\n953\n424\n354\n415\n286\n187\n659\n139\n769\n811\n415\n424\n331\n139\n953\n354\n659\n769\n380\n286\n187\nThen, counting sort is used to sort the numbers based on only the next least-significant digit (the hundreds digit).\n811\n415\n424\n331\n139\n953\n354\n659\n769\n380\n286\n187\n139\n187\n286\n331\n354\n380\n415\n424\n659\n769\n811\n953\nAfter sorting on the most significant digit of the input, the entire container is sorted. Since the number of counting sort passes we need is equal\nto the number of digits in the largest value, the time complexity of radix sort is Θ(𝑑(𝑛+𝑘)), where 𝑑is the number of digits in the largest value,\n𝑛is the input size, and 𝑘is the number of distinct keys that need to be sorted; with integers, 𝑘is the base that the input is expressed in (e.g.,\nbase 10 in the above example). This is because each pass of counting sort takes time, and 𝑑passes need to be performed before theΘ(𝑛+𝑘)\ncontainer is fully sorted. The auxiliary space used by radix sort is the same as that of counting sort, or Θ(𝑛+𝑘).\nBest Time\nAverage Time\nWorst Time\nAuxiliary Space\nStable?\nAdaptive?\nΘ(𝑑(𝑛+𝑘))\nΘ(𝑑(𝑛+𝑘))\nΘ(𝑑(𝑛+𝑘))\nΘ(𝑛+𝑘)\nYes\nNo\n14.11\nIndex Sorting\nIndex sorting is a method that can be used to access elements of a container in sorted order without actually sorting the container’s data itself.\nThis method is useful if you want to know the sorted order of a collection of elements without changing its original order, or if the container you\nwant to sort contains large objects that are too expensive to move around.\nRecall that an element’s represents its position in a container. The very first element is located at index 0, the second element isindex\nlocated at index 1, and so on. The goal of index sorting is to sort a that serves as a proxy for the data in the originalseparate container of indices\ncontainer. The indices, however, are not sorted in ascending order themselves; the \"sorted\" ordering of indices instead depends on the order of\ni j,elements in the original data container. For instance, if the element at index of the original container is smaller than the element at index\ni jthen would come before in the vector of indices. By sorting indices in this manner, you will be able to access the original data elements in\ndoublesorted order without having to physically sort the original elements themselves. For example, consider the following vector of objects,\nvec:which we will call\nvec\n7.5\n4.4\n6.7\n3.2\n5.5\n1.3\n9.6\n8.3\n0\n1\n2\n3\n4\n5\n6\n7\ndoubles vecSuppose we wanted to know the sorted order of the in this vector without modifying the order of elements in itself. We can\nidx = [0, 1, 2, 3, 4, 5, 6, 7])this vector by initializing a separate vector of indices (e.g., and sort these indices based on theindex sort\ndouble vec 1.3,value they reference in the original vector. For example, the smallest number in is which is at index 5. Thus, the value\n5 idx 3.2 idxends up at index 0 in the sorted vector. The next smallest value is at index 3, so the second value in the sorted vector is 3.\nidx [5, 3, 1, 4, 2, 0, 7, 6].Continuing this logic, the final contents of after index sorting would be\nvec vec idx,This way, we can retrieve the elements of in sorted order without modifying the contents of itself. After sorting we know\nvec idx[0], idx[1],that the smallest element of is located at index the second smallest element is located at index and so on. As an\nvecexample, the following code can be used to print all the elements of in sorted order:\n1\nfor (size_t i = 0; i < idx.size(); ++i) {\n2\nstd::cout << vec[idx[i]] << '\\n';\n3\n} // for i", "word_count": 779, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b29f9fe1-681e-51e3-88fb-1bae83244d12", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 446, "real_page_number": null, "text": "434\nChapter 14. Sorting Algorithms\nstd::sort() compHow can we implement index sorting in a program? Recall that the function accepts an optional comparator that can be\nused to determine the ordering of elements:\nvoid std::sort(RandomAccessIterator first, RandomAccessIterator last, Compare comp);\nInstead of sorting the vector of indices in ascending order (which would have been the default without a comparator passed in), we want to pass\nstd::sort() idx vec.in a comparator that tells to sort the indices in based on the contents of Thus, our comparator will need to somehow\nknow the values of our original data, so that it knows how the indices should be ordered.\nThis can be done by using the comparator’s internal state to store a to the original data that you want to index sort. A reference isreference\npreferred because you do not want to make a separate copy of the container every time a comparator is constructed (e.g., if you had a giant\nvector of large objects, the comparator should not make an entirely new copy of this giant vector just to sort the indices). Because references\nmust be initialized upon creation, you must use a constructor with an initializer list to initialize the comparator. An example is shown below:\n1\ntemplate <typename T>\n2\nclass IndexSortComparator {\n3\nconst std::vector<T>& data; // reference to original data to index sort\n4\npublic:\n5\nIndexSortComparator(const std::vector<T>& vec)\n6\n: data{vec} {}\n7\n...\n8\n};\noperator().Since this is a comparator, you will have to implement Normally, to implement a comparator that can be used to sort a container\nof integers in ascending order, you would just return whether one value is less than the other (if two elements are passed into the comparator, a\ntruereturn value of indicates that the first element comes before the second after sorting):\n1\nbool operator() (uint32_t uint32_t consti, j) {\n2\nreturn i < j;\n3\n} // operator()\ni jHowever, with index sort, we aren’t looking at the values of and when determining which one should come first in the sorted vector of\ni jindices. Instead, we want to look at the elements at positions and of the underlying data container! That is, when an index sort is done, the\ni j i j.index should come before if the element at index of the underlying data container is less than the element at index This requires the\noperator():following modification to our overloaded\n1\nbool operator() (uint32_t uint32_t consti, j) {\n2\nreturn data[i] < data[j];\n3\n} // operator()\ndefinition:2Putting this all together, we have the following comparator\n1\ntemplate <typename T>\n2\nclass IndexSortComparator {\n3\nconst std::vector<T>& data;\n4\npublic:\n5\nIndexSortComparator(const std::vector<T>& vec)\n6\n: data{vec} {}\n7\nbool operator() (uint32_t uint32_t consti, j) {\n8\nreturn data[i] < data[j];\n9\n} // operator()\n10\n};\nWith this comparator, we can sort the container of indices so that the sorted order of these indices is determined by the ordering of elements in\nthe underlying data container. An example is shown below:\n1\nstd::vector<double> vec = {7.5, 4.4, 6.7, 3.2, 5.5, 1.3, 9.6, 8.3};\n2\n// initialize vector of indices\n3\nstd::vector<int32_t> idx(vec.size());\n4\n// initialize indices of idx vector\n5\nstd::iota(idx.begin(), idx.end(), 0);\n6\n// initialize comparator by passing in a reference to vec\n7\nIndexSortComparator<double> idx_comp{vec};\n8\n// index sort vector of indices by passing idx_comp into sort function\n9\nstd::sort(idx.begin(), idx.end(), idx_comp);\n10\n// final contents of idx vector after index sort is {5, 3, 1, 4, 2, 0, 7, 6}\n2For operator< T.thistowork, mustbesupportedbythetype", "word_count": 612, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "93fe5273-e527-56a0-9723-9b6b2e03bd78", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 447, "real_page_number": null, "text": "14.11 Index Sorting\n435\nChapter 14 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. If you want to sort an array of 𝑛elements, which of the following statements is FALSE?\nA) The worst-case time complexity of mergesort is Θ(𝑛log(𝑛))\nB) The worst-case time complexity of quicksort is Θ(𝑛log(𝑛))\nC) The worst-case time complexity of heapsort is Θ(𝑛log(𝑛))\nΘ(𝑛2)D) The worst-case time complexity of bubble sort is\nΘ(𝑛2)E) The worst-case time complexity of insertion sort is\n2. You decided to run the same sorting algorithm on two different arrays of size 𝑛, one of which is nearly sorted, while the other is not close\nto being sorted at all. After running the sorting algorithm, you notice that it took less time to sort the nearly-sorted vector. Which of the\nfollowing is most likely to be the sorting algorithm that you used?\nA) Insertion sort\nB) Selection sort\nC) Heapsort\nD) Mergesort\nE) Quicksort\n3. The university registrar has developed a new method for selecting people off the waitlist. Students with more credits are given priority over\nstudents with fewer credits. Ties in credit hours are settled with arrival order — that is, students who signed up for the waitlist earlier are\nselected over students who signed up later. If you could only look at the number of credits, which of the following sorts would NOT allow\nthe registrar to accomplish these goals?\nA) Bubble sort\nB) Selection sort\nC) Insertion sort\nD) Mergesort\nE) All of the above sorts would satisfy the registrar’s requirements\n4. Which of the following statements is TRUE?\nA) Bubble sort can be implemented to be either adaptive or stable, but not both\nB) Insertion sort is fast on nearly sorted arrays but requires auxiliary spaceΘ(𝑛)\nC) Selection sort performs only comparisons on an already sorted arrayΘ(𝑛)\nΘ(𝑛2)D) The best-case time complexity of selection sort is\nE) None of the above\n5. Two of your EECS 281 friends, Daniel and Ryan, want your help with selecting a sorting algorithm that best suits their needs. Daniel has an\narray that holds identical keys, and he does not want the sorting algorithm to modify the order of these identical elements. However, he does\nnot care about the auxiliary space that his sorting algorithm may require. Ryan, on the other hand, desires the opposite: he wants a sorting\nalgorithm that can guarantee him auxiliary space in the worst case for sorting his array, but he does not mind if the sort randomizes theΘ(1)\norder of identical keys in the process. Both students want a sorting algorithm with a worst-case time complexity of Θ(𝑛log(𝑛)). Given these\nrequirements, which of the following is an acceptable recommendation for both Daniel and Ryan?\nA) Daniel should use heapsort, and Ryan should use quicksort\nB) Daniel should use quicksort, and Ryan should use heapsort\nC) Daniel should use heapsort, and Ryan should use mergesort\nD) Daniel should use mergesort, and Ryan should use heapsort\nE) More than one of the above\n6. One way to classify sorting algorithms is by their adaptability. What is an advantage of a non-adaptive sorting algorithm?\nA) Non-adaptive sorting algorithms may perform less work depending on the order of the data\nB) Non-adaptive sorting algorithms are always faster than adaptive algorithms\nC) Non-adaptive sorting algorithms may change its sequence of operations based on the input\nD) Non-adaptive sorting algorithms may be simpler to implement than adaptive algorithms\nE) Non-adaptive sorting algorithms are always guaranteed to be stable\n7. If you want to sort a collection of values that is too large to fit in the main memory of a computing device, one strategy is to use an external\nalgorithm that divides the input into smaller chunks that are small enough to fit in main memory, sorts them individually, andsorting\nrecombines the sorted chunks to obtain the full sorted output. This external sort is most similar to which of the following sorting algorithms?\nA) Bubble sort\nB) Selection sort\nC) Insertion sort\nD) Heapsort\nE) Mergesort", "word_count": 713, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "33e65bf8-baf8-5382-bfc0-5c949fb176cb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 448, "real_page_number": null, "text": "436\nChapter 14. Sorting Algorithms\n8. Which of the following sorting algorithms can be used to sort a linked list of size 𝑛in time, while ensuring that identical elementsΘ(𝑛log(𝑛))\nremain in the same relative order before and after the sort?\nA) Selection sort\nB) Heapsort\nC) Mergesort\nD) Quicksort\nE) None of the above\n9. Which of the following statements is/are TRUE?\nI. When sorting an array of size 𝑛, mergesort requires auxiliary space.Θ(𝑛)\nII. The fastest possible comparison sort has a worst-case time complexity no better than Θ(𝑛log(𝑛)).\nIII. Quicksort will always sort an array faster than bubble sort.\nA) I only\nB) II only\nC) I and II only\nD) I and III only\nE) I, II, and III\n10. You are using quicksort to sort the following 9 elements in ascending order:\nint32_t arr[] = {43, 82, 50, 65, 24, 19, 78, 16, 37}\nWhich of the following elements would be the best choice for the pivot?\nA) 43\nB) 50\nC) 24\nD) 37\nE) None of the above\nΘ(𝑛2)11. Your friend is attempting to implement top-down mergesort, but they incorrectly implemented their merging algorithm to take time\ninstead of time, given an input of size 𝑛. Assuming that all other steps of the mergesort algorithm are implemented correctly, what isΘ(𝑛)\nthe time complexity of your friend’s mergesort algorithm?\nA) Θ(𝑛log(𝑛))\nΘ(𝑛2)B)\nΘ(𝑛2C) log(𝑛))\nΘ(𝑛3)D)\nΘ(𝑛3E) log(𝑛))\n12. Consider two implementations of quicksort. Quicksort A always chooses the rightmost (last) element within the subarray as the pivot, while\nquicksort B always chooses the smallest value within the subarray as the pivot. For example, given the subarray [5, 3, 1, 2, 4], quicksort A\nwould choose 4 as the pivot, while quicksort B would choose 1 as the pivot. If both algorithms can find the correct pivot in constant time\nwith each partition, which of the following statements are TRUE?\nA) The average-case time complexity of quicksort A is than the average-case time complexity of quicksort B.better\nThe worst-case time complexity of quicksort A is than the worst-case time complexity of quicksort B.better\nB) The average-case time complexity of quicksort A is to the average-case time complexity of quicksort B.equal\nThe worst-case time complexity of quicksort A is to the worst-case time complexity of quicksort B.equal\nC) The average-case time complexity of quicksort A is than the average-case time complexity of quicksort B.worse\nThe worst-case time complexity of quicksort A is than the worst-case time complexity of quicksort B.worse\nD) The average-case time complexity of quicksort A is than the average-case time complexity of quicksort B.worse\nThe worst-case time complexity of quicksort A is to the worst-case time complexity of quicksort B.equal\nE) The average-case time complexity of quicksort A is than the average-case time complexity of quicksort B.better\nThe worst-case time complexity of quicksort A is to the worst-case time complexity of quicksort B.equal\n13. You are given three mystery sorting algorithms, sorts A, B, and C. To determine their identities, you run these sorts multiple times on three\ndifferent arrays, each containing 100000 integer values. The results you obtain are shown in the table below:\nSort A\nSort B\nSort C\naverage runtime on unsorted array (seconds)\n6.62\n0.00759\n7.69\naverage runtime on nearly sorted array (seconds)\n6.41\n3.88\n0.000211\naverage runtime on sorted array (seconds)\n6.13\n4.27\n0.000073\nKnowing this information, which one of the following conclusions is most consistent with the observed data? You may assume that\nquicksort always selects the last element as the pivot.\nA) Sort A is insertion sort, sort B is mergesort, sort C is quicksort\nB) Sort A is insertion sort, sort B is heapsort, sort C is bubble sort\nC) Sort A is insertion sort, sort B is quicksort, sort C is mergesort\nD) Sort A is selection sort, sort B is heapsort, sort C is quicksort\nE) Sort A is selection sort, sort B is quicksort, sort C is bubble sort", "word_count": 673, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "62633001-960e-5354-b99a-b8a69ed133a4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 449, "real_page_number": null, "text": "14.11 Index Sorting\n437\n14. What is the time complexity and auxiliary space required to run heapsort on an array of 𝑛elements?\nA) time,Θ(𝑛)\nauxiliary spaceΘ(1)\nB) time, auxiliary spaceΘ(𝑛log(𝑛)) Θ(1)\nC) time, auxiliary spaceΘ(𝑛log(𝑛)) Θ(𝑛)\nΘ(𝑛2)D) time,\nauxiliary spaceΘ(1)\nΘ(𝑛2)E) time,\nauxiliary spaceΘ(𝑛)\n15. Suppose you had the following unsorted array:\n[48, 25, 98, 41, 33, 64, 87, 38, 19]\nYou decided to sort this array using heapsort with bottom-up heapify, and you took six snapshots of the array while it was being sorted.\nHowever, you forgot to keep track of the order of your snapshots, so you must look at the contents of each array to determine which came\nfirst. Given these six snapshots below, in what order were they taken?\n[33, 19, 25, 38, 41, 48, 64, 87, 98]I.\n[48, 41, 25, 38, 33, 19, 64, 87, 98]II.\n[19, 33, 25, 38, 41, 48, 64, 87, 98]III.\n[98, 41, 87, 38, 33, 64, 48, 25, 19]IV.\n[87, 41, 64, 38, 33, 19, 48, 25, 98]V.\n[33, 38, 25, 19, 41, 48, 64, 87, 98]VI.\nA) IV, V, II, VI, I, III\nB) III, I, VI, II, V, IV\nC) IV, V, VI, II, I, III\nD) IV, V, II, I, VI, III\nE) IV, V, II, VI, III, I\n16. Suppose you had the following unsorted array:\n[70, 44, 69, 40, 42, 37, 46, 39]\nYou decided to sort this array using heapsort, and you took four snapshots of the array while it was being sorted. However, you somehow\nended up with five snapshots of this array — and one of them does not belong! The five snapshots are shown below. Which snapshot was\nNOT taken while the array was being sorted using heapsort?\n[39, 40, 37, 42, 44, 46, 69, 70]I.\n[37, 44, 39, 40, 42, 46, 69, 70]II.\n[39, 44, 46, 40, 42, 37, 69, 70]III.\n[42, 40, 39, 37, 44, 46, 69, 70]IV.\n[44, 42, 39, 40, 37, 46, 69, 70]V.\nA) Snapshot I\nB) Snapshot II\nC) Snapshot III\nD) Snapshot IV\nE) Snapshot V\npartition(),17. Consider the following implementation of where the last element in the array is always chosen as the pivot.\n1\nint32_t partition(int32_t int32_t int32_ta[], left, right) {\n2\nint32_t pivot = --right;\n3\nwhile (true) {\n4\nwhile (a[left] < a[pivot])\n5\n++left;\n6\nwhile (left < right && a[right - 1] >= a[pivot])\n7\n--right;\n8\nif (left >= right)\n9\nbreak;\n10\nstd::swap(a[left], a[right - 1]);\n11\n} // while\n12\nstd::swap(a[left], a[pivot]);\n13\nreturn left;\n14\n} // partition()\nSuppose you had the following unsorted array:\nint32_t arr[] = {5, 7, 4, 1, 9, 8, 2, 6, 3};\npartition(arr, 0, 9)?What are the contents of this array after one call to\n[1, 2, 3, 4, 5, 6, 7, 8, 9]A)\n[2, 1, 3, 5, 9, 8, 7, 6, 4]B)\n[2, 1, 3, 7, 9, 8, 5, 6, 4]C)\n[3, 1, 2, 5, 7, 4, 9, 8, 6]D)\nE) None of the above", "word_count": 523, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ccd2a1c3-94db-58b2-beff-a00e0c9f7fe9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 450, "real_page_number": null, "text": "438\nChapter 14. Sorting Algorithms\n18. You are given a string of size 𝑛that only contains lowercase English letters from ‘a’ to ‘z’. What is the time complexity of returning a\nstring with all the characters of the original string in sorted order, if you use the most efficient algorithm? For example, given the string\n\"eecsisfun\", \"ceefinssu\".you would want to return the string\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n19. Which of the following comparison sorting algorithms never compare the same pair of elements more than once during the sorting process?\nI. Insertion sort\nII. Selection sort\nIII. Heapsort\nA) I only\nB) II only\nC) III only\nD) I and II only\nE) I and III only\n20. Consider a quicksort algorithm that always chooses the median value as the pivot of each subarray in time. Given an array of 𝑛distinctΘ(1)\nvalues, what is the worst-case time complexity of performing this quicksort algorithm to sort this array?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n21. Which of the following statements is/are TRUE?\nI. Heapsort guarantees worst-case performance on a container that supports random access.Θ(𝑛log(𝑛))\nII. If you pick the best pivot with every partition, the average-case time complexity of quicksort may be better than the average-case\ntime complexity of mergesort (for sorting the same array of values).\nIII. In the average case, mergesort requires more memory usage to sort an array of size 𝑛compared to quicksort.\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) I, II, and III\n22. Which of the following correctly depicts the minimum amount of auxiliary space required to sort an array of size 𝑛using recursive quicksort\nand mergesort, assuming the most efficient implementation? Include the space required for stack frames.\nA) Quicksort: Θ(1),\nMergesort: Θ(1)\nB) Quicksort: Θ(1),\nMergesort: Θ(𝑛)\nC) Quicksort: Θ(log(𝑛)),\nMergesort: Θ(1)\nD) Quicksort: Θ(log(𝑛)),\nMergesort: Θ(𝑛)\nE) Quicksort: Θ(𝑛),\nMergesort: Θ(𝑛)\n23. Which of the following sorting algorithms is best suited for sorting a small array of integers with relatively few inversions?\nA) Insertion sort\nB) Selection sort\nC) Quicksort\nD) Mergesort\nE) Both (C) and (D)\n24. How many inversions exist in this array: [0, 2, 8, 1, 3, 7, 6] (assume ascending order is the desired sorting order)?\nA) 2\nB) 3\nC) 4\nD) 5\nE) 6\n25. What is the maximum number of inversions that can exist in an array of size 7?\nA) 14\nB) 15\nC) 20\nD) 21\nE) 28", "word_count": 424, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "35590556-e671-5de3-82e4-af68f07d7c63", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 451, "real_page_number": null, "text": "14.11 Index Sorting\n439\n26. Suppose you are given an unsorted array with mystery values, as shown. Three of these mystery values are denoted as 𝑎, 𝑏, and 𝑐.\n[..., a, ..., b, ..., c, ..., 17]\nYou prepare to quicksort this array in ascending order by selecting 17 as your first pivot. After completing the first partition, you notice that\n𝑏ended up at index 0 of the array, 𝑎ended up at index 1 of the array, 17 ended up at index 2 of the array, and 𝑐ended up at the last position\nin the array, as shown.\n[b, a, 17, ..., c]\nKnowing this, which of the following statements is/are TRUE?\nI. The value of 𝑏must be less than or equal to 𝑎.\nII. The value of 𝑎+𝑏must be less than 35.\nIII. 𝑐must be the largest value in the array.\nA) II only\nB) III only\nC) I and II only\nD) II and III only\nE) I, II, and III\n27. You have a spreadsheet with post data from the Spring 2018 EECS 281 Piazza site. A subset of the data is shown below:\nPost ID\nView Count\nSubject Line\n23\n225\nLecture schedule and timing\n57\n92\nStudy Group for Lab, Project, and Exams\n79\n115\nStack vs. Queue\n140\n194\nStyle and Organization Recommendation\n232\n63\nUse of Classes for Project 1\n519\n78\nWHERE IS THE LAB SLIDES?\nYou insert these posts into a vector in this order and index sort them on the number of views, as shown by the following comparator:\n1\nstruct int32_t int32_tPost { id; views; std::string subject; };\n2\n3\nclass IdxSort {\n4\nconst std::vector<Post>& _posts;\n5\npublic:\n6\nIdxSort(const std::vector<Post>& temp) : _posts(temp) {}\n7\nbool operator()(size_t size_t consti, j) {\n8\nreturn _posts[i].views < _posts[j].views;\n9\n} // operator()()\n10\n};\n11\n12\nvoid index_sort(const std::vector<Post>& posts) {\n13\nIdxSort idx_comp(posts);\n14\nstd::vector<int32_t> indices(posts.size());\n15\nstd::iota(indices.begin(), indices.end(), 0);\n16\nstd::sort(indices.begin(), indices.end(), idx_comp);\n17\n} // index_sort\nindicesWhat are the contents of after the index sort on line 16 is complete?\n0 2 1 3 4 5A)\n5 4 3 1 2 0B)\n4 5 1 2 3 0C)\n0 3 2 1 5 4D)\nE) None of the above\n28. Which of the following statements about selection sort is/are TRUE?\nI. Selection sort only needs to perform comparisons if the provided array is nearly sorted.Θ(𝑛)\nΘ(𝑛2)II. Selection sort runs in time for containers that support random access, and time for containers that do not.Θ(𝑛log(𝑛))\nIII. Selection sort is useful if you want to minimize the total number of swaps performed during the sorting process.\nA) I only\nB) III only\nC) I and II only\nD) I and III only\nE) I, II, and III", "word_count": 469, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0cbcbc43-5669-5f4d-a352-07f5c7506975", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 452, "real_page_number": null, "text": "440\nChapter 14. Sorting Algorithms\n29. For which of the following reasons would it be preferable to use heapsort instead of quicksort to sort an array of 𝑛distinct values?\nI. You want to ensure that the relative ordering of equal elements is maintained before and after the sort.\nII. You want to ensure that the sort will take no worse than time.Θ(𝑛log(𝑛))\nIII. You want to ensure that the sort can be done with auxiliary space.Θ(1)\nA) I only\nB) II only\nC) III only\nD) II and III only\nE) I, II, and III\nFor questions 30-34, suppose you had the following unsorted array:\n[22, 9, 13, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n30. A snapshot is taken during execution of a sorting algorithm. If the snapshot of the array is:\n[9, 13, 22, 52, 66, 74, 28, 59, 71, 11, 35, 47]\nwhich of the following sorts could have been run on this array?\nA) Bubble sort\nB) Insertion sort\nC) Selection sort\nD) Quicksort\nE) Mergesort\n31. A snapshot is taken during execution of a sorting algorithm. If the snapshot of the array is:\n[9, 11, 13, 22, 28, 74, 66, 59, 71, 35, 52, 47]\nwhich of the following sorts could have been run on this array?\nA) Bubble sort\nB) Insertion sort\nC) Selection sort\nD) Quicksort\nE) Mergesort\n32. A snapshot is taken during execution of a sorting algorithm. If the snapshot of the array is:\n[22, 9, 13, 11, 35, 28, 47, 59, 71, 66, 52, 74]\nwhich of the following sorts could have been run on this array?\nA) Bubble sort\nB) Insertion sort\nC) Selection sort\nD) Quicksort\nE) Mergesort\n33. A snapshot is taken during execution of a sorting algorithm. If the snapshot of the array is:\n[9, 13, 22, 28, 52, 59, 35, 66, 11, 47, 71, 74]\nwhich of the following sorts could have been run on this array?\nA) Bubble sort\nB) Insertion sort\nC) Selection sort\nD) Quicksort\nE) Mergesort\n34. A snapshot is taken during execution of a sorting algorithm. If the snapshot of the array is:\n[9, 13, 22, 28, 52, 59, 66, 74, 71, 35, 11, 47]\nwhich of the following sorts could have been run on this array?\nA) Bubble sort\nB) Insertion sort\nC) Selection sort\nD) Quicksort\nE) Mergesort", "word_count": 394, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e7b5bc9b-98d3-5a3d-9f8e-832a558f0660", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 453, "real_page_number": null, "text": "14.11 Index Sorting\n441\n35. You are given two snapshots of an array during the execution of a sorting algorithm, where the first snapshot was taken before the second:\n[13, 11, 12, 14, 17, 21, 15, 18, 22, 16, 19, 20]Snapshot 1:\n[11, 12, 13, 14, 17, 21, 15, 18, 22, 16, 19, 20]Snapshot 2:\nWhich of the following sorts could have possibly been run on this array? Note: for quicksort, do not restrict yourselfSelect all that apply.\nto the implementation where the last element is always chosen as the pivot; you may consider an implementation where any value could\nhave been chosen as the pivot, and the selected pivot could have been swapped to either the front or back of the array before partitioning.\nA) Bubble sort\nB) Insertion sort\nC) Selection sort\nD) Quicksort\nE) Mergesort\nChapter 14 Exercise Solutions\nΘ(𝑛2),The correct answer is (B). The worst-case time complexity of quicksort on an array of size 𝑛is and not Θ(𝑛log(𝑛)). This happens1.\nif you get unlucky with your pivot choice and end up with the smallest or largest elements as your pivot at every step, which causes the\nΘ(𝑛2).quicksort recurrence to become 𝑇(𝑛−1)+𝑛, which evaluates to𝑇(𝑛)=\n2. The correct answer is (A). Of the 5 sorting algorithms, insertion sort is adaptive, which allows it to perform better on nearly sorted input.\n3. The correct answer is (B). If the registrar has to sort a list of waitlisted students by credit hours to determine waitlist priority, such a sort\ncannot disturb the arrival order due to the possibility of ties. Thus, the registrar needs a sort that is stable, as stable sorts can preserve the\nrelative order of students when there are duplicates present. Selection sort is the only sort provided that is not stable.\n4. The correct answer is (D). Option A is false because bubble sort can be implemented to be both adaptive and stable (see section 14.2 for\nthis exact implementation). Option B is false because insertion sort can be done in-place without any additional auxiliary space. Option C\nΘ(𝑛2)is false because selection will still need to perform comparisons, regardless of what the input is (for this reason, option D is true,\nsince these comparisons represent a majority of the work done while running a selection sort).\n5. The correct answer is (D). Both students require a sort that has a worst-case time complexity of Θ(𝑛log(𝑛)), so quicksort is not an option.\nOf the remaining comparison sorts, both heapsort and mergesort can be done in worst-case time. Mergesort is stable, so itΘ(𝑛log(𝑛))\nwould allow Daniel to keep the relative order of elements in his array intact. Heapsort is not stable, so it would not work for Daniel. On the\nother hand, heapsort can be done in-place, which would work for Ryan (mergesort would not, since it requires auxiliary space on anΘ(𝑛)\narray). Thus, Daniel should use mergesort, and Ryan should use heapsort.\n6. The correct answer is (D). Non-adaptive sorting algorithms may be simpler to implement than adaptive algorithms, as they do not have to\nexhibit different behavior depending on the outcomes of comparisons. However, this inability to change their operations depending on the\ninput may cause them to run slower in special situations, such as sorting a nearly-sorted array.\n7. The correct answer is (E). The best external sorts are divide-and-conquer sorting algorithms, which allow the input to be divided across\nmultiple workers, individually sorted, and then merged together. Of the provided sorts, mergesort falls into this category.\n8. The correct answer is (C). Mergesort is the only one of the provided sorts that is stable. In addition, selection sort and quicksort have a\nΘ(𝑛2),worst-case time complexity of and heapsort’s time complexity relies on random access, which linked lists do not support.Θ(𝑛log(𝑛))\n9. The correct answer is (C). Statements I and II are true. Mergesort does require an auxiliary array of the same size when sorting the\noriginal array, and the fastest possible comparison sort has a worst-case time complexity no better than (see section 14.8.2 forΘ(𝑛log(𝑛))\nan explanation why). Statement III is false because quicksort’s better time complexity does not mean that it will always sort faster, but\nΘ(𝑛2),rather that its performance degrades slower as input size grows (also, the worst-case time complexity of quicksort is still which\nmatches bubble sort).\n10. The correct answer is (A). The best choice for the pivot is the median, which lets you partition the input into two evenly sized subarrays.\nIn this case, the median value is 43.\nThe correct answer is (B). The recurrence relation for mergesort is 2𝑇(𝑛∕2)+Θ(𝑛), since it recursively calls itself twice with half11. 𝑇(𝑛)=\nmerge(),the input size, then calls which takes linear time. However, since your friend incorrectly implemented the algorithm so that the\nΘ(𝑛2) 2𝑇(𝑛∕2)+Θ(𝑛2).merging step takes time, the recurrence relation now becomes Using the Master Theorem where 2,𝑇(𝑛) 𝑎==\n𝑏𝑐, Θ(𝑛𝑐) Θ(𝑛2).2, and 2, we see that which implies that the time complexity of this recurrence is𝑏= 𝑐= 𝑎< =\n12. The correct answer is (E). Quicksort A is the standard version of quicksort that was discussed in this chapter, with an average-case time\nΘ(𝑛2).complexity of and a worst-case time complexity of Quicksort B always chooses the worst-case pivot, so the performanceΘ(𝑛log(𝑛))\nΘ(𝑛2).of quicksort B will always be the worst-case of Thus, the average-case time complexity of quicksort A is better than that of quicksort\nB, but their worst-case time complexities are the same.", "word_count": 930, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "776f39a5-55ca-511f-89fd-14f4fcca35c1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 454, "real_page_number": null, "text": "442\nChapter 14. Sorting Algorithms\n13. The correct answer is (E). We can see from the data that sort A performs roughly the same regardless of whether the array is unsorted,\nnearly sorted, or fully sorted. This indicates that sort A is not adaptive; this eliminates options (A), (B), and (C), since insertion sort is\nadaptive. Sort B performs the best on unsorted arrays, while sort C performs the best on sorted arrays. This indicates that sort C is adaptive,\nsince it is able to use the fact that its array is sorted to improve its performance — of the two remaining options, bubble sort makes the most\nsense for sort C. This leaves sort B as quicksort, which makes sense given that the quicksort algorithm used always selects the last element\nas the pivot; for nearly sorted arrays, quicksort will tend to select bad pivots with each partition (since it will pick pivot values that are close\nΘ(𝑛2).to the largest within each subarray rather than the median), which causes the performance to degenerate from toΘ(𝑛log(𝑛))\n14. The correct answer is (B). The process of fixing down takes time, and it is done times during the heapsort process. Thus,Θ(log(𝑛)) 𝑛−1\nthe time complexity of heapsort is Θ(𝑛log(𝑛)). Heapsort can be done in-place, so it requires no additional memory.\n15. The correct answer is (E). The process of heapsort is shown below (bold = element fixed in the heap):\nStep\nHeap\nSnapshot\nStart\n[48, 25, 98, 41, 33, 64, 87, 38, 19]\nTurn into a max heap\n[98, 41, 87, 38, 33, 64, 48, 25, 19]\n(iv)\nSwap top and bottom\n[19, 41, 87, 38, 33, 64, 48, 25, 98]\nCall fix down to fix the heap\n[87, 41, 64, 38, 33, 19, 48, 25, 98]\n(v)\nSwap top and bottom\n[25, 41, 64, 38, 33, 19, 48, 87, 98]\nCall fix down to fix the heap\n[64, 41, 48, 38, 33, 19, 25, 87, 98]\nSwap top and bottom\n[25, 41, 48, 38, 33, 19, 64, 87, 98]\nCall fix down to fix the heap\n[48, 41, 25, 38, 33, 19, 64, 87, 98]\n(ii)\nSwap top and bottom\n[19, 41, 25, 38, 33, 48, 64, 87, 98]\nCall fix down to fix the heap\n[41, 38, 25, 19, 33, 48, 64, 87, 98]\nSwap top and bottom\n[33, 38, 25, 19, 41, 48, 64, 87, 98]\n(vi)\nCall fix down to fix the heap\n[38, 33, 25, 19, 41, 48, 64, 87, 98]\nSwap top and bottom\n[19, 33, 25, 38, 41, 48, 64, 87, 98]\n(iii)\nCall fix down to fix the heap\n[33, 19, 25, 38, 41, 48, 64, 87, 98]\n(i)\nSwap top and bottom\n[25, 19, 33, 38, 41, 48, 64, 87, 98]\nCall fix down to fix the heap\n[25, 19, 33, 38, 41, 48, 64, 87, 98]\nSwap top and bottom\n[19, 25, 33, 38, 41, 48, 64, 87, 98]\n16. The correct answer is (A). The process of heapsort is shown below (bold = element fixed in the heap):\nStep\nHeap\nSnapshot\nStart (already a max heap)\n[70, 44, 69, 40, 42, 37, 46, 39]\nSwap top and bottom\n[39, 44, 69, 40, 42, 37, 46, 70]\nCall fix down to fix the heap\n[69, 44, 46, 40, 42, 37, 39, 70]\nSwap top and bottom\n[39, 44, 46, 40, 42, 37, 69, 70]\n(iii)\nCall fix down to fix the heap\n[46, 44, 39, 40, 42, 37, 69, 70]\nSwap top and bottom\n[37, 44, 39, 40, 42, 46, 69, 70]\n(ii)\nCall fix down to fix the heap\n[44, 42, 39, 40, 37, 46, 69, 70]\n(v)\nSwap top and bottom\n[37, 42, 39, 40, 44, 46, 69, 70]\nCall fix down to fix the heap\n[42, 40, 39, 37, 44, 46, 69, 70]\n(iv)\nSwap top and bottom\n[37, 40, 39, 42, 44, 46, 69, 70]\nCall fix down to fix the heap\n[40, 37, 39, 42, 44, 46, 69, 70]\nSwap top and bottom\n[39, 37, 40, 42, 44, 46, 69, 70]\nCall fix down to fix the heap\n[39, 37, 40, 42, 44, 46, 69, 70]\nSwap top and bottom\n[37, 39, 40, 42, 44, 46, 69, 70]\n17. The correct answer is (C). We select 3 as the pivot and set a left and right pointer to begin partitioning:\n5\n7\n4\n1\n9\n8\n2\n6\n3\nL\nR\nWe keep on incrementing the left pointer until we find a value greater than the pivot value of 3 (in this case, we already have such an\nelement in 5). Then, we decrement the right pointer until we find a value less than 3 (the first of which is 2).\n5\n7\n4\n1\n9\n8\n2\n6\n3\nL\nR\nLeft and right are not pointing to the same value, so we swap the two elements:\n2\n7\n4\n1\n9\n8\n5\n6\n3\nL\nR\nWe then increment the left pointer until we find a value greater than 3; this lands the left pointer at 7. We then decrement the right pointer\nuntil we find a value less than 3; this lands the right pointer at 1. Since left and right are not the same value, we swap the elements:", "word_count": 880, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "92f43960-a9cc-5ad0-8515-aab4e6d6d4cb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 455, "real_page_number": null, "text": "14.11 Index Sorting\n443\n2\n7\n4\n1\n9\n8\n5\n6\n3\nL\nR\n2\n1\n4\n7\n9\n8\n5\n6\n3\nL\nR\nWe then increment the left pointer until we find a value greater than 3; this lands the left pointer at 4. We then decrement the right pointer\nuntil we find a value less than 3 — however, this cases the left and right pointers to point to the same value. When this happens, we swap\nthe pivot with the element at the position where the two pointers meet. Thus, 3 and 4 swap places to produce our final array:\n2\n1\n4\n7\n9\n8\n5\n6\n3\nLR\n2\n1\n3\n7\n9\n8\n5\n6\n4\nLR\n18. The correct answer is (C). The number of possible characters in the string is limited to 26 possible characters: ‘a’ to ‘z’. Because of this,\nwe can use counting sort to sort the string in linear time.\n19. The correct answer is (A). Only insertion sort never compares the same pair of elements more than once during the sorting process, since\neach element is only ever compared with the values that come before it. Selection sort may compare the same pair of elements more than\nonce: for example, given [3, 2, 1], selection sort would compare 2 and 3 on the first pass when determining the smallest element to place at\nindex 0, then it would swap 3 and 1, and then would compare 2 and 3 again on the second pass to determine the next smallest element to\nplace at index 1. Heapsort may also compare the same pair of elements more than once since you cannot guarantee the organization of\nelements within the heap throughout the sorting process: for instance, in the example shown at the beginning of section 14.5, the values of\n46 and 17 are compared more than once.\n20. The correct answer is (C). If the quicksort algorithm can always choose the median value as the pivot in time, then each partitionΘ(1)\nwill always split its subarray in half. This gives us the following recurrence relation for quicksort in all cases:\n𝑇(𝑛)=\n{\n1,\nif or𝑛=0 1\n2𝑇(𝑛∕2)+𝑛,\nif 𝑛>1\nThis evaluates to Θ(𝑛log(𝑛)); essentially, if the median is chosen as the pivot every time, then the worst-case of our original quicksort\nimplementation will never happen.\n21. The correct answer is (D). Statement I is true because heapsort guarantees time if given random access, since it involves aΘ(𝑛log(𝑛)) 𝑛−1\ncalls to fix down, which takes time. Statement II is false because the average-case time complexity of mergesort is Θ(𝑛log(𝑛)),Θ(log(𝑛))\nand it is not possible for quicksort to be better than that as a comparison sort. Statement III is true, since the auxiliary space required by\nquicksort on an array is Θ(log(𝑛)), compared to for mergesort.Θ(𝑛)\n22. The correct answer is (D). The auxiliary space used by quicksort is for the stack frames required (if we always make sure weΘ(log(𝑛))\nrecurse into the larger subarray last to ensure it is tail recursive). The auxiliary space used by mergesort is for the auxiliary arrayΘ(𝑛)\nneeded to store the sorted/merged output.\n23. The correct answer is (A). An array with few inversions is one that is nearly sorted, as there are few pairs of elements that are out of order\nrelative to each other. Insertion sort is the only adaptive sorting algorithm among the ones provided, so it can perform better for a small,\nnearly sorted array.\n24. The correct answer is (E). There are six inversions in this array: (2, 1), (8, 1), (8, 3), (8, 7), (8, 6), and (7, 6).\n25. The correct answer is (D). The maximum number of inversions occurs if the array is in reverse sorted order, for which every pair of values\nis an inversion. For an array of size 7, there are 6 inversions that start with the first value, 5 inversions that start with the second value,\n4 inversions that start with the third value, 3 inversions that start with the fourth value, 2 inversions that start with the fifth value, and 1\ninversion that starts with the sixth value. This comes out to inversions. In general, the maximum number of6+5+4+3+2+1 = 21\npossible inversions in an array of size 𝑛is + + + 1. To illustrate with another example, the inversions in the array [4, 3, 2,(𝑛−1) (𝑛−2) …\n1] are (4, 3), (4, 2), (4, 1), (3, 2), (3, 1), and (2, 1) — there are total of 3 + 2 + 1 = 6 inversions for an array of size 4.\n26. The correct answer is (A). The only thing we can conclude after the first partition is that the first pivot ends up in the correct position in\n≤pivot, ≥pivot.sorted order, and that all values to the left are and all values to the right are You cannot conclude anything about the order\nof values in these two subarrays outside of the pivot value. In this case, we know that 17 is the third smallest value in the array since it\nended up at index 2 after the first partition, and that 𝑏and 𝑎are each no greater than 17, and 𝑐is no less than 17. Thus, the only statement\nwe can guarantee is statement II, since 𝑎+𝑏cannot be greater than 34 if neither 𝑎nor 𝑏can be larger than 17.\n27. The correct answer is (C). The index sort sorts the Piazza posts in order of view count, where the index of the lowest viewed post comes\nfirst and the index of the highest viewed post comes last. In this case, the sorted order of views is post 232 (index 4) →post 519 (index 5)\n→post 57 (index 1) →post 79 (index 2) →post 140 (index 3) →post 23 (index 0).", "word_count": 996, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "44adb10f-d3ee-5bfd-ab97-f464c31a2212", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 456, "real_page_number": null, "text": "444\nChapter 14. Sorting Algorithms\n28. The correct answer is (B). Only statement III is true, since selection sort only requires swaps in the worst-case, when given an input𝑛−1\nΘ(𝑛2)size of 𝑛(one swap per iteration). Statement I is false since selection sort will still need to perform comparisons (Θ(𝑛) comparisons\nΘ(𝑛2)per iteration to determine which value is the next smallest), and statement II is false because selection sort takes time regardless of\nwhether random access is supported.\n29. The correct answer is (D). Statements II and III are true: heapsort can be done in worst-case time and auxiliary space,Θ(𝑛log(𝑛)) Θ(1)\nΘ(𝑛2)while quicksort requires worst-case time and auxiliary space. Statement I is false because heapsort is not stable.Θ(log(𝑛))\n30. The correct answer is (E). You can see that the elements can be split into sorted subsections:\n[9, 13, 22]\n[52, 66, 74]\n[28, 59, 71]\n[11, 35, 47]\nThis indicates the array is in the process of being sorted with mergesort.\n31. The correct answer is (C). The process of selection sort is shown below; you can see that the smallest values are swapped with the values\nthat were at the beginning of the original array.\n[22, 9, 13, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n[9, 22, 13, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n11, 22,[9, 13, 52, 66, 74, 28, 59, 71, 35, 47]\n13,[9, 11, 52, 66, 74, 28, 59, 71, 35, 22, 47]\n22, 52,[9, 11, 13, 66, 74, 28, 59, 71, 35, 47]\n28, 66,[9, 11, 13, 22, 74, 59, 71, 35, 52, 47]\n32. The correct answer is (D). Notice how 47 acts as a pivot, with numbers to the left of 47 being less than 47, and numbers to the right of 47\nbeing greater than 47. This hints that the array is being sorted with quicksort (and you can prove this by walking through the first few steps):\n47,[22, 9, 13, 11, 35, 28, 59, 71, 66, 52, 74]\n33. The correct answer is (A). Notice that the larger elements are bubbling to the right side of the array. This hints that bubble sort is being\nexecuted on this array. The following steps show that this is true:\n[22, 9, 13, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n[9, 22, 13, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n13, 22,[9, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n28, 74,[9, 13, 22, 52, 66, 59, 71, 35, 11, 47]\n59, 74,[9, 13, 22, 52, 66, 28, 71, 35, 11, 47]\n71, 74,[9, 13, 22, 52, 66, 28, 59, 35, 11, 47]\n35, 74,[9, 13, 22, 52, 66, 28, 59, 71, 11, 47]\n11, 74,[9, 13, 22, 52, 66, 28, 59, 71, 35, 47]\n47, 74][9, 13, 22, 52, 66, 28, 59, 71, 35, 11,\n28, 66,[9, 13, 22, 52, 59, 71, 35, 11, 47, 74]\n59, 66,[9, 13, 22, 52, 28, 71, 35, 11, 47, 74]\n35, 71,[9, 13, 22, 52, 28, 59, 66, 11, 47, 74]\n11, 71,[9, 13, 22, 52, 28, 59, 66, 35, 47, 74]\n47, 71,[9, 13, 22, 52, 28, 59, 66, 35, 11, 74]\n28, 52,[9, 13, 22, 59, 66, 35, 11, 47, 71, 74]\n35, 66,[9, 13, 22, 28, 52, 59, 11, 47, 71, 74]\nWhile this variation of bubble sort bubbles larger values to the right side of the array, this is not the only possible implementation of this\nsorting algorithm. It is also valid for a bubble sort implementation to bubble the smaller values to the left side of the array.\n34. Thecorrectansweris(B).Forinsertionsort,youwillnoticethatthevaluesaremovedtothefrontofthearrayinamannerthatincrementally\nexpands the portion of the array that is sorted from left to right. The process of insertion sort is shown below; the underlined values\nrepresent the portion of the array that is sorted, and the bolded value represents the new value that was added to the sorted subarray.\n[22, 9, 13, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n[9, 22, 13, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n13,[9, 22, 52, 66, 74, 28, 59, 71, 35, 11, 47]\n52,[9, 13, 22, 66, 74, 28, 59, 71, 35, 11, 47]\n66,[9, 13, 22, 52, 74, 28, 59, 71, 35, 11, 47]\n74,[9, 13, 22, 52, 66, 28, 59, 71, 35, 11, 47]\n28,[9, 13, 22, 52, 66, 74, 59, 71, 35, 11, 47]\n59,[9, 13, 22, 28, 52, 66, 74, 71, 35, 11, 47]\nThe correct answers are (A), (B), (C), (D), and (E). All of the provided sorts could have been used. For bubble sort, the second snapshot35.\ncould have been taken after 13 bubbled past 12 to its correct position. For insertion sort, the second snapshot could have been taken after 12\nwas shifted before 13 when considering the first three elements. For selection sort, the second snapshot could have been taken after 13 was\nfirst swapped with 11 on the first pass, and then 12 on the second pass. For quicksort, 12 could have been chosen as the first pivot and then\nswapped to the front, and the second snapshot could have been taken after the other elements are partitioned. For mergesort, the elements in\nthe second snapshot can be split into sorted subarrays of size 3.", "word_count": 930, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d05f550d-8923-5451-a1a7-a8982a6ca7c2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 457, "real_page_number": null, "text": "Chapter 15\nBinary Search and Additional Algorithms\n15.1\nOrdered and Sorted Containers\nNow that we have covered several different data structures, we will introduce a new way of categorizing different container types based on their\nbehavior. These new categorizations include and containers.ordered sorted\nWe define an ordered container as a container that maintains its current ordering of elements, regardless of how the container is modified.\nElements in an ordered container must maintain their relative positions unless they are removed. For example, if element 𝐴comes before\nelement 𝐵in an ordered container, and we insert element 𝐶into the container, then 𝐴would still be before 𝐵after the insertion. Similarly, if 𝐶\nwere then removed, 𝐴would still come before 𝐵in the container. As long as 𝐴and 𝐵are in the ordered container, 𝐴will always be positioned\nbefore 𝐵in the container’s ordering, irrespective of other elements that may be inserted or removed. In the STL, ordered containers typically\nsupport an insertion method that accepts an iterator, which defines the position at which the new element should be inserted at.\nOn the other hand, we define a sorted container as a sequential container whose elements are always in some predefined order. In a sorted\ncontainer, you cannot insert elements wherever you want in the container; a newly inserted element’s position must adhere to this predefined\nordering designated by a sorted container. For example, if you have a sorted container that contains the values 1 and 3, and then you insert 2\ninto this container, then 2 must be inserted in between 1 and 3 (assuming the sorted container sorts its values in ascending order).\nSo far, we have seen several examples of ordered and sorted containers. Linked lists, arrays, and deques are good examples of ordered\ncontainers on their own, since the insertion or deletion of elements from these containers do not change the relative ordering of other elements.\nOn their own, these containers are also not sorted by default, as there are no rules that dictate the order in which values must be positioned.\nHowever, these containers can also be used as the underlying container of other data structures that may be unordered or sorted. For example,\nthe unordered sequence container used to implement a priority queue in section 10.2.2 is an unordered container, since popping off an element\nmay change the relative ordering of other elements in the container:\n16\n22\n15\n11\n19\n.pop()\n16\n19\n15\n11\n19\n22 19overwrite with\npop back underlying vector\n16\n19\n15\n11", "word_count": 418, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "63e615fd-0644-5596-b09f-ab77aa836458", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 458, "real_page_number": null, "text": "446\nChapter 15. Binary Search and Additional Algorithms\nSimilarly, the sorted sequence container in section 10.2.3 implements a priority queue using an underlying array whose elements are required to\nfollow a predefined order. In this case, we consider this underlying array as a sorted container.\n13\n17\n24\n33\n42\n.push(19)\n13\n17\n19\n24\n33\n42\n13\n17\n19\n24\n33\n42\n.pop()\n13\n17\n19\n24\n33\nAs another example, the set implementation introduced in section 13.1.2 uses a sorted array as its underlying structure, as this invariant allowed\nus to perform set operations efficiently. Therefore, we would also categorize this set implementation as a sorted container.\nWhen implementing an ordered or sorted container, arrays, linked lists, and deques are typically good candidates for storing the container’s\nunderlying data. The ideal selection of container depends on the requirements of the application that uses it, and knowledge of operations that\nare called frequently can aid in this decision (for example, if you need something that relies on random access, a list will not be the way to go).\nThe table below summarizes the worst-case complexities of operations on an container, using different underlying containers to store itsordered\ndata (review chapters 6-9 if you are not sure where these complexities come from).\nOperation\nArray\nLinked List\nDeque\nadd_value(val)\nΘ(1)\nΘ(1)\nΘ(1)\n(inserts value anywhere in container)\nremove(val)\nΘ(𝑛)\nΘ(𝑛)\nΘ(𝑛)\n(remove value from container, but you must find it first)\nremove(iterator)\nΘ(𝑛)\nif singly-linkedΘ(𝑛)\nΘ(𝑛)\n(remove value from container, but you are given its iterator)\nif doubly-linkedΘ(1)\nfind(value)\nΘ(𝑛)\nΘ(𝑛)\nΘ(𝑛)\n(find a value in the container)\niterator::operator*()\nΘ(1)\nΘ(1)\nΘ(1)\n(dereference an iterator)\noperator[](index)\nΘ(1)\nΘ(𝑛)\nΘ(1)\n(access element with a position at index)\ninsert_after(iterator, val)\nΘ(𝑛)\nΘ(1)\nΘ(𝑛)\n(insert an element after a provided iterator)\ninsert_before(iterator, val)\nΘ(𝑛)\nif singly-linkedΘ(𝑛)\nΘ(𝑛)\n(insert an element before a provided iterator)\nif doubly-linkedΘ(1)\nThe following table summarizes the worst-case complexities of a container, with each container type holding is underlying data.sorted\nOperation\nArray\nLinked List\nDeque\nadd_value(val)\nΘ(𝑛)\nΘ(𝑛)\nΘ(𝑛)\n(inserts value anywhere in container)\nremove(val)\nΘ(𝑛)\nΘ(𝑛)\nΘ(𝑛)\n(remove value from container, but you must find it first)\nremove(iterator)\nΘ(𝑛)\nif singly-linkedΘ(𝑛)\nΘ(𝑛)\n(remove value from container, but you are given its iterator)\nif doubly-linkedΘ(1)\nfind(value)\nΘ(log(𝑛))\nΘ(𝑛)\nΘ(log(𝑛))\n(find a value in the container)\niterator::operator*()\nΘ(1)\nΘ(1)\nΘ(1)\n(dereference an iterator)\noperator[](index)\nΘ(1)\nΘ(𝑛)\nΘ(1)\n(access element with a position at index)\ninsert_after(iterator, val)\nN/A\nN/A\nN/A\n(insert an element after a provided iterator)\ninsert_before(iterator, val)\nN/A\nN/A\nN/A\n(insert an element before a provided iterator)\nFor arrays, lists, and deques, there are a few notable differences in their time complexities based on whether they are ordered and/or sorted. The\ntime complexity of adding a value in a sorted container becomes Θ(𝑛), since each new element must be inserted in a position that adheres to the\ncorrect sorted order (which takes linear time for a list since you cannot use binary search, and linear time for vectors and deques since elements\nmay need to be shifted after the insertion). The time complexity of finding an element improves to for the random-access containersΘ(log(𝑛))\nsince we can now use binary search, but remains for a list since they do not provide random access. Lastly, inserting before and after aΘ(𝑛)\ngiven iterator is no longer supported, since you do not have the freedom to insert values wherever you want in a sorted container.", "word_count": 581, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d932cc2c-6b63-5172-bf08-f4cb66216786", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 459, "real_page_number": null, "text": "15.2 Binary Search\n447\nExample 15.1 Is the underlying data vector used to implement a binary heap sorted? Is it ordered? If not, provide an example.\nThe underlying data vector used to implement a binary heap is neither sorted nor ordered. This may seem counterintuitive, since a binary heap\ncertainly seems like a container that is sorted by priority. However, if you recall the definition of a binary heap, the relationship between nodes\nof the heap only requires no parent to have a lower priority than any of its descendants. Because of this, there are multiple ways to represent the\nsame elements in the underlying container of a binary heap.\n1\n3\n6\n2\n5\n4\n1\n2\n3\n4\n5\n6\n1\n2\n4\n3\n5\n6\n1\n3\n2\n6\n5\n4\nBoth of these variations are valid structures for a min-binary heap, and we can clearly see that the data in the underlying container does not need\nto be in a specific, predefined order. Thus, the underlying vector used to implement a binary heap is not a sorted container.\nThe underlying vector in a binary heap is also not an ordered container. To illustrate why this is the case, consider the following array\nrepresentation of a max-binary heap:\n8\n4\n2\n8\n2\n4\nSuppose we insert 9 into this binary heap. 9 would get inserted to the back of the heap, and then fixed up to its correct position.\n8\n4\n2\n9\n8\n2\n4\n9\n8\n4\n9\n2\n8\n9\n4\n2\n9\n4\n8\n2\n9\n8\n4\n2\nNotice that the addition of 9 to this binary heap changed the relative ordering of 2 and 4! Prior to insertion, 2 was before 4 in the underlying\nvector; however, after 9 was added, 2 is after 4. This shows that the binary heap’s underlying data vector is not an ordered container.\n15.2\nBinary Search\nAs mentioned earlier, searching in a sorted container is much more efficient if the container is able to support binary search. Binary search is a\nsearching algorithm that can be used to find a value in logarithmic time if the underlying data is sorted. During the binary search process, we\nfirst look at the middle element and compare it with the value we are trying to find. If the middle element is larger than the element we want to\nfind, then we know that the our target element must be to the left of the middle value. On the other hand, if the middle element is smaller than\nthe element we want to find, then we know that our target element must be to the right of the middle value. We are then able to repeat this\nprocess until we find (or fail to find) our target value, eliminating half the search space every time. For example, suppose we wanted to search\nfor 21 in the following sorted vector:\n2\n3\n5\n7\n13\n21\n25\n31\n42\n0\n1\n2\n3\n4\n5\n6\n7\n8\nWe will first look at the element in the middle, 13. Since 21 is greater than 13, we know that 21 (if it exists), must be to the right of 13. As a\nresult, we can eliminate all the elements to the left of 13 as possible solutions.\n2\n3\n5\n7\n13\n21\n25\n31\n42\n0\n1\n2\n3\n4\n5\n6\n7\n8\nNext, we would look at the middle value of the remaining elements, or 25 (integer division chooses the value on the left if there are two middle\nvalues). Since 21 is less than 25, it must be to the left of 25 (if it exists). Thus, we can eliminate all the elements to the right of 25.\n2\n3\n5\n7\n13\n21\n25\n31\n42\n0\n1\n2\n3\n4\n5\n6\n7\n8\nWe then look at the remaining element to the left of 25. It is equal to our target of 21, so the value has been found.\n2\n3\n5\n7\n13\n21\n25\n31\n42\n0\n1\n2\n3\n4\n5\n6\n7\n8", "word_count": 694, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "da3c32c8-1802-5dda-9185-63e400ef8812", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 460, "real_page_number": null, "text": "448\nChapter 15. Binary Search and Additional Algorithms\n-1The following code implements binary search. It returns the index of the target element if found, and otherwise:\n1\nint32_t binary_search(const std::vector<int32_t>& int32_t int32_t int32_tvec, target, left, right) {\n2\nwhile (right > left) {\n3\nint mid = left + (right - left) / 2;\n4\nif (target == vec[mid]) {\n5\nreturn mid;\n6\n} // if\n7\nelse if (target < vec[mid]) {\n8\nright = mid;\n9\n} // else if\n10\nelse {\n11\nleft = mid + 1;\n12\n} // else\n13\n} // while\n14\nreturn -1;\n// target not found\n15\n} // binary_search()\ntarget [left, right) leftHere, represents the value you want to find, and represents the range you want to search in. At the beginning,\nright size whileshould be 0 and should be if you want to search the entire range. The loop on line 2 runs as long as there are elements that\n(left + right)/2;still need to be searched. On line 3, we calculate the middle index of our range. Note that we did not use this is because\nleft rightadding and could lead to overflow if our indices are very large. The if-else statements from line 4-13 deal with the comparisons;\nsince half of the search space is eliminated at each iteration, the complexity of binary search is for a search space of size 𝑛.Θ(log(𝑛))\n== true,However, this code is not entirely optimal, and there are ways we can speed it up. First, observe that the on line 4 rarely returns\n< >since it is more likely for the target to be less than or greater than the first element you check. Thus, it would be more efficient to check and\n==,before checking as that could potentially save you a few comparison checks per iteration. This change is made below:\n1\nint32_t binary_search(const std::vector<int32_t>& int32_t int32_t int32_tvec, target, left, right) {\n2\nwhile (right > left) {\n3\nint32_t mid = left + (right - left) / 2;\n4\nif (target < vec[mid]) {\n5\nright = mid;\n6\n} // if\n7\nelse if (target > vec[mid]) {\n8\nleft = mid + 1;\n9\n} // else if\n10\nelse {\n11\nreturn mid;\n12\n} // else\n13\n} // while\n14\nreturn -1;\n// target not found\n15\n} // binary_search()\nBinary search can also be implemented recursively. In the recursive approach, a recursive call is made on the right half if the middle element is\nsmaller than the target value, and a recursive call is made on the left half if the middle element is larger than the target value. The code for a\nrecursive binary search approach is shown below:\n1\nint32_t binary_search(const std::vector<int32_t>& int32_t int32_t int32_tvec, target, left, right) {\n2\nif (left > right) {\n// base case, failed to find value\n3\nreturn -1;\n4\n} // if\n5\nint32_t mid = left + (right - left) / 2;\n6\nif (vec[mid] < target) {\n7\nreturn binary_search(vec, target, mid + 1, right);\n8\n} // if\n9\nelse if (vec[mid] > target) {\n10\nreturn binary_search(vec, target, left, mid - 1);\n11\n} // else if\n12\nelse {\n13\nreturn mid;\n14\n} // else\n15\n} // binary_search()", "word_count": 564, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f7d6bf53-ce1f-5e02-a60d-c83f454f8083", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 461, "real_page_number": null, "text": "15.2 Binary Search\n449\nA similar approach to binary search would be finding the of a target element. When calculating a lower bound, you aren’tlower bound\nfinding if an element actually exists in a range; instead, you are finding the index that an element should be found at if it did exist. A possible\nimplementation for the lower bound operation is shown below; since there is no need to check for equality, the algorithm only makes two\ncomparisons per loop (rather than three, as was the case before).\n1\nint32_t lower_bound(const std::vector<int32_t>& int32_t int32_t int32_tvec, target, left, right) {\n2\nwhile (right > left) {\n3\nint32_t mid = left + (right - left) / 2;\n4\nif (vec[mid] < target) {\n5\nleft = mid + 1;\n6\n} // if\n7\nelse {\n8\nright = mid;\n9\n} // else\n10\n} // while\n11\nreturn left;\n12\n} // lower_bound()\nFor example, suppose we wanted to find the lower bound of 20 in the following sorted vector:\nmid\nleft\nright\n2\n3\n5\n7\n13\n21\n25\n31\n42\n0\n1\n2\n3\n4\n5\n6\n7\n8\nleftWe will first look at the element in the middle, 13. Since 20 is greater than 13, we update to the position of 21.\nmid\nleft\nright\n2\n3\n5\n7\n13\n21\n25\n31\n42\n0\n1\n2\n3\n4\n5\n6\n7\n8\nmid rightNow, refers to 25, which is larger than 20. Thus, we would set to the position of 25.\nright\nmid\nleft\n2\n3\n5\n7\n13\n21\n25\n31\n42\n0\n1\n2\n3\n4\n5\n6\n7\n8\nmid rightThe value of is now 21, which is still larger than 20. Thus, is updated to the position of 21.\nright\nmid\nleft\n2\n3\n5\n7\n13\n21\n25\n31\n42\n0\n1\n2\n3\n4\n5\n6\n7\n8\nleft right while leftBoth and refer to the same element, so the loop on line 2 exits, and is returned (in this case, index 5). Even though\nleft20 itself does not exist in the sorted array, the position of is the index that 20 would be located at if it did exist in the array. This can be\nuseful if you want to insert an element into a sorted container, as the lower bound operation would give you the correct position of insertion.", "word_count": 409, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6b94e6d2-d27e-5a62-bd71-045c2494011d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 462, "real_page_number": null, "text": "450\nChapter 15. Binary Search and Additional Algorithms\nThe binary search algorithm is a classic topic for many interview problems. We will discuss several examples below:\nExample 15.2 You are given an array that is first increasing and then decreasing. Find the maximum value in this array. For example,\n[1, 3, 50, 10, 9, 7, 6], 50.given the array you would return\nNotice that the maximum value splits the array into two segments with unique properties. All of the elements to the left of the maximum value\nare in ascending order, while all of the elements to the right of the maximum value are in descending order. This allows us to easily determine\nwhether an element is to the left or right of the maximum value, making binary search an efficient algorithm for solving this problem.\n1\n3\n50\n10\n9\n7\n6\n0\n1\n2\n3\n4\n5\n6\nascending order\ndescending order\nFirst, we would look at the middle element and compare it with the elements that are adjacent to it. If the middle element is larger than both\nadjacent elements, it must be the maximum, and we return this value. If the middle element is greater than the element to its left and smaller\nthan the element to its right, we are on the ascending portion of the array, and the maximum value must be to the right of the middle value (this\nallows us to eliminate all elements to the left immediately). Similarly, if the middle element is smaller than the element to its left and greater\nthan the element to its right, we are on the descending portion of the array, and the maximum value must be to the left of the middle value (this\nallows us to eliminate all elements to the right immediately). The code for this algorithm is shown below:\n1\nint32_t find_max_value(const std::vector<int32_t>& int intvec, left, right) {\n2\nint32_t start = left, end = right - 1;\n3\nwhile (start < end) {\n4\nint32_t mid = start + (end - start) / 2;\n5\n// if this condition is true, the target must be to the right of mid\n6\nif (vec[mid] < vec[mid + 1] && vec[mid] > vec[mid - 1]) {\n7\nstart = mid + 1;\n8\n} // if\n9\n// otherwise, the target must be to the left of mid\n10\nelse {\n11\nend = mid;\n12\n} // else\n13\n} // while\n14\nreturn vec[start];\n15\n} // find_max_value()\nSince we are simply performing a binary search, the time complexity of this solution is Θ(log(𝑛)), where 𝑛is the size of the vector. The auxiliary\nspace of this solution is Θ(1), since the amount of memory allocated is not dependent on the input size.\nExample 15.3 You are given a sorted array that is rotated at a certain position. Return the minimum element in this array. For example,\n[6, 7, 1, 2, 3, 4, 5], 1.given the array you would return\nSimilar to the previous example, the minimum element can be used to partition the array into two segments that have different properties. All\nthe elements to the left of the minimum value must be larger than the element at the back of the rotated array, and all the elements to the right of\nthe minimum value must be less than or equal to the element at the back of the array. We can use this information to conduct a binary search.\n6\n7\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n> last element (5)\n≤last element (5)\nFirst, we look at the element in the middle of the array and compare it with the last element. If this middle element is smaller than the last\nelement, we can eliminate every element to the right of the middle element since they cannot be the minimum. If this middle element is larger\nthan the last element, we can similarly eliminate every element to the left of the middle element. The code is shown below:\n1\nint32_t find_min_rotated(const std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\nint32_t start = left,\nend = right - 1;\n3\nwhile (start < end) {\n4\nint32_t mid = start + (end - start) / 2;\n5\n// if this condition is true, the target must be to the right of mid\n6\nif (vec[mid] > vec[end]) {\n7\nstart = mid + 1;\n8\n} // if\n9\n// otherwise, the target must be to the left of mid\n10\nelse {\n11\nend = mid;\n12\n} // else\n13\n} // while\n14\nreturn vec[start];\n15\n} // find_min_rotated()\nSimilar to the previous problem, the time complexity of this solution is Θ(log(𝑛)), and the auxiliary space usage is Θ(1).", "word_count": 802, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "60c17f5f-85ab-527c-844d-cc807531e1a3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 463, "real_page_number": null, "text": "15.3 Moore’s Voting Algorithm\n451\nExample 15.4 You are given a sorted array where every number occurs twice except for one number. Return the number that only appears\n[1, 1, 2, 2, 3, 4, 4], 3.once. For example, given the array you would return\nOnce again, the element that appears once can be used to partition elements to the left and right into two groups with different properties. This\nmay be a bit tricky to notice at first, but consider the following array:\n1\n1\n2\n2\n3\n4\n4\n0\n1\n2\n3\n4\n5\n6\n1st duplicate at even index\n1st dup. at odd index\n1 2Notice that if a pair occurs before the single element, the first element of the pair has an even index (e.g., the first and are located at indices\n40 and 2). However, if a pair occurs after the single element, the first element of the pair would have an odd index (e.g., the first is located at\nindex 5). We can use this information to conduct a binary search.\nmid mid + 1First, we look at the middle index. If the middle index is even, we check if the element at and are the same. If the middle\nmid mid - 1index is odd, we check if the element at and are the same. If this is true, the single element must be to the right; otherwise, it\nmust be to the left. The code is shown below:\n1\nint32_t find_single_element(const std::vector<int32_t>& int32_t int32_tvec, left, right) {\n2\nint32_t start = left,\nend = right - 1;\n3\nwhile (start < end) {\n4\nint mid = start + (end - start) / 2;\n5\n// if this condition is true, the target must be to the right of mid\n6\nif ((mid % 2 == 0 /* even */ && vec[mid] == vec[mid + 1]) ||\n7\n(mid % 2 == 1 /* odd\n*/ && vec[mid] == vec[mid - 1])) {\n8\nstart = mid + 1;\n9\n} // if\n10\n// otherwise, the target must be to the left of mid\n11\nelse {\n12\nend = mid;\n13\n} // else\n14\n} // while\n15\nreturn vec[start];\n16\n} // find_single_element()\nSimilar to the previous problems, the time complexity of this solution is Θ(log(𝑛)), and the auxiliary space usage is Θ(1).\n15.3\nMoore’s Voting Algorithm (✽)\nSuppose you are given an array of size 𝑛, and you are told that one element in the array is repeated over times (i.e., it is a majority element).𝑛∕2\nHow can you identify what this majority element is? One algorithm that you can use is Moore’s voting algorithm, which can be used to\ndetermine the majority element in a container, if there is one, in time.Θ(𝑛)\nWhen running this algorithm, you will need to keep track of a counter and a \"candidate\" element that could potentially be the majority\nelement. The counter is initialized to zero. Then, begin a traversal of the array using the following steps:\n1. Initialize the candidate to the first element and the counter to 1.\n2. Continue traversing the array. If an element is equal to the candidate value, increment the counter.\n3. If an element is not equal to the candidate value, decrement the counter.\n4. If the counter ever hits 0, update the candidate value to the element currently being visited, and set the counter back to 1.\n5. Repeat steps 2-4 until the entire array is processed. The \"candidate\" element that remains at the end of the traversal is guaranteed to be\nthe majority element if a majority element exists.\n6. If a majority element is not guaranteed to exist, traverse the array again, but this time check to see if the candidate actually appears 𝑛∕2\ntimes. This is done to ensure that the candidate is indeed a majority element.\nLet’s explain this algorithm using an example. Consider the following array, where 3 is the majority element:\n3\n1\n3\n3\n1\n0\n1\n2\n3\n4\nWhen traversing the array, the first element we encounter is 3, which is equal to our current candidate value. Thus, we would increment the\ncounter to 1.\nCandidate: 3\nCounter: 1\n3\n1\n3\n3\n1\n0\n1\n2\n3\n4\nNext, we visit element 1. Since 1 is not equal to our candidate, we decrement the counter.\nCandidate: 3\nCounter: 0\n3\n1\n3\n3\n1\n0\n1\n2\n3\n4", "word_count": 755, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4edb749a-8506-55c5-9f2b-cb5c512761a4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 464, "real_page_number": null, "text": "452\nChapter 15. Binary Search and Additional Algorithms\nNotice that our counter is now 0. When this happens, we update the candidate to the current value we are on, or 1. The counter gets reset to 1.\nCandidate: 1\nCounter: 1\n3\n1\n3\n3\n1\n0\n1\n2\n3\n4\nThe next element is 3. Since 3 is not equal to our candidate, we decrement the counter.\nCandidate: 1\nCounter: 0\n3\n1\n3\n3\n1\n0\n1\n2\n3\n4\nThe counter is zero, so we will need to update our candidate. The candidate becomes 3, and the counter gets set to 1.\nCandidate: 3\nCounter: 1\n3\n1\n3\n3\n1\n0\n1\n2\n3\n4\nThe next element is 3, which is equal to our candidate. Thus, we increment the counter.\nCandidate: 3\nCounter: 2\n3\n1\n3\n3\n1\n0\n1\n2\n3\n4\nThe next element is 1, which is not equal to our candidate. Thus, we decrement the counter.\nCandidate: 3\nCounter: 1\n3\n1\n3\n3\n1\n0\n1\n2\n3\n4\nWe are done traversing the array, and our candidate is 3. Therefore, we can conclude that if a majority element exists in the array, then it must be\n⌊5∕2⌋=3. We must do another traversal to make sure that 3 appears more than times. This additional traversal is needed in case there is no2\nmajority element. To see why this additional traversal is important, consider the following:\nCandidate: 4\nCounter: 0\n4\n4\n3\n3\n1\n0\n1\n2\n3\n4\nIf we follow the rules above, we would obtain the following after the traversal:\nCandidate: 1\nCounter: 1\n4\n4\n3\n3\n1\n0\n1\n2\n3\n4\nHowever, it is clear that 1 is not our majority element! In fact, there is no majority element, since no element appears more than 2 times. This is\nwhy the additional traversal at the end is needed, as we need to ensure our candidate is actually a majority element.\nThe correctness of Moore’s voting algorithm is actually quite intuitive. Each element \"votes\" for itself as the candidate, and opposing votes\ncancel each other out. If there exists a majority element, then the candidate that remains after the traversal must have had enough votes to not be\nnegated by the other elements. The code for this algorithm is shown on the next page.", "word_count": 402, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab5a0c07-3268-5026-a809-f79b50f4a4fb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 465, "real_page_number": null, "text": "15.3 Moore’s Voting Algorithm\n453\n-1The code for Moore’s voting algorithm is shown below, to identify a majority element. is returned if no majority element exists.\n1\nint32_t majority_element(const std::vector<int32_t>& vec) {\n2\nint32_t counter = 0, candidate;\n3\nfor (int32_t val : vec) {\n4\nif (counter == 0) {\n5\ncandidate = val;\n6\n} // if\n7\ncounter += (val == candidate) ? 1 : -1;\n8\n} // for val\n9\nint32_t threshold = vec.size() / 2;\n10\ncounter = 0;\n11\nfor (int32_t val : vec) {\n12\nif (val == candidate) {\n13\n++counter;\n14\n} // if\n15\n} // for val\n16\nif (counter > threshold) {\n17\nreturn candidate;\n18\n} // if\n19\nreturn -1;\n20\n} // majority_element()\n21\n22\nint main() {\n23\nstd::vector<int32_t> vec = {3, 1, 3, 3, 1};\n24\nint32_t majority = majority_element(vec);\n25\nif (majority != -1) {\n26\nstd::cout << \"The majority element is \" << majority << '\\n';\n27\n} // if\n28\nelse {\n29\nstd::cout << \"There is no majority element.\\n\";\n30\n} // else\n31\n} // main()\n-1 -1Remark: In the above code, is returned if there is no majority element. However, what if were actually the majority element? This\nleads to an interesting conundrum: we need to return a value that indicates that the function failed to find a majority value, but we also need to\nstd::optional<>,ensure that the value we return upon failure cannot be the majority element itself! This would be a good place to use\nwhichisanobjectthatmanagesanoptionalvalue. Thatway,thefunctionwouldonlyneedtoreturnanintegerifamajorityelementexists,and\nstd::optional<> std::optional<>,we can check the contents of the to determine the existence of the majority element. To use\n#include <optional>.you must The modified code is shown below:\n1\nstd::optional<int32_t> majority_element(const std::vector<int32_t>& vec) {\n2\nint32_t counter = 0, candidate;\n3\nfor (int32_t val : vec) {\n4\nif (counter == 0) {\n5\ncandidate = val;\n6\n} // if\n7\ncounter += (val == candidate) ? 1 : -1;\n8\n} // for\n9\nint32_t threshold = vec.size() / 2;\n10\ncounter = 0;\n11\nfor (int32_t val : vec) {\n12\nif (val == candidate) {\n13\n++counter;\n14\n} // if\n15\n} // for val\n16\nif (counter > threshold) {\n17\nreturn candidate;\n18\n} // if\n19\nreturn std::nullopt;\n// return optional with no value if no majority element\n20\n} // majority_element()\n21\n22\nint main() {\n23\nstd::vector<int32_t> vec = {4, 4, 3, 3, 1};\n24\nstd::optional<int32_t> majority = majority_element(vec);\n25\nif (majority.has_value()) {\n// if a majority element exists\n26\nstd::cout << \"The majority element is \" << *majority << '\\n';\n27\n} // if\n28\nelse {\n29\nstd::cout << \"There is no majority element.\\n\";\n30\n} // else\n31\n} // main()\nstd::optional<>,For more on review section 11.13.2.", "word_count": 502, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9909f657-ae41-5dd9-8a07-516b60da886f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 466, "real_page_number": null, "text": "454\nChapter 15. Binary Search and Additional Algorithms\n15.4\nDutch National Flag Algorithm (✽)\nThe Dutch national flag algorithm can be used to sort a container in linear time using no auxiliary space, provided that the container only\ncontains elements that take on three distinct values (named after the Dutch flag, with three colored stripes). For example, suppose you are given\nan array filled with 0s, 1s, and 2s, and you want to sort the array in linear time:\n[2, 1, 0, 1, 2, 0]\nThe goal behind the Dutch national flag algorithm is to swap all the 0s to the left end of the array, and swap all of the 2s to the right end of the\narray. At the very end, all the 1s will be left in the middle, leaving the array fully sorted. The steps of the algorithm are as follows:\n1. Create a index that references the first element in the array, and a index that references the last element in the array.low high\nmid mid2. Create a index that starts from the beginning of the array and iterates through each element. The algorithm stops once passes\nhigh. mid.During the iteration, the following steps are followed based on the value at index\nvec[mid] vec[mid] vec[low] low mid• If is 0, swap with and increment both and by 1.\nvec[mid] mid• If is 1, don’t swap anything and increment by 1.\nvec[mid] vec[mid] vec[high] high• If is 2, swap with and decrement by 1.\nThe code for the Dutch national flag algorithm is shown below, for a vector containing only 0s, 1s, and 2s:\n1\nvoid dutch_national_flag(std::vector<int32_t>& vec) {\n2\nsize_t low = 0, high = vec.size() - 1;\n3\nfor (size_t mid = 0; mid <= high; ) {\n4\nif (vec[mid] == 0) {\n5\nstd::swap(vec[mid++], vec[low++]);\n6\n} // if\n7\nelse if (vec[mid] == 2) {\n8\nstd::swap(vec[mid], vec[high--]);\n9\n} // else if\n10\nelse {\n11\n++mid;\n12\n} // else\n13\n} // for mid\n14\n} // dutch_national_flag()\nLet’s go through this algorithm using the example above:\nmid\nlow\nhigh\n2\n1\n0\n1\n2\n0\n0\n1\n2\n3\n4\n5\nvec[mid] vec[mid] vec[high] highis 2, so we swap with and decrement by 1.\nmid\nlow\nhigh\n0\n1\n0\n1\n2\n2\n0\n1\n2\n3\n4\n5\nvec[mid] vec[mid] vec[low] low mid lowis 0, so we swap with and increment both and by 1. Nothing is swapped here since and\nmid refer to the same index.\nmid\nlow\nhigh\n0\n1\n0\n1\n2\n2\n0\n1\n2\n3\n4\n5\nvec[mid] midis 1, so we increment by 1.\nlow\nmid\nhigh\n0\n1\n0\n1\n2\n2\n0\n1\n2\n3\n4\n5\nvec[mid] vec[mid] vec[low] low midis 0, so we swap with and increment both and by 1.\nlow\nmid\nhigh\n0\n0\n1\n1\n2\n2\n0\n1\n2\n3\n4\n5\nvec[mid] mid mid high,is 1, so we increment by 1. is now equal to so the array is fully sorted.\nmid vec[mid] vec[mid]It is important to note that is incremented if is 0 or 1, but it is incremented if is 2. To see why, considernot\n[1, 2, 0]. mid vec[mid] == 2the array If we increment in the case, we would swap 0 and 2, but we would never be able to swap 0 with 1.\n[1, 0, 2], mid highAs a result, our final array would be which is not sorted. Thus, to prevent from exceeding before the array is fully\nmid vec[mid]sorted, we do not increment if is 2.", "word_count": 618, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d9562b2e-d8b5-5477-a04c-5f2477c35834", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 467, "real_page_number": null, "text": "15.5 Two Pointer Technique\n455\n15.5\nTwo Pointer Technique (✽)\nThe two pointer technique is an algorithmic technique that can be used to solve several different types of programming problems. With the two\npointer technique, we start with two pointers (or indices): one that references the first element and one that references the last element. We then\nmove the two pointers toward each other, using the values they reference at each step to help us solve the problem. The Dutch national flag\nalgorithm covered earlier is an example of this technique. Two additional examples of this algorithmic approach are provided below.\nExample 15.5 You are given an array of positive integers that is sorted in ascending order. Write an algorithm that, when given this array\n[3, 4, 7, 9,and a target number, returns a pair of two numbers in the array that sums to the target number. For example, given the array\n10] 11, [4, 7]. [-1, -1]and a target of you would return Return if there is no way to sum to the target number.\nΘ(𝑛2)Anaïvesolutionwouldbetosumupeverypossiblepairinthearrayandcheckitagainstthetargetvalue, whichwouldtake time. However,\nsince we know that the underlying array is sorted, we can use the two pointer technique to reduce the runtime to Θ(𝑛). The idea is as follows.\nleft right,First, we initialize two indices, and that refer to the first and last element in the vector.\nleft\nright\n3\n4\n7\n9\n10\n0\n1\n2\n3\n4\nleft right? 13,What happens when we add the values at and We get a sum of which is not the target value we want. However, this actually\nrightgives us valuable information — we know that our sum is too large. To decrease the sum, we will have to decrement the index.\nleft\nright\n3\n4\n7\n9\n10\n0\n1\n2\n3\n4\nleft right, 12. rightNow, if we add the values at and we get a sum of This is still too large, so we decrement the index by one.\nleft\nright\n3\n4\n7\n9\n10\n0\n1\n2\n3\n4\n10, left.Now, the sum is which is too small. The only way to increase the sum is to increment Note that both indices only move toward the\nmiddle of the array. This ensures that every value is only visited once and guarantees a runtime.Θ(𝑛)\nleft\nright\n3\n4\n7\n9\n10\n0\n1\n2\n3\n4\nleft right 11, left rightThe sum of and is now equal to our target value, so we can return this pair. Notice that if and ever meet, there\nmust exist no pair that sums to the given target. The code for this solution is shown below:\n1\nstd::pair<int32_t, int32_t> target_sum(const std::vector<int32_t>& int32_tvec, target) {\n2\nsize_t left = 0, right = vec.size() - 1;\n3\nwhile (left < right) {\n4\nif (vec[left] + vec[right] < target) {\n5\n++left;\n6\n} // if\n7\nelse if (vec[left] + vec[right] > target) {\n8\n--right;\n9\n} // else if\n10\nelse {\n11\nreturn {vec[left], vec[right]};\n12\n} // else\n13\n} // while\n14\nreturn {-1, -1};\n// no solution\n15\n} // target_sum()\nSince we are moving the left and right pointers toward each other and terminating the algorithm once they cross, we only complete a single pass\nof the input vector, and the time complexity of this algorithm is Θ(𝑛). The memory allocated by this solution does not depend on the input size,\nso the auxiliary space usage of this solution is Θ(1).", "word_count": 616, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cfd64a2e-b5c5-546d-95fe-905f7ad67195", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 468, "real_page_number": null, "text": "456\nChapter 15. Binary Search and Additional Algorithms\nExample 15.6 You are given an array of 𝑛non-negative integers that represent the height of vertical bars on a map. For example, the array\n[1, 4, 9, 5, 8, 10, 7, 6, 3] represents the following map:\n10\n8\n6\n4\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nEach bar is a distance of 1 away from adjacent bars. Implement a function that returns the maximum amount of water that can be held by the\nbars on the map. The most water that can be held in the example above is 30:\n10\n8\n6\n4\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nLike with the previous example, a naïve solution would be to check every possible pair of bars to determine the maximum amount of water that\ncan be held. However, we can use the two pointer approach to reduce the complexity of this problem to linear time. To do so, we first start by\nconsidering both the leftmost and rightmost bars, as shown below. Here, the amount of water we can hold is 8.1×(9−1)=\n10\n8\n6\n4\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNext, we will move the two pointers (or indices) toward each other in order to discover potentially better solutions. Which one do we move first?\nSince we are trying to maximize the amount of water we can hold, we should move the pointer referencing the shorter bar. In fact, we should\ncontinue moving this pointer until we encounter a bar that is taller than the ones we have encountered before, as only a taller bar can potentially\nyield a better solution. In this case, we will increment the left pointer to the right by one.\n10\n8\n6\n4\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9", "word_count": 319, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab14ce25-45da-59fa-96c7-bf3f41947048", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 469, "real_page_number": null, "text": "15.5 Two Pointer Technique\n457\nThe amount of water we can hold is now 21. This is better than our previous solution of 8, so we will keep track of 21 as the largest3×(9−2)=\nvalue we have encountered so far. At this point, the bar on the right is now the smaller one under consideration, so we decrement its pointer until\nwe encounter a bar that is higher.\n10\n8\n6\n4\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThe amount of water we can hold is now 24, which is better than our previous solution of 21. 24 now becomes the largest value we4×(8−2)=\nhave seen so far. Since the bar on the left is the smaller of the two, we then increment the left pointer until we encounter a bar that is higher.\n10\n8\n6\n4\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThe amount of water we can hold is now 30. This is better than 24, so 30 becomes the best we have seen so far. Since the bar on6×(8−3)=\nthe right is shorter, we decrement its pointer until we encounter a bar that is taller.\n10\n8\n6\n4\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThe amount of water we can hold is now 28. This is not better than 30, so we ignore this result. The bar on the right is the shorter7×(7−3)=\nof the two, so we decrement its pointer until we encounter a taller bar.\n10\n8\n6\n4\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThe amount of water we can hold is now 27. This is not better than 30, so we ignore this result. The bar on the left is the shorter of9×(6−3)=\nthe two, so we increment its pointer until we encounter a taller bar (note that this allows us to skip bars 4 and 5, since they cannot produce a\nbetter solution). This causes our left and right pointers to point to the same bar, which indicates that our algorithm is complete. Since 30 was the\nbest value we encountered, it must be the solution for this example.", "word_count": 379, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7c06f3ab-6654-5e8c-b5d5-82b7ba100277", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 470, "real_page_number": null, "text": "458\nChapter 15. Binary Search and Additional Algorithms\nAn implementation of this solution is shown below. This solution uses indices, but you can also use iterators to traverse the bars instead.\n1\nint32_t max_water(const std::vector<int32_t>& bars) {\n2\nsize_t left = 0, right = bars.size() - 1;\n3\nint32_t best = 0;\n4\nwhile (left < right) {\n5\nint32_t min_height = std::min(bars[left], bars[right]);\n6\nbest = std::max(best, min_height (right - left));*\n7\nwhile (bars[left] <= min_height && left < right) {\n8\n++left;\n9\n} // while\n10\nwhile (bars[right] <= min_height && left < right) {\n11\n--right;\n12\n} // while\n13\n} // while\n14\nreturn best;\n15\n} // max_water()\nSimilar to the previous problem, we only complete a single pass of the input vector, so the time complexity of this solution is Θ(𝑛). The memory\nallocated by this solution does not depend on input size, so the auxiliary space usage is Θ(1).\n15.6\nSliding Window Technique (✽)\nThe sliding window technique is another technique that can be used to solve different programming problems. The sliding window technique\nconsiders individual subsets of data and expands or shrinks that subset based on the conditions of the problem, giving it the effect of \"sliding\" a\nwindow through the data. In many cases, the sliding window technique is a useful tool for solving problems where you are given an ordered and\niterable container (like a vector or a string) and asked for information on a subrange of that container (such as the longest or shortest subrange\ncondition).1that satisfies a given We will look at a few problems that can be solved using a sliding window approach below.\nExample 15.7 str,Given a string find the length of the longest substring without any repeating characters. You may assume that the\nstring you are given only contains the letters a-z, all in lowercase.\n\"eecsiseecs\", 4, (\"ecsi\")Example: Given the string return since the longest substring without any repeating letters has length 4.\nThe simplest solution would be to generate all possible substrings of the given string, and among the substrings that have no duplicate letters,\nidentify the one with the largest length. However, such an approach is clearly inefficient. Instead, since the problem deals with a contiguous\nsubrange that needs to satisfy a given condition, we will approach it using the sliding window technique.\nIt turns out that the sliding window approach can indeed be used to solve this problem efficiently. In this solution, we start at the left\nedge of the string, increasing the size of our window one character at a time. Along the way, we keep track of which letters appear in our\nwindow, allowing us to immediately detect the presence of a duplicate letter. We also keep track of the length of the longest viable string we\nhave encountered so far. If the right boundary of our window ever encounters a duplicate letter, we must increment the left boundary until the\nduplicate is gone. This process is illustrated in the example below.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 0\nLetters currently in window:\neFirst, we add to our sliding window. The best window size encountered becomes 1.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\n1Best window size encountered:\neLetters currently in window:\nslide\ne e eNext, we add the second to our sliding window. However, already exists in our window, so the addition of this new would make our\newindow contain a duplicate letter. To fix this, we would continuously increment the left boundary of our window until there is only one\nremaining. In this case, the left boundary only needs to move forward by one position. Our new window is shown below:\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 1\nLetters currently in window: e\nslide\nc.We then slide the right boundary of the window forward, adding in This is still a valid window without any duplicate letters, so we update the\ncbest window size encountered to 2 and add to the collection of letters currently in our window.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\n2Best window size encountered:\ne, cLetters currently in window:\nslide\n1Thisisjustaruleofthumb,andthereisnoguaranteethataslidingwindowcanbeusedtosolveallproblemsthatfollowthisstructure. Forcertainproblems,a\nmoreadvancedalgorithmictechniquemaybenecessary,suchasdynamicprogrammingordivide-and-conquer(bothofwhichwillbecoveredlater). However,\nthisisstillagoodpatterntohaveinyouralgorithmtoolbox,asitcanhelpyoucomeupwithsolutionstoproblemsthatcanbesolvedusingthistechnique.", "word_count": 805, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b4fd54e8-02dc-50ba-ad0c-d968f3bf063a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 471, "real_page_number": null, "text": "15.6 Sliding Window Technique\n459\ns. sWe slide our window forward again, adding in There are no duplicate letters yet, so we update the best window encountered to 3 and add to\nthe collection of letters currently in our window.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\n3Best window size encountered:\ne, c, sLetters currently in window:\nslide\ni. iWe slide our window forward again, adding in There are no duplicate letters yet, so we update the best window encountered to 4 and add to\nthe collection of letters currently in our window.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\n4Best window size encountered:\ne, c, s, iLetters currently in window:\nslide\ns. s sNext, we slide our window forward again, adding in However, there is already an in our window, so adding this new would result in a\nsduplicate letter. Since our window is invalid, we will need to shift forward the left boundary of our window until there is only one remaining\n(updating the collection of letters in the window as necessary).\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\nc, s, iLetters currently in window:\nslide\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\ns, iLetters currently in window:\nslide\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\niLetters currently in window:\nslide\ns sAfter removing the other from the window, we can safely add the new without introducing a duplicate letter. The window size now becomes\n2, which is not better than the best known length encountered so far, so the best window size is not updated.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\ni, sLetters currently in window:\nslide\ne,We slide our window forward and add which does not introduce a duplicate letter.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\ni, s, eLetters currently in window:\nslide\ne, eThe next however, introduces a duplicate letter, so we increment the left boundary of our window until the other is gone.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\ns, eLetters currently in window:\nslide\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\neLetters currently in window:\nslide\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\nLetters currently in window:\nslide\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\neLetters currently in window:\nslide", "word_count": 467, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9f60abc9-34b6-5a38-86b2-aeb1533faaef", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 472, "real_page_number": null, "text": "460\nChapter 15. Binary Search and Additional Algorithms\nc,We slide our window forward and add which does not introduce a duplicate letter.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\ne, cLetters currently in window:\nslide\ns,We slide our window forward and add which does not introduce a duplicate letter.\ne\ne\nc\ns\ni\ns\ne\ne\nc\ns\nBest window size encountered: 4\ne, c, sLetters currently in window:\nslide\nThe window has now reached the end, so we are done iterating. At this point, the best window size encountered (in this case, 4) is the length of\nthe longest substring without any duplicate letters, and thus the solution to our problem. An implementation of this solution is shown below:\n1\nint32_t length_of_longest_substring(const std::string& str) {\n2\nstd::vector<int8_t> count(26);\n// keeps track of whether each letter is in our window\n3\nsize_t left_idx = 0, right_idx = 0;\n4\nint32_t best = 0;\n5\n6\nwhile (right_idx < str.length()) {\n7\n++count[str[right_idx] - 'a'];\n// increment count for letter at right_idx\n8\nwhile (count[str[right_idx] - 'a'] > 1) {\n// while loop runs if there is a duplicate\n9\n--count[str[left_idx] - 'a'];\n// remove character at left_idx from count\n10\n++left_idx;\n// slide left_idx forward\n11\n} // while\n12\n13\nint32_t current_length = right_idx - left_idx + 1;\n14\nbest = std::max(best, current_length);\n// update if current length is better than best so far\n15\n++right_idx;\n16\n} // while\n17\n18\nreturn best;\n19\n} // length_of_longest_substring()\nΘ(𝑛2)What is the time complexity of this solution? There is a nested loop, so it may initially seem that this algorithm runs in time, where 𝑛is\nthe length of the given string. However, this assumption is incorrect because there is a dependency involved between the two loops: the inner\nright_idxloop only runs as long as the count of the character at is greater than one, but the count itself is bounded by the number of times\nright_idx is incremented within the outer loop! Because the inner loop can only run at most once for each iteration of the outer loop (which\nruns 𝑛times), the combined time complexity of the two loops must also be Θ(𝑛).\nThe auxiliary space used by this solution is constant, since we only declared a few variables and a vector of size 26, none of which depend\non the size of the given string itself.\nExample 15.8 str target,You are given two strings, and with lengths of 𝑠and 𝑡, respectively. Write a function that returns the shortest\nstr targetsubstring of that contains every character in (with the same frequency if there are duplicates). If there is no such substring in\nstr target \"\".such that every letter is in with the same frequency, then return the empty string For simplicity, you may assume that the\ncharacters in the string can only contain the letters a-z, all in lowercase, and that there is only one unique solution.\nstr = \"transcendences\" target = \"eecs\", \"ences\",Example: Given and you would return since this is the shortest substring\n\"transcendences\" \"eecs\"in that contains all the letters in with the same frequency (2 e’s, 1 c, 1 s).\nAs with before, a simple solution would be to generate all possible substrings of the original string and find the shortest one that contains all of\nthe characters in the target string. However, this approach is inefficient. Instead, we can use a sliding window approach to solve the problem\nwith a time complexity that is linear on the lengths of the two given strings.\nSimilar to the previous problem, we will slide our window forward using two indices. The right index is responsible for expanding the\nwindow, while the left index is responsible for contracting the window. The movement of these indices depends on the contents of our window.\ntargetIn our example, we will continuously increment the right index until we encounter a window that has all of the characters in (with\nmatching frequencies for duplicate letters), which we will define as a window. Once we find a viable window, we remember its contentsviable\nas a possible solution. Then, we will move the left index forward to see if we can get a viable window with a smaller length. As long as our\nwindow remains viable, we will continuously move the left index forward and store the result if it is the best solution we have encountered so\nfar. However, once our window is no longer viable, we stop incrementing the left index and begin incrementing the right index once again,\nexpanding our window forward to find the next viable solution. This process continues until the right index reaches the end of the string, which\nindicates that there are no more windows to consider. At this point, we would return the best solution we encountered so far.\nLet’s look at this process visually using the example provided with the problem. First, we continuously increment the right index until our\nwindow stores a viable solution that contains all the letters in our target string.\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide", "word_count": 888, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "897b180e-35cd-5df8-b5ac-9757ee410d13", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 473, "real_page_number": null, "text": "15.6 Sliding Window Technique\n461\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nYes.Does the window contain a viable solution?\n\"transcende\"Best solution encountered so far:\nslide\nAt this point, we have a viable solution in our window. We keep track of this solution, and then we continuously increment the left index until\nthe window no longer stores a viable solution (updating the best known solution along the way).\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? Yes.\n\"ranscende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? Yes.\n\"anscende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? Yes.\n\"nscende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? Yes.\n\"scende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nNo.Does the window contain a viable solution?\n\"scende\"Best solution encountered so far:\nslide", "word_count": 453, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "86a6e308-968d-59d7-877e-83d01e79dbce", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 474, "real_page_number": null, "text": "462\nChapter 15. Binary Search and Additional Algorithms\nThe window no longer holds a viable solution, so we will start incrementing the right index again until we encounter another viable solution.\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"scende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"scende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? No.\n\"scende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nYes.Does the window contain a viable solution?\n\"scende\"Best solution encountered so far:\nslide\nThe window is viable again, so we will begin incrementing the left index to see how small we can make our window, updating our best solution\nif we ever encounter a viable window that is smaller.\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? Yes.\n\"scende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? Yes.\n\"scende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? Yes.\n\"scende\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nDoes the window contain a viable solution? Yes.\n\"ences\"Best solution encountered so far:\nslide\nt\nr\na\nn\ns\nc\ne\nn\nd\ne\nn\nc\ne\ns\nNo.Does the window contain a viable solution?\n\"ences\"Best solution encountered so far:\nslide\nThe window is no longer viable, so we will resume incrementing the right index. However, this moves the right index off the end of the string.\n\"ences\"This indicates that we have checked all valid windows, so the current best solution of must be the solution to the entire problem. An\nimplementation of this solution is provided below:\n1\nmin_window_substring(const conststd::string std::string& str, std::string& target) {\n2\nif (str.length() == 0 || target.length() == 0) {\n3\nreturn \"\";\n// no solution if either string is empty\n4\n} // if\n5\n6\n// vector keeps track of character frequency in window ('a' = 0, 'b' = 1, ..., 'z' = 25)\n7\nstd::vector<int32_t> count_in_window(26);\n8\n// vector keeps track of character frequency in target ('a' = 0, 'b' = 1, ..., 'z' = 25)\n9\nstd::vector<int32_t> count_in_target(26);\n10\nfor (size_t i = 0; i < target.length(); ++i) {\n11\n++count_in_target[target[i] - 'a'];\n12\n} // for i\n13\n14\n// counts the number of characters that are matched in the window\n15\n// if this value ever equals the length of the target string, we have a viable window\n16\nsize_t characters_matched = 0;\n17\n// left and right indices\n18\nsize_t left_idx = 0, right_idx = 0;\n19\n// best solution\n20\nstd::string best;\n21\n22\n/* ... continued on next page ... */", "word_count": 555, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3c1f413d-80a5-5af8-93b9-52ffb5b84dd3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 475, "real_page_number": null, "text": "15.7 Monotonic Stacks and Queues\n463\n23\n// iterate while right_idx is not off the end\n24\nwhile (right_idx < str.length()) {\n25\nchar right_idx_char = str[right_idx];\n26\n++count_in_window[right_idx_char - 'a'];\n27\n// if character in target string added (and not over frequency), increment characters matched\n28\nif (count_in_window[right_idx_char - 'a'] <= count_in_target[right_idx_char - 'a']) {\n29\n++characters_matched;\n30\n} // if\n31\n32\n// if viable, try to contract the window (move left_idx forward) until window no longer viable\n33\nwhile (left_idx <= right_idx && characters_matched == target.length()) {\n34\n// update best known solution if needed\n35\nsize_t window_length = right_idx - left_idx + 1;\n36\nif (best.empty() || window_length < best.length()) {\n37\nbest = str.substr(left_idx, window_length);\n// store window as substring\n38\n} // if\n39\n40\nchar left_idx_char = str[left_idx];\n41\n--count_in_window[left_idx_char - 'a'];\n42\n// if character removed was in target, decrement characters_matched\n43\n// this means the window is no longer viable, and would terminate this while loop\n44\nif (count_in_target[left_idx_char - 'a'] > 0\n45\n&& count_in_window[left_idx_char - 'a'] < count_in_target[left_idx_char - 'a']) {\n46\n--characters_matched;\n47\n} // if\n48\n49\n// move the left index forward, contracting the window\n50\n++left_idx;\n51\n} // while\n52\n53\n// window is not viable, so move the right index forward, expanding the window\n54\n++right_idx;\n55\n} // while\n56\nreturn best;\n57\n} // min_window_substring()\nstr,Since the work done by the sliding window (lines 24-55) is linear on the size of the input string and we iterate over the characters of the\ntarget string once on lines 10-12, the overall time complexity of this solution is Θ(𝑠+𝑡), where 𝑠and 𝑡are the lengths of the two strings.\n15.7\nMonotonic Stacks and Queues (✽)\nA monotonic stack/queue is a stack or queue whose elements are stored in a strictly ascending or descending order. You can think of these\ncontainers as similar to their regular stack or queue counterparts, but with a key distinction for the push operation: before pushing an element\ninto a monotonic stack or queue, you must first check if it breaks the monotonic condition (i.e., the strictly ascending or descending order\nof elements). If it does, you must pop out all elements that violate the monotonic condition before pushing the new element in. Monotonic\ncontainers can be useful in solving problems such as finding the next largest or smallest element in a list, or identifying the maximum or\nminimum value in a sliding window. We will go over some of these problems in this section.\nExample 15.9 tempsYou are given a vector of integers that stores the daily temperature forecasts for the next few days. Implement a\nfunction that, for each index of the input vector, stores the number of days you would need to wait for a warmer temperature. If there is no\nfuture day where this is possible, a value of 0 should be stored.\n[25, 32, 16, 22, 21, 20, 21, 23, 33], [1, 7, 1, 4, 3, 1, 1, 1,Example: Given the vector you would return the solution\n0], since you would need to wait 1 day on day 0 for a warmer temperature (25→32), 7 days on day 1 for a warmer temperature (32→33),\n1 day on day 2 for a warmer temperature (16→22), 4 days on day 3 for a warmer temperature (22→23), and so on.\nA naïve brute-force force approach would iterate through the entire vector and, for each day, iterate again over all the remaining days to find a\nΘ(𝑛2) tempswarmer temperature. This approach utilizes a nested loop that runs in time, where 𝑛is the number of days in the provided vector.\nHow can we do better? One key insight to notice is that, if there are multiple days where temperatures are in descending order, then those\ndays will share the same \"warmer temperature day,\" if there is one. Instead of looping over the entire vector for each day, we can \"defer\" finding\nthe solution for days whose temperatures are in descending order until we encounter a warmer temperature, at which we can move backward and\ntempsassign the solution for all these days at the same time. As an example, consider the following vector:\n[18, 17, 16, 15, 14, 13, 12, 11, 19]\nSince the first eight days have decreasing temperatures (18→11), the solution for all of these eight days is to wait until the ninth day. Instead of\nlooping over the vector for each of the first eight days, we will wait until we encounter a warmer temperature (in this case, 19 on the ninth day)\nand then backfill this result to the previous eight days.\nTo accomplish this, we would need to store our data in a container that allows us to access all the days that are still waiting on a warmer\ntempsevery time we consider a new temperature in the vector. Since we want to consider previous temperatures every time wetemperature\nconsider a new temperature value, this container will need to support last-in, first-out (LIFO) behavior, which matches the functionality of a\nstack. In addition, we should only defer finding a solution if the temperatures are in descending order, as these days may end up waiting for the\nsame higher temperature value. Because of this, our stack will need to keep track of the temperatures in order so that we can quicklydescending", "word_count": 907, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "37686a5f-ca70-5081-aa2b-475429c4dd2c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 476, "real_page_number": null, "text": "464\nChapter 15. Binary Search and Additional Algorithms\ndetermine if the temperature value we are currently considering is higher than any of the temperatures waiting for a solution in our stack. This is\nwhere a monotonic stack can come into play.\n[25, 32, 16, 22, 21, 20, 21, 23, 33].As an example, consider the initial example provided in our problem: We will iterate\nwaiting)over all the temperatures in our vector, pushing them into a descending monotonic stack (which we will call if we do not know\nof a warmer temperature. However, once we discover a warmer temperature, we can pop temperatures off the monotonic stack and assign\nthem a solution. Since we want to know the number of days we need to wait, we will store the indices of the days in our stack rather than the\ntemperatures themselves (which we can easily access by indexing into the temperatures vector). This allows us to more easily compute the\nnumber of days we need to wait to encounter a warmer temperature.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n(monotonic stack)\nsolution\nWe start off at day 0, with a temperature of 25. We do not yet know of a warmer temperature, so we push day 0 into the waiting stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n0\n(25)\nsolution\nNext, we consider day 1, with a temperature of 32. This is warmer than the temperature on day 0, so we pop 0 out of the stack and update the\nsolution of day 0 to 1 - 0 = 1 (the wait time between day 0 and day 1). Then, we push day 1 into the stack, since we have not yet discovered a\nwarmer temperature for day 1.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n(32)\nsolution\n1\nNext, we consider day 2, with a temperature of 16. This is not warmer than the day at the top of our stack, so we push 2 into the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n2\n(32) (16)\nsolution\n1\nNext, we consider day 3, with a temperature of 22. The day at the top of the stack, 2, has a temperature of 16. Since 22 is warmer than 16, we\nknow that day 3 is the first day that is warmer than day 2, and that we have to wait 3 - 2 = 1 day to experience a warmer temperature starting\nfrom day 2. Therefore, we set the solution for day 2 to 1 and pop day 2 out of the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n(32)\nsolution\n1\n1\nThe day at the top of the stack, 1, has a temperature of 32. 22 is not warmer than 32, so we push day 3 into the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n3\n(32) (22)\nsolution\n1\n1", "word_count": 560, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e290b6db-d985-558f-aab8-e888463c7774", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 477, "real_page_number": null, "text": "15.7 Monotonic Stacks and Queues\n465\nNext, we consider day 4, with a temperature of 21. This is not warmer than the day at the top of the stack, so we push day 4 into the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n3\n4\n(32) (22) (21)\nsolution\n1\n1\nNext, we consider day 5, with a temperature of 20. This is not warmer than the day at the top of the stack, so we push day 5 into the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n3\n4\n5\n(32) (22) (21) (20)\nsolution\n1\n1\nNext, we consider day 6, with a temperature of 21. The day at the top of the stack, 5, has a temperature of 20. Since 21 is warmer than 20, we\nknow that we have to wait 6 - 5 = 1 day to experience a warmer temperature starting from day 5. Therefore, we set the solution for day 5 to 1\nand pop day 5 out of the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n3\n4\n(32) (22) (21)\nsolution\n1\n1\n1\nThe day at the top of the stack, 4, has a temperature of 21. The current temperature of 21 is not warmer than 21, so day 6 is pushed into the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n3\n4\n6\n(32) (22) (21) (21)\nsolution\n1\n1\n1\nNext, we consider day 7, with a temperature of 23. The day at the top of the stack, 6, has a temperature of 21. Since 23 is warmer than 21, we\nknow that we have to wait 7 - 6 = 1 day to experience a warmer temperature starting from day 6. Therefore, we set the solution for day 6 to 1\nand pop day 6 out of the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n3\n4\n(32) (22) (21)\nsolution\n1\n1\n1\n1\nThe current temperature of 23 is still warmer than the day at the top of the stack (day 4), which has a temperature of 21. This indicates that\nday 7 is also the first day with a warmer temperature starting from day 4, and that we had to wait 7 - 4 = 3 days for this warmer temperature.\nTherefore, we set the solution for day 4 to 3 and pop day 4 out of the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n3\n(32) (22)\nsolution\n1\n1\n3\n1\n1\nThe current temperature of 23 is still warmer than the day at the top of the stack (day 3), which has a temperature of 22. With similar reasoning\nas above, we know that we have to wait 7 - 3 = 4 days for a warmer temperature starting from day 3. Therefore, we set the solution for day 3 to 4\nand pop day 3 out of the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n(32)\nsolution\n1\n1\n4\n3\n1\n1", "word_count": 598, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8f141164-d8ea-5f02-8936-06b2de478e31", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 478, "real_page_number": null, "text": "466\nChapter 15. Binary Search and Additional Algorithms\nThe current temperature of 23 is no longer warmer than the day waiting at the top of the stack, so we push day 7 into the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n7\n(32) (23)\nsolution\n1\n1\n4\n3\n1\n1\nNext, we consider day 8, with a temperature of 33. The day at the top of the stack, 7, has a temperature of 23. Since 33 is warmer than 23, we\nknow that day 8 is the first day with a warmer temperature after day 7, and that we would have to wait 8 - 7 = 1 day to experience a warmer\ntemperature starting from day 7. Therefore, we set the solution for day 7 to 1 and pop day 7 out of the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n1\n(32)\nsolution\n1\n1\n4\n3\n1\n1\n1\nThe current temperature of 33 is still warmer than the day at the top of the stack (day 1), which has a temperature of 32. We can therefore pop\nday 1 out of the stack and assign its solution to 8 - 1 = 7 days, since we need to wait 7 days for a warmer temperature starting from day 1.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\nsolution\n1\n7\n1\n4\n3\n1\n1\n1\nThe stack is empty, so we push day 8 into the stack.\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\n8\n(33)\nsolution\n1\n7\n1\n4\n3\n1\n1\n1\ntempsWe are now down iterating over the vector. At this point, there are no more warmer temperatures to consider, so all days remaining in\nthe queue must have a solution 0 (the value to store if there is no solution).\n25 32 16 20 23 3322 21 21\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntemps\nwaiting\nsolution\n1\n7\n1\n4\n3\n1\n1\n1\n0\nThis completes our algorithm, and our solution vector stores the number of days we need to wait for a warmer temperature on each day. An\nimplementation of this problem is shown below:\n1\nstd::vector<int32_t> warmer_temperatures(const std::vector<int32_t>& temps) {\n2\nstd::vector<int32_t> solution(temps.size());\n3\nstd::stack<int32_t> waiting;\n4\nfor (size_t curr_day = 0; curr_day < temps.size(); ++curr_day) {\n5\nint32_t curr_temp = temps[curr_day];\n6\n// pop until the current day's temp is not warmer than the day at the top of the stack\n7\nwhile (!waiting.empty() && temps[waiting.top()] < curr_temp) {\n8\n// curr_temp is warmer, so we found a solution for prev_day\n9\nint32_t prev_day = waiting.top();\n10\nsolution[prev_day] = curr_day - prev_day;\n11\nwaiting.pop();\n12\n} // while\n13\n// after all lower temps are popped out of the monotonic stack, push in the current day\n14\nwaiting.push(curr_day);\n15\n} // for curr_day\n16\nreturn solution;\n17\n} // warmer_temperatures()", "word_count": 535, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "27e9481a-2312-537c-83c5-f0842dc028a7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 479, "real_page_number": null, "text": "15.7 Monotonic Stacks and Queues\n467\nΘ(𝑛2)What is the time complexity of this solution? It may initially seem that the time complexity should be given 𝑛temperatures, since there is\nwhile fora nested loop instead a loop. However, notice that each element can only be added to the stack once, which indicates that the stack\nwhileonly experiences 𝑛pops. Furthermore, each iteration of the inner loop performs exactly one pop; since we know the stack only gets\nwhilepopped 𝑛times, this means the loop must not perform more than 𝑛pops in total. Because each element only gets pushed and popped\nonce, the time complexity of this solution is actually Θ(𝑛). This is actually a common outcome when analyzing monotonic stack or queue\nproblems: because of how elements are pushed or popped in, a solution may seem to have a quadratic time complexity when it is actually linear!\nExample 15.10 numYou are given a string that represents a non-negative integer, as well as an integer 𝑘. Implement a function that\nnum.returns the smallest integer obtainable after removing exactly 𝑘digits from\nnum = \"453178629\" \"17629\",Example: Given and 4, you would return since 17629 is the smallest integer that can be created𝑘=\nafter removing four digits from 453178629.\nTo obtain the smallest possible number, our goal is to remove as many big digits as possible in the most significant positions of the original\nnumber. This can be done by iterating over the string in order of decreasing significance position (i.e., from left to right) and tracking the digits\nencountered in a container that allows us to identify\n• its significance position (i.e., the container must be to know the order in which its digits were considered)\n• whether the digit should be removed (i.e., the digits must be stored in sorted order)\nA monotonic stack fits the bill, since it provides both LIFO behavior while also maintaining its values in sorted order. To solve the problem, we\nwill create an ascending monotonic stack that stores the digits we want to keep in our string. The largest element encountered will be at the top\nof the stack, which allows us to pop it off if we ever encounter a smaller digit that is more worthwhile to keep. Since we are limited in the\n𝑘thnumber of digits we can remove, we will restrict the stack to exactly 𝑘pops — after the pop, all remaining digits in the string are kept. If we\nend up visiting all digits without performing 𝑘pops, we will pop any excess values off the top of the stack so that we end up with exactly 𝑘pops\n(this is okay because the stack is sorted, so the larger digits are popped first). Furthermore, we will iterate over the string from left to right,\nensuring that digits with higher significance positions are prioritized when determining which digits to remove (since a large digit in a higher\nsignificance position is worse than a large digit in a lower significance position). Let’s walk through the process using the provided example.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\nPops remaining: 4\nThe first digit is 4. Our stack is currently empty, so there are no values to compare to, and we can push 4 into the stack.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n4\nPops remaining: 4\nThe next digit is 5. This is not better than the digit at the top of the stack, 4, so we do not need to pop 4 out. 5 is then pushed into the stack.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n4\n5\nPops remaining: 4\nThe next digit is 3. This is better than the digit at the top of the stack, 5, so we pop 5 out and decrement our counter.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n4\n3Pops remaining:\n3 is still better than the digit at the top of the stack, 4, so we also pop 4 out an decrement our counter.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n2Pops remaining:\nThere are no more values in the stack, so we can push in 3.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n3\nPops remaining: 2\nThe next digit is 1. This is better than the digit at the top of the stack, 3, so we pop 3 out and decrement our counter.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n1Pops remaining:\nThere are no more values in the stack, so we can push in 1.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n1\nPops remaining: 1\nThe next digit is 7. This is not better than the digit at the top of the stack, 1, so we do not need to pop 1 out. 7 is then pushed into the stack.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n1\n7\nPops remaining: 1\nThe next digit is 8. This is not better than the digit at the top of the stack, 7, so we do not need to pop 7 out. 8 is then pushed into the stack.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n1\n7\n8\nPops remaining: 1", "word_count": 899, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d0446422-b17d-58eb-9e8b-722f257430ea", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 480, "real_page_number": null, "text": "468\nChapter 15. Binary Search and Additional Algorithms\nThe next digit is 6. This is better than the digit at the top of the stack, 8, so we pop 8 out and decrement our counter.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n1\n7\n0Pops remaining:\nWe are now out of pops, since the counter has reached zero. This means that none of the remaining digits can be removed, so we will push them\nall into the stack.\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n1\n7\n6\nPops remaining: 0\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n1\n7\n6\n2\nPops remaining: 0\n4\n5\n3\n1\n7\n8\n6\n2\n9\nnum\nmonostack\n1\n7\n6\n2\n9\nPops remaining: 0\n(\"17629\")The contents of our stack make up our final solution. An implementation is shown below:\n1\nmin_remove_k_digits(const int32_tstd::string std::string& num, k) {\n2\nint32_t counter = k;\n3\n// use vector for iterator support, but this behaves like a stack (.top() == .back())\n4\nstd::vector<char> monostack;\n5\nfor (char curr_digit : num) {\n6\nwhile (counter > 0 && !monostack.empty() && monostack.back() > curr_digit) {\n7\nmonostack.pop_back();\n8\ncounter -= 1;\n9\n} // while\n10\n// curr_digit != 0 check prevents leading zeros in the solution string\n11\nif (!monostack.empty() || curr_digit != '0') {\n12\nmonostack.push_back(curr_digit);\n13\n} // if\n14\n} // for curr_digit\n15\n// if counter > 1, not all available digits are popped, so keep popping off the monotonic stack\n16\n// this is okay because stack is sorted, so the larger digits get popped first\n17\nwhile (!monostack.empty() && counter-- != 0) {\n18\nmonostack.pop_back();\n19\n} // while\n20\n// contents of stack forms out final solution (if stack is empty, solution must be zero)\n21\nreturn monostack.empty() ? \"0\" : std::string{monostack.begin(), monostack.end()};\n22\n} // min_remove_k_digits()\nstd::string .push_back() .pop_back(),Itshouldalsobenotedthat supports and soyoucanuseastringinsteadofastackcontainer\n(on line 4 below). This allows you to return the solution directly without a conversion; we will go over string operations in the next chapter.\n1\nmin_remove_k_digits(const int32_tstd::string std::string& num, k) {\n2\nint32_t counter = k;\n3\n// use a string to emulate a stack\n4\nstd::string monostack;\n5\nfor (char curr_digit : num) {\n6\nwhile (counter > 0 && !monostack.empty() && monostack.back() > curr_digit) {\n7\nmonostack.pop_back();\n8\ncounter -= 1;\n9\n} // while\n10\n// curr_digit != 0 check prevents leading zeros in the solution string\n11\nif (!monostack.empty() || curr_digit != '0') {\n12\nmonostack.push_back(curr_digit);\n13\n} // if\n14\n} // for curr_digit\n15\n// if counter > 1, not all available digits are popped, so keep popping off the monotonic stack\n16\n// this is okay because stack is sorted, so the larger digits get popped first\n17\nwhile (!monostack.empty() && counter-- != 0) {\n18\nmonostack.pop_back();\n19\n} // while\n20\n// contents of stack forms out final solution (if stack is empty, solution must be zero)\n21\nreturn monostack.empty() ? \"0\" : monostack;\n22\n} // min_remove_k_digits()\nSimilar to the previous problem, the nested loops in the solution may be deceiving. Each digit can only be pushed into the stack once, which\nnum.indicates that the stack can only experience at most 𝑛pops, where 𝑛is the number of digits in the original string Since each iteration of the\nwhileinner loop performs exactly one pop, this means that the loop cannot perform more than 𝑛pops in total — and because each digit only\ngets pushed and popped from the stack at most once during the lifetime of the algorithm, the time complexity of the solution is actually Θ(𝑛).", "word_count": 635, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "651ccba6-30dc-5ed1-91c1-e66fb01672ac", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 481, "real_page_number": null, "text": "15.7 Monotonic Stacks and Queues\n469\nExample 15.11 nums.You are given a vector of integers A sliding window of size 𝑘is moving from the left of the vector to the right, and\nyou are only able to see the 𝑘numbers in the window. Each time the sliding window moves right by one position. Return the maximum\nvalue of each sliding window in a separate vector. You may assume 𝑘is not greater than the size of the vector.\nnums = [1, 9, 8, 6, 7, 2, 4, 5], [9, 9, 8, 7, 7, 5],Example: Given a vector and 3, you would return the output vector𝑘=\nwhich stores the maximum value for each iteration of the sliding window of size 3.\nWindow Position\nMaximum Window Value\n[1, 9, 8], 6, 7, 2, 4, 5\n9\n[9, 8, 6],1, 7, 2, 4, 5\n9\n[8, 6, 7],1, 9, 2, 4, 5\n8\n[6, 7, 2],1, 9, 8, 4, 5\n7\n[7, 2, 4],1, 9, 8, 6, 5\n7\n[2, 4, 5]1, 9, 8, 6, 7,\n5\nThe simplest solution would be to iterate over all possible sliding windows and find the maximum value for each window. However, this solution\nnums,would end up taking time, where 𝑛is the size of the vector since you would have to iterate over all possible windows,Θ(𝑛𝑘) 𝑛−𝑘+1\neach of size 𝑘. Is there a way to do better?\nAnother method for solving this problem is to use a priority queue that stores an integer pair containing the value and its index. This allows\nus to determine the largest value that is also at a valid index of our current window. This solution is shown below:\n1\nstd::vector<int32_t> max_sliding_window(const std::vector<int32_t>& int32_tnums, k) {\n2\nstd::priority_queue<std::pair<int32_t, int32_t>> pq;\n// pair<element value, index>\n3\nstd::vector<int32_t> solution;\n4\nsolution.reserve(nums.size() - k + 1);\n5\n// push elements of first window into pq\n6\nfor (size_t i = 0; i < k; ++i) {\n7\npq.emplace(nums[i], i);\n8\n} // for i\n9\n// push largest value of first window in solution vector\n10\nsolution.push_back(pq.top().first);\n11\n// repeat for the remaining windows\n12\nfor (size_t j = k; j < nums.size(); ++j) {\n13\npq.emplace(nums[j], j);\n14\n// remove elements that are no longer in the current window\n15\nwhile (!(pq.top().second > j - k)) {\n16\npq.pop();\n17\n} // while\n18\nsolution.push_back(pq.top().first);\n19\n} // for j\n20\nreturn solution;\n21\n} // max_sliding_window()\nHowever, this particular solution is not the most ideal either. This is because pushing in new elements and removing older elements both take\ntime. Preferably, we want a container that allows us to insert new values and remove old values in constant time, while also allowingΘ(log(𝑛))\nus to efficiently determine the largest value in each window. This is where a monotonic queue comes into play.\nWith a standard queue, we would push in new sliding window values to the back of the queue and pop out expired sliding window values\nout the front of the queue. However, a key insight to notice is that each new element gives us information on which values in the sliding window\nare promising and which ones are not. Whenever we want to push a new element in, we know that the values in the window that are smaller than\nthis new element cannot ever be the sliding window maximum (since they will leave the window before this new element does). Because of this,\nit is safe to pop out all values that are smaller whenever we consider a new value to insert into our sliding window, ensuring that our queue\nstores its values in monotonically descending order. This allows us to identify the maximum value for any sliding window in time byΘ(1)\nsimply querying the value at the front of the monotonic queue.\nLet’s illustrate this process using the provided example. Similar to the warmer temperatures problem, we will store indices in our monotonic\nqueue rather than the values themselves, as this allows us to easily determine whether a value is no longer in the current sliding window (and the\nnumsindex also allows us to get its corresponding value by indexing into the vector).\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\nmonoqueue\nThe value at index 0 is 1. The queue is currently empty, so we push index 0 in.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\nmonoqueue\n0", "word_count": 764, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "30831db7-53a4-5373-929e-6440663a3fcd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 482, "real_page_number": null, "text": "470\nChapter 15. Binary Search and Additional Algorithms\nThe value at index 1 is 9. The value at the back of the queue (index 0, with a value of 1) is smaller, so we will pop it out before pushing index 1 in.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\nmonoqueue\n1\nThe value at index 2 is 8. The value at the back of the queue (index 1, with a value of 9) is larger, so we will push index 2 into the queue without\nremoving anything.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\nmonoqueue\n1\n2\nSince our sliding windows have size 3, our first window is ready to be processed. Since our monotonic queue stores the indices in order of𝑘=\nnumsdecreasing value, we know that the value at index 1 of (or 9) must be the maximum value of this first window, since it is at the front of the\nqueue. We therefore push 9 into our solution vector.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\nmonoqueue\n1\n2\nWe now increment our window forward by one position. This requires us to remove index 0 from our queue; however, 0 has already been\nremoved earlier (to keep our queue monotonic, since we determined index 0 cannot be the maximum of any remaining window). We will then\ninsert index 3 (the new value in our window) into the queue after removing all values that are smaller. Since the other two indices in the queue\nhave larger values of 9 and 8, nothing is added before index 3 is pushed in.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\nmonoqueue\n1\n2\n3\nThe maximum value of this sliding window must be the value at index 1 (or 9), since index 1 is at the front of the queue. We therefore push 9\ninto our solution vector.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\nmonoqueue\n1\n2\n3\nWe now increment our window forward by one position. This requires us to remove index 1 from our queue, since it is no longer in our sliding\nwindow. We will then insert index 4 into the queue after removing all values that are smaller (in this case index 3, since 6 < 7).\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\nmonoqueue\n2\n3\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\nmonoqueue\n2\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\nmonoqueue\n2\n4", "word_count": 500, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bd9f8e67-5ccc-5920-8f6f-03845db84d9b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 483, "real_page_number": null, "text": "15.7 Monotonic Stacks and Queues\n471\nThe maximum value of this sliding window is the value at index 2 (or 8), so we push 8 into our solution vector.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\nmonoqueue\n2\n4\nWe now increment our window forward by one position. This requires us to remove index 2 from our queue. Since the new value at index 5 (or\n2) is not larger than any of the values in the queue (index 4, which has a value of 7), we can push index 5 in without popping anything else out.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\nmonoqueue\n4\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\nmonoqueue\n4\n5\nThe maximum value of this sliding window is the value at index 4 (or 7), so we push 7 into our solution vector.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\n7\nmonoqueue\n4\n5\nWe now increment our window forward by one position. This requires us to remove index 3 from our queue (this step is trivial because index 3\nhad already been removed earlier). We then pop out all values that are smaller than the new value in our window (in this case, we pop out index\n5 with a value of 2) before pushing index 6 in.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\n7\nmonoqueue\n4\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\n7\nmonoqueue\n4\n6\nThe maximum value of this sliding window is the value at index 4 (or 7), so we push 7 into our solution vector.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\n7\n7\nmonoqueue\n4\n6", "word_count": 369, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7738342a-6163-5e5c-a98b-4fac59a363c8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 484, "real_page_number": null, "text": "472\nChapter 15. Binary Search and Additional Algorithms\nWe now increment our window forward by one position. This requires us to remove index 4 from our queue. We then pop out all values that are\nsmaller than the new value in our window (in this case, we pop out index 6 with a value of 4) before pushing index 7 in.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\n7\n7\nmonoqueue\n6\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\n7\n7\nmonoqueue\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\n7\n7\nmonoqueue\n7\nThe maximum value of this sliding window is the value at index 7 (or 5), so we push 5 into our solution.\n1\n9\n8\n6\n7\n2\n4\n5\nnums\n0\n1\n2\n3\n4\n5\n6\n7\nsolution\n9\n9\n8\n7\n7\n5\nmonoqueue\n7\n[9, 9, 8, 7, 7, 5].We have finished processing our final sliding window, so our algorithm has completed with a solution of An\nstd::deque<>implementation of this process is shown in the code below. Notice that we use a to implement our monotonic queue; this is\nbecause we need to be able to pop at both ends of the container (the front to discard values that exit the sliding window, and the back to discard\nvalues that are smaller than the new value that enters the sliding window).\n1\nstd::vector<int32_t> max_sliding_window(const std::vector<int32_t>& int32_tnums, k) {\n2\nif (nums.empty()) {\n3\nreturn nums;\n4\n} // if\n5\nstd::deque<int32_t> monoqueue;\n6\nstd::vector<int32_t> solution;\n7\nsolution.reserve(nums.size() - k + 1);\n8\nfor (int32_t curr_idx = 0; curr_idx < nums.size(); ++curr_idx) {\n9\n// remove old values that are no longer in the sliding window\n10\nwhile (!monoqueue.empty() && monoqueue.front() < curr_idx - k + 1) {\n11\nmonoqueue.pop_front();\n12\n} // while\n13\n// remove values that are smaller than the newest value in the window,\n14\n// since they cannot be the maximum value for any future windows\n15\nwhile (!monoqueue.empty() && nums[monoqueue.back()] < nums[curr_idx]) {\n16\nmonoqueue.pop_back();\n17\n} // while\n18\n// push curr_idx into monotonic queue\n19\nmonoqueue.push_back(curr_idx);\n20\n// write sliding window maximum to solution\n21\nif (curr_idx - k + 1 >= 0) {\n22\nsolution.push_back(nums[monoqueue.front()]);\n23\n} // if\n24\n} // for curr_idx\n25\nreturn solution;\n26\n} // max_sliding_window()\nnumsMuch like the previous two problems, the time complexity of this solution is Θ(𝑛), where 𝑛is the size of the vector. This is because each\nelement can only be pushed and popped out of the monotonic queue once, so the total work performed by the loop on line 8 ends up being linear\non the size of the initial vector.", "word_count": 492, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2c092bc7-fef1-5568-87a5-4e4d40d78842", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 485, "real_page_number": null, "text": "15.8 Median Finding Algorithms\n473\n15.8\nMedian Finding Algorithms (✽)\nFinding the median of a given collection of values may seem like a trivial task, but trying to accomplish this in an asymptotically optimal\nmanner is not entirely straightforward. The simplest way to find the median is to sort the container and take the value at index (if the𝑛∕2\n⌊𝑛∕2⌋−1container has an odd size) or the average of the values at indices and (if the container has an even size):𝑛∕2\n1\ndouble find_median(const std::vector<double>& vec) {\n2\nstd::vector<double> sorted{vec};\n3\nsize_t sz = sorted.size();\n4\nif (sz == 0) {\n5\nstd::cerr << \"Vector cannot be empty!\" << std::endl;\n6\nthrow std::invalid_argument(\"Cannot find median of an empty vector\");\n7\n} // if\n8\nstd::sort(sorted.begin(), sorted.end());\n9\nif (sorted.size() % 2 == 1) {\n// odd\n10\nreturn sorted[sz / 2];\n11\n} // if\n12\nelse {\n// even\n13\nreturn 0.5 (sorted[sz / 2 - 1] + sorted[sz / 2]);*\n14\n} // else\n15\n} // find_median()\nHowever, the time complexity of such an approach is Θ(𝑛log(𝑛)), since the sorting step is a bottleneck that dictates the overall performance. Is\nthere a way to do better? In this section, we will discuss two different median finding algorithms designed to find the median in linear time.\n¸ 15.8.1\n(✽)Quickselect\n𝑘thThe quickselect algorithm can be used to find the smallest element of a collection of elements in average-case time, and it is related toΘ(𝑛)\nthe partitioning step of the quicksort algorithm. Recall that, in the partitioning step:\n1. A pivot element is selected.\n2. The remaining elements in the array are partitioned so that all elements to the left of the pivot are lesser than the pivot, and all elements\nto the right of the pivot are greater than the pivot.\nWe can use the quickselect algorithm to help us find the median: after partitioning, one side of the pivot element must contain the median (if the\n𝑘thmedian is not the pivot value itself). Suppose we know that the median is the element among the sorted values. If there are more than 𝑘\nelements to the left of the pivot after partitioning, then the median must be located to the left of the pivot. Otherwise, if there are fewer than 𝑘\nelements to the left of the pivot after partitioning, then the median must be located to the right of the pivot. We can use this strategy to write an\nalgorithm that finds the median by repeatedly recursing into the side that contains the median value. An example is illustrated below:\n9\n4\n3\n5\n2\n8\n7\n1\n6\nSelect pivot.\n1\n4\n3\n5\n2\n6\n7\n9\n8\nPartition the remaining values around this pivot.\n1\n4\n3\n5\n2\n6\n7\n9\n8\nThe median must be to the left of the pivot 6.\n1\n4\n3\n5\n2\n6\n7\n9\n8\nRecurse into the left side and select a pivot.\n1\n2\n3\n5\n4\n6\n7\n9\n8\nPartition the remaining values around this pivot.\n1\n2\n3\n5\n4\n6\n7\n9\n8\nThe median must be to the right of the pivot 2.\n1\n2\n3\n5\n4\n6\n7\n9\n8\nRecurse into the right side and select a pivot.\n1\n2\n3\n4\n5\n6\n7\n9\n8\nPartition the remaining values around this pivot.\n1\n2\n3\n4\n5\n6\n7\n9\n8\nThe median must be to the right of the pivot 4.\n1\n2\n3\n4\n5\n6\n7\n9\n8\nRecurse into the right side. 5 must be the median.", "word_count": 613, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fe561a80-d3ac-5bb1-acb5-49946363012d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 486, "real_page_number": null, "text": "474\nChapter 15. Binary Search and Additional Algorithms\nIn summary, the quickselect algorithm can be used to find the median as follows:\n1. Select a pivot element from the container of values.\n2. Partition the remaining items into two groups: values less than the pivot, and values greater than the pivot.\n3. One of these groups contains the median. Assuming the median should be at index 𝑘:\n• If 𝑘is less than the pivot index, recurse on the left side of the pivot.\n• If 𝑘is greater than the pivot index, recurse on the right side of the pivot.\n• If 𝑘is equal to the pivot, then the value at the pivot position must be the median.\nA recursive implementation of quickselect is shown in the code below:\n1\nint32_t partition(std::vector<double>& int32_t int32_tvec, left, right) {\n2\nint32_t pivot = --right;\n3\nwhile (true) {\n4\nwhile (vec[left] < vec[pivot]) {\n5\n++left;\n6\n} // while\n7\nwhile (left < right && vec[right - 1] >= vec[pivot]) {\n8\n--right;\n9\n} // while\n10\nif (left >= right) {\n11\nbreak;\n12\n} // if\n13\nstd::swap(vec[left], vec[right - 1]);\n14\n} // while\n15\nstd::swap(vec[left], vec[pivot]);\n16\nreturn left;\n17\n} // partition()\n18\n19\ndouble quickselect(std::vector<double>& int32_t int32_t int32_tvec, left, right, target_idx) {\n20\n// handles case where there is only one element in the range\n21\nif (left + 1 >= right) {\n22\nreturn vec[left];\n23\n} // if\n24\nint32_t pivot_idx = partition(vec, left, right);\n25\n// if target_idx (index of median) is equal to pivot_idx, return\n26\nif (target_idx == pivot_idx) {\n27\nreturn vec[target_idx];\n28\n} // if\n29\n// if target_idx < pivot_idx, recurse on left side\n30\nelse if (target_idx < pivot_idx) {\n31\nreturn quickselect(vec, left, pivot_idx, target_idx);\n32\n} // else if\n33\n// if target_idx > pivot_idx, recurse on right side\n34\nelse {\n35\nreturn quickselect(vec, pivot_idx + 1, right, target_idx);\n36\n} // else\n37\n} // quicksort()\n38\n39\ndouble find_median(std::vector<double>& vec) {\n40\nif (vec.empty()) {\n41\nstd::cerr << \"Vector cannot be empty!\" << std::endl;\n42\nthrow std::invalid_argument(\"Cannot find median of an empty vector\");\n43\n} // if\n44\nif (vec.size() % 2 == 1) {\n// odd\n45\nreturn quickselect(vec, 0, vec.size(), vec.size() / 2);\n46\n} // if\n47\n// even\n48\nreturn 0.5 *\n49\n(quickselect(vec, 0, vec.size(), vec.size() / 2) +\n50\nquickselect(vec, 0, vec.size(), vec.size() / 2 - 1));\n51\n} // find_median()\nThe average-case time complexity of quickselect is Θ(𝑛). To see why, suppose each pivot splits the list into two roughly equally sized partitions.\nSince we only need to recurse on the side with the median, the work we do at each iteration decreases by approximately half. This infinite\ngeometric series yields us an complexity:Θ(𝑛)\n𝑛𝑇(𝑛) 𝑛+=\n𝑛+2\n𝑛+4\n𝑛+8\n2𝑛=Θ(𝑛)+…=16\nThat being said, this is obviously an optimistic assumption. Nonetheless, much like our analysis of quicksort’s average-case time complexity\nin the previous chapter, we can use the formula for the sum of a geometric series to show that the time complexity would stilll be a constant\nmultiple of 𝑛for any other constant factor split.\nΘ(𝑛2)However, much like quicksort, the worst-case time complexity is if you get unlucky and pick the worst-case pivot at every iteration\n(which could end up decreasing the search range by only one instead of half). In practice, however, this scenario rarely happens, and quickselect\nexhibits a linear-time performance for most practical use cases. That being said, there is an approach that can be used to help us find the median\nof a collection of items in time known as the algorithm.Θ(𝑛)worst-case median of medians", "word_count": 632, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a49ec623-380d-5459-bb5a-ce9f0467b1ff", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 487, "real_page_number": null, "text": "15.8 Median Finding Algorithms\n475\n¸ 15.8.2\n(✽)Median of Medians\nΘ(𝑛2),To avoid the worst-case time complexity of you must guarantee that each pivot value partitions the input into roughly equal halves. We\ncould use randomization to reduce the likelihood of the worst-case scenario occuring (in fact, randomization is a very effective strategy in\npractice), but this does not prevent us from experiencing the worst case altogether. In this section, we will discuss the median of medians\nalgorithm, which can be used to select optimal pivots that prevent the worst case from happening at all. The algorithm is as follows:\n1. Divide the elements into smaller groups of size five (there can be fewer than five elements in the last group if the total number of elements\npivot).2is not divisible by five; you may also drop this last group from consideration completely, since this step is only used to find a good\n2. Determine the median of each of these smaller subgroups, either by sorting or brute force (since the groups have at most five elements,\nwe can consider sorting each group as a constant time process).\n3. Store the medians of the subgroups in a separate array.\n4. Use the quickselect algorithm to find the median of this array, recursively using the median of medians approach to choose the pivot.\n5. Use the pivot obtained from the previous step to partition the original container of values. If more than half the elements are to the left of\nthis pivot after the partition, the median must be smaller, so recursively run this process again on the elements to the left. If more than\nhalf the elements are to the right of this pivot after the partition, the median must be larger, so recursively run this process again on the\nelements to the right. If the pivot ends up at the index of the median, return its value (with logic to handle an even number of elements).\nAn illustration of this process is shown below. Here, 13 would be chosen as our pivot for partitioning.\n12 11\n4\n6\n1617 22\n1\n23 19\n9\n3\n5\n10 18 2014\n8\n13\n2\n21\n7\n15\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟\nSelect medians from subgroups of size 5\n1911\n9\n13 15\nQuickselect using median of medians to select pivot\n13\npivot\nThe median of medians approach allows us to select a reasonable pivot for each partition, but how from the worst case can this pivot be? It turns\nout that, if the median of medians strategy is used to select a pivot, we are guaranteed to remove 30% of the values with each partition. Let’s\ntake a look at why this is the case. Consider the example above, with the individual subgroups listed in order of their median values:\n3\n5\n9\n10\n14\n4\n6\n11\n12\n17\n2\n8\n13\n18\n20\n7\n15\n21\n1\n16\n19\n22\n23\nThe values to the northwest of the median of medians cannot be larger, and the values to the southeast of the median of medians cannot be smaller.\n3\n5\n9\n10\n14\n4\n6\n11\n12\n17\n2\n8\n13\n18\n20\n7\n15\n21\n1\n16\n19\n22\n23\nThese values cannot be larger than 13.\n3\n5\n9\n10\n14\n4\n6\n11\n12\n17\n2\n8\n13\n18\n20\n7\n15\n21\n1\n16\n19\n22\n23\nThese values cannot be smaller than 13.\n2The choice of fiveis intentional. Wewant an odd number, which makes medianfinding a lot more straightforward, and wealso want the size to beas small as\npossible so that it is easier to find the median of each of the subgroups. If we divide the elements into groups of three, we do not end up with a worst-case\ntimecomplexityofΘ(𝑛). Ifwedividetheelementsintogroupsofsevenormore,wedoendupwithaworst-casetimecomplexityofΘ(𝑛),butthecoefficient\noverheadislargersinceitismoreexpensivetoprocesssubgroupsthatarelarger.", "word_count": 685, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5031c977-cdf5-58a1-8288-62fdc75b019e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 488, "real_page_number": null, "text": "476\nChapter 15. Binary Search and Additional Algorithms\nHow many values must be in each of these two regions if there are 𝑛values in total? Since we are grouping elements into groups of five, there\nare a total of groups available. For half of these groups, three of its five values must be to the northwest or southeast of the median of𝑛∕5\nmedians. Therefore, the total number of values that are at least as large or small as the median of medians is:\n(\n1\n2𝑛\n)\n×\n(\n3\n5\n)\n3=\n10𝑛\nThis indicates that at least of all the values as the median of medians. Thus, even in the worst case where themust be at least as extreme3∕10\nmedian of medians approach produces a pivot that is as far away from the median as possible, at least 30% of the values are still guaranteed to be\npartition).3even farther away (which allows them to be discarded after each\nWe can use this information to devise a recurrence relation that describes our median finding algorithm’s performance if the worst-case\npivot (30% away from the end) is chosen at every step. Notice that we perform the following steps at each iteration of the algorithm:\n• Identify the median for each of the smaller groups of size five. Since each group has at most five elements, the complexity of finding the\nmedian for a single group is constant. There are a total of groups, so the total work done at this step is Θ(𝑛).𝑛∕5 𝑛∕5×Θ(1)=\n• Store the medians of the subgroups into a separate array of size and recursively apply this algorithm to find the median of this array.𝑛∕5\nThis requires us to recursively solve a subproblem that is 1/5 the size of the original data range.\n• Use the pivot obtained from the previous step to partition the original values. This takes time.Θ(𝑛)\n• After the pivot ends up in its correct position after partitioning, recurse into the side that contains the median. Since we are analyzing the\nworst case, this side includes 70% of all the values (since we proved that at least 30% of the values can be discarded at each partition).\nThus, this step solves a subproblem that is 7/10 the size of the original data range.\nThe algorithm can therefore be expressed using the following recurrence relation:\n𝑇(𝑛) 𝑇=\n(\n𝑛\n5\n)\n+𝑇\n(\n7𝑛\n10\n)\n+𝑛\nThis recurrence relation ends up evaluating to Θ(𝑛). We can see why by using a recursion tree to express the work done at each level of recursion:\n𝑛\n7𝑛\n10\n49𝑛\n100\n⋮\n⋮\n1\n1\n⋮\n1\n1\n⋮\n⋮\n1\n1\n⋮\n1\n1\n7𝑛\n50\n⋮\n⋮\n1\n1\n⋮\n1\n1\n⋮\n⋮\n1\n1\n⋮\n1\n1\n𝑛\n5\n7𝑛\n50\n⋮\n1\n1\n⋮\n1\n1\n𝑛\n25\n⋮\n1\n1\n⋮\n1\n1\nlog10∕7(𝑛)\nlog5(𝑛)\n𝑛On the top level, we do 𝑛work. On the next level, we do\n7𝑛+5\n9𝑛=10\n𝑛work. On the next level, we do10\n7𝑛+25\n7𝑛+50\n49𝑛+50\n81𝑛=100\nwork. In fact,100\nthis pattern can be used to show that the total work done cannot exceed:\n𝑇(𝑛) 𝑛+𝑛=\n(\n9\n10\n)\n+𝑛\n(\n9\n10\n)2\n+𝑛\n(\n9\n10\n)3\n+…+𝑛\n(\n9\n10\n)log10∕7(𝑛)\nThis is because 𝑛\n(\n9\n10\n)𝑘\n𝑘thwork is done at the level of recursion, and the longer recursive branch takes levels to reach the base caselog10∕7(𝑛)\n(notice that this solution is larger than the work actually done, since the left branch terminates after levels). Factoring out the 𝑛, we getlog5(𝑛)\n𝑇(𝑛) 𝑛=\n[(\n9\n10\n)0\n+\n(\n9\n10\n)1\n+\n(\n9\n10\n)2\n+\n(\n9\n10\n)3\n+…+\n(\n9\n10\n)log10∕7(𝑛)]\nUsing the sum of an infinite geometric series, we know that the term multiplied by 𝑛must be a constant that is smaller than 10:\n∞\n∑\n𝑖=0\n(\n9\n10\n)𝑖\n=\n1\n1−9\n10\n=10\nTherefore, must be bounded by Θ(𝑛). Since our analysis assumed that the worst possible pivot is chosen every time, we have successfully𝑇(𝑛)\nproved that the worst-case time complexity of finding the median using the median of medians approach is also Θ(𝑛).\nEven though the median of medians approach is asymptotically optimal in the worst case, selecting a pivot randomly is almost always\nsufficient in practice. This is because computing the median of medians is not a trivial task, and the likelihood of ending up with a worst-case\npivot every time is quite low, even for randomized pivot selections. Thus, for most use cases, the overhead involved in computing the median of\nmedians is typically not a worthwhile tradeoff, even if it is asymptotically more efficient.\n3Notethatgroupswithfewerthanfiveelementsmayaffectthisresult,butonlybyaconstantamount(sincethereareonlyatmostfiveelementspergroup,and\nonlyonegroupmayhavefewerthanfiveelements). Thus,thiscaveatdoesnotactuallychangetheoutcomeofouroverallanalysis.", "word_count": 873, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6954e7fb-38df-528c-a538-455bb291dd16", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 489, "real_page_number": null, "text": "15.8 Median Finding Algorithms\n477\nChapter 15 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. You are given two containers of size 𝑛: a sorted array and a sorted linked list. What are the time complexities of searching for a value in each\nof these two containers, if you use the most efficient algorithm?\nA) Sorted array: Θ(log(𝑛)),\nSorted linked list: Θ(log(𝑛))\nB) Sorted array: Θ(log(𝑛)),\nSorted linked list: Θ(𝑛)\nC) Sorted array: Θ(𝑛),\nSorted linked list: Θ(log(𝑛))\nD) Sorted array: Θ(𝑛),\nSorted linked list: Θ(𝑛)\nE) None of the above\n2. You are given a mystery container of size 𝑛, and you are told that inserting an element into the position directly before any given iterator\ninsert_before(iterator, val)).takes time (i.e., Which of the following could be the mystery container you were given?Θ(1)\nA) Array\nB) Deque\nC) Singly-linked list\nD) Doubly-linked list\nE) More than one of the above\n3. Which one of the following statements is TRUE about the contents of the underlying data vector used to implement a binary heap?\nA) The binary heap’s underlying data vector is an ordered and sorted container\nB) The binary heap’s underlying data vector is an ordered container, but it is not a sorted container\nC) The binary heap’s underlying data vector is a sorted container, but it is not an ordered container\nD) The binary heap’s underlying data vector is neither an ordered nor sorted container\nE) None of the above\n4. If you are given a sorted array of 𝑛integers, what is the worst-case time complexity of finding removing a specified value from thisand\narray, if you use the most efficient algorithm?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n5. You are given a sorted array of size 𝑛that contains all but one integer in the range [0,𝑛]. What is the worst-case time complexity of finding\nthe missing number, if you use the most efficient algorithm? Assume all integers in the array are unique. For example, given the array\n[0,1,2,4,5], the algorithm would return 3.\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n6. Suppose you are given an unsorted array of 𝑛integers (where 𝑛is odd), and you are told that all but one of the values also have their additive\ninverse inside the array. What is the worst-case time complexity of finding the value without an additive inverse inside the array if you use\nthe most efficient algorithm? For example, given the array [4, 8, -3, -2, -8, 5, -5, -4, 3], the algorithm would return -2.\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n7. You are given an array of positive integers that is sorted in ascending order, and you are told that two numbers in this array sumunsorted\nup to a target number 𝑇. What is the worst-case time complexity of finding the two numbers that sum to 𝑇, if you use the most efficient\nalgorithm that does NOT create any additional containers? For example, given the array [5, 9, 13, 2, 7] and 11, the algorithm would𝑇=\nreturn the pair [9, 2], since this is the pair of values that sum to 11. If multiple pairs sum to 𝑇, the algorithm may return any of these pairs.\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)", "word_count": 596, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "acbcbe6b-3f7b-5d57-988e-9dabc4fd6d88", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 490, "real_page_number": null, "text": "478\nChapter 15. Binary Search and Additional Algorithms\n8. Given a sorted (ascending) array of integers of size 𝑛, implement a function that returns a sorted (ascending) array that contains the square of\neach number. For example, given the array [-7, -3, -2, 1, 4, 6], you would return the array [1, 4, 9, 16, 36, 49].\nstd::vector<int32_t> sorted_squares(const std::vector<int32_t>& vec);\nYour solution must perform BETTER than time.Θ(𝑛log(𝑛))\nYou are given a non-empty vector of distinct elements, and you want to return a vector that stores the previous greater element that exists9.\n-1before each index. If no previous greater element exists, is stored.\nvec = [11, 16, 15, 13], [-1, -1, 16, 15].Example 1: Given you would return\nvec = [19, 18, 12, 14, 13], [-1, 19, 18, 18, 14].Example 2: Given you would return\nstd::vector<int32_t> previous_greater_element(const std::vector<int32_t>& vec);\nYour solution should be implemented in time and with auxiliary space, where 𝑛is the length of the vector.𝑂(𝑛) 𝑂(𝑛)\n10. You are given a sorted array consisting of integers where every element appears exactly twice, except for one element which appears exactly\nonce. Implement a function that returns the single element.\nvec = [1, 1, 2, 3, 3, 4, 4], 2.Example: Given you would return\nint32_t find_single_element(const std::vector<int32_t>& vec);\nYour solution should be implemented in time and with auxiliary space, where 𝑛is the length of the vector.𝑂(log(𝑛)) 𝑂(1)\n11. You are given a collection of intervals. Implement a function that merges all of the overlapping intervals.\nvec = [[1, 3], [2, 6], [8, 10], [15, 18]], [[1, 6], [8, 10], [15, 18]].Example1: Given youwouldreturn\nvec = [[4, 5], [1, 4]], [[1, 5]].Example 2: Given you would return\nstruct Interval {\nint32_t start;\nint32_t end;\n};\nmerge_intervals(conststd::vector<Interval> std::vector<Interval>& vec);\nYour solution should be implemented in time and with auxiliary space, where 𝑛is the length of the vector of intervals.𝑂(𝑛log(𝑛)) 𝑂(𝑛)\nvec,12. You are given a vector of integers, and you are told to implement a function that moves all elements with a value of 0 to the end of the\nvector elements.while maintaining the relative order of the non-zero\n[0, 1, 0, 4, 3],Example: Given the initial vector you should rearrange the contents of the vector so that the final ordering of\n[1, 4, 3, 0, 0].elements is\nvoid shift_zeros(std::vector<int32_t>& vec);\nYour solution should be implemented in time and with auxiliary space, where 𝑛is the length of the vector.𝑂(𝑛) 𝑂(1)\n13. You are given a 𝑚×𝑛matrix in the form of a vector of vectors that has the following properties:\n• integers in each row are sorted in ascending order from left to right\n• integers in each column are sorted in ascending order from top to bottom\nImplement a function that searches for a value in this matrix and returns whether the element can be found.\nExample: Given the following matrix:\n[ [ 1,\n4,\n7, 11, 15],\n[ 2,\n5,\n8, 12, 19],\n[ 3,\n6,\n9, 16, 22],\n[10, 13, 14, 17, 24],\n[18, 21, 23, 26, 30]\n]\ntrue. false.and a target value of 5, you would return Given a target value of 20, you would return\nbool is_value_in_matrix(const std::vector<std::vector<int32_t>>& int32_tmatrix, target);\nYour solution should be implemented in time and with auxiliary space.𝑂(𝑚+𝑛) 𝑂(1)\n14. You are given a vector of integers that is sorted in ascending order. However, this vector has been rotated at some pivot unknown to you\n[1, 2, 3, 4, 5] [3, 4, 5, 1, 2],beforehand. For instance, the vector may be given to you in the form where the vector is\nrotated at 3. You may assume that no duplicate exists in this array. Implement a function that finds the minimum element in the vector.\n[3, 4, 5, 1, 2],Example: Given the input vector you would return 1.\nint32_t find_rotated_minimum(const std::vector<int32_t>& vec);\nYour solution should be implemented in time and with auxiliary space, where 𝑛is the length of the vector.𝑂(log(𝑛)) 𝑂(1)", "word_count": 673, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0284dfcc-5709-5b92-8eb9-06676b234155", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 491, "real_page_number": null, "text": "15.8 Median Finding Algorithms\n479\n15. You are given a vector of integers and a target value 𝑘. Implement a function that returns the pair of elements whose sum is closest to 𝑘.\nThe smaller element should go first in the pair that is returned.\n[29, 30, 40, 10, 28, 22] [22, 30].Example: Given the input vector and a target of 54, you would return the pair𝑘=\nstd::pair<int32_t, int32_t> closest_sum_to_k(const std::vector<int32_t>& int32_tvec, k);\nYour solution should be implemented in time and with auxiliary space, where 𝑛is the length of the vector.𝑂(𝑛log(𝑛)) 𝑂(log(𝑛))\nvec target.16. You are given an array of positive integers and a positive integer Implement a function that returns the length of the smallest\ntarget. std::numeric_limits<int32_t>::max()subarraywhosesumisgreaterthanorequalto Ifthereisnosuchsubarray,return\n(INT_MAX). [2, 3, 4] [1, 2, 3, 4, 5]).A subarray is a contiguous section of an array (e.g., is a subarray of\n[4, 2, 9, 1, 2, 8, 5, 3] target 12, 2,Example: Given the input vector and a value of you would return since the smallest\n[8, 5],subarray that sums to a value of at least 12, has a length of 2.\nint32_t min_subarray_length(const std::vector<int32_t>& int32_tvec, target);\nYour solution should be implemented in time and with auxiliary space, where 𝑛is the length of the vector.𝑂(𝑛) 𝑂(1)\nChapter 15 Exercise Solutions\n1. The correct answer is (B). The time complexity of finding an element in a sorted array is because you are able to use binaryΘ(log(𝑛))\nsearch. However, the performance of binary search is only possible if random access is available, which linked lists do notΘ(log(𝑛))\nsupport. Hence, even if a linked list is sorted, the time complexity of finding a value would still require a traversal in some form.Θ(𝑛)\n2. The correct answer is (D). The doubly-linked list is the only container that supports before any given iterator. Arrays and dequesΘ(1)\nmay require elements to be shifted after the insertion point, which could take time. Singly-linked lists do not provide direct access toΘ(𝑛)\nprevious node, so you may need a traversal to find and update the next pointer of the the node directly before the insertion point.Θ(𝑛)\n[1, 3, 2, 6, 5, 4]3. The correct answer is (D). The data vector of a binary heap is neither sorted nor ordered. For example, is a\nvalid min-binary heap, but the contents of the vector are not sorted in ascending order. Additionally, adding an element to a binary heap\nmay change the relative ordering of elements already in the heap, which indicates that the data vector is also not ordered. To illustrate this,\n[8, 2, 4]:consider the array representation of\n8\n4\n2\nNow, let’s add 9 to the binary heap and fix the heap invariant:\n8\n4\n2\n9\n8\n4\n9\n2\n9\n4\n8\n2\n[9, 8, 4, 2].The underlying data vector of the binary heap now stores the values Notice that the relative order of 2 and 4 were\nswitched after the insertion, with 2 now ending up after 4 when it was before 4 earlier.\n4. The correct answer is (C). Finding a given value in a sorted array takes time due to binary search, but removing any elementΘ(log(𝑛))\nmay take up to in an array (since elements may need to be shifted after the removal point). The cost of removal is dominant, so theΘ(𝑛)\noverall time complexity of finding and removing any element ends up being Θ(𝑛).\n5. The correct answer is (B). This is actually a binary search question. Consider the following array where the number 7 is missing.\n0\n1\n2\n3\n4\n5\n6\n8\n9\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNotice that all numbers before the missing number share their values with their index positions (e.g., 0 at index 0, 1 at index 1, etc.), while\nall numbers after the missing number have a value one larger than their index positions (e.g., 8 at index 7, 9 at index 8, etc.). We can use\nthis knowledge to do a binary search. First, we visit the element in the middle and see if its value is equal to its index. If it is, then the\nmissing number has not occurred yet and must be to the right of the middle value. Otherwise, something before the middle value must have\nmessed the indices up, and the missing element must be to the left. Since binary search cuts the search space in half at every iteration, the\nworst-case time complexity of solving this problem is Θ(log(𝑛)).", "word_count": 783, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "458614c4-8d09-5347-a2a1-2f79d3fce64c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 492, "real_page_number": null, "text": "480\nChapter 15. Binary Search and Additional Algorithms\n6. The correct answer is (C). The most efficient solution can be obtained with the following insight: since a number and its additive inverse\nmust sum to zero, we can find the value without an additive inverse by just summing up the entire vector (since all other additive inverse\npairs cancel each other out). Thus, a sum of the values would give us our solution, which takes time.Θ(𝑛)\n7. The correct answer is (D). This is a problem that can be solved efficiently if the vector were sorted beforehand. Without sorting, we would\nΘ(𝑛2)have to compare each element with all the remaining elements, which would take time. However, if the vector were sorted, we could\nkeep track of two pointers, one moving from the left to right and the other moving from the right to left. If the sum of left and right is\ngreater than the target value, we decrement the right pointer. If the sum is less than the target value, we increment the left pointer. This\nallows us to obtain the solution with just a linear pass of the vector. Since sorting acts as the bottleneck of this entire process, the overall\ntime complexity is Θ(𝑛log(𝑛)).\n8. The core challenge of this problem comes from the fact that our solution must be better than time, which prevents us from justΘ(𝑛log(𝑛))\nsquaring each value and sorting the entire array afterward. It turns out that this problem can be solved in linear time using the two-pointer\ntechnique, thanks to the fact that the initial array is sorted. We start with two pointers (or indices), one that points to the leftmost element,\nand one that points to the rightmost element. Then, we use the absolute values of the two values pointed to by left and right to construct the\nsorted order of squares in descending order, since we know the largest square must be one of these two values. Consider the input array in\nthe provided example:\n-7\n-3\n-2\n1\n4\n6\nL\nR\nOutput\nThe value at the left pointer is -7, while the value at the right pointer is 6. -7 has the larger absolute value, so we know that its square of 49\nmust be the largest value in our sorted output. We write this value to the end of our output vector and increment the left pointer.\n-7\n-3\n-2\n1\n4\n6\nL\nR\n49\nOutput\nThe value at the left pointer is -3, while the value at the right pointer is 6. 6 has the larger absolute value, so we know that its square of 36\nmust be the next largest value in our sorted output. We write this value to the end of our output vector and decrement the right pointer.\n-7\n-3\n-2\n1\n4\n6\nL\nR\n36\n49\nOutput\nThe value at the left pointer is -3, while the value at the right pointer is 4. 4 has the larger absolute value, so we know that its square of 16\nmust be the next largest value in our sorted output. We write this value to the end of our output vector and decrement the right pointer.\n-7\n-3\n-2\n1\n4\n6\nL\nR\n16\n36\n49\nOutput\nContinuing this process, we will eventually build our entire solution in sorted order.\n-7\n-3\n-2\n1\n4\n6\nLR\n1\n4\n9\n16\n36\n49\nOutput\nAn implementation of this solution is shown below:\n1\nstd::vector<int32_t> sorted_squares(const std::vector<int32_t>& vec) {\n2\nstd::vector<int32_t> result(vec.size());\n3\nint32_t left = 0, right = vec.size() - 1;\n4\nfor (int32_t i = vec.size() - 1; i >= 0; --i) {\n5\nif (std::fabs(vec[right]) > std::fabs(vec[left])) {\n6\nresult[i] = vec[right] vec[right];*\n7\n--right;\n8\n} // if\n9\nelse {\n10\nresult[i] = vec[left] vec[left];*\n11\n++left;\n12\n} // else\n13\n} // for i\n14\nreturn result;\n15\n} // sorted_squares()", "word_count": 663, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fdeed3e3-ef97-5a3e-a27e-c1ff8c281bf9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 493, "real_page_number": null, "text": "15.8 Median Finding Algorithms\n481\n9. The naïve solution would be to run a doubly-nested loop, where the outer loop visits every element and the inner loop finds the previous\nΘ(𝑛2)element that is greater. This solution would take time. Can we use our knowledge of data structures to improve this solution? It\nturns out that we can use a stack to solve this problem in linear time. Since we want to identify the largest element directly before each\ngiven element, pushing each of the elements into the stack (from left to right) gives us the ability to access the most recent element that\nis larger, provided that we pop out all of the smaller elements every time we push in a new value. For example, consider the example\n[19, 18, 12, 14, 13]. 19,We know that the is no previous greater element for the first value, so the first value of our solution\n-1. 19array is However, we also push the value into an auxiliary stack, which will be used to solve the remaining elements in our input:\n19\n18\n12\n14\n13\n0\n1\n2\n3\n4\nvec\naux\n19\nsolution\n-1\n18. 18. 19,The next value in our input is We look at the top of the auxiliary stack and compare its value with Since the top value, is larger,\n19 18. 19 18must be the previous greater element of We push into our solution array, and onto the auxiliary stack.\n19\n18\n12\n14\n13\n0\n1\n2\n3\n4\nvec\naux\n19\n18\nsolution\n-1\n19\n12. 12. 18,The next value in our input is We look at the top of the auxiliary stack and compare its value with Since the top value, is larger,\n18 12. 18 12must be the previous greater element of We push into our solution array, and onto the auxiliary stack.\n19\n18\n12\n14\n13\n0\n1\n2\n3\n4\nvec\naux\n19\n18\n12\nsolution\n-1\n19\n18\n14. 14. 12,The next value in our input is We look at the top of the auxiliary stack and compare its value with However, the top value, is\n14. 12 18,smaller than the current value of This means that is not the previous greater element of so we pop it out of the stack to reveal the\n18. 18 14, 14 18, 18next most recent value, which is is larger than so the previous greater element of is and we push into our solution\n14 12 12array and onto the auxiliary stack. Notice that we are able to safely pop out because cannot be the previous greater element for\n14 12 14.any future value, given that the next element of is already larger, and any element for which is greater is also less than\n19\n18\n12\n14\n13\n0\n1\n2\n3\n4\nvec\naux\n19\n18\n14\nsolution\n-1\n19\n18\n18\n13. 14, 14The last value in our input is The top element in the auxiliary stack, is larger, so is our final previous greater element. This\n[-1, 19, 18, 18, 14].gives us our final solution of An implementation of this solution is shown below:\n1\nstd::vector<int32_t> previous_greatest_element(const std::vector<int32_t>& vec) {\n2\nstd::stack<int32_t> s;\n3\ns.push(vec[0]);\n4\nstd::vector<int32_t> result = {-1};\n// first value is always -1\n5\nint32_t left = 0, right = nums.size() - 1;\n6\nfor (size_t i = 1; i < vec.size(); ++i) {\n7\nwhile (!s.empty() && s.top() < vec[i]) {\n8\ns.pop();\n9\n} // while\n10\ns.empty() ? result.push_back(-1) : result.push_back(s.top());\n11\ns.push(vec[i]);\n12\n} // for i\n13\nreturn result;\n14\n} // previous_greatest_element()\nWhy does this solution run in time? Notice that each of the 𝑛elements can only be pushed/popped from the stack at most once. SinceΘ(𝑛)\neach push/pop takes time, and this serves as the bottleneck of the entire algorithm, the total cost of our solution is Θ(𝑛).𝑛×Θ(1)Θ(1) =", "word_count": 668, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a7eb9af2-072f-5ee3-83e1-bc55f3ac5474", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 494, "real_page_number": null, "text": "482\nChapter 15. Binary Search and Additional Algorithms\n10. This problem can be solved with binary search. To see why, consider the example:\n1\n1\n2\n3\n3\n4\n4\n0\n1\n2\n3\n4\n5\n6\n2. 2The number missing a second value is All of the values before have their first occurrence located at an even index, while all values\n2after have their first occurrence located at an odd index. Therefore, you can use the index position of the first repeated value to halve\nthe search space: if the first occurrence of a number has an even index, then the single number must be to the right; otherwise, if the first\noccurrence has an odd index, then the single number must be to the left. An implementation of this is shown below:\n1\nint32_t find_single_element(const std::vector<int32_t>& vec) {\n2\nsize_t left = 0, right = vec.size() - 1;\n3\nwhile (left < right) {\n4\nsize_t mid = (left + right) / 2;\n5\nif ((mid % 2 == 0 && vec[mid] == vec[mid + 1]) ||\n6\n(mid % 2 == 1 && vec[mid] == vec[mid - 1])) {\n7\nleft = mid + 1;\n8\n} // if\n9\nelse {\n10\nright = mid;\n11\n} // else\n12\n} // while\n13\nreturn vec[left];\n14\n} // find_single_element()\nTo merge the intervals together, it would be helpful for the initial input to be sorted by the start time. Then, you would be able to iterate11.\nover the sorted input and use the end time of each value to determine if it should be merged with the most recent interval inserted into the\nsolution. If the end time of the input interval being considered is larger than or equal to the end time of the most recent interval in the\nsolution, then the solution interval may be extended. Otherwise, there is no overlap, and the new interval can be pushed into the solution.\nAn implementation of this solution is shown below:\n1\nstruct Interval {\n2\nint32_t start;\n3\nint32_t end;\n4\n};\n5\n6\nmerge_intervals(conststd::vector<Interval> std::vector<Interval>& vec) {\n7\nstd::vector<Interval> result;\n8\nif (vec.empty()) {\n9\nreturn result;\n10\n} // if\n11\nstd::vector<Interval> sorted_input{vec};\n12\n[](const conststd::sort(sorted_input.begin(), sorted_input.end(), Interval& a, Interval& b) {\n13\nreturn a.start < b.start;\n// sorts input by start time, can also use comparator instead\n14\n});\n15\nresult.push_back(sorted_input.front());\n16\nfor (size_t i = 1; i < sorted_input.size(); ++i) {\n17\nif (result.back().end < sorted_input[i].start) {\n18\nresult.push_back(sorted_input[i]);\n19\n} // if\n20\nelse {\n21\nresult.back().end == std::max(result.back().end, sorted_input[i].end);\n22\n} // else\n23\n} // for i\n24\nreturn result;\n25\n} // merge_intervals()\n12. This problem can be solved with a linear-time loop over the input, where we move non-zero elements to the beginning of the array. While\nshifted_index,looping over the input, we will keep track of another index, which represents the next available position that a non-zero\nelement can be moved to — every time we move a value, we increment this shifted index. Then, after we have moved all the non-zero\n0 shifted_indexelements over, we write a value to all positions from to the end of the array. Note that we are able to overwrite\nelements while iterating over the input because once we move past an element during our iteration, we no longer need to reference it again.\nAn implementation of this solution is shown below:\n1\nvoid shift_zeros(std::vector<int32_t>& vec) {\n2\nsize_t shifted_index = 0;\n3\nfor (size_t i = 0; i < vec.size(); ++i) {\n4\nif (vec[i] != 0) {\n5\nvec[shifted_index++] = vec[i];\n6\n} // if\n7\n} // for\n8\nwhile (shifted_index < vec.size()) {\n9\nvec[shifted_index++] = 0;\n10\n} // while\n11\n} // shift_zeros()", "word_count": 637, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ac049ee1-0e30-564a-b412-f37b04ce45d0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 495, "real_page_number": null, "text": "15.8 Median Finding Algorithms\n483\n13. If you try to look through all the values in the matrix to search for the target value, then your worst-case time complexity would be Θ(𝑚𝑛)\nsince there are 𝑚𝑛values in the matrix. Instead, we can use the sorted nature of the matrix to reduce the number of elements we have\nto check to at most 𝑚+𝑛. This can be done by starting at the bottom left or top right corner of the matrix and comparing its value with\nthe target value to determine which direction to search. For example, if we start from the top right, and the target value is smaller, we\ncontinue the search by looking at the value to the left (since this value is smaller); if the target value is larger, we look at the value below\n(since this value is larger) — if we start from the bottom left instead, we would look up and to the right, respectively. If our search cannot\ncontinue because we have reached the edge of the matrix, then the value must not be found. For example, when searching for 10 in this\nmatrix starting from the top right corner, our search path would be →10.→11→7→8→9→14→1315\n[ [ 1,\n4,\n7, 11, 15],\n[ 2,\n5,\n8, 12, 19],\n[ 3,\n6,\n9, 16, 22],\n[10, 13, 14, 17, 24],\n[18, 21, 23, 26, 30]\n]\nNote that we cannot start our search from the top left or bottom right corners, since these are the smallest and largest values in the matrix,\nand thus comparing this with the target value can help use conclude whether the target exists in the matrix, but not where to continue the\nsearch if it does. An implementation of this solution is shown below:\n1\nbool matrix_search(const std::vector<std::vector<int32_t>>& int32_tmatrix, target) {\n2\nif (matrix.empty()) {\n3\nreturn false;\n4\n} // if\n5\nint32_t curr_row = 0, curr_col = matrix[0].size() - 1;\n6\nwhile (curr_row < matrix.size() && curr_col >= 0) {\n7\nif (matrix[curr_row][curr_col] == target) {\n8\nreturn true;\n9\n} // if\n10\nelse if (matrix[curr_row][curr_col] > target) {\n11\n--curr_col;\n12\n} // else if\n13\nelse {\n14\n++curr_row;\n15\n} // else\n16\n} // while\n17\nreturn false;\n18\n} // matrix_search()\nThis is another problem that can be solved with binary search, since there is information that allows us to get rid of half the search space14.\nwith every value we check in the array. Consider the input provided in the example:\n3\n4\n5\n1\n2\n0\n1\n2\n3\n4\nIf the value we are checking is larger than the value at the last position of our search space, then the point of rotation must be to the right;\notherwise, if a value is smaller than the value at the last position of our search space, then the point of rotation must be to the left. In the\nexample, the value 4 is smaller than the last value in the array, 2, so the point of rotation must be to the right of the value 4. A binary search\nsolution is provided below:\n1\nint32_t find_rotated_minimum(const std::vector<int32_t>& vec) {\n2\nint32_t left = 0, right = vec.size() - 1;\n3\nwhile (left < right) {\n4\nint32_t mid = left + (right - left) / 2;\n5\nif (vec[mid] > vec[right]) {\n6\nstart = mid + 1;\n7\n} // if\n8\nelse {\n9\nright = mid;\n10\n} // else\n11\n} // while\n12\nreturn vec[left];\n13\n} // find_rotated_minimum()", "word_count": 605, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ad92e61f-3f18-5dd8-8f29-3027f2c3baae", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 496, "real_page_number": null, "text": "484\nChapter 15. Binary Search and Additional Algorithms\n15. This is similar to the solution for question 7 — we can sort the vector and use the two pointer approach to determine which pair of elements\ntargetsums closest to (moving the pointers to adjust whether the target is larger or smaller than the current sum). The main difference is\nthat you are no longer guaranteed that there are two values that sum to the target value; this can be addressed by keeping tracking of the best\nsolution encountered so far while running the algorithm. An implementation of this solution is shown below:\n1\nstd::pair<int32_t, int32_t> closest_sum_to_k(const std::vector<int32_t>& int32_tvec, k) {\n2\nstd::vector<int32_t> sorted_input{vec};\n3\nstd::sort(sorted_input.begin(), sorted_input.end());\n4\n5\nstd::pair<int32_t, int32_t> idx;\n6\nint32_t std::numeric_limits<int32_t>::max();left = 0, right = vec.size() - 1, best =\n7\nwhile (left < right) {\n8\nint32_t curr = std::fabs(sorted_input[left] + sorted_input[right] - k);\n9\nif (curr < best) {\n10\nidx.first = left;\n11\nidx.second = right;\n12\nbest = curr;\n13\n} // if\n14\nif (sorted_input[left] + sorted_input[right] > k) {\n15\n--right;\n16\n} // if\n17\nelse {\n18\n++left;\n19\n} // else\n20\n} // while\n21\nreturn std::make_pair(sorted_input[idx.first], sorted_input[idx.second]);\n22\n} // closest_sum_to_k()\nThis problem can be efficiently solved using the sliding window approach, and most similar matches the solution to the shortest substring16.\nproblem demonstrated in example 15.8. Instead of determining whether the window contains every character in a target string (as with this\nexample), we will want to determine whether the window sums to a value larger than the target and keep track of the shortest window\nencountered that satisfies this condition. We will begin our sliding window from the beginning of the array and extend it rightward as long\nas the sum is less than the target value. Once the sum is greater than or equal to the target value, we compare it with the best solution we\nhave encountered so far to determine if it could be a viable solution, and then we contract the sliding window by removing values on the\nleft. This is done until the window sum goes below the target value again. After repeating this until the window reaches the end of the input\narray, the best solution encountered must also be the solution to the entire problem. An implementation of this is shown below:\n1\nint32_t min_subarray_length(const std::vector<int32_t>& int32_tvec, target) {\n2\nint32_t std::numeric_limits<int32_t>::max();left = 0, window_sum = 0, min_len =\n3\nfor (int32_t right = 0; right < vec.size(); ++right) {\n4\nwindow_sum += vec[right];\n5\nwhile (window_sum >= target) {\n6\nmin_len = std::min(min_len, right - left + 1);\n7\nwindow_sum -= vec[left];\n8\n++left;\n9\n} // while\n10\n} // while\n11\nreturn min_len;\n12\n} // min_subarray_length()", "word_count": 469, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "62b34551-55c2-5c28-b702-068f692e0863", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 497, "real_page_number": null, "text": "Chapter 16\nStrings and Sequences\n16.1\nC Strings\nIn this chapter, we will look at the string data type, which can be used to represent text data using sequences of characters. Strings are one of\nthe most important components of programming, and much of the technology that we use today relies on strings and string operations. In fact,\n(\"Hello, World!\")for many developers, a string object was the thing that welcomed them to the programming world. Since strings are so\nubiquitous and useful, the course would not be complete without a discussion on how strings and string algorithms work.\n(std::string).In C++, there exist two primary abstractions for string objects, C strings and C++ strings For the majority of this\nclass, we will be working with C++ strings. However, this section will provide a brief overview of C strings, which serve as a foundation for the\nmore complex string objects that we will cover later.\n('\\0').A C string is an array of characters that is terminated by a null-character, or sentinel character The sentinel is important because it\nallows the program to determine where a C string ends in memory, and therefore it must be included at the end of every C string.\nA C string can be declared in many ways. The following methods can all be used to declare a C string that stores the word \"EECS\". Note\nthat, in the first two methods, the language is smart enough to add a null terminator to the end of the string for you. However, if you explicitly\nspecify the size of the character array like in methods three and four, you will need to explicitly assign the null character to the last index.\nconst char* cstr = \"EECS\";\n// read-only\nchar cstr[] = \"EECS\";\nchar cstr[5] = {'E', 'E', 'C', 'S', '\\0'}\nchar* new char[5];cstr =\ncstr[0] = 'E'; cstr[1] = 'E'; cstr[2] = 'C'; cstr[3] = 'S'; cstr[4] = '\\0';", "word_count": 324, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "411e8950-abad-57ec-84cd-d6b67b0b4288", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 498, "real_page_number": null, "text": "486\nChapter 16. Strings and Sequences\ncstrIn memory, is a pointer to a character array that stores the characters of the string \"EECS\":\ncstr\n'E'\n'E'\n'C'\n'\\0''S'\ncharBehind the scenes, each is actually stored as a numerical value. This is because C strings use ASCII, a standard that dictates what\nnumerical value each character is assigned to. These values are shown in the table below:\nASCII characters 0-31 have been omitted from the table because they are unprintable characters. However, some notable characters in this range\n'\\0' '\\t' '\\n'are the null character (ASCII value 0), the tab character (ASCII value 9), the newline character (ASCII value 10), and the\n'\\r'carriage return character (ASCII value 13).\nUsing the table, we can convert the characters of our C string to their respective ASCII values. This is what the array above looks like under\nthe hood:\ncstr\n'E'\n'E'\n'C'\n'\\0''S'\ncstr\n69\n69\n67\n83\n0\nBecause characters are actually numbers behind the scenes, we can compare characters using comparison operators and iterate through letters in\na loop, as follows:\n1\nfor (char current = 'a'; current <= 'z'; ++current) {\n2\nstd::cout << current;\n// prints \"abcdefghijklmnopqrstuvwxyz\"\n3\n} // for current\nBecause C strings are arrays of characters, they behave just like arrays. Thus, if you try to access the value of a C string variable, you will\noperator==;end up getting a pointer to the first character. As a result, you cannot compare two C strings using built-in operators like\n#includespecial operations are required to work with C strings. Some of these operations are detailed below, and they can be used if you\n<cstring> in your code:\nsize_t strlen(const char* str);\nstr.Returns the length of the C string\n1\nchar str[] = \"apple\";\n2\nstd::cout << strlen(str) << '\\n';\n// prints 5\nchar* strcpy(char* const char*dest, source);\nsource dest,Copies the C string pointed to by into the array pointed to by including the sentinel character. The size of the array pointed\ndest source, sourceto by needs to be long enough to hold the contents of and it should not overlap with in memory. After the copy,\ndest is returned.\n1\nchar s1[] = \"hi\";\n2\nchar s2[3];\n// allocate enough space\n3\nstrcpy(s2, s1);\n// s2 also stores {'h','i','\\0'} since s1 was copied over", "word_count": 397, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "081c3cfa-f548-571a-a236-e6f901166d10", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 499, "real_page_number": null, "text": "16.1 C Strings\n487\nchar* strncpy(char* const char* size_tdest, source, num);\nnum source dest. source numCopies the first characters of to If the entirety of is copied before characters have been copied over (i.e.,\nsource num dest num sourceif has fewer than characters), is padded with zeros until a total of characters have been written. If is\nnum num destlonger than characters, only the first characters are copied over and the sentinel character! After the copy, is returned.not\n1\nchar s1[] = \"eecs281\";\n2\nchar s2[5];\n3\nstrncpy(s2, s1, 4);\n// s2 now stores \"eecs\" but with no sentinel\n4\ns2[4] = '\\0';\n// sentinel must be added for s2 to be a valid C string\nchar* strcat(char* const char*dest, source);\nsource dest destConcatenates a copy of the C string to the end of the C string. The sentinel of is overwritten by the first character of\nsource, destbut a new sentinel is added to the end of the resultant C string after concatenation. After the concatenation, is returned.\n1\nchar s[8];\n2\nstrcpy(s, \"eecs\");\n// copies {'e','e','c','s','\\0'} to s\n3\nstrcat(s, \"281\");\n// appends {'2','8','1','\\0'}, s now stores {'e','e','c','s','2','8','1','\\0'}\nchar* strncat(char* const char* size_tdest, source, num);\nnum source dest, source numConcatenates the first characters of to plus a sentinel. If has fewer than characters, only the characters\ndestup to the sentinel is copied. After the concatenation, is returned.\n1\nchar s1[] = \"281282283\";\n2\nchar s2[] = \"eecs\";\n3\nstrncat(s2, s1, 3);\n// s2 now stores {'e','e','c','s','2','8','1','\\0'}\nint strcmp(const char* const char*str1, str2);\nstr1 str2. str1 str2Compares the two C strings and If the two C strings are equal, the function returns 0. If is smaller than (using\nstr1 str2,ASCII values to determine ordering), a negative number is returned. If is larger than a positive number is returned. To\ndetermine which C string comes first, the function compares the characters of the strings one-by-one until a mismatch (or sentinel) is\nencountered. The ASCII values of the mismatched characters are then compared to determine which C string comes first.\n1\nchar s1[] = \"apple\";\n2\nchar s2[] = \"apple\";\n3\nchar s3[] = \"banana\";\n4\nif (strcmp(s1, s2) == 0) {\n5\nstd::cout << \"This prints if s1 == s2\" << '\\n';\n6\n} // if\n7\nif (strcmp(s1, s3) < 0) {\n8\nstd::cout << \"This prints if s1 < s3\" << '\\n';\n9\n} // if\nint strncmp(const char* const char* size_tstr1, str2, num);\nnumCompares up to characters of the two C strings to determine equality. The return value rules for this function are the same as the rules\nstrcmp()fo (i.e., 0 if equal, etc.).\n1\nchar s1[] = \"apple\";\n2\nchar s2[] = \"application\";\n3\nif (strncmp(s1, s2, 4) == 0) {\n4\nstd::cout << \"This prints because first four chars are equal\" << '\\n';\n5\n} // if\n6\nif (strncmp(s1, s2, 5) < 0) {\n7\nstd::cout << \"This prints because apple < appli\" << '\\n';\n8\n} // if\n<cstring>This is not a comprehensive list of all the functions that are provided by the library. Additional C string operations can be found\nby looking through documentation. However, we will not be going over these additional operations because C strings will not be a focus of this\nclass. Instead, you will primarily be working on C++ strings, which will are covered in the next section.\nstd::cout. std::coutLastly, it should be noted that C strings exhibit special behavior when used with C++ output streams like If is\nchar*, ('\\0')passed in a it will treat the pointer as a C string and print out every character up until the first null character it encounters. If a\nchar* std::cout,that is not null-terminated is passed into you would get undefined behavior.\n1\nchar str1[] = {'e', 'e', 'c', 's', '\\0'};\n2\nchar str2[] = {'e', 'e', 'c', 's'};\n3\n4\nstd::cout << str1 << '\\n';\n// prints \"eecs\"\n5\nstd::cout << str2 << '\\n';\n// undefined behavior", "word_count": 679, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7caa5915-5788-5172-84d1-65c904b7d21e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 500, "real_page_number": null, "text": "488\nChapter 16. Strings and Sequences\n16.2\nC++ Strings\n¸ 16.2.1\n(✽)String Implementations and Short String Optimization\nstd::string,WhenyouworkwithCstrings,youaredirectlyworkingwiththecharactervaluesinmemory. Ontheotherhand,aC++string,or\nstd::stringis a class-type object that encapsulates a string’s underlying character array and manages its memory for you. The is defined\n<string> std::string std::vector<>in the library. You can think of a as similar in concept to a — like a vector, the string class\nstores an internal array with its underlying data and provides member functions that can be used to operate on that data.\nstd::stringTo gain insight into how a works under the hood, let’s look at a common implementation of the C++ string class in\nmemory.1 std::stringIn g++ version 5 and above, a uses 32 bytes of memory on the stack to store the following information: (1) a\npointer to the beginning of the underlying character array, (2) the length of the string, and (3) the capacity of the underlying character array (not\nincluding the sentinel). An illustration is shown below:\nstd::string str = \"EECS 281 is very fun\";\n0x3010\n20\n20\nø\ndata\nlength\ncapacity\nunused\nstack\nheap\nE E C S\n82 1\nsi\nyv e r\nu n \\0f\nThe pointer, size, and capacity values each take up 8 bytes of memory. This adds up to a total of 24 bytes. So, why are there 8 bytes of unused\ncapacitystack space floating around in each string? It turns out that the member variable actually does double duty. If we were to look at\nwhat a C++ string stores internally, we would essentially see something like this (this is a simplified version of the actual implementation):\n1\nclass string {\n2\nchar* data;\n3\nsize_t length;\n4\nunion {\n5\nsize_t capacity;\n6\nchar local_buffer[16];\n7\n};\n8\n};\nunionYou do not need to know this for this class, but a is a special type of object that can only take on one of its data members at any time. In\nstringthis case, the object holds three member variables: a data pointer, the string’s length, and a special variable that can store one of either:\nsize_t• a 8 byte value that stores the capacity of the heap-allocated array OR\n• a 16 byte character array\nstd::stringThis additional memory allows the to take advantage of something known as short string optimization (SSO). As mentioned\nin chapter 6, the program stack size is rather limited compared to the heap, and arrays on the stack need to have fixed sizes at compile time. Thus,\nC++ strings typically initialize their data on the heap (and keep stack usage capped at 32 bytes) to avoid potential overflow issues. However,\nthere is a trade-off: dynamically-allocated arrays tend to have slower performance compared to arrays on the stack.\nFor larger strings, there is no way around this. The majority of strings, however, tend to be short, and it would be inefficient to dynamically\nstd::stringallocate them every time. This is where the additional stack memory comes in handy. If the length of a is fewer than 16\ncharacters, the memory that would have been used to store the capacity value (along with the extra 8 bytes of memory at the end) is used to store\nthe actual string instead.\nstd::string str = \"EECS 281 is fun\";\n0x7f30\n15\nE E C S\n82 1\nsi\nu n \\0f\ndata\nlength\nin-situ capacity\nstd::stringBy repurposing the last 16 bytes of stack space to hold short strings, the is able to take advantage of performance benefits from\narrays that live on the stack. Only when the length of the string exceeds 15 characters does the string begin allocating memory on the heap.\nNote that a string of length 16 does not fit on the stack, since an additional byte is needed to store the sentinel character (and thus requires 17\nbytes in total).\n1Likewithavector,astring’sstandardlibraryimplementationmaybeplatform-specific. Inthischapter,wewillbelookingattheGCCstringimplementationin\nlibstdc++,whichiscommonlyusedbymanyplatforms.", "word_count": 710, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e537ce80-975a-501e-b458-7d9552808a34", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 501, "real_page_number": null, "text": "16.2 C++ Strings\n489\n¸ 16.2.2\nString Library Operations\nstd::string operator= <,>, ==.Objectsofthe classsupportassignmentusing andcomparisonsusingoperators and Stringsalsosupport\n.begin() .end()random access iterators, which can be initialized using variations of and (similar to other random access containers).\n<string> std::stringThe library provides many operations that can be used to operate on a object. An outline of several string\nmember functions is provided below. Several of these operations have different variations depending on the parameters that are passed in, but\nthis list will only cover the most common ones (you are encouraged to look at other documentation for these additional behaviors).\nsize_t std::string::size();\nsize_t std::string::length();\nReturns the number of characters in the string, not including the sentinel. Both functions do the same thing.\n1\nstd::string str = \"EECS281\";\n2\nstd::cout << str.size() << '\\n';\n// prints 7\n3\nstd::cout << str.length() << '\\n';\n// prints 7\nvoid std::string::clear();\nClears the contents of a string.\nbool std::string::empty();\nReturns whether a string is empty.\n1\nstd::string str = \"EECS281\";\n2\nstr.clear();\n3\nstd::cout << std::boolalpha << str.empty() << '\\n';\n// prints \"true\"\nchar& std::string::operator[](size_t idx);\nidxReturns a reference to the character at index of the string.\n1\nstd::string str = \"EECS281\";\n2\nstd::cout << str[2] << '\\n';\n// prints \"C\"\nstd::string::operator+=(conststd::string& std::string& str);\nstd::string::operator+=(const char*std::string& cstr);\nstd::string::operator+=(charstd::string& c);\nAppends the given value to the back of the string and returns the modified string.\n1\nstd::string str = \"hello \";\n2\nstr += \"world\";\n3\nstd::cout << str << '\\n';\n// prints \"hello world\"\nvoid std::string::push_back(char c);\ncAppends the character to the back of the string and increases length by one.\nvoid std::string::pop_back();\nDeletes the character at back of string and reduces length by one. This operation causes undefined behavior if used on an empty string.\n1\nstd::string str = \"EECS28\";\n2\nstr.push_back('1');\n// str now stores \"EECS281\"\n3\nstr.pop_back();\n// str now stores \"EECS28\"\nchar& std::string::front();\nReturns a reference to the first character of the string. Causes undefined behavior if used on an empty string.\nchar& std::string::back();\nReturns a reference to the last character of the string. Causes undefined behavior if used on an empty string.\n1\nstd::string str = \"EECS281\";\n2\nstd::cout << str.front() << '\\n';\n// prints the first character, or 'E\"\n3\nstd::cout << str.back() << '\\n';\n// prints the last character, or '1'\nstd::string::insert(size_t conststd::string& pos, std::string& str);\nstr posInserts a copy of directly before the character at position and returns the modified string.\n1\nstd::string str = \"281\";\n2\nstr.insert(0, \"EECS\");\n3\nstd::cout << str << '\\n';\n// prints \"EECS281\"\n4\n5\nstd::string str1 = \"pot\";\n6\nstd::string str2 = \"arr\";\n7\nstd::string str3 = str1.insert(1, str2);\n8\nstd::cout << str3 << '\\n';\n// prints \"parrot\"", "word_count": 466, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6cd1ca55-b3a4-5c09-a5e6-3685bfeec15f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 502, "real_page_number": null, "text": "490\nChapter 16. Strings and Sequences\nstd::string::erase(size_t size_tstd::string& pos = 0, len = std::string::npos);\npos lenErases the portion of the string that starts at index and spans characters, or until the end of the string, whichever comes first. If not\npos len npos.specified, defaults to 0 and defaults to Returns a reference to the modified string.\niterator std::string::erase(iterator pos);\npos. endErases the character pointed to by Returns an iterator pointing to the character directly following the character erased, or the\niterator if no such character exists.\niterator std::string::erase(iterator first, iterator last);\n[first, last). lastErases all characters in the range Returns an iterator referencing the character pointed to before the erase, or the\nend iterator if no such character exists.\n1\nstd::string str1 = \"carrot\";\n2\nstr1.erase(2, 3);\n3\nstd::cout << str1 << '\\n';\n// prints \"cat\"\n4\n5\nstd::string str2 = \"carrot\";\n6\nstr2.erase(3);\n// erases everything starting from index 3\n7\nstd::cout << str2 << '\\n';\n// prints \"car\"\n8\n9\nstd::string str3 = \"carrot\";\n10\nstr3.erase(str3.begin(), str3.begin() + 3);\n11\nstd::cout << str3 << '\\n';\n// prints \"rot\"\nstd::string::replace(size_t size_t conststd::string& pos, len, std::string& str);\npos len strReplaces the portion of the string that begins at index and spans characters with the contents of and returns a reference to the\nmodified string.\nconststd::string& std::string::replace(iterator i1, iterator i2, std::string& str);\n[i1, i2) strReplaces the portion of the string that falls in iterator range with the contents of and returns the modified string.\n1\nstd::string str = \"eecs 370 is fun\";\n2\nstr.replace(5, 3, \"281\");\n// replaces 3 chars starting at index 5\n3\nstd::cout << str << '\\n';\n// prints \"eecs 281 is fun\"\nconst char* std::string::c_str();\nReturns a pointer to the underlying C string that stores the current data of the string object.\n1\nstd::string str = \"eecs 281\";\n2\nconst char* cstr = str.c_str();\n3\nstd::cout << cstr << '\\n';\n// prints \"eecs 281\"\nsize_t std::string::find(const size_tstd::string& str, pos = 0);\npos str nposSearches the string starting at index for the first occurrence of and returns the index position of the first match (or if no\nposmatches were found). defaults to 0 if not specified.\n1\nstd::string str = \"pineapple\";\n2\nsize_t idx1 = str.find(\"apple\", 0);\n// idx1 == 4\n3\nsize_t idx2 = str.find(\"orange\");\n// idx2 == npos\n4\nsize_t idx3 = str.find(\"apple\", 6);\n// idx3 == npos\nsize_t std::string::find_first_of(const size_tstd::string& str, pos = 0);\npos strSearches the string starting at index for the first character that matches any of the characters in and returns the position of the first\nnpos posmatch (or if no matches were found). defaults to 0 if not specified.\n1\nstd::string str = \"pineapple\";\n2\nsize_t idx1 = str.find_first_of(\"apple\");\n// idx1 == 0 ('p')\n3\nsize_t idx2 = str.find_first_of(\"orange\");\n// idx2 == 2 ('n')\n4\nsize_t idx3 = str.find_first_of(\"pie\", 4);\n// idx3 == 5 ('p')\nsize_t std::string::find_last_of(const size_tstd::string& str, pos = std::string::npos);\nstr nposSearches the string for the last character that matches any of the characters in and returns position of first match (or if no matches\npos poswere found). Only characters at or before index are considered. defaults to 0 if not specified.\n1\nstd::string str = \"pineapple\";\n2\nsize_t idx1 = str.find_last_of(\"apple\");\n// idx1 == 8 ('e')\n3\nsize_t idx2 = str.find_last_of(\"orang\");\n// idx2 == 4 ('a')\n4\nsize_t idx3 = str.find_last_of(\"pie\", 4);\n// idx3 == 5 ('p')\nsize_t std::string::find_first_not_of(const size_tstd::string& str, pos = 0);\npos strSearches the string starting at index for the first character that does not match any of the characters in and returns its position (or\nnpos posif no matches were found). defaults to 0 if not specified.\nsize_t std::string::find_last_not_of(const size_tstd::string& str, pos = std::string::npos);\nstr nposSearches the string for the last character that does not match any of the characters in and returns its position (or if no matches\npos pos nposwere found). Only characters at or before index are considered. defaults to if not specified.", "word_count": 676, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b4a13817-3b96-5e0e-93da-767c1472c6dc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 503, "real_page_number": null, "text": "16.2 C++ Strings\n491\nstd::string::substr(size_t size_tstd::string pos = 0, len = std::string::npos);\npos lenReturns a newly constructed string that is a substring of the original string. The substring begins at index and spans characters (or\nuntil the end of the string, whichever comes first).\n1\nstd::string str = \"pineapple\";\n2\nstd::string substring = str.substr(4, 3);\n// start at index 4 with length 3\n3\nstd::cout << substring << '\\n';\n// prints \"app\"\noperator+(const conststd::string std::string& lhs, std::string& rhs);\nCan be used to concatenate two strings (this is not a member function of the string class, but it is useful to know).\n1\nstd::string str1 = \"pine\";\n2\nstd::string str2 = \"apple\";\n3\nstd::string str3 = str1 + str2;\n4\nstd::cout << str3 << '\\n';\n// prints \"pineapple\"\nThis is not a comprehensive list of all available string operations, but these are the most common. You may have noticed a value known as\nstd::string::npos nposin the function definitions above. The value is a special constant that is used when working with strings. This\nsize_t, len posvariable stores the value of the largest possible and its purpose depends on how it is used. When used as the value for the or\nnposparameter of a string member function, it tells the function to complete an operation until the end of the string. If is returned by a variant\nfind()of the function, it indicates that no match was found.\n¸ 16.2.3\n(✽)std::string_view\nOne major pitfall of strings is that they can be quite expensive to copy, especially if dynamic memory allocation is involved (recall that heap\nallocation is not a cheap process). However, there are cases where an algorithm may only need a copy of the string it is working with.read-only\nIn these situations, there is no need to give a separate copy of a string to every method that needs to use it. Instead, since usage of the string is\nintended to be read-only, we can simply pass in a \"view\" of the exact same string object to all the methods that need it.\nstd::string_view. std::string_view,ThisbehaviorwassimplifiedinC++17withtheintroductionof Whenyougivesomethinga\nyou are giving it else, instead of making a separate copy of that string. Not only are stringa view of an existing string that is owned by something\nviews more performant, they are more lightweight in size as well. If you look at what a string view stores internally, you would see something\nlike this (again, this is an oversimplification, but it provides an insight into what this type provides):\n1\nclass string_view {\n2\nsize_t len;\n// length of the string view\n3\nconst char* str;\n// pointer to the first character\n4\n};\nIf used correctly, string views can greatly improve the performance of many string operations, from substring searches to string comparisons and\nstd::string_viewconcatenation. Let’s look at an example where usage of a may be prudent. Suppose you have a giant file containing\nautograder submission data that you want to process:\n1\nvoid process_data(const conststd::string& timestamp, std::string& uniqname,\n2\nconst std::string& assignment) {\n3\n// do stuff...\n4\n} // process_data()\n5\n6\nint main() {\n7\nstd::string line;\n8\nwhile (std::getline(std::cin, line)) {\n9\nsize_t last = 0, next = 0;\n10\n11\n// get timestamp\n12\nnext = line.find(\";\", last);\n13\nstd::string timestamp_str = line.substr(last, next - last);\n14\nstd::string timestamp = timestamp_str.substr(timestamp_str.find(\":\") + 1);\n15\nlast = next + 1;\n16\n17\n// get uniqname\n18\nnext = line.find(\";\", last);\n19\nstd::string uniqname_str = line.substr(last, next - last);\n20\nstd::string uniqname = uniqname_str.substr(uniqname_str.find(\":\") + 1);\n21\nlast = next + 1;\n22", "word_count": 618, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5b3b6481-7966-5944-9264-456446752308", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 504, "real_page_number": null, "text": "492\nChapter 16. Strings and Sequences\n23\n// get assignment\n24\nnext = line.find(\";\", last);\n25\nstd::string assignment_str = line.substr(last, next - last);\n26\nstd::string assignment = assignment_str.substr(assignment_str.find(\":\") + 1);\n27\nlast = next + 1;\n28\n29\nprocess_data(timestamp, uniqname, assignment);\n30\n} // while\n31\n} // main()\nNotice that this code allocates more strings than necessary, which results in a significant performance overhead. First, we read in the contents of\nstd::getline()a line using on line 8, which invokes a heap allocation:\nstd::getline(std::cin, line);\n0x3110\n63\n120\nø\ndata\nlength\ncapacity\nunused\nstack\nheap\n. . . ;p q: :e s a a e am m n n mt UT 50 0 9 0 3i 2 2 1 2 1 1 1 4 i\n…\ntimestamp_str:Then, on line 13, we perform another heap allocation to initialize\nstd::string timestamp_str = line.substr(last, next - last);\n0x3310\n27\n27\nø\ndata\nlength\ncapacity\nunused\nstack\nheap\n. . .p :e s am mtT \\050 0 9 0 3i 2 2 1 2 1 1 1 4\ntimestamp process_data()On line 14, we perform yet another heap allocation to initialize the string to be passed into the method:\nstd::string timestamp = timestamp_str.substr(timestamp_str.find(\":\") + 1);\n0x3410\n17\n17\nø\ndata\nlength\ncapacity\nunused\nstack\nheap\n. . . \\050 0 9 0 32 2 1 2 1 1 1 4\nprocess_data()This same inefficiency also applies to the extraction of uniqname and assignment ID from each line. However, since the\nmethod does not need to change the contents of any of its string parameters (hence the const reference), we do not need to allocate entirely new\nline.strings to pass into the function. Rather, it is sufficient to pass in a view of the original string that we initially read into the variable", "word_count": 320, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e436f831-a33a-5f02-88bc-636266246198", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 505, "real_page_number": null, "text": "16.2 C++ Strings\n493\nstd::string_viewA modified implementation using is shown below, which does just that:\n1\nvoid process_data(std::string_view timestamp, std::string_view uniqname,\n2\nstd::string_view assignment) {\n3\n// do stuff...\n4\n} // process_data()\n5\n6\nint main() {\n7\nstd::string line;\n8\nwhile (std::getline(std::cin, line)) {\n9\nsize_t last = 0, next = 0;\n10\n11\n// get timestamp\n12\nnext = line.find(\";\", last);\n13\nstd::string_view timestamp_str_v{line.c_str() + last, next - last};\n14\nstd::string_view timestamp = timestamp_str_v.substr(timestamp_str_v.find(\":\") + 1);\n15\nlast = next + 1;\n16\n17\n// get uniqname\n18\nnext = line.find(\";\", last);\n19\nstd::string_view uniqname_str_v{line.c_str() + last, next - last};\n20\nstd::string_view uniqname = uniqname_str_v.substr(uniqname_str_v.find(\":\") + 1);\n21\nlast = next + 1;\n22\n23\n// get assignment\n24\nnext = line.find(\";\", last);\n25\nstd::string_view assignment_str_v{line.c_str() + last, next - last};\n26\nstd::string_view assignment = assignment_str_v.substr(assignment_str_v.find(\":\") + 1);\n27\nlast = next + 1;\n28\n29\nprocess_data(timestamp, uniqname, assignment);\n30\n} // while\n31\n} // main()\nInstead of performing multiple string allocations, this implementation only makes a single allocation for each line of the file. All of the string\nviews that are initialized (on lines 13-14, 19-20, and 25-26) simply refer to the contents of this first allocation of the string instead of making a\nnew copy of their own. This improves the overall performance, since allocating a new string can be an expensive process. To illustrate how this\nlineimplementation works, consider the value of after the first iteration of the loop:\n0x3110\n63\n120\nø\nline\ndata\nlength\ncapacity\nunused\nstack\nheap\n. . . ;p q: :e s a a e am m n n mt UT 50 0 9 0 3i 2 2 1 2 1 1 1 4 i\n…\nstd::string_viewOn line 13, we create a corresponding to the timestamp portion of the original string. Instead of making a brand new\nstring, this expression simply creates a read-only view of a subsection of the original string.\n0x3110\n63\n120\nø\nline\n0x3110\n27\ntimestamp_str_v\nview\nlength\ndata\nlength\ncapacity\nunused\nstack\nheap\n. . . ;p q: :e s a a e am m n n mt UT 50 0 9 0 3i 2 2 1 2 1 1 1 4 i\n…\ntimestamp_str_v provides a view of the following portion of the string", "word_count": 401, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "39266a6d-8e42-5bbd-a471-2bfcc43dc861", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 506, "real_page_number": null, "text": "494\nChapter 16. Strings and Sequences\ntimestamp_str_vLikewise, line 14 does the same thing and creates a new view using a substring of the view.\n0x3110\n63\n120\nø\nline\n0x3110\n27\ntimestamp_str_v\n0x311a\n17\ntimestamp\nview\nlength\ndata\nlength\ncapacity\nunused\nview\nlength\nstack\nheap\n. . . ;p q: :e s a a e am m n n mt UT 50 0 9 0 3i 2 2 1 2 1 1 1 4 i\n…\ntimestamp provides a view of this part of the string\nstd::string_view <string_view>If you are using C++17 or beyond, you can use by including the header. As shown in the previous\ncode, you can create a string view of an existing string by passing in a pointer to the first character along with the size of the view:\n1\nstd::string str = \"EECS281\";\n2\nstd::string_view str_v{str.c_str() + 4, 3};\n3\nstd::cout << str_v << std::endl;\n// prints \"281\"\nstd::string_view std::string.There is a lot of overlap between the methods provided by and the methods provided by As shown\n.substr() std::string_view,previously, we can use to get a substring of a much like how we can use the same method to get a\nstd::string.substring of a Other read-only methods, like the find methods, are also supported by string views.\n.remove_prefix() .remove_suffix(),Two additional methods provided by string views are and which can be used to shrink the\n.remove_prefix() .remove_suffix()sizeofaview. Astheirnamesimply, shrinksaviewbymovingitsstartpositionforward, while\nshrinks a view by moving its end position backward. Examples of these methods are shown below:\n1\nstd::string str = \"xxxEECS281xx\";\n2\nstd::string_view str_v = str;\n3\nstr_v.remove_prefix(3);\n// trim first three characters of view\n4\nstr_v.remove_suffix(2);\n// trim last two characters of view\n5\nstd::cout << str_v << std::endl;\n// prints \"EECS281\"\nstd::string_view,Remark: With the introduction of we have an alternative way to express the existence of a read-only string. Now,\nconst std::string&) std::string_view:there are cases where immutable references to strings (e.g., may be replaced with a\n1\n// before C++17: use immutable reference to std::string\n2\nstd::vector<std::string> strs = { /* values */ };\n3\nfor (const std::string& str : strs) {\n4\n// do stuff\n5\n} // for str\n6\n7\n// after C++17: use std::string_view\n8\nfor (std::string_view str_v : strs) {\n9\n// do stuff\n10\n} // for str_v\nstd::string_view std::stringThere are situations where using a may be faster than using an immutable reference. Consider the\nconst std::string&:following function definition, which takes in a\nvoid foo(const std::string& str);\nfoo():Now, consider what happens when we pass a string literal into\nfoo(\"This string is way too long to qualify for SSO\");\nfoo() std::string, const char*) std::stringThe method expects a so the string literal (stored as is first converted into a\nobject, which is then destroyed after the function returns. However, this forces a string allocation on the heap, which is not efficient. Instead,\nfoo() std::string_viewwe can define the method to take in a instead:\nvoid foo(std::string_view str);\nfoo()Here, an object is still created to hold the string literal, but an expensive string allocation is no longer needed since the parameter of\nstd::string. foo()does not require a Instead, simply takes in a pointer to a view of the literal, which is much more efficient.", "word_count": 569, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1cf403e6-1cbb-5a90-b499-a741e72a1269", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 507, "real_page_number": null, "text": "16.3 Lexicographical String Comparisons\n495\nThere are a few things to be aware of when working with string views. First, a string view only provides a read-only view of a string, but it does\nown any string data on its own. Because of this, you have to make sure that a string view does not outlive its underlying string! Second, anot\nstring view is not guaranteed to be null-terminated, as it instead uses its length to identify where its contents end. As a result, string views will\nnot be safe if you need to work with C-style strings and methods that expect null termination.\nstd::string_viewWhen passing immutable, read-only strings into functions, a may be faster, and they are more flexible with C-style\nstring arguments (such as string literals). However, if your function expects null termination or calls some other function that takes in a\nstd::string const std::string& std::string_viewparameter, then passing in a may be a better option, as is not guaranteed to\nstd::stringbe null-terminated, and it would be wasteful to have to convert a view back into a if it needs to be in that form anyway.\nstd::string_viewRemark: In general, should be passed by value instead of const reference. This is because string views are small,\ntrivially copyable types that can be optimized by compilers if passed by value, and doing so also removes a level of pointer indirection when\naccessing the contents of the view (i.e., if you pass a string view by reference, the function that is called (or the callee) would need to access\nthe address of the reference first before it can access the address of the view’s data itself).\n¸ 16.2.4\n(✽)Additional String Formatting Methods\nstd::format() <format>C++20 introduced the method in the header, which makes it easier to format strings with custom values. This\n{ }method takes in a format string that consists of ordinary characters, plus the special characters of and that delineate the position of custom\nstd::format()arguments. For instance, the following uses to format a string using custom arguments:\n1\n// This prints \"My favorite class is EECS 281!\"\n2\nstd::string category = \"EECS\";\n3\nstd::string number = \"281\";\n4\nstd::cout << std::format(\"My favorite class is {} {}!\", category, number) << '\\n';\nYou can also specify the order of the arguments in the format string by including an index within the curly braces. This is shown below:\n1\n// This prints \"Have you taken EECS 281? There is no doubt that 281 is the best class in EECS!\"\n2\nstd::string category = \"EECS\";\n3\nstd::string number = \"281\";\n4\nstd::cout <<\n5\nstd::format(\"Have you taken {0} {1}? There is no doubt that {1} is the best class in {0}!\",\n6\ncategory, number) << '\\n';\nstd::print(),C++23 takes string formatting up a notch with the introduction of which can be used to format a string and also print it\nstd::format() std::print()to a customized output stream. We will not go over and in too much detail here, but they are definitely\nworthwhile to understand if you are using versions of C++ that support them.\n16.3\nLexicographical String Comparisons\nC++stringshaveoverloadedcomparisonoperatorsthatallowthemtobecompared. TheorderingofstringobjectsisdeterminedbyASCIIvalue.\nAs a result, uppercase letters are considered to be \"less\" than lowercase letters, since uppercase letters have lower ASCII values. Otherwise,\nstrings follow lexicographical (dictionary) order, e.g.,\n\"\"< \"Zebra\"< \"a\"< \"ab\"< \"b\"< \"bb\"< \"bc\"< \"bc0\"\n<algorithm> std::lexicographical_compare()TheC++ libraryprovidesthe function,whichcanbeusedtocheckifthecontents\nof one iterator range is lexicographically less than the contents of a second iterator range. A lexicographical comparison involves going through\nthe elements in both iterator ranges sequentially until a character in the first range does not compare equal to one in the second. The values of\nthese two elements are then compared, and the result determines which range comes first lexicographically.\ntemplate <typename typename typename typenameInputIterator1, InputIterator2, OutputIterator, Compare>\nbool std::lexicographical_compare(InputIterator1 first1, InputIterator1 last1,\nInputIterator2 first2, InputIterator2 last2, Compare comp);\n[first1, last1) [first2, last2),Returns true if the range compares lexicographically less than the range and false otherwise.\noperator<The comparator is optional, and is used if it is not provided.\n\"apple\" \"banana\":For example, the following code checks if the string is lexicographically less than the string\n1\nstd::string str1 = \"apple\";\n2\nstd::string str2 = \"banana\";\n3\nbool b = std::lexicographical_compare(str1.begin(), str1.end(), str2.begin(), str2.end());\n// true\nThe following rules are used when two strings are lexicographically compared:\n• Two ranges are compared element by element.\n• The first mismatching element defines which range is lexicographically less or greater than the other.\n• If one range is a prefix of another, the shorter range is lexicographically less than the other.\n• If two ranges have equivalent elements and are the same length, the ranges are lexicographically equal.\n• An empty range is lexicographically less than any non-empty range.\n• Two empty ranges are lexicographically equal.\nUsing these rules, the following lexicographical comparison would be false, as uppercase letters have lower ASCII values than lowercase ones:\n1\nstd::string str1 = \"apple\";\n2\nstd::string str2 = \"Banana\";\n3\nbool b = std::lexicographical_compare(str1.begin(), str1.end(), str2.begin(), str2.end());\n// false", "word_count": 875, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b58ff5ac-dedb-5c84-abca-b66d20dabc7c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 508, "real_page_number": null, "text": "496\nChapter 16. Strings and Sequences\n\"app\" \"apple\",Similarly, using the prefix rule, the string would compare lexicographically less than the string and an empty string would\n\"app\".compare lexicographically less than the string\nstd::lexicographical_compare()Because checks if a range is strictly lesser, the function will return false if both strings passed\nin are equal. Furthermore, the function is templated so that it can take in any container that supports input iterators. For instance, the following\nis a valid usage of the function, even though we are passing in iterators belonging to different containers:\n1\nstd::string str1 = \"apple\";\n2\nstd::deque<char> str2 = {'b', 'a', 'n', 'a', 'n', 'a'};\n3\nbool b = std::lexicographical_compare(str1.begin(), str1.end(), str2.begin(), str2.end());\n// true\n'3' '3'If you do something like this, make sure to watch the types! For instance, the character compares greater than the integer 3, since has\n'3'an ASCII value of 51. Thus, a 3 in a container of integers is very different from a in a container of characters!\n16.4\nSTL Sequence Operations\n<algorithm>The STL library provides several functions that can be used to work with sequences of data. In this section, we will discuss\nstd::equal() std::unique().two of these functions, and\n¸ 16.4.1\nstd::equal\nstd::equal()The function can be used to check if the contents of two interator ranges are equal. Two ranges are considered equal if they\nhave the same number of elements every character in one range matches the character at the same relative position of the other range.and\ntemplate <typename typename typenameInputIterator1, InputIterator2, BinaryPredicate>\nbool std::equal(InputIterator1 first1, InputIterator1 last1, InputIterator2 first2, BinaryPredicate pred);\ntrue [first, last) [first2, first2 +Returns if the elements in the iterator range are equal to the elements in the iterator range\n(last1 - first1)), false operator==and otherwise. The predicate is optional, and is used to compare values if not provided.\nConsider the following snippet of code:\n1\nstd::string str1 = \"app\";\n2\nstd::string str2 = \"apple\";\n3\nstd::cout << std::equal(str1.begin(), str1.end(), str2.begin());\n// true\nstd::equal() [str1.begin(), str1.end())When is called, it checks if all the characters in the range match the characters in the\nstr2.begin(). \"app\"range beginning at Note that the first range determines the length of the sequence that is checked. Since the string is\n\"apple\" \"app\".used to determine the length of the iterator range, only the first three characters of are checked to see if it matches with As a\nstd::equal() true std::equal() false,result, returns in this example. Using similar logic, the following call returns since there\nexists no match for the last two letters of \"apple\" in the string \"app\":\n1\nstd::string str1 = \"app\";\n2\nstd::string str2 = \"apple\";\n3\nstd::cout << std::equal(str2.begin(), str2.end(), str1.begin());\n// false\nstd::equal()The operation is helpful if you want to check if one string is a prefix of another:\n1\nbool is_prefix(const conststd::string& prefix, std::string& full_str) {\n2\nreturn std::equal(prefix.begin(), prefix.end(), full_str.begin());\n3\n} // is_prefix()\nstd::equal()As an additional example, the following function checks if a string is a palindrome using in just one line:\n1\nbool is_palindrome(const std::string& str) {\n2\nreturn std::equal(str.begin(), str.begin() + str.size() / 2, str.rbegin());\n3\n} // is_palindrome()\n¸ 16.4.2\nstd::unique\nstd::unique(),Another useful STL sequence operation is which filters out duplicates in a container of values.sorted\ntemplate <typename typenameForwardIterator, BinaryPredicate>\nForwardIterator std::unique(ForwardIterator first, ForwardIterator last, BinaryPredicate pred);\nRemoves all duplicates in a sorted iterator range by eliminating all but the first element from every consecutive group of identical elements\n[first, last).from the range An iterator pointing one past the last non-removed element is returned. The predicate is optional, and\noperator== is used to compare values if not provided.\nLike with other STL algorithms, the function cannot physically modify the container that the underlying data is stored in. As a result,\nstd::unique() only ensures that the elements to be kept are moved to the front of the container. This is why the function returns an iterator\nstd::unique(),one past the last element not removed. After calling you must separately erase all the elements from this returned iterator\n.erase()to the end using the member of the container the data is stored in.\nstd::unique()It is also important to note that the function does not sort the elements for you. If you want to remove duplicates using\nstd::unique(). std::unique()this function, you must sort the underlying container before passing it into Otherwise, would behave\nas if the data were sorted, which could lead to incorrect results. Consider the following code:\n1\nstd::vector<int32_t> vec = {5, 5, 5, 3, 4, 4, 4, 3, 3, 3, 6, 7, 7, 6, 7};\n2\nstd::unique(vec.begin(), vec.end());", "word_count": 778, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2208fc8b-a3d5-5d64-9341-14fab0927555", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 509, "real_page_number": null, "text": "16.5 Brute Force String Searching\n497\nstd::unique().Here, the vector was not sorted prior to passing it into What happens here? When the function is called, it completes a\nlinear pass of the sequence. If it sees a consecutive sequence of elements that are the same, it only keeps the first of the sequence and flags the\nstd::uniqueremaining elements for removal. For example, the bolded elements would be \"removed\" by on this unsorted container.\n5\n5\n5\n3\n4\n4\n4\n3\n3\n3\n6\n7\n7\n6\n7\nstd::unique()Since only looks at elements that are identical, there is no guarantee that all duplicates will be removed if theconsecutive\nunderlying container is not sorted. In the above example, there are multiple 3s, 6s, and 7s left over. To ensure all duplicates are removed, the\nstd::unique()range passed into must be sorted.\n1\nstd::vector<int32_t> vec = {5, 5, 5, 3, 4, 4, 4, 3, 3, 3, 6, 7, 7, 6, 7};\n2\nstd::sort(vec.begin(), vec.end());\n3\nstd::unique(vec.begin(), vec.end());\n3\n3\n3\n3\n4\n4\n4\n5\n5\n5\n6\n6\n7\n7\n7\nstd::unique()\n3\n4\n5\n6\n7\n3\n3\n3\n4\n4\n5\n5\n6\n7\n7\nstd::unique()Since cannot physically remove elements from the vector itself, they must be removed explicitly, as shown:\n1\nstd::vector<int32_t> vec = {5, 5, 5, 3, 4, 4, 4, 3, 3, 3, 6, 7, 7, 6, 7};\n2\nstd::sort(vec.begin(), vec.end());\n3\nauto it = std::unique(vec.begin(), vec.end());\n// iterator to first elt to erase\n4\nvec.erase(it, vec.end());\n// erases all elts from 'it' to the end\nThe last two lines of the above code can be combined (skipping the temporary variable). The following code does the same thing as above:\n1\nstd::vector<int32_t> vec = {5, 5, 5, 3, 4, 4, 4, 3, 3, 3, 6, 7, 7, 6, 7};\n2\nstd::sort(vec.begin(), vec.end());\n3\nvec.erase(std::unique(vec.begin(), vec.end()), vec.end());\n3\n4\n5\n6\n7\n3\n3\n3\n4\n4\n5\n5\n6\n7\n7\nvec.erase()\n3\n4\n5\n6\n7\n16.5\nBrute Force String Searching\nString searching algorithms are undoubtedly one of the most important groups of algorithms in the field of computer science. These algorithms\ncan be used to search for specific text patterns within a large body of text, and they play a pivotal role in search engines, word processing\nsoftware, and even the field of bioinformatics, just to name a few.\nThe process of searching for specific sequences of characters in a larger string is sometimes denoted as a problem.needle in a haystack\nThis is because string searching is akin to searching for a needle in a haystack, where the needle is the string pattern you are trying to find, and\nthe haystack is the text corpus that you are trying to search in. The goal is to search for instances of the needle in the haystack and return the\nposition of any match. Consider the following needle and haystack:\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca\nHow would you find the needle in the haystack? A naïve approach is to use a brute force sliding window approach. In this solution,theneedle\"abca\"\nwould be compared with all substrings of length 4 by sliding through the haystack until a match is found.\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\n...\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(match found!)", "word_count": 582, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f23e5279-31d8-5bc6-9855-99771361bcfe", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 510, "real_page_number": null, "text": "498\nChapter 16. Strings and Sequences\nWhat is the time complexity of this brute force algorithm? In the average case, only the first few letters of the needle are enough to determine\nthat a window does not match. For example, any haystack window that begins with a letter other than ‘a’ can be eliminated immediately in\nconstant time:\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(only one character needs to be checked to see that this won’t work)\nWith this information, we can show that the time required to check each window is constant in the average case, since only the first few letters\nof the needle are often enough to determine a mismatch. If we denote the length of the needle as 𝑛and the length of the haystack as ℎ, the\naverage-case time complexity of the sliding window approach is Θ(ℎ), since there are a total of windows that can each be checked inΘ(ℎ)\naverage-case time.Θ(1)\nWhat about the worst-case time complexity? This happens when every character in the needle needs to be compared for every window in\nthe haystack.\nHaystack:\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaab\nNeedle:\naaaaaab →(not equal, so slide window)\nHere, we need to check every character of the needle \"aaaaaab\" before we can tell that the window does not match. Since the worst-case time\ncomplexity of checking each window is Θ(𝑛), and there are a total of windows that need to be checked, the worst-case time complexity ofΘ(ℎ)\nthe brute force string searching algorithm is Θ(𝑛ℎ). The code for brute force is shown below:\n1\nstd::vector<size_t> sliding_window(const conststd::string& needle, std::string& haystack) {\n2\nstd::vector<size_t> matches;\n3\nconst size_t len_needle = needle.length();\n4\nconst size_t len_haystack = haystack.length();\n5\nfor (size_t i = 0; i <= len_haystack - len_needle; ++i) {\n6\nsize_t j = 0;\n7\nfor (; j < len_needle; ++j) {\n8\nif (haystack[i + j] != needle[j]) {\n9\nbreak;\n// character mismatch\n10\n} // if\n11\n} // for\n12\n// if for loop completes, the needle matches the window, so store starting index in solution\n13\nif (j == len_needle) {\n14\nmatches.push_back(i);\n15\n} // if\n16\n} // for i\n17\nreturn matches;\n18\n} // sliding_window()\nEven though a worst-case time complexity of is not ideal, the brute force approach is by no means a terrible algorithm, and it can be quiteΘ(𝑛ℎ)\nfast in practice. This is because the worst-case scenario rarely happens. However, if the worst case does show up, there are ways to improve\nperformance. One such method is the Rabin-Karp algorithm, which we will cover in the next section.\nString Search Method\nAverage-Case Time\nWorst-Case Time\nSliding Window\nΘ(ℎ)\nΘ(𝑛ℎ)\n16.6\nRabin-Karp String Searching\n¸ 16.6.1\nRabin Fingerprinting\nIf we use the brute force approach, the worst-case time complexity for finding a match is Θ(𝑛ℎ). This is because a single window comparison\nmay take up to time if every character of the needle needs to be compared. The Rabin-Karp string searching algorithm addresses thisΘ(𝑛)\ninefficiency by guaranteeing a comparison for each window, regardless of the contents of the string.Θ(1)\nIn the previous example, checking the equality of \"aaaaaaa\" and \"aaaaaab\" took linear time because we had to compare every letter. The\nRabin-Karp algorithm avoids this problem by comparing instead of the actual strings themselves. In this approach, eachstring fingerprints\nstring is first converted into an integer in time. Then, the integers are compared to determine the equality of the strings. This improves theΘ(1)\nworst-case performance because an integer comparison, unlike a string comparison, takes time. The function we use to convert each stringΘ(1)\ninto an integer is our fingerprint function, and the integer that the string gets converted to is its fingerprint. A string’s fingerprint value may\nalso be referred to as a hash.rolling\nTo gain an intuition for how string fingerprinting works, consider a simple fingerprint function that converts a string to an integer by adding\nthe ASCII value of each character.\n1\nint32_t simple_fingerprint(const std::string& str) {\n2\nint32_t sum = 0;\n3\nfor (char c : str) {\n4\nstatic_cast<int32_t>(c);sum +=\n5\n} // for c\n6\nreturn sum;\n7\n} // simple_fingerprint()\nUsing this fingerprint function, the string \"cat\" would get converted to the integer 312. This is because ‘c’ has an ASCII value of 99, ‘a’ has an\nASCII value of 97, and ‘t’ has an ASCII value of 116 — the sum of these three numbers is 312. With string fingerprinting, we can use the\ninteger 312 to do comparisons instead of the string \"cat\". If any string does not have a fingerprint of 312, it cannot possibly be the string \"cat\"!", "word_count": 780, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "245f9a91-500c-5054-8726-f7e289d3878b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 511, "real_page_number": null, "text": "16.6 Rabin-Karp String Searching\n499\nHowever, what if we were given another string that also has a fingerprint of 312? Can we conclude that this other string also has the value \"cat\"?\nIn this case, no — match. If a string does not have a fingerprint of 312,fingerprints can only tell us if strings differ, but they cannot tell us if they\nit cannot be the string \"cat\", but if a string does have a fingerprint of 312, we would not know anything without first comparing the two strings\nthemselves! The other string could be \"cat\", but it could also be any string that has the letters ‘c’, ‘a’, and ‘t’ (e.g., \"act\"). In fact, the other string\ndoes not even need this combination of letters; any string whose ASCII values sum to 312 would work (e.g., \"kid\" = 107 + 105 + 100 = 312).\nThus, if two strings have the same fingerprint, they are not guaranteed to be identical — we must check the character compositions of the two\nstrings for equality before we can make such a conclusion.\nIt turns out that simply adding ASCII values produces a really bad fingerprint function. This is because it produces a lot of collisions, where\ntwo different strings are assigned to the same fingerprint value. Consider the following:\nFP 312:\n• \"act\"\n• \"are\"\n• \"cat\"\n• \"doe\"\n• \"ear\"\n• \"gap\"\n• \"gel\"\n• \"jam\"\n• \"kid\"\n• \"leg\"\n• \"sad\"\nFP 430:\n• \"best\"\n• \"draw\"\n• \"even\"\n• \"fast\"\n• \"frog\"\n• \"girl\"\n• \"hero\"\n• \"link\"\n• \"memo\"\n• \"park\"\n• \"them\"\nFP 536:\n• \"alert\"\n• \"bound\"\n• \"drink\"\n• \"equal\"\n• \"fresh\"\n• \"index\"\n• \"level\"\n• \"light\"\n• \"paper\"\n• \"shell\"\n• \"there\"\nFP 641:\n• \"beyond\"\n• \"broken\"\n• \"center\"\n• \"choose\"\n• \"either\"\n• \"finish\"\n• \"medium\"\n• \"method\"\n• \"prince\"\n• \"random\"\n• \"recent\"\nFP 769:\n• \"airport\"\n• \"control\"\n• \"convert\"\n• \"instant\"\n• \"present\"\n• \"protect\"\n• \"protein\"\n• \"running\"\n• \"through\"\n• \"violent\"\n• \"working\"\nFP 843:\n• \"accuracy\"\n• \"critical\"\n• \"diameter\"\n• \"distance\"\n• \"electric\"\n• \"flexible\"\n• \"generate\"\n• \"intended\"\n• \"leverage\"\n• \"marginal\"\n• \"practice\"\nThe table above provides lists of strings that all map to the same fingerprint value. For instance, all of the words in the first column map to a\nfingerprint of 312. Collisions are not good because they result in false positives, which result in unnecessary string comparisons.\nA good fingerprint function should minimize collisions. For instance, if \"cat\" is assigned a fingerprint of 312, the fingerprint function should\n(ideally) not assign any other string a fingerprint of 312. That way, it would be extremely rare to encounter a string with a fingerprint of 312 not\nequal to \"cat\". To do this, we will use a technique known as Rabin fingerprinting. Unlike the previous approach, the Rabin fingerprinting\napproach also takes into account the of each character alongside its ASCII value when calculating the fingerprint of a string.position\nConsider the string \"cat\" again. Instead of simply adding the ASCII values together, we will also multiply each intermediate result with a\nmultiplier value that depends on a character’s position. In the example below, we will use 10 as the multiplier:\nOld fingerprinting function (position not considered): \"cat\" →99 + 97 + 116 = 312\nNew fingerprinting function (multiply fingerprint by a constant value every time a new letter is added; here = 10):k k\n\"c\" →99 (since ‘c’ has an ASCII value of 99)\n\"ca\" →(10 99) + 97 = 1087 (multiply the fingerprint of \"c\" by 10, then add the ASCII value of ‘a’)×\n\"cat\" →(10 1087) + 116 = 10986 (multiply the fingerprint of \"ca\" by 10, then add the ASCII value of ‘t’)×\nUsing this method, the string \"cat\" has a fingerprint of 10986. Since we taking the position of each character into account, it is much more\ndifficult for two different strings to end up with the same fingerprint. Consider the string \"act\", which had the same fingerprint as \"cat\" using our\ninitial fingerprinting approach.\nOld fingerprinting function (position not considered): \"act\" →97 + 99 + 116 = 312\nNew fingerprinting function (multiply fingerprint by a constant value every time a new letter is added; here = 10):k k\n\"a\" →97 (since ‘c’ has an ASCII value of 97)\n\"ac\" →(10 97) + 99 = 1069 (multiply the fingerprint of \"a\" by 10, then add the ASCII value of ‘c’)×\n\"act\" →(10 1069) + 116 = 10806 (multiply the fingerprint of \"ac\" by 10, then add the ASCII value of ‘t’)×\nNotice that \"act\" and \"cat\" now have different fingerprints, even though they are made up of the same characters. In fact, if we look at the new\nfingerprints of the strings in the previous list, we can see that they are now very different:\nFP 312:\n• \"act\" →10806\n• \"are\" →10941\n• \"cat\" →10986\n• \"doe\" →11211\n• \"ear\" →11184\n• \"gap\" →11382\n• \"gel\" →11418\n• \"jam\" →11679\n• \"kid\" →11850\n• \"leg\" →11913\n• \"sad\" →12570\nFP 430:\n• \"best\" →109366\n• \"draw\" →112489\n• \"even\" →113920\n• \"fast\" →112966\n• \"frog\" →114613\n• \"girl\" →114748\n• \"hero\" →115351\n• \"link\" →119707\n• \"memo\" →120301\n• \"park\" →122947\n• \"them\" →127519\nFP 536:\n• \"alert\" →1089356\n• \"bound\" →1103900\n• \"drink\" →1125707\n• \"equal\" →1135778\n• \"fresh\" →1145354\n• \"index\" →1171130\n• \"level\" →1193918\n• \"light\" →1196456\n• \"paper\" →1229324\n• \"shell\" →1265288\n• \"there\" →1275341", "word_count": 922, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "13dd7027-85ca-5f3f-8304-6671b870d8af", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 512, "real_page_number": null, "text": "500\nChapter 16. Strings and Sequences\nFP 641:\n• \"beyond\" →10943300\n• \"broken\" →11062820\n• \"center\" →11032724\n• \"choose\" →11063351\n• \"either\" →11277524\n• \"finish\" →11371754\n• \"medium\" →12021779\n• \"method\" →12037610\n• \"prince\" →12457091\n• \"random\" →12491219\n• \"recent\" →12520316\nFP 769:\n• \"airport\" →108764356\n• \"control\" →111328618\n• \"convert\" →111329356\n• \"instant\" →117276916\n• \"present\" →124536316\n• \"protect\" →124637206\n• \"protein\" →124637260\n• \"running\" →126921703\n• \"through\" →127663834\n• \"violent\" →129729316\n• \"working\" →131358703\nFP 843:\n• \"accuracy\" →1080194811\n• \"critical\" →1115775978\n• \"diameter\" →1115903724\n• \"distance\" →1117769091\n• \"electric\" →1129218549\n• \"flexible\" →1139415981\n• \"generate\" →1143134961\n• \"intended\" →1172731110\n• \"leverage\" →1193934831\n• \"marginal\" →1199547078\n• \"practice\" →1244817591\nNow, it is very difficult for two differing strings to have the same fingerprint. If you wanted to search for the word \"cat\" in a body of text, the\nlikelihood of encountering a different string with the same fingerprint of 10986 is near zero. What are the implications of this? Consider the\nworst-case scenario we introduced earlier:\nHaystack:\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaab\nNeedle:\naaaaaab\nIf we used brute force, we would have to check all the characters of the needle before we can determine that the two strings are different.\nHowever, if we instead converted \"aaaaaaa\" and \"aaaaaab\" to their respective fingerprints, 107777767 and 107777768, comparing the equality\nof these two integers only takes constant time!\nBut wait... doesn’t the fingerprint conversion process take linear time? If we wanted to convert \"aaaaaaa\" to its fingerprint, we would need\nto visit every letter of the string, multiply it by a constant, and add an ASCII value. Wouldn’t we be doing the same amount of work as brute\nforce, since we need to visit every character of each haystack window regardless? It is indeed true that we need to visit every character of a\nstring to calculate its fingerprint. However, the math behind Rabin fingerprinting works out so that we only need to calculate a fingerprint\nfrom scratch: once for the needle and once for the first haystack window we check. All other fingerprints needed for the algorithm can betwice\ncomputed in time. Consider the following example:Θ(1)\nHaystack:\nabcde\nNeedle:\ncde\nFirst, let’s calculate the fingerprint of the needle, using a multiplier of 10:𝑘=\n\"c\" →99 (since ‘c’ has an ASCII value of 99)\n\"cd\" →(10 99) + 100 = 1090 (multiply the fingerprint of \"c\" by 10, then add the ASCII value of ‘d’)×\n\"cde\" →(10 1090) + 101 = 11001 (multiply the fingerprint of \"cd\" by 10, then add the ASCII value of ‘e’)×\nWe then repeat this process to find the fingerprint of our first haystack window, \"abc\":\n\"a\" →97 (since ‘a’ has an ASCII value of 97)\n\"ab\" →(10 97) + 98 = 1068 (multiply the fingerprint of \"a\" by 10, then add the ASCII value of ‘b’)×\n\"abc\" →(10 1068) + 99 = 10779 (multiply the fingerprint of \"ab\" by 10, then add the ASCII value of ‘c’)×\nBecause the two fingerprints differ (10779 != 11001), we shift to the next window:\nHaystack:\nabcde\nNeedle:\ncde →(10779\n⏟⏟⏟\nwindow\n!= 11001\n⏟⏟⏟\nneedle\n, so slide window)\nNow, we have to compare the fingerprints of \"cde\" and \"bcd\" to determine if there is a match.\nHaystack:\nabcde\nNeedle:\ncde\nWe know from before that the fingerprint of \"cde\" is 11001 since we calculated it previously. However, what is the fingerprint of \"bcd\"? Do we\nneed to calculate this fingerprint from scratch?\nIt turns out that we can do some math to elegantly obtain the fingerprint value of \"bcd\" in time, given that we already know theΘ(1)\nfingerprint of the previous haystack window (in this case, \"abc\"). To do so, we first need to remove the leftmost character of the previous\nwindow (i.e., the character that is leaving the window as we slide it forward) from the fingerprint value. To remove this character, we can first\n𝑘𝑛−1,multiply its ASCII value by where 𝑘is the base multiplier and 𝑛is the length of the needle. Then, we subtract this value from the previous\nfingerprint value, multiply the result by the multiplier, and add the ASCII value of the new character that entered the window.\nLet’s go through the previous example and look at how the fingerprint of \"abc\" (10779) can be converted to the fingerprint of \"bcd\" in\n𝑘𝑛−1 103−1constant time. In our example, we are using and 3, so we need to multiply the leftmost character by and subtract𝑘= 𝑛=10 = =100\nit from the fingerprint of \"abc\".\n\"abc\" →10779\n\"bc\" →10779 - (100 97) = 1079 (subtract the contribution of ‘a’ from the fingerprint)×\nWe are now able to add ‘d’ using the standard fingerprint calculation process.\n\"bc\" →1079\n\"bcd\" →(10 1079) + 100 = 10890 (multiply the fingerprint of \"bc\" by 10, then add the ASCII value of ‘d’)×\nThus, \"bcd\" has a fingerprint of 10890. We were able to obtain this value from the fingerprint of \"abc\" in time.Θ(1)", "word_count": 838, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "027c6bcc-e060-53d0-80f4-a166d10b884a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 513, "real_page_number": null, "text": "16.6 Rabin-Karp String Searching\n501\nRemark: Why were we able to calculate the fingerprint of \"bcd\" in constant time, given that we already know the fingerprint of \"abc\"?\nNotice what is happening when we calculate our Rabin fingerprints:\n\"a\" →97 (since ‘a’ has an ASCII value of 97)\n\"ab\" →(10 97) + 98 = 1068 (multiply the fingerprint of \"a\" by 10, then add the ASCII value of ‘b’)×\n\"abc\" →(10 1068) + 99 = 10779 (multiply the fingerprint of \"ab\" by 10, then add the ASCII value of ‘c’)×\nIf we combined all three operations into one equation, we get\nfingerprint of \"abc\"=10×((10×97)+98)+99= 10×10×97\n⏟⏞⏞⏞⏞⏟⏞⏞⏞⏞⏟\ncontributionof‘a’\n+\n10×98\n⏟⏟⏟\ncontributionof‘b’\n+\n99\n⏟⏟⏟\ncontributionof‘c’\nTo obtain the fingerprint of \"bcd\" from the fingerprint of \"abc\", we need to\n1. Remove the contribution of ‘a’ from the fingerprint of \"abc\" to get the fingerprint of \"bc\".\n2. Add the contribution of ‘d’ to the fingerprint of \"bc\" to get the fingerprint of \"bcd\".\nIn general, if you have a string of length 𝑛with characters that have ASCII values 𝑐1, 𝑐2, 𝑐3, …, 𝑐𝑛and a base multiplier of 𝑘, the string’s\nfingerprint can be calculated using the following equation:\n𝑘𝑛−1𝑐1+𝑘𝑛−2𝑐2+𝑘𝑛−3𝑐3+…+𝑘1𝑐𝑛−1+𝑘0𝑐𝑛\nEverytimeweshifttheslidingwindow,weremoveacharacterfromtheleftofthewindowandaddoneontheright. Wecaneasilymanipulate\nthe formula above to obtain the fingerprint of one window from the fingerprint of another. The process of turning the fingerprint of \"abc\"\ninto the fingerprint of \"bcd\" is shown again below (where the ASCII values of ‘a’, ‘b’, and ‘c’ are 97, 98, and 99, respectively):\nOriginal fingerprint of \"abc\":\n102×97\n⏟⏞⏟⏞⏟\ncontributionof‘a’\n+\n101×98\n⏟⏞⏟⏞⏟\ncontributionof‘b’\n+\n100×99\n⏟⏞⏟⏞⏟\ncontributionof‘c’\n10779=\n102Remove the contribution of ‘a’ from the fingerprint by subtracting 97:×\n102×97−102×97\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nremovecontributionof‘a’\n+\n101×98\n⏟⏞⏟⏞⏟\ncontributionof‘b’\n+\n100×99\n⏟⏞⏟⏞⏟\ncontributionof‘c’\n10210779 - 97= ×\n101×98\n⏟⏞⏟⏞⏟\ncontributionof‘b’\n+\n100×99\n⏟⏞⏟⏞⏟\ncontributionof‘c’\n1079=\nAdd ‘d’ to the fingerprint by multiplying the current fingerprint by the multiplier and adding the ASCII of ‘d’:\n10×(101×98+100×99)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nfingerprintof\"bc\"multipliedby10\n+\n100\n⏟⏟⏟\nASCIIof‘d’\n18090=\nThis simplifies to the following, which is the fingerprint value of \"bcd\".\n102×98\n⏟⏞⏟⏞⏟\ncontributionof‘b’\n+\n101×99\n⏟⏞⏟⏞⏟\ncontributionof‘c’\n+\n100×100\n⏟⏞⏞⏟⏞⏞⏟\ncontributionof‘d’\n10890=\nBefore we continue, there are two things that should be mentioned. First, even though we used 10 as our multiplier for the above examples, it is\noften better to use a power of two instead. This is because multiplication is faster with powers of two (due to a process known as bit shifting,\nwhich you do not have to worry about for this class).\nSecond, for extremely long strings, fingerprint calculations can lead to overflow errors if you end up with a fingerprint that is larger than the\nlargest number representable using its underlying data type. To account for this, we can use modular arithmetic to ensure our fingerprints do not\nexceed the largest possible value that we can represent. This is done by picking a very large prime number and taking the modulus of the result\nwith the prime number after each calculation (i.e., result % prime). (Why prime? We will get into more detail when we discuss hash tables in\nthe next chapter, but prime numbers are better at producing uniformly distributed modulus results, and they reduce the likelihood of collisions.)\nAn example of this process is shown below:\n\"potatobo\" →1243698191\n\"potatobot\" →(10 1243698191) + 116 = 12436982026 (multiply the fingerprint of \"potatobo\" by 10, then add the ASCII value of ‘t’)×\nIf you are using a 32-bit integer to represent your fingerprint, this value is above the largest possible value that the integer can hold. To address\nthis, we can take the modulus of each computed fingerprint with a large prime number to get a more reasonable value, as shown:\n\"potatobo\" →1243698191\n\"potatobot\" →((10 1243698191) + 116) %× 183203281\n⏟⏞⏞⏞⏟⏞⏞⏞⏟\nlargeprime\n= 162362199", "word_count": 720, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0b5a9679-54e7-56b0-a245-82f13463f5af", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 514, "real_page_number": null, "text": "502\nChapter 16. Strings and Sequences\n¸ 16.6.2\nImplementing Rabin-Karp String Search\nNow that we have covered Rabin fingerprinting, we can use it to implement the Rabin-Karp string searching algorithm, which can be used\nto solve the needle-in-a-haystack problem more efficiently than brute force. Recall that brute force compares the needle with every possible\nwindow in the haystack:\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(not equal, so slide window)\n...\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca →(match found!)\nHowever, this could potentially take worst-case time if every window had to check all 𝑛characters in the needle.Θ(𝑛ℎ)\nThe Rabin-Karp algorithm addresses this issue by comparing integers instead of strings, and it does so by using Rabin fingerprinting to\nconvert strings to integers. Rabin-Karp is implemented using the following procedure:\n1. The fingerprint of the needle is calculated in time, where 𝑛is the length of the needle.Θ(𝑛)\n2. The fingerprint of the first 𝑛characters of the haystack is calculated, also in time.Θ(𝑛)\n3. The fingerprint of the needle is compared with the fingerprint of the window.\n4. If the two fingerprint values match, a brute force comparison is done to confirm that the contents of the two strings are indeed the same\n(this needs to be done in case two different strings end up with the same fingerprint).\n5. If the two fingerprints do not match, the needle slides to the right, and the fingerprint is updated for the new window. This is done by\n(𝑘𝑛−1×• Removing the character on the left of the window by subtracting ASCII) from the existing fingerprint.\n• Multiplying this intermediate value by 𝑘(the base multiplier).\n• Adding the ASCII value of the new character in the window to the result.\n6. As long as there exist more characters in the haystack, repeat steps 4 and 5.\nThe Rabin-Karp process is shown below, using a base multiplier of 32.𝑘=\nHaystack:\nabcdabcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca\nFirst, we calculate the fingerprints of the needle (\"abca\") and the first window of the haystack (\"abcd\"):\n323×97+322×98+321×99+320FP(\"abca\") = ×97=3282113\n323×97+322×98+321×99+320FP(\"abcd\") = ×100=3282116\nWe then compare the two fingerprints. Since they do not match, we know that the strings in the first window also do not match.\nHaystack:\n3282116\n⏞⏞⏞\nabcd abcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca\n⏟⏟⏟\n3282113\nBecause the window does not match, we slide the needle one character to the right and update the fingerprint of the window, which can be done\nin constant time. This process is repeated until a match is found.\nHaystack:\n3282116\n⏞⏞⏞\nabcd abcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca\n⏟⏟⏟\n3282113\n→(3282116 != 3282113, so slide window and update fingerprint)\nHaystack:\na\n3315937\n⏞⏞⏞\nbcda bcdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca\n⏟⏟⏟\n3282113\n→(3315937 != 3282113, so slide window and update fingerprint)", "word_count": 496, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4bce6ddf-4b2b-539c-811a-e1918a019606", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 515, "real_page_number": null, "text": "16.6 Rabin-Karp String Searching\n503\nHaystack:\nab\n3349634\n⏞⏞⏞\ncdab cdabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca\n⏟⏟⏟\n3282113\n→(3349634 != 3282113, so slide window and update fingerprint)\nHaystack:\nabc\n3379363\n⏞⏞⏞\ndabc dabcdabcbadbccacbabdcabcadabcd\nNeedle:\nabca\n⏟⏟⏟\n3282113\n→(3379363 != 3282113, so slide window and update fingerprint)\n...\nHaystack:\nabcdabcdabcdabcbadbccacbabdc\n3282113\n⏞⏞⏞\nabca dabcd\nNeedle:\nabca\n⏟⏟⏟\n3282113\n→(3282113 == 3282113, match found!)\nLet’s code the implementation of Rabin-Karp. First, let’s initialize a few variables that we will need throughout the algorithm:\n1\nconstexpr int32_t base = 128;\n// this is k, or our multiplier\n2\nconstexpr int64_t prime = 376370281280203183;\n// this prime will be used for our modulus\n3\nconst size_t len_needle = needle.length();\n// length of needle\n4\nconst size_t len_haystack = haystack.length();\n// length of haystack\n5\nstd::vector<size_t> matches;\n// stores indices of matches\n(𝑘𝑛−1×To remove the leftmost character from our window, we will need to subtract ASCII) from our fingerprint. Let’s create a variable to store\n𝑘𝑛−1 primethe value of (as mentioned, we take its modulus with to prevent overflow).\nconst int64_t static_cast<int64_t>(std::pow(base,left_multiplier = len_needle - 1)) % prime;\nThen, we will calculate the fingerprint of the needle and the first search window of the haystack.\n1\nint64_t fp_needle = 0;\n2\nint64_t fp_haystack = 0;\n3\nfor (size_t i = 0; i < len_needle; ++i) {\n4\nfp_needle = (base fp_needle + needle[i]) % prime;*\n5\nfp_haystack = (base fp_haystack + haystack[i]) % prime;*\n6\n} // for i\nlen_haystack - len_needleAfter this, we will need to iterate through all the windows of the haystack, as shown. We iterate up to so\nthat we do not iterate off the end of the haystack when comparing with the needle.\n1\nfor (size_t i = 0; i <= len_haystack - len_needle; ++i) {\n2\nif (fp_needle == fp_haystack) {\n3\n// do work if fingerprints are equal\n4\n} // if\n5\n// slide the needle one window to the right and calculate the fingerprint\n6\n// of the new window (subtract left char and add right char)\n7\n} // for i\nIf the fingerprints ever match, we will still need to do a string comparison to make sure the strings are actually the same. This is because it is\nstill possible for two strings to end up with the same fingerprint value.\n1\nfor (size_t i = 0; i <= len_haystack - len_needle; ++i) {\n2\nif (fp_needle == fp_haystack) {\n3\nsize_t j = 0;\n4\nfor (; j < len_needle; ++j) {\n5\nif (haystack[i + j] != needle[j]) {\n6\nbreak;\n7\n} // if\n8\n} // for\n9\n// if for loop completes without breaking, the strings are equal\n10\nif (j == len_needle) {\n11\nmatches.push_back(i);\n12\n} // if\n13\n} // if\n14\n...\n15\n} // for i", "word_count": 475, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fe0134a0-d3bf-5533-9cb4-122701f73ac2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 516, "real_page_number": null, "text": "504\nChapter 16. Strings and Sequences\nThen, we would check the next window by sliding the needle rightward and recalculating the window’s fingerprint:\n1\nfor (size_t i = 0; i <= len_haystack - len_needle; ++i) {\n2\n...\n// continued from line 14 previously\n3\n// fingerprint with leftmost character removed\n4\nint64_t fp_no_left = fp_haystack - haystack[i] left_multiplier;*\n5\nfp_haystack = (base fp_no_left + haystack[i + len_needle]) % prime;*\n6\n// if fingerprint becomes negative, add the prime to make it positive\n7\n// this is okay because of modular arithmetic\n8\nif (fp_haystack < 0) {\n9\nfp_haystack = fp_haystack + prime;\n10\n} // if\n11\n} // for i\nPutting this all together, we have the following Rabin-Karp function to find all instances of a needle within a larger string:\n1\nstd::vector<size_t> rabin_karp(const conststd::string& needle, std::string& haystack) {\n2\nstd::vector<size_t> matches;\n3\nconstexpr int32_t base = 128;\n4\nconstexpr int64_t prime = 376370281280203183;\n5\nconst size_t len_needle = needle.length();\n6\nconst size_t len_haystack = haystack.length();\n7\nconst int64_t static_cast<int64_t>(std::pow(base,left_multiplier = len_needle - 1)) % prime;\n8\nint64_t fp_needle = 0;\n9\nint64_t fp_haystack = 0;\n10\nfor (size_t i = 0; i < len_needle; ++i) {\n11\nfp_needle = (base fp_needle + needle[i]) % prime;*\n12\nfp_haystack = (base fp_haystack + haystack[i]) % prime;*\n13\n} // for i\n14\nfor (size_t i = 0; i <= len_haystack - len_needle; ++i) {\n15\nif (fp_needle == fp_haystack) {\n16\nsize_t j = 0;\n17\nfor (; j < len_needle; ++j) {\n18\nif (haystack[i + j] != needle[j]) {\n19\nbreak;\n20\n} // if\n21\n} // for\n22\nif (j == len_needle) {\n23\nmatches.push_back(i);\n24\n} // if\n25\n} // if\n26\nint64_t fp_no_left = fp_haystack - haystack[i] left_multiplier;*\n27\nfp_haystack = (base fp_no_left + haystack[i + len_needle]) % prime;*\n28\nif (fp_haystack < 0) {\n29\nfp_haystack = fp_haystack + prime;\n30\n} // if\n31\n} // for i\n32\nreturn matches;\n33\n} // rabin_karp()\n¸ 16.6.3\nRabin-Karp Complexity Analysis\nTheefficiencyofRabin-Karpdependsonthefrequencyatwhichcollisionsoccur. Everytimetwostringshavethesamefingerprint,thealgorithm\nmust do a string comparison to ensure the strings are indeed equal. Ideally, two different strings should never share the same fingerprint; ifΘ(𝑛)\nthis is the case, the runtime of Rabin-Karp would be in the worst case, since checking each window would only take time.Θ(ℎ) Θ(1)\nHowever, this is not entirely a guarantee; if the fingerprinting function is awful and produces a false positive at every step, then you could\nend up with a worst-case time complexity of Θ(𝑛ℎ). When this happens, the algorithm would think it found a match at every position of the\nhaystack and could potentially perform a comparison to verify that the needle actually matches for all positions of the haystack. ThisΘ(𝑛) Θ(ℎ)\nmatches the worst-case time complexity of brute force! That being said, while this scenario is theoretically possible, it is extremely improbable\nin practice if a good fingerprinting function is used, especially since Rabin-Karp is designed to safeguard against this happening.\nTheaverage-casetimecomplexityofRabin-KarpisΘ(𝑛+ℎ). Thisisbecauseittakes timetocheckeachwindowifagoodfingerprintingΘ(1)\nfunction is used (which makes it rare for two different strings to share the same fingerprint). There are a total of windows that need to beΘ(ℎ)\nchecked in the haystack, and each match requires a pass over the needle to verify that the window actually matches (to make sure it is not aΘ(𝑛)\nfalse positive where two differing strings end up with the same fingerprint).\nString Search Method\nAverage-Case Time\nWorst-Case Time\nRabin-Karp\nΘ(𝑛+ℎ)\nΘ(𝑛ℎ)", "word_count": 631, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fc06292b-be19-5055-bc17-588ad713963f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 517, "real_page_number": null, "text": "16.7 Knuth-Morris-Pratt (KMP) String Searching\n505\n16.7\nKnuth-Morris-Pratt (KMP) String Searching (✽)\n¸ 16.7.1\n(✽)Preﬁx Tables\nIn this section, we will be discussing the Knuth-Morris-Pratt (KMP) string searching algorithm, which is another important string matching\nalgorithm that can be used to improve the worst-case performance of string searching. The KMP algorithm differs from brute force in that it\nkeeps track of information from previous comparisons to reduce the number of comparisons that may be needed later on. To understand how\nKMP works, consider the following example that was introduced earlier:\nHaystack:\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaab\nNeedle:\naaaaaab →(not equal, so slide window)\nIf we used the brute force approach, we would compare \"aaaaaaa\" with \"aaaaaab\", see that the two strings are not equal, and slide the window\nforward by one. Then, we would compare \"aaaaaaa\" with \"aaaaaab\" again, as shown.\nHaystack:\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaab\nNeedle:\naaaaaab →(not equal, so slide window)\nHowever, much of this work is unnecessary. When we detected a mismatch in the first window (‘a’ != ‘b’), we had already discovered that the\nfirst six characters matched. Thus, we do not need to compare these characters again when we move on to the second window, since we already\nknow they match from the comparisons we made in the previous window. This is where the KMP algorithm comes into play.\nTo better illustrate this, consider the following modified example:\nHaystack:\nabdcdabdcabdbcdbccaabdcdabcabdcdabcbab\nNeedle:\nabdcdabcb →(not equal, so slide window)\nHere, the first difference is at the second to last position of the needle, which is ‘d’ in the haystack but ‘c’ in the needle.\nHaystack:\nabdcdab d cabdbcdbccaabdcdabcabdcdabcbab\nNeedle:\nbabdcdab c\nNotice that the sequence of characters that comes directly before the mismatch is \"ab\" (which is underlined below).\nHaystack:\nabdcdab d cabdbcdbccaabdcdabcabdcdabcbab\nNeedle:\nc babdcdab\nHowever, this same sequence \"ab\" also appears at the of our window.beginning\nHaystack:\nabdcdab d cabdbcdbccaabdcdabcabdcdabcbab\nNeedle:\nabdcdab c b\nWecantakeadvantageofthisinformationtoskipseveralcomparisons. Sincetheneedlestartswiththecharacters\"ab\",andthecharactersdirectly\nbefore the mismatch are also \"ab\", we can align the first \"ab\" of the needle with the last \"ab\" of our previous haystack window. Furthermore,\nsince we already know that the \"ab\" sequences match, we can start our comparisons at the position of the two ‘d’ characters (which are boxed\nbelow) instead of the beginning of the window:\nHaystack:\nabdcdab d cabdbcdbccaabdcdabcabdcdabcbab\nNeedle:\n→ab d cdabcb\nWith this optimization, we are able to traverse the haystack without ever having to move backwards. This idea forms the basis for how KMP\nworks. Now, let’s look at how KMP is implemented.\nFirst, we will introduce some terms that are important for the problem. The prefix of a string is any substring of that string that begins at the\nfirst character. For example, the following are prefixes of the string \"abdcdabcb\":\n\"a\", \"ab\", \"abd\", \"abdc\", \"abdcd\", \"abdcda\", \"abdcdab\", \"abdcdabc\", \"abdcdabcb\"\nOn the other hand, the suffix of a string is any substring of that string that ends at the last character. For example, the following are suffixes of\nthe string \"abdcdabcb\":\n\"b\", \"cb\", \"bcb\", \"abcb\", \"dabcb\", \"cdabcb\", \"dcdabcb\", \"bdcdabcb\", \"abdcdabcb\"\nTo perform KMP string search, we must first preprocess our needle and store additional information that can be used to determine how far we\ncan shift the needle after a mismatch. This is done by constructing something known as a prefix table (also known as a or an table,𝜋table LPS\nwhere LPS stands for \"longest prefix suffix\"). The prefix table is an array that has the same length as the needle, and each index 𝑖of the prefix\ntable stores the length of the longest suffix that is also a prefix of the substring up to index 𝑖(ignoring the suffix that matches with the entire\nstring). Because we never consider the suffix that matches with the entire string, index 0 of the prefix table will always have a value of 0.", "word_count": 657, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "26ca6ce4-15bf-51b4-aa21-a88cbabcdf92", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 518, "real_page_number": null, "text": "506\nChapter 16. Strings and Sequences\nFor example, let’s use the string \"abdcdabcb\" to construct a prefix table. Since this string has a length of 9, the prefix table will also have a\nlength of 9. The value at index 0 of the prefix table is set to 0. This is shown below:\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nWe will now go through all the prefixes of the string and identify the length of the longest suffix that is also a prefix. Index 1 of the prefix table\nstores the length of the longest suffix that is also a prefix of the substring up to index 1 (\"ab\"). Ignoring the case where the entire string matches\nwith itself, there is no suffix that is also a prefix. Thus, index 1 of the prefix table stores 0.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nIndex2oftheprefixtablestoresthelengthofthelongestsuffixthatisalsoaprefixofthesubstringuptoindex2(\"abd\"). Ignoringtheself-match\ncase, there is no suffix that is also a prefix. Thus, index 2 of the prefix table also stores 0.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nIf we continue this process for indices 3 and 4 (the substrings \"abdc\" and \"abdcd\"), we would see that there is no suffix that is also a prefix of the\nstring (ignoring the self-match case). As a result, indices 3 and 4 of the prefix table are both 0.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nIndex 5 of the prefix table stores the length of the longest suffix that is also a prefix of the substring up to index 5, or \"abdcda\". Here, there is a\nsuffix that matches with a prefix: \"abdcda\". This suffix has a length of 1, so index 5 of the prefix table stores 1.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\nIndex 6 of the prefix table stores the length of the longest suffix that is also a prefix of the substring up to index 6, or \"abdcdab\". Here, there is a\nsuffix that matches with a prefix: \"abdcdab\". This suffix has a length of 2, so index 6 of the prefix table stores 2.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\nIndex 7 of the prefix table stores the length of the longest suffix that is also a prefix of the substring up to index 7, or \"abdcdabc\". Here, there is\nno longer a suffix that matches a prefix (ignoring the self-match case). Thus, index 7 of the prefix table stores 0. This is also true for index 8.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nThis is our final prefix table for the needle \"abdcdabcb\".", "word_count": 585, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "341babb3-1efa-5141-9253-e4cd34883d0e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 519, "real_page_number": null, "text": "16.7 Knuth-Morris-Pratt (KMP) String Searching\n507\nExample 16.1 Suppose you are using the KMP string searching algorithm to find instances of the string \"aabcbaaabcab\" (the needle)\nwithin a larger body of text (the haystack). Construct the prefix table for this given needle.\nTo solve this problem, find the longest suffix that is also a prefix for the substring up to each index of the string. This process is shown below\n(remember that index 0 of the prefix table will always have a value of 0):\n• Index 1: \"aa\" →longest matching suffix has a length of 1\n• Index 2: \"aab\" →no suffix matches a prefix, so longest matching suffix has a length of 0\n• Index 3: \"aabc\" →no suffix matches a prefix, so longest matching suffix has a length of 0\n• Index 4: \"aabcb\" →no suffix matches a prefix, so longest matching suffix has a length of 0\n• Index 5: \"aabcba\" →longest matching suffix has a length of 1\n• Index 6: \"aabcbaa\" →longest matching suffix has a length of 2\n• Index 7: \"aabcbaaa\" →longest matching suffix has a length of 2\n• Index 8: \"aabcbaaab\" →longest matching suffix has a length of 3\n• Index 9: \"aabcbaaabc\" →longest matching suffix has a length of 4\n• Index 10: \"aabcbaaabca\" →longest matching suffix has a length of 1\n• Index 11: \"aabcbaaabcab\" →no suffix matches a prefix, so longest matching suffix has a length of 0\nThus, the final prefix table is:\nneedle\na\na\nb\nc\nb\na\na\na\nb\nc\na\nb\nprefix table\n0\n1\n0\n0\n0\n1\n2\n2\n3\n4\n1\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nExample 16.2 Suppose you are using the KMP string searching algorithm to find instances of the string \"aabaabaaa\" (the needle) within a\nlarger body of text (the haystack). Construct the prefix table for this given needle.\nTo solve this problem, find the longest suffix that is also a prefix for the substring up to each index of the string. This process is shown below\n(remember that index 0 of the prefix table will always have a value of 0):\n• Index 1: \"aa\" →longest matching suffix has a length of 1\n• Index 2: \"aab\" →no suffix matches a prefix, so longest matching suffix has a length of 0\n• Index 3: \"aaba\" →longest matching suffix has a length of 1\n\"aabaa\" →longest matching suffix has a length of 2• Index 4:\n• Index 5: \"aabaab\" →longest matching suffix has a length of 3\n• Index 6: \"aabaaba\" →longest matching suffix has a length of 4\nRemark: Notice that overlaps are fine, as long as you are overlapping the suffix with the entire string (e.g., for index 2, you cannotnot\nmatch \"aab\" with \"aab\" and claim a longest matching suffix length of 3).\n• Index 7: \"aabaabaa\" →longest matching suffix has a length of 5\n• Index 8: \"aabaabaaa\" →longest matching suffix has a length of 2\nThus, the final prefix table is:\nneedle\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n4\n5\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n¸ 16.7.2\n(✽)Implementing Knuth-Morris-Pratt String Search\nAfter we construct the prefix table for the needle, we can now begin our KMP string search. We will iterate through the haystack, using the\nprefix table to identify how many comparisons we can skip after each mismatch. Let’s return to our previous haystack and needle as an example:\nHaystack:\nabdcdabdcabdbcdbccaabdcdabcabdcdabcbab\nNeedle:\nabdcdabcb\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8", "word_count": 634, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d6458d65-82d5-5ced-8dd9-80db20cf963b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 520, "real_page_number": null, "text": "508\nChapter 16. Strings and Sequences\nWe first begin by comparing the characters in the haystack one by one, until we encounter a mismatch. The first mismatch we encounter is the\nsecond to last letter in the needle, which is ‘d’ in the haystack but ‘c’ in the needle.\nHaystack:\nabdcdab d cabdbcdbccaabdcdabcabdcdabcbab\nNeedle:\nabdcdab c b\nAfter encountering this mismatch, we go to the prefix table and look up the value associated with the character directly before the mismatch. In\nthis case, the character before the mismatch is the ‘b’ highlighted below:\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nThe value in the prefix table is 2, so we will align the character at index 2 of the needle (the ‘d’) with the mismatched character in the haystack\n(shown below). Then, we will continue comparing the characters in the haystack, mismatch. We do not comparestarting from the position of the\nthe first \"ab\" again because we already know they match! This allows us to traverse the haystack without ever moving backwards to check\ncharacters that have already been compared before.\nHaystack:\nabdcdab d cabdbcdbccaabdcdabcabdcdabcbab\nNeedle:\n→ab d cdabcb\nThe next mismatch happens at this point:\nHaystack:\nabdcdabdc a bdbcdbccaabdcdabcabdcdabcbab\nNeedle:\nabdc d abcb\nThe character in the needle directly before the mismatch is the ‘c’, which has a prefix table value of 0.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nThus, we will align the character at index 0 of the needle with the mismatched character in the haystack, as shown:\nHaystack:\nabdcdabdc a bdbcdbccaabdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb\nThe next mismatch happens at this point:\nHaystack:\nabdcdabdcabd b cdbccaabdcdabcabdcdabcbab\nNeedle:\ndabcb→abd c\nThe character directly before the mismatch is the ‘d’, which has a prefix value of 0.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nThus, we will align the character at index 0 of the needle with the mismatched character in the haystack, as shown:\nHaystack:\nabdcdabdcabd b cdbccaabdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb\nNotice here that there is a mismatch on the first character of the needle. When this happens, you would just slide the needle forward by one\ncharacter, since there are no comparisons that you are able to immediately skip.\nHaystack:\nabdcdabdcabdb c dbccaabdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb (mismatch on first character, so slide window forward by one)", "word_count": 451, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bb35e44b-28e3-50d3-8856-504cfbcea364", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 521, "real_page_number": null, "text": "16.7 Knuth-Morris-Pratt (KMP) String Searching\n509\nHaystack:\nabdcdabdcabdbc d bccaabdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb (mismatch on first character, so slide window forward by one)\nHaystack:\nabdcdabdcabdbcd b ccaabdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb (mismatch on first character, so slide window forward by one)\nHaystack:\nabdcdabdcabdbcdb c caabdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb (mismatch on first character, so slide window forward by one)\nHaystack:\nabdcdabdcabdbcdbc c aabdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb (mismatch on first character, so slide window forward by one)\nHaystack:\nabdcdabdcabdbcdbcc a abdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb\nIn this window, the next mismatch is at the second character:\nHaystack:\nabdcdabdcabdbcdbcca a bdcdabcabdcdabcbab\nNeedle:\n→a b dcdabcb\nThe prefix value of the character before the mismatch is 0:\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nThus, we will align the character at index 0 of the needle with the mismatched character in the haystack, as shown:\nHaystack:\nabdcdabdcabdbcdbcca a bdcdabcabdcdabcbab\nNeedle:\n→a bdcdabcb\nAt this point, our next mismatch occurs at the last letter of the needle:\nHaystack:\nbdcdabcbababdcdabdcabdbcdbccaabdcdabc a\nNeedle:\n→abdcdabc b\nThe character directly before the mismatch is the ‘c’, which has a prefix value of 0.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nThus, we will align the character at index 0 of the needle with the mismatched character in the haystack, as shown.\nHaystack:\nabdcdabdcabdbcdbccaabdcdabc a bdcdabcbab\nNeedle:\n→a bdcdabcb\nWe now have a match, so we record its position. To continue finding matches, we would align the character in the needle whose index is the\nlast value in the prefix table (in this case, index 0) with the character directly after the current window of the haystack and repeat the above\nprocedure, as shown below. We use the last value of the prefix table to determine where to continue comparisons; for instance, if the last value\nin the prefix table had been something else like 2, we would instead align the next character in the haystack with index 2 of our needle.\nneedle\na\nb\nd\nc\nd\na\nb\nc\nb\nprefix table\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nHaystack: ... ccaabdcdabcabdcdabcb a b ...\nNeedle:\n→a bdcdabcb", "word_count": 406, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f55712a6-9142-5ec9-ab83-2f645c9ee307", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 522, "real_page_number": null, "text": "510\nChapter 16. Strings and Sequences\nThe code for the KMP string search algorithm is provided below:\n1\nstd::vector<size_t> compute_prefix_array(const std::string& needle) {\n2\nstd::vector<size_t> prefix_arr(needle.length());\n3\nsize_t j = 0;\n4\nfor (size_t i = 1; i < needle.length(); ++i) {\n5\nwhile (j > 0 && needle[j] != needle[i]) {\n6\nj = prefix_arr[j - 1];\n7\n} // while\n8\nif (needle[j] == needle[i]) {\n9\n++j;\n10\n} // if\n11\nprefix_arr[i] = j;\n12\n} // for i\n13\nreturn prefix_arr;\n14\n} // compute_prefix_array()\n15\n16\nstd::vector<size_t> knuth_morris_pratt(const conststd::string& needle, std::string& haystack) {\n17\nstd::vector<size_t> matches;\n18\nstd::vector<size_t> prefix_arr = compute_prefix_array(needle);\n19\nsize_t idx = 0;\n20\nfor (size_t curr = 0; curr < haystack.length(); ++curr) {\n21\nwhile (idx > 0 && needle[idx] != haystack[curr]) {\n22\nidx = prefix_arr[idx - 1];\n23\n} // while\n24\nif (needle[idx] == haystack[curr]) {\n25\n++idx;\n26\n} // if\n27\nif (idx == needle.length()) {\n28\nmatches.push_back(curr - needle.length() + 1);\n29\n} // if\n30\n} // for curr\n31\nreturn matches;\n32\n} // knuth_morris_pratt()\nTo illustrate how this works, let’s walk through the code for building a prefix table (lines 1-14) using the following example:\nneedle\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni j, i jWe first initialize two indices, and that are used to iterate through the needle. is initially set to 1 and is initially set to 0.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nj while if true, jBecause is 0, the loop on line 5 does not run. We hit the check on line 8, which returns so is incremented to index 1, and\nprefix_arr[1] jis set to 1.=\nneedle\nji\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8", "word_count": 344, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c07c9d3e-a79e-5665-bf25-bd9047ef9800", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 523, "real_page_number": null, "text": "16.7 Knuth-Morris-Pratt (KMP) String Searching\n511\ni foris then incremented for the next iteration of the loop.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\nj needle[j] needle[i] j prefix_arr[j - 1],is greater than 0, and and do not match. As a result, gets set to or index 0.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni j if prefix_arr[2] j,The characters at and still do not match, so the check on line 8 does not run, and gets set to or 0.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni for j whileis incremented for the next iteration of the loop. is 0, so the loop on line 5 does not run.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni j j prefix_arr[3]The characters at positions and are the same (both ‘a’), so is incremented to index 1, and is also set to 1.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni foris incremented for the next iteration of the loop.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8", "word_count": 288, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d35c20c5-ff0d-5e2f-a609-9818fac291a4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 524, "real_page_number": null, "text": "512\nChapter 16. Strings and Sequences\ni j while if true, jThecharactersatpositions and arethesame(both‘a’),sothe loopdoesnotrun. The conditionreturns so isincremented\nprefix_arr[4]to index 2, and is also set to 2.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni foris incremented for the next iteration of the loop.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni j while if true, jThecharactersatpositions and arethesame(both‘b’),sothe loopdoesnotrun. The conditionreturns so isincremented\nprefix_arr[5]to index 3, and is also set to 3.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRepeating the process for the next two iterations, we would end up with the following:\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni for i jisincrementedonemoretimeforthelastiterationofthe loop. Thecharactersatpositions and arenotthesameaftertheincrementation,\nj while j prefix_arr[j - 1],and is not 0. Thus, the loop runs, and moves back to or the prefix value of the character directly before\nprefix_arr[j - 1] jthe mismatch. In this case, is 2, so moves back to index 2.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n7\n8", "word_count": 327, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "429418b0-c56c-586c-b125-bea25c98bdf4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 525, "real_page_number": null, "text": "16.7 Knuth-Morris-Pratt (KMP) String Searching\n513\nThe characters at and still do not match, so the loop continues iterating, and moves back to again. In thisi j while j prefix_arr[j - 1]\nprefix_arr[j - 1] jexample, is 1, so moves back to index 1.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n7\n8\ni j while ifThe characters at and now match, so the loop completes. Since the characters at these two positions are the same (both ‘a’), the\ntrue, j prefix_arr[8]statement returns and is incremented to index 2. The value of is then set to 2 as well.\nneedle\nj\ni\na\na\nb\na\na\nb\na\na\na\nprefix table\n0\n1\n0\n1\n2\n3\n4\n5\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\nThis is our final prefix table. The code for the actual KMP search (lines 16-32) is fairly straightforward and closely follows the description of the\nforalgorithm on the previous few pages. The loop on line 20 iterates through the haystack one character at a time. If there is a mismatch\nwhilebetween the needle and the haystack, the loop on line 21 correctly aligns the needle with the haystack based on the prefix value of\n(prefix_arr[idx - 1]). ifthe character directly before the mismatch The check on line 24 takes care of the comparison process,\nincrementing the index of the needle as long as it matches the corresponding character in the haystack. If the index of the needle ever reaches\nthe end (line 27), the entire window must have matched, so we record the position of the match (i.e., the haystack index of the first character in\nthe match) in the output vector. The code is reproduced below for convenience.\n1\nstd::vector<size_t> compute_prefix_array(const std::string& needle) {\n2\nstd::vector<size_t> prefix_arr(needle.length());\n3\nsize_t j = 0;\n4\nfor (size_t i = 1; i < needle.length(); ++i) {\n5\nwhile (j > 0 && needle[j] != needle[i]) {\n6\nj = prefix_arr[j - 1];\n7\n} // while\n8\nif (needle[j] == needle[i]) {\n9\n++j;\n10\n} // if\n11\nprefix_arr[i] = j;\n12\n} // for i\n13\nreturn prefix_arr;\n14\n} // compute_prefix_array()\n15\n16\nstd::vector<size_t> knuth_morris_pratt(const conststd::string& needle, std::string& haystack) {\n17\nstd::vector<size_t> matches;\n18\nstd::vector<size_t> prefix_arr = compute_prefix_array(needle);\n19\nsize_t idx = 0;\n20\nfor (size_t curr = 0; curr < haystack.length(); ++curr) {\n21\nwhile (idx > 0 && needle[idx] != haystack[curr]) {\n22\nidx = prefix_arr[idx - 1];\n23\n} // while\n24\nif (needle[idx] == haystack[curr]) {\n25\n++idx;\n26\n} // if\n27\nif (idx == needle.length()) {\n28\nmatches.push_back(curr - needle.length() + 1);\n29\n} // if\n30\n} // for curr\n31\nreturn matches;\n32\n} // knuth_morris_pratt()", "word_count": 486, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a78fc8bd-b34e-52ad-a023-264095bdfecc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 526, "real_page_number": null, "text": "514\nChapter 16. Strings and Sequences\n¸ 16.7.3\n(✽)Knuth-Morris-Pratt Complexity Analysis\nWhat are the time and space complexities of the KMP algorithm? The KMP process consists of two steps: constructing the prefix table and\niterating through the haystack. If the needle has length 𝑛and haystack has length ℎ, the overall time complexity of KMP is Θ(𝑛+ℎ), since\nbuilding the prefix table takes time, and iterating through the haystack takes time.Θ(𝑛) Θ(ℎ)\nwhileWhy is this the case? First, let’s consider the work required to build the prefix table. Since the number of times the inner loop runs\nfor(line 5) depends on the outer loop (line 4), we have a loop dependency, so we will analyze the runtime of the two loops together. Notice\nj jthat starts at 0 and is only incremented per iteration of the outer loop (on line 9). Thus, the value of cannot exceed 𝑛−1,at most once\niwhich is the total number of iterations the outer loop runs. Furthermore, notice that is incremented once per iteration of the outer loop.always\nj j i, j i.Because is incremented once per iteration of the outer loop, and starts off smaller than the value of cannot ever be larger thanat most\n3\nsize_t j = 0;\n4\nfor (size_t i = 1; i < needle.length(); ++i) {\n5\nwhile (j > 0 && needle[j] != needle[i]) {\n6\nj = prefix_arr[j - 1];\n7\n} // while\n8\nif (needle[j] == needle[i]) {\n9\n++j;\n10\n} // if\n11\nprefix_arr[i] = j;\n12\n} // for i\nprefix_arr[i] j j i, prefix_arr[j - 1]Since is set to (line 11), and cannot be larger than we can conclude that must be less than\nj. while j, j prefix_arr[j - 1]As a result, each iteration of the inner loop ends up the value of since we are assigning todecreasing\nj). j(whose value must be smaller than the original value of However, in order for to decrease in value from the assignment on line 6, it\njmust have increased its value earlier in the algorithm (for example, in order for to fall from a value of 2 to 0 in the inner loop, it must have\njincremented to a value of 2 sometime earlier in the algorithm, which requires at least two iterations of the outer loop to run). Because cannot\nj ifall below 0, the inner loop can only decrease at most as many times as the total increase of in the outer loop. Thus, the number of iterations\nwhile icompleted by the inner loop is bounded by the total increase of in the outer loop, or Θ(𝑛). Since the remaining work done in𝑛−1=\nforthe loop (lines 9 to 11) takes time, the total work done by the entire loop dependency (lines 4 to 12) is Θ(𝑛).Θ(𝑛)+Θ(1)Θ(1) =\nWe can use a similar analysis to show that the time complexity of iterating through the haystack (after calculating the prefix table) is Θ(ℎ).\nfor idx prefix_arr[idx - 1]For each iteration of the loop on line 20, the value of increases by at most 1 (on line 25). Because is\nidx, while idx. idxalways less than the inner loop on line 21 always decreases the value of Since cannot be negative, the number of times\nidx idx, forwe can decrease is bounded by the value of which is itself bounded by the number of times the outer loop has run. Since the\nforouter loop runs at most times, and the remaining work in the outer loop runs in constant time, the time complexity of the entire loopΘ(ℎ)\ndependency (lines 20 to 30) is also Θ(ℎ).\n19\nsize_t idx = 0;\n20\nfor (size_t curr = 0; curr < haystack.length(); ++curr) {\n21\nwhile (idx > 0 && needle[idx] != haystack[curr]) {\n22\nidx = prefix_arr[idx - 1];\n23\n} // while\n24\nif (needle[idx] == haystack[curr]) {\n25\n++idx;\n26\n} // if\n27\nif (idx == needle.length()) {\n28\nmatches.push_back(curr - needle.length() + 1);\n29\n} // if\n30\n} // for curr\nLastly, it should be noted that the auxiliary space used by the KMP algorithm is Θ(𝑛), where 𝑛is the length of the needle. This is because we\nneed to construct a separate prefix table of length 𝑛to run the algorithm.\nString Search Method\nAverage-Case Time\nWorst-Case Time\nKnuth-Morris-Pratt\nΘ(𝑛+ℎ)\nΘ(𝑛+ℎ)", "word_count": 748, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b55c9625-dbbb-55d7-9ca7-ee69c7608e3c", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 527, "real_page_number": null, "text": "16.7 Knuth-Morris-Pratt (KMP) String Searching\n515\nChapter 16 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Consider the following four strings:\n1\nstd::string str1 = \"a\";\n2\nstd::string str2 = \"ab\";\n3\nstd::string str3 = \"ab9\";\n4\nstd::string str4 = \"ab10\";\nWhich of the following correctly lists these strings in lexicographical order?\nstr1 < str2 < str3 < str4A)\nstr1 < str2 < str4 < str3B)\nstr4 < str3 < str1 < str2C)\nstr4 < str3 < str2 < str1D)\nE) None of the above\nstd::lexicographical_compare()2. For which of the following would return false? Assume that the first word is entered as an\nargument before the second word.\n\"Apple\" \"apple\"A) Comparing the strings and\n\"eecs280\" \"eecs281\"B) Comparing the strings and\n\"lettuce\" \"lettuce\"C) Comparing the strings and\nD) More than one of the above\nE) None of the above\n3. Which of the following statements is/are TRUE?\n(char*), std::equal() operator==.I. If you want to compare the equality of two C strings you should choose over\nstd::equal()II. can be used to check if a string shares a matching prefix with another string.\noperator< operator==III. Only and are needed to implement the other four comparison operators.\nA) I only\nB) II only\nC) I and II only\nD) II and III only\nE) I, II, and III\n4. In which of the following cases will\nstd::equal(InputIterator1 first1, InputIterator1 last1, InputIterator2 first2)\nInputIterator1 InputIterator2always return false? Note that and represent iterators to two different containers.\n[first1, last1) first2A) When the range is larger than the container of\n[first1, last1) first2B) When the range is smaller than the container of\nfirst1 first2C) When the container of and the container of are identical\nfirst1 first2D) When the container of and the container of are identicalnot\nstd::equal()E) None of the above (i.e., can return true for all of the above)\n5. Consider the following snippet of code:\n1\nstd::string str = \"This pizza is really good!!!\";\n2\nsize_t size_init = str.size();\n3\n4\nauto it = std::unique(str.begin(), str.end());\n5\nstr.resize(it - str.begin());\n6\nsize_t size_final = str.size();\n7\n8\nstd::cout << size_init - size_final << std::endl;\nstd::unique()What does the above code output? one pastHint: returns an iterator that points the last element that is not removed.\n0A)\n4B)\n5C)\n9D)\n10E)", "word_count": 440, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "28d44f13-8a5c-572e-a030-f1dbc574fd86", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 528, "real_page_number": null, "text": "516\nChapter 16. Strings and Sequences\n6. Consider the following snippet of code:\n1\nconstexpr size_t size = 10;\n2\nchar a1[size] = {'E', 'E', 'C', 'S', '3', '7', '\\0', 'F', '2', '1'};\n3\nchar a2[size];\n4\nfor (size_t i = 0; i < size; ++i) {\n5\na2[i] = a1[size - i - 1];\n6\n} // for i\n7\nstd::cout << a2 << std::endl;\nWhat does the above code output?\nEECS37A)\n73SCEEB)\nF21C)\n12FD)\nSegmentation faultE)\n7. Consider the following mystery function:\n1\nbool mystery(const std::string& str) {\n2\nreturn std::equal(str.rbegin(), str.rbegin() + str.size() / 2, str.begin());\n3\n} // mystery()\nFor which of the following strings would this function return true?\nabcdefA)\ntartarB)\nkayakC)\nbananaD)\nE) More than one of the above\nIf you are searching for a string of length 𝑛within a string of length ℎusing brute force string searching, the average-case time complexity8.\nwould be ___________ and the worst-case time complexity would be ___________.\nA) Θ(𝑛), Θ(ℎ)\nB) Θ(𝑛), Θ(𝑛+ℎ)\nΘ(𝑛2)C) Θ(𝑛),\nΘ(ℎ2)D) Θ(ℎ),\nE) Θ(ℎ), Θ(𝑛ℎ)\n9. If you are searching for a string of length 𝑛within a string of length ℎusing Rabin-Karp string searching, the worst-case time complexity\nwould be ___________.\nA) Θ(1)\nB) Θ(𝑛)\nC) Θ(ℎ)\nD) Θ(𝑛+ℎ)\nE) Θ(𝑛ℎ)\nstd::sort() std::vector<>10. What is the worst-case time complexity of calling on a of 𝑛strings, each of length 𝑚characters?\nA) Θ(𝑚log(𝑛))\nB) Θ(𝑛log(𝑛))\nC) Θ(𝑚𝑛log(𝑛))\nΘ(𝑚2𝑛log(𝑛))D)\nΘ(𝑚𝑛2)E)\n11. Which of the following is the best explanation as to why string fingerprinting is useful?\nA) It reduces the cost of comparing strings by only checking the first character of each string rather than the entire string\nB) It reduces the cost of comparing strings by traversing through only one string instead of both strings during a comparison\nC) It reduces the cost of comparing strings by assigning a unique integer that can be used to distinctively identify different strings\nD) It reduces the cost of comparing strings by performing a binary search instead of a linear search on each stringΘ(log(𝑛)) Θ(𝑛)\nE) It reduces the cost of comparing strings by mapping strings to an integer identifier that can be more efficiently compared\n12. You are given the Rabin-Karp fingerprints of two non-empty strings. The original strings are unknown, but their fingerprints are the same.\nWhich of the following statements are guaranteed to be true?\nI. The strings are the same.\nII. The strings contain the same characters.\nIII. The strings have the same length.\nIV. The strings start with the same letter.\nA) III only\nB) IV only\nC) II and III only\nD) I, II, III, and IV\nE) None of the above", "word_count": 456, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e40ed60b-94d4-558d-a5af-1f00e64cc3f9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 529, "real_page_number": null, "text": "16.7 Knuth-Morris-Pratt (KMP) String Searching\n517\n13. You are given the Rabin-Karp fingerprints of two non-empty strings. The original strings are unknown, but their fingerprints are different.\nUnder which of the following conditions could it be possible for these two strings to be identical?\nA) The two strings are palindromes (i.e., spelled the same forward and backward)\nB) The length of each string is a prime number\nC) The number chosen as the fingerprint modulus is a prime number\nD) The number chosen as the fingerprint modulus is a power of two\nE) None of the above\n14. Given a sorted array of 𝑛strings, each of length 𝑚, what is the worst-case time complexity of finding whether a given string of length 𝑚\nexists in the array, if you use the most efficient algorithm?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑚))\nD) Θ(𝑚log(𝑛))\nE) Θ(𝑛𝑚)\n15. You are given a string 𝑆of length 𝑛, and you want to return the longest repeated substring in 𝑆. A repeated substring is a substring that\noccurs more than once in a string, and occurrences of a repeated substring may overlap. If you the length of the longestalready know\nrepeated substring in 𝑆, what is the time complexity of returning a longest repeated substring, if you use the most efficientaverage-case\nalgorithm? For example, given the string 𝑆= \"soonoonoontm\", you would return \"oonoon\", since it is the longest substring that occurs\nmore than once in 𝑆: \"soonoonoontm\" and \"soonoonoontm\".\nA) Θ(𝑛)\nB) Θ(𝑛log(𝑛))\nΘ(𝑛log2(𝑛))C)\nΘ(𝑛2)D)\nΘ(𝑛2E) log(𝑛))\n(const char*)16. You are given two identical strings of length 𝑛, except that one string is in the form of a C string while the other is in the\nstd::string. strlen()form of a What are the time complexities of finding the length of the two strings, provided that you use on\n.size() std::string?the C string and on the\nA) C string: Θ(1),\nstd::string: Θ(1)\nB) C string: Θ(1),\nstd::string: Θ(𝑛)\nC) C string: Θ(𝑛),\nstd::string: Θ(1)\nD) C string: Θ(𝑛),\nstd::string: Θ(𝑛)\nE) None of the above\nstd::lexicographical_compare()17. Consider the following possible implementation of in the standard algorithm library:\n1\ntemplate <class classInputIterator1, InputIterator2>\n2\nbool lexicographical_compare(InputIterator1 first1, InputIterator1 last1,\n3\nInputIterator2 first2, InputIterator2 last2) {\n4\nfor (void)(; (first1 != last1) && (first2 != last2); ++first1, ++first2) {\n5\nif (__I__)\n6\nreturn true;\n7\nif (__J__)\n8\nreturn false;\n9\n} // for\n10\n11\nreturn (first1 == last1) && (first2 != last2);\n12\n} // lexicographical_compare()\nI JWhich of the following could be correctly inserted into the missing condition statements and on lines 5 and 7?\nI: *first1 < *first2A)\nJ: *first1 == *first2\nI: *first1 == *first2B)\nJ: *first1 < *first2\nI: *first1 > *first2C)\nJ: *first2 > *first1\nI: *first1 < *first2D)\nJ: *first2 < *first1\nE) None of the above", "word_count": 483, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2487ce7a-4caa-556f-9ddb-c24faae54d78", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 530, "real_page_number": null, "text": "518\nChapter 16. Strings and Sequences\n18. You are given two strings, as well as a fingerprint function that can be used to convert a string into an integer in constant time.\nint32_t get_string_fingerprint(const std::string& str);\nYour friend implements the following function, which they claim is able to check if two strings are equal in constant time.\n1\nbool are_strings_equal(const conststd::string& s1, std::string& s2) {\n2\nint32_t fingerprint1 = get_string_fingerprint(s1);\n3\nint32_t fingerprint2 = get_string_fingerprint(s2);\n4\nif (fingerprint1 == fingerprint2) {\n5\nreturn true;\n6\n} // if\n7\nreturn false;\n8\n} // are_strings_equal()\nIs your friend’s implementation guaranteed to work as intended? Why or why not?\nstr,Implement a function that, given a string returns the longest prefix of the string that is also a suffix (excluding the entire word itself).19.\nIf no such prefix exists, return an empty string.\n(\"edited\")\"edited\", \"ed\",Example: Given the string you would return since that is the longest prefix that is also a suffix of the\n(\"edited\").same word\nlongest_prefix_suffix(conststd::string std::string& str);\nΘ(𝑛2),You should implement your solution so that the average-case time complexity is better than where 𝑛is the length of the input string.\nΘ(𝑛2)(Reducing the worst-case time complexity below is also possible, but not required.)\nChapter 16 Exercise Solutions\n\"a\", \"ab\", \"ab10\", \"ab9\" (\"ab10\" \"ab9\"1. The correct answer is (B). The correct order is then then then comes before since 9 is\nlarger lexicographically compared to 1).\n2. Thecorrectansweris(C).Option(A)returnstruebecausecapitalletterscomebeforelowercaseletterslexicographically. Option(B)returns\n(\"0\" \"1\") \"eecs280\" \"eecs281\"true because the first character difference between the two strings and puts before lexicographically.\nstd::lexicographical_compare()Option (C) returns false because returns false if two strings are identical.\nstd::equal() operator==3. The correct answer is (E). All three statements are true. works on C strings while does not, and it can\nalso be used to check for a matching prefix (since you can pass in an iterator range representing the prefix you want to search). Statement III\nis true since only < and == are needed to implement the other four comparison operators:\n• a != b is the same as !(a == b)\n• a > b is the same as b < a\n• a <= b is the same as !(b < a)\n• a >= b is the same as !(a < b)\n4. The correct answer is (A). If you want to find an input range 𝐴within another sequence 𝐵, then you would never be able to find 𝐴\nstd::equal()in 𝐵if the length of 𝐵is smaller than 𝐴. Thus, is guaranteed to return false if the input range you are searching for\n[first1, last1) first2).is larger than the container you are searching in (represented by\nstd::unique()5. The correct answer is (C). The function eliminates all but the first element from every consecutive group of equivalent\npizza really[first, last). str \"This iselements in the provided range Thus, the following five letters of are removed:\ngood!!!\".\na1 a2.6. The correct answer is (D). The loop on line 4 reverses the contents of and inserts them into Therefore, when the loop completes its\na2 {'1', '2', 'F', '\\0', '7', '3', 'S', 'C', 'E', 'E'}. a2last iteration, the contents of are Printing would read\n'\\0', \"12F\".every character up to the first sentinel character so the output would be\n7. The correct answer is (C). The function returns true if the back half of the string is equal to the front half (notice the use of reverse iterators\nin the input range). Thus, the function would only return true if given a palindrome (a word spelled the same forward and backward), which\nis only true for option (C).", "word_count": 633, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "6ba72854-2f24-536a-9e20-cb61240f21d2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 531, "real_page_number": null, "text": "16.7 Knuth-Morris-Pratt (KMP) String Searching\n519\n8. The correct answer is (E). With brute force string searching, we take the needle of length 𝑛and use it as a \"window\" to compare with\nsubstrings of length 𝑛within the haystack. For instance, if we had a needle of length 4 and a haystack of length 20, we would first check if\nthe needle matches the haystack from characters 1-4, then if the needle matches with characters 2-5, then 3-6, then 4-7, etc., until we find a\nmatch. In the average case, we would be able to tell quickly if a match is found, as we expect the first letter of the needle and the haystack\nsubstring to be different if there is no match. Thus, we would check the first letter of the needle with each letter of the haystack, moving\nforward if a match is not found. We would only have to do approximately ℎcomparisons while doing this, where ℎis the length of the\nhaystack, so this gives us an average-case time complexity of Θ(ℎ). However, in the worst case, we would have to check all 𝑛characters\nin the needle for substring of length 𝑛in the haystack. Thus, for the approximately ℎcomparisons we have to do when traversingevery\nthrough the haystack, we would have to do an additional 𝑛comparisons in the worst case (e.g., in our previous example, we would have to\ncompare the 4 characters of the needle with haystack character from 1-4 to see if there is a match, then with haystack characterevery every\nfrom 2-5, then 3-6, then 4-7, etc.). This gives us a worst-case time complexity of Θ(𝑛ℎ). See section 16.5 for a more in-depth explanation.\n9. The correct answer is (E). Although improbable, two strings having the same fingerprint does not guarantee that they are equal. As a\nresult, even if you encounter a fingerprint match while searching the string, you would still need to do a string comparison to determine if\nthe two strings are actually equal. In the worst case, every fingerprint is a false positive, which forces you to do a comparison for allΘ(𝑛)\npositions of the haystack. This yields the same worst-case time complexity as brute force, which is Θ(𝑛ℎ).Θ(ℎ)\n10. The correct answer is (C). The worst-case time complexity of sorting a vector of 𝑛elements is if comparisons take time.Θ(𝑛log(𝑛)) Θ(1)\nIn our case, the worst-case time complexity of comparing two strings of length 𝑚is Θ(𝑚). The overall worst-case time complexity occurs\nwhen every comparison during the sort takes time, which yields a result of Θ(𝑚×𝑛log(𝑛)).Θ(𝑚)\n11. The correct answer is (E). String fingerprinting maps strings to an integer comparator that improves the efficiency of string comparisons.\n\"aaaaaaaaaa\" \"aaaaaaaaab\"For instance, checking if the strings and are identical may be inefficient using the standard method of\ncomparing characters up to the first mismatch. If we map these strings to a unique fingerprint, however, we can simplify this process by\nchecking if the two integers are identical, as integer comparisons are a lot more lightweight. Note that string fingerprints are not guaranteed\nto be unique, as two different strings can still map to the same integer value.\n12. The correct answer is (E). If two strings have the same fingerprint, they may be the same, but that is not a guarantee. This is due to the\nchance that two different strings could still be mapped to the same fingerprint value. You cannot conclude any of the statements provided\nwithout actually checking the characters of the two strings for equivalence.\n13. The correct answer is (E). If two strings have different fingerprints, the contents of the strings themselves also be different, as it ismust\nimpossible for the same string to map to two different fingerprint values. This holds in all cases, so the provided scenarios are all irrelevant.\n14. The correct answer is (D). If the array is sorted, you can find whether a string exists or not using a binary search, which involves Θ(log(𝑛))\ncomparisons. Each comparison takes time, so the overall time complexity would be Θ(𝑚log(𝑛)).Θ(𝑚)\n15. The correct answer is (A). Since repeated substrings may overlap, Rabin-Karp can be a useful strategy for this problem. To solve this\nproblem efficiently, you can keep track of a rolling fingerprint value for substrings of length 𝑘, where 𝑘is the length of the longest repeated\nsubstring. Slide through the original string, and once you find a fingerprint you have seen before, check the strings for equality and return\nif the strings are equal. If the fingerprint function is good (and there are few collisions), each window in the string can be checked in\naverage-case time. Since there are windows, the average-case time complexity of the entire problem is Θ(𝑛).Θ(𝑛)Θ(1)\nstd::string16. The correct answer is (C). A stores information internally that allows it to calculate the length of its stored string in Θ(1).\nThis is not true for C strings, which are essentially plain character arrays — to find its length, you would need to iterate over the characters\nuntil you encounter the sentinel character, which would take time for a C string of length 𝑛.Θ(𝑛)\nstd::lexicographical_compare()17. The correct answer is (D). The function returns true if the contents of the first iterator range\n*first1compares lexicographically less than the contents of the second iterator range. Therefore, we want to return true if is less than\n*first2, *first1 *first2.and false if is greater than This matches option (D).", "word_count": 918, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "41741986-55ab-593d-ae02-b85f7c542cc7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 532, "real_page_number": null, "text": "520\nChapter 16. Strings and Sequences\nget_string_fingerprint()18. Your friend’s implementation is not guaranteed to work in the case where returns the same fingerprint\nfor two different strings. Even if two fingerprints match, the actual contents of the strings should still be compared with each other to ensure\nthat the strings themselves also match.\n19. One strategy for solving this problem is to incrementally compare the ends of the input string and keep track of the longest matching\n\"abcdabc\".prefix/suffix encountered so far. For instance, consider the string Here, we would perform the following comparisons:\n1:𝑛=\n\"abcdabc\"\n\"abcdabc\"\n✗\n2:𝑛=\n\"abcdabc\"\n\"abcdabc\"\n✗\n3:𝑛=\n\"abcdabc\"\n\"abcdabc\"\n✓\n4:𝑛=\n\"abcdabc\"\n\"abcdabc\"\n✗\n5:𝑛=\n\"abcdabc\"\n\"abcdabc\"\n✗\n6:𝑛=\n\"abcdabc\"\n\"abcdabc\"\n✗\nΘ(𝑛2)However, comparing strings can be expensive, and this would give us an algorithm on average. If we recall the rolling hash strategy\nemployed in the Rabin-Karp algorithm, we can actually calculate the fingerprint of the first 𝑛characters of a string in time, providedΘ(1)\nthat we know the fingerprint of the first characters. See the first remark of section 16.6 for why this is the case, but the idea is that we𝑛−1\ncan easily convert our prefixes and suffixes into integers, which are more efficient to compare than the contents of the strings themselves.\nGiven a string of length 𝑛with characters that have ASCII values 𝑐1,𝑐2,…,𝑐𝑛and a base multiplier of 𝑘, a string’s fingerprint can be\ncalculated using the following equation:\n𝑘𝑛−1𝑐1+𝑘𝑛−2𝑐2+𝑘𝑛−3𝑐3+…+𝑘1𝑐𝑛−1+𝑘0𝑐𝑛\nIf we want to increase the length of our prefix by one character to the right, we would just need to multiply our existing prefix fingerprint\nby the base multiplier 𝑘and add the value of the new character (e.g., for a string of length 2, the fingerprint of the prefix of length 2 is\n𝑘1𝑐1+𝑘0𝑐2, 𝑘2𝑐1+𝑘1𝑐2+𝑘0𝑐3, (𝑘1𝑐1+𝑘0𝑐2) 𝑘2𝑐1+𝑘1𝑐2+𝑘0𝑐3the fingerprint of the prefix of length 3 is and for →3).∗𝑘+𝑐3 𝑛== 2\nOn the other hand, if we want to increase the length of our suffix by one character to the left, we would just need to multiply the new\n𝑘𝑚andcharacter by add it to the existing suffix fingerprint value, where 𝑚is the distance from the last character (e.g., for a string\n𝑘1𝑐2 𝑘0𝑐3, 𝑘2𝑐1 𝑘1𝑐2 𝑘0𝑐3,of length 3, the fingerprint of the suffix of length 2 is the fingerprint of the suffix of length 3 is and+ + +\n(𝑘1𝑐2+𝑘0𝑐3)+𝑘2𝑐1 𝑘2𝑐1+𝑘1𝑐2+𝑘0𝑐3 for →3). An implementation of this solution is shown below; note that a prefix and suffix𝑛== 2\nsharing the same hash value does not imply that they are necessarily equal (due to the potential of two differing strings mapping to the same\ninteger value via a hash collision), so a string comparison must still be made if the fingerprints of a prefix and suffix match.\n1\nlongest_prefix_string(conststd::string std::string& str) {\n2\nconstexpr int32_t base = 128;\n// this is k, or our multiplier (can be different)\n3\nconstexpr int64_t prime = 281280281;\n// large prime to be used as modulus (can be different)\n4\n5\nint64_t prefix_hash = 0, suffix_hash = 0, suffix_multiplier = 1;\n6\nint32_t best_prefix_length = 0;\n// length of best match encountered\n7\nfor (size_t i = 0; i < str.length() - 1; ++i) {\n8\nprefix_hash = (base prefix_hash + str[i]) % prime;*\n9\nsuffix_hash = (suffix_hash + suffix_multiplier str[str.length() - i - 1]) % prime;*\n10\nsuffix_multiplier = suffix_multiplier base % prime;*\n11\nif (prefix_hash == suffix_hash && str.substr(0, i + 1) == str.substr(str.length() - i - 1)) {\n12\nbest_prefix_length = i + 1;\n13\n} // if\n14\n} // for\n15\n16\nreturn str.substr(0, best_prefix_length);\n17\n} // longest_prefix_string()\nNot required for this problem:\nSince string comparisons cannot be avoided even in the case where two fingerprints match, the worst-case time complexity of this rolling\nΘ(𝑛2).hash solution is still If we want to drop this worst-case complexity down, we can instead use the KMP approach, which allows us to\nobtain a linear time solution. Notice that the prefix table that needs to be constructed for KMP solves the same prefix problem: each index 𝑖\nof the prefix table stores the length of the longest suffix that is also a prefix of the substring up to index 𝑖. Thus, the solution for this problem\ncan be obtained by simply constructing the prefix table and returning the prefix of that length. An implementation is shown below (this is\nthe same implementation as the prefix table code explained in section 16.7 — refer to that section for an explanation of how this works):\n1\nlongest_prefix_string(conststd::string std::string& str) {\n2\nstd::vector<size_t> prefix_arr(str.length());\n3\nsize_t j = 0;\n4\nfor (size_t i = 1; i < str.length(); ++i) {\n5\nwhile (j > 0 && str[j] != str[i]) {\n6\nj = prefix_arr[j - 1];\n7\n} // while\n8\nif (str[j] == str[i]) {\n9\n++j;\n10\n} // if\n11\nprefix_arr[i] = j;\n12\n} // for i\n13\n14\nreturn str.substr(0, j);\n15\n} // longest_prefix_string()", "word_count": 860, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2473191f-5c42-5ae2-8a9b-d74ced55f8a6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 533, "real_page_number": null, "text": "Chapter 17\nHash Tables and Collision Resolution\n17.1\nThe Dictionary ADT\nSuppose you wanted to write a program for a local grocery store that, when a user enters in a shopping item, returns the price of the item that\nwas entered. When the program begins, it loads in a data set containing the prices of all the items in the store by item name. How would you\nstore this data in your program to support this lookup?\nFood Item\nPrice\nApple\n$3.99\nAvocado\n$4.99\nBanana\n$2.49\nFlour\n$2.29\nGinger\n$2.69\nMilk\n$2.09\nTofu\n$3.79\n\"Tofu\"User queries:\n3.79Program returns:\nOne way to accomplish this is to use an abstract data type known as a dictionary. A dictionary is a lookup container comprised of a collection\nof key-value pairs, where the is an object you can query information for, and the is the data associated with a given key. In most cases,key value\neach key in a dictionary is unique. By utilizing key-value pairs, each key of a dictionary is \"mapped\" to a corresponding value that can be\nqueried using the key’s value. This makes a dictionary a good container type if you want to look up information associated with a given object.", "word_count": 204, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9bb720f3-6e08-5fe4-bfdd-f137fe6387e8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 534, "real_page_number": null, "text": "522\nChapter 17. Hash Tables and Collision Resolution\nRemark: The moniker is quite fitting for this abstract data type, since its behavior closely resembles that of an actual dictionary!dictionary\nWhen you use a dictionary, you are taking a word and looking up its definition. Here, the \"key\" is the word you want to look up, and the\n\"value\" is the definition of the word. The dictionary abstract data type (ADT) applies this idea to data retrieval in a program: if you use a\ndictionary ADT, you can look up information associated with any object by storing this lookup data in the form of key-value pairs.\nUsing the shopping example from earlier, we could use a dictionary ADT to support lookup for the price of any item given its name. In the\ndictionary, we would store the item-price relationships as key-value pairs, where the name of the item is the key (the object that you want to\nlook up data for), and the price is the value (the information associated with each key).\nFood Item\nPrice\nApple\n$3.99\nAvocado\n$4.99\nBanana\n$2.49\nFlour\n$2.29\nGinger\n$2.69\nMilk\n$2.09\nTofu\n$3.79\nThere are several basic operations that a dictionary should be able to support:\n• the insertion of new key-value pairs\n• the retrieval (or lookup) of values associated with a given key\nIt should be noted that a dictionary is an type, so it is simply an for a container that supports key-value lookup. Asabstract data interface\nmentioned, if you have a dictionary, you can insert key-value pairs, and you can look up the value associated with any key. However, the actual\nimplementation details of a dictionary can vary greatly based on the features that a specific lookup container should provide. A lookup container\nthat allows sorted access to its key-value pairs is implemented very differently from a container that does not need to exhibit this behavior, even\nthough both are considered as dictionaries as long as they support the dictionary interface.\nAs such, our exploration of the dictionary ADT will be split into two chapters. In this chapter, we will discuss the implementation of a\ndictionary that does not need to store its key-value pairs in any particular order. This implementation is known as a or a map.hash table hash\nBecause they do not need to maintain an internal ordering of elements, hash tables can be implemented to support key-value lookups in constant\ntime. The fast performance of hash tables makes them very useful for solving a variety of problems that would be less efficient if implemented\nusing other containers, and knowledge of this data structure in your coding repertoire will be immensely beneficial to you as a programmer.\nHowever, there are situations where you may want a dictionary to support more than just key-value lookup. For instance, you may also want\na lookup container to provide access to its key-value pairs. To implement a dictionary that stores its keys in sorted order, we will use asorted\nstructure known as a tree. These sorted dictionaries are versatile and support more features than a standard, unordered hash table,binary search\nbut these features come at the expense of constant time lookup. We will discuss this implementation in greater detail in chapter 18.\n17.2\nIntroduction to Hashing\n¸ 17.2.1\nHash Tables\nLet’s return to the shopping example introduced earlier in this chapter. How would you efficiently implement a dictionary that allows you to\nquery the price of an item as quickly as possible? Using the data structures we have covered so far, this is not immediately apparent. We could\ntry to store the values in an array-like container and loop through the container whenever a user queries an item. An example is shown below:\n1\nstruct Item {\n2\nstd::string name;\n3\ndouble price;\n4\n};\n5\n6\n// loop through all items and find the one that matches the target\n7\ndouble get_price(const conststd::vector<Item>& items, std::string& target) {\n8\nfor (const Item& item : items) {\n9\nif (item.name == target) {\n10\nreturn item.price;\n11\n} // if\n12\n} // for\n13\nthrow std::invalid_argument(\"Item does not exist!\");\n14\n} // get_price()\nHowever, this approach requires us to perform a traversal of all the items in our container whenever we want to query the price of a singleΘ(𝑛)\nitem. This is not the best way to do things! Instead, we want to store the data in a way that supports efficient lookup: if we query any item, we\nshould be able to get back its price without having to look at all other items in the container. How can this be done?", "word_count": 778, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "58e5dc2a-e54c-58ee-9428-2b790078d8c3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 535, "real_page_number": null, "text": "17.2 Introduction to Hashing\n523\nTo gain some intuition to this better method, let’s adjust the previous example a bit and replace each item’s name with an ID number starting\nfrom zero, as shown below:\nFood Item\nPrice\n0\n$3.99\n1\n$4.99\n2\n$2.49\n3\n$2.29\n4\n$2.69\n5\n$2.09\n6\n$3.79\nHow can we store this data in a way that allows us to efficiently retrieve a food item’s price given its ID number? The straightforward solution\nwould be to insert all the price values into an underlying data array, where the ID of an item is represented by each value’s index:\n3.99\n4.99\n2.49\n2.29\n2.69\n2.09\n3.79\n0\n1\n2\n3\n4\n5\n6\nvec[1]If we were given an item number, we could just query the price of that item by using its item number as an index (e.g., for the price of\nitem 1). This provides lookup in constant time! Yet, this is not very different from the problem we had earlier; we simply replaced each item\nvec[\"Apple\"]name with an integer ID instead. The main difference is that strings cannot be used to index directly into an array (e.g., does\nnot work), but a numeric ID can, provided that it is non-negative.\nAlthough strings cannot be used to directly index into an array, we can resolve this issue by simply converting each string into a numeric\nindex that can be used to index an array. This concept applies to any key type as well: if we can convert a key into an integer index, then we can\nsupport lookup on that key by first converting it into an index, and then using that index to access a value from an underlying array (with some\ncaveats of course, which will be discussed in this chapter). If this conversion takes constant time, then lookup can also be done in constant time.\nKey\nconvert key to integer index\nuse this converted index to index into an underlying array that stores the values for each key\nThis basic idea forms the foundation for the hash table data structure. A hash table, alternatively known as a hash map, is an implementation\nof the dictionary ADT that supports key-value insertion, lookup, and removal in average-case time with respect to the total number ofconstant\nitems in the hash table. This constant time access is made possible using a process known as hashing, which is designed to translate a key into\nan index value that is then used to access an underlying array that stores the values associated with each key.\n¸ 17.2.2\nHash and Compression Functions\nWhen a hash table lookup is requested on a key, the key is first passed into something known as a hash function, which performs arithmetic\noperations to convert that key into an integer. This integer is then compressed into a valid index, which is then used to index into an underlying\ndata vector storing the hash table’s key-value pairs. To illustrate this process, consider the following hash function that converts a string key into\nan integer based on its first letter. We will use this hash function to implement a hash table that stores its data in an underlying array of size 10.\n1\nint32_t hash(const std::string& s) {\n2\nif (s.empty()) {\n3\nreturn 0;\n4\n} // if\n5\n// returns 0 if first char is 'A', 1 if 'B', 2 if 'C', ..., 25 if 'Z'\n6\nreturn (s[0] - 'A');\n7\n} // hash()\nKey\nValue\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9", "word_count": 596, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "61d4eb65-a419-5d6e-ae86-48c42d8552c6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 536, "real_page_number": null, "text": "524\nChapter 17. Hash Tables and Collision Resolution\nSuppose we wanted to insert the following key-value pair into this hash table:\n{\"Apple\", 3.99}\n\"Apple\"To determine which index of the underlying vector this key-value pair falls into, we will pass the key into the hash function. Since\n\"Apple\" 'A', {\"Apple\", 3.99}begins with the character the hash function returns 0, and the key-value pair ends up at index 0. Here,\n\"Apple\".we consider 0 as the hash value of the key\n\"Apple\"\nhash(\"Apple\")\nKey\n\"Apple\"\nValue\n3.99\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNow, suppose we wanted to insert the following key-value pair into this hash table:\n{\"Milk\", 2.09}\n\"Milk\"If we were to pass into our hash function, we would get back a hash value of 12. However, 12 is not a valid index number since our\nvector only has a size of 10. This is where the compression step plays an important role. During the compression step, the result obtained from\nthe hash function is compressed into a valid index in the range [0,𝑀), where 𝑀is the size of the underlying array. The simplest compression\n𝑀).1 {\"Milk\",method is to take the modulo of the hashed integer with the size of the table (e.g., mod In this case, 12 mod 10 is 2, soℎ(𝑡)\n2.09} would end up at index 2.\n1\nint32_t compress(int32_t hash_result) {\n2\nreturn hash_result % table_size;\n3\n} // compress()\n\"Milk\"\nhash(\"Milk\")\ncompress(12)\nKey\n\"Apple\"\n\"Milk\"\nValue\n3.99\n2.09\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"Apple\". \"Apple\"Now, suppose we wanted to retrieve the price of With hashing, we can do this in time: we first pass the string intoΘ(1)\n\"Apple\",the hash function, which returns 0. Then, we go to index 0, confirm that the key is indeed and then return the corresponding value\n(3.99) to the user. Notice that, as long as the hash and compression functions take constant time, the entire process of finding the price of\nany item (and similarly inserting or removing items) also takes time. If we were to insert the remaining foods into the table (excludingΘ(1)\n\"Avocado\", which produces a special scenario that we will cover later), we would get the following hash table:\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.49\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1If canbenegative,anabsolutevalueshouldbeappliedtothehashvaluebeforetakingthemodulus.ℎ(𝑡)", "word_count": 422, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "09231444-dc6e-5ea8-96ae-2dcd6f7782c2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 537, "real_page_number": null, "text": "17.2 Introduction to Hashing\n525\n¸ 17.2.3\nProperties of Hash and Compression Functions\nIn the previous example, we used a hash function that calculated a string’s hash value using the distance between the first letter of the string and\n\"A\".the letter However, this is not the only hash function we could have used. In fact, we could have used any function that converts a key into\na numeric hash value, as long as it has the following properties:\n• It must be easy to compute. If a hash function is so complicated that running it takes longer than constant time, it defeats the purpose of\nhaving a hash table in the first place.\n• It must compute a hash for every key. The hash function should be able to convert any valid key into an integer. For instance, the hash\nfunction should not return integers for some keys and segfault for others.\n• It must compute the same hash for the same key. The benefit of hashing comes from the fact that the index each key maps to is\ndeterministic. If the hash function converts the string \"Apple\" to the integer 0, then it should always convert \"Apple\" to 0. It shouldn’t\noccasionally convert \"Apple\" to something else, since that would prevent us from getting the efficient lookup we need from a hash table.\nThere is another property that a hash function have to be able to provide consistent lookup. This property is not required for a hashΘ(1)should\nfunction to be valid, but having it allows a hash table to perform its operations efficiently.\n• It should distribute keys evenly. For instance, \"Apple\" and \"Avocado\" are hashed to the same location using the hash function in\nthe previous example (we will deal with this issue in the next section). When two keys map to the same index, a occurs. If acollision\ncollision happens, we will have to do additional work to resolve the collision, which may take more than time. There is no way toΘ(1)\neliminate collisions entirely, but they can be minimized if keys are distributed evenly by the hash function.\nFor instance, a hash function that always returns 0 for all keys is a valid hash function (since it satisfies the first three mandatory requirements),\nbut it is a very poor hash function since it causes collisions every time a key is inserted.\nThe compression step also plays a role in the efficiency of a hash table. There are two primary methods of compressing the output of the\nhash function into an index in the range [0,𝑀), where 𝑀is the table size.ℎ(𝑡)\n|ℎ(𝑡)|• The Division Method: mod 𝑀. Ideally, the table size 𝑀should be a prime number.\n|𝑎∗ℎ(𝑡)+𝑏|• The MAD (Multiply and Divide) Method: mod 𝑀, where 𝑎and 𝑏are prime numbers. This method can be used if you\ncannot control the underlying table size 𝑀. In this method, 𝑎mod 𝑀must not equal 0.\nIf you could choose the table size 𝑀, why should 𝑀be prime? It turns out that prime number table sizes reduce collisions and improve the\nperformance of a hash table. For example, if you created a hash table with a size of 100 and all your keys ended up hashing to multiples of 5,\nyour hash function would fail to distribute keys evenly (e.g., no element would be mapped to indices 1-4, 6-9, …). However, if you changed the\ntable size to 101 (a prime number), you would get a more even distribution of keys, even if every key were hashed to a multiple of 5.\nIn general, non-prime values of 𝑀tend to cause more collisions because every hash value that shares a common factor with 𝑀will end up\nat an index that is a multiple of this factor (which prevents the keys from being distributed evenly). However, if 𝑀were prime, you wouldn’t\nhave to worry about keys sharing a common factor with 𝑀(since the only factors of a prime number are 1 and itself).\nClaim: Every key 𝐾that shares a common factor with the table size 𝑀will be hashed to an index that is a multiple of this common factor.\nProof: Let 𝐾and 𝑀be natural numbers (∈ℕ) such that 𝐾and 𝑀share a common factor. Without loss of generality, let the natural number\n𝑡be one of the common factors of 𝐾and 𝑀. Since 𝐾and 𝑀have a common factor 𝑡, we know that 𝑡𝑢and 𝑡𝑣for some natural𝐾= 𝑀=\nnumbers 𝑢and 𝑣. Furthermore, the key 𝐾will be hashed to some index 𝑖, where 𝐾mod 𝑀. Since 𝑖is the remainder when 𝐾is divided𝑖=\nby 𝑀(per the definition of a modulus), there must exist an integer 𝑎such that 𝐾−𝑎𝑀. Replacing 𝐾with 𝑡𝑢and 𝑀with 𝑡𝑣, we get the𝑖=\nequation 𝑡𝑢−𝑎𝑡𝑣. This can be simplified to 𝑡(𝑢−𝑎𝑣). We have now proven that the index 𝑖is a multiple of the common factor 𝑡. Thus,𝑖= 𝑖=\nall keys that share a common factor with the table size will end up at an index that is a multiple of the common factor. Note that this holds for\nall common factors between 𝐾and 𝑀.\nIf you could guarantee perfect hashing with no collisions, what are the time complexities of insertion, search, and removal? In the case of\ninsertion, if you can guarantee that every key had its own index, you would immediately know where each key should go in the hash table. Thus,\ninsertion would always take time as long as the hash function also takes constant time. The same applies to search and removal — youΘ(1)\nknow exactly where each key can be found in the hash table, so you can immediately retrieve its value or erase it in time. However, perfectΘ(1)\nhashing is a theoretical ideal that is hard to attain in practice. This is because the hashing process is rarely collision-free, and additional work\nneeds to be done to resolve collisions if they do occur. We will discuss collision resolution techniques in the next section.\n¸ 17.2.4\nAddressing Methods\nThere are two primary methods we can use to search a lookup table when given a key. The first, direct addressing, can be used if the universe\nof possible keys is relatively small. If we know that the possible keys can be drawn from the set for some value of 𝑚that is not{0,1,…,𝑚−1}\ntoo large, then we can build an array of size 𝑚and use the value of the key to directly address this array (hence the name addressing).direct\n∙\n0\n∙\n1\n∙\n4\n∙\n6\n∙\n3\n∙\n2\n∙\n5\n∙\n7\n0\n1\n2\n3\n4\n5\n6\n7\nUniverse of possible keys\nActual keys in table\nThe universe of possible keys can only take on the values of 0-7, so direct addressing can be used with a table of size 8.", "word_count": 1139, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c304a3ac-3374-5061-b881-5640924ea45c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 538, "real_page_number": null, "text": "526\nChapter 17. Hash Tables and Collision Resolution\nHowever, if the universe of possible keys is so large, or if it greatly exceeds the actual number of keys that are stored in the table, then direct\naddressing is no longer feasible. In this case, it would be preferable to use hashed addressing, where only the actual keys in a hash table are\nmapped to a position in the underlying array. With hashed addressing, we apply a hash function (and a compression function 𝑐) that maps eachh\nkey in the universe of possible keys to a value in the set {0,1,…,𝑚−1}, where 𝑚is the size of the underlying table.\nSince the application of hash and compression is not one-to-one, it is inevitable that multiple keys may end up at the same position of the\nunderlying array. As mentioned, this is known as a collision and will need to be resolved using a collision resolution technique.\n∙\n𝑘2\n∙\n𝑘1\n∙\n𝑘3\n∙\n𝑘4\n𝑐(ℎ(𝑘1))\n𝑐(ℎ(𝑘2))\n𝑐(ℎ(𝑘3))\n𝑐(ℎ(𝑘4))\n0\n1\n2\n3\n4\n5\n6\n7\nUniverse of possible keys\nActual keys in table\nHash and compression are applied to a key to determine where to address its value in a table.\n𝑘2 𝑘3Because hashing is not one-to-one, collisions may occur (as illustrated with and above).\n17.3\nCollision Resolution Techniques\nLet’s return to the hash table of prices that we introduced earlier:\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.49\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nSuppose we try to add the remaining food item that has not been added yet:\n{\"Avocado\", 4.99}\n\"Avocado\" \"Apple\"If we were to hash the string using our hash function, we would get an index of 0. However, is already sitting at index\n0! In this case, we have a collision, where multiple keys hash to the same index. In this section, we will discuss four collision resolution\ntechniques that can be used to resolve collisions:\n• separate chaining\n• linear probing\n• quadratic probing\n• double hashing\n¸ 17.3.1\nSeparate Chaining\nThe first collision resolution technique we will discuss is separate chaining. With separate chaining, each index of the hash table stores a linked\nlist that holds all the keys that hash to that index. That is, index 0 would store a list of all items that hashed to index 0, index 1 would store a list\nof all items that hashed to index 1, and so on.\n\"Avocado\"If the above hash table used separate chaining, we would resolve the collision by appending to the list at index 0. This would\nproduce the following result:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"Apple\"\n3.99\n\"Avocado\"\n4.99\n\"Banana\"\n2.49\n\"Milk\"\n2.09\n\"Flour\"\n2.29\n\"Ginger\"\n2.69\n\"Tofu\"\n3.79\nIf we had 𝑁keys that were evenly distributed across all 𝑀indices, the average linked list in our hash table would have a length of 𝑁∕𝑀.\nThe average-case time complexity of searching for a key would therefore be Θ(𝑁∕𝑀), since it takes constant time to identify which list a key\nbelongs to (via hashing), and there are an average of 𝑁∕𝑀elements that you have to search once you know the correct list. (In fact, if we have\na good distribution and a big enough 𝑀, the value of 𝑁∕𝑀can actually be treated as a constant.)", "word_count": 561, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d99fec3b-4dac-527a-be2d-a21b773fa62f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 539, "real_page_number": null, "text": "17.3 Collision Resolution Techniques\n527\nThe complexities of hash table operations using separate chaining are summarized in the table below:\nOperation\nAverage-Case Complexity\nInsert Key\nif duplicate keys are allowed,Θ(1)\nif duplicates not allowedΘ(𝑁∕𝑀)\nSearch for Key\nΘ(𝑁∕𝑀)\nErase Key\nΘ(𝑁∕𝑀)\nInsertion takes time on average if duplicate keys are not allowed. This is because you need to check if the key already exists beforeΘ(𝑁∕𝑀)\nadding it in. This requires you to iterate through its corresponding list, which has a length of 𝑁∕𝑀, provided we have a good hash function.\n¸ 17.3.2\nLinear Probing\nWith separate chaining, every element is guaranteed to end up at the index it hashes to. However, this is not always true using the other collision\nresolution techniques. Linear probing, quadratic probing, and double hashing are all examples of open addressing techniques, which use empty\nlocations in the table to resolve collisions. Unlike separate chaining, these methods do not allow an index to store more than a single key-value pair.\nBefore we introduce these techniques, we will first introduce the concept of probing. A probe is a check that is made to determine whether\n\"Apple\"an index in the table is occupied. For example, if we wanted to insert into the hash table, we must first probe index 0 of the table to\ndetermine if something is already there. There are four possible outcomes of a probe:\n• Empty: The probe finds an empty cell in the table at an index. An empty cell has never held an item before.\n• Deleted: The probe finds a cell that once held an item, but is not currently holding an item.\n• Hit: The probe finds an occupied cell that contains an item whose key matches the search key.\n• Full: The probe finds an occupied cell, but the key does not match the search key.\nWith linear probing, we resolve collisions by continuously probing sequential indices until we find an open position (looping back to the\nbeginning if we reach the end). Consider the example table we had before:\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.49\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"Avocado\"If we attempted to insert into this table with linear probing as our collision resolution method, we would first probe index 0 to see\n\"Avocado\" \"Apple\"if something is already there (as hashes to 0). Since is already there, we would probe sequential indices until we find\nan open spot — that is, we would probe index (0 + 1) mod 𝑀, then index (0 + 2) mod 𝑀, then index (0 + 3) mod 𝑀, etc. until we find an\n\"Avocado\"empty spot. In this case, the first empty cell we probe is at index 3, so is placed at index 3. (𝑀is our table size, which is 10.)\nProbe index 0\nFULL\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nProbe index\n(0+1) mod10\nFULL\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nProbe index\n(0+2) mod10\nFULL\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9", "word_count": 568, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "223bb0b7-6ff7-5d00-acd5-826281a15601", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 540, "real_page_number": null, "text": "528\nChapter 17. Hash Tables and Collision Resolution\nProbe index\n(0+3) mod10\nEMPTY\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nInsert key\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n{\"Jelly\", 1.99} \"Jelly\"As another example, suppose we wanted to add the key-value pair to our hash table. We would first hash using\n\"Tofu\"our hash function, which returns an index of 9. Then, we would probe index 9 to see if it is available. Since already occupies that index,\n\"Jelly\"we then check indices (9 + 1) mod 𝑀, (9 + 2) mod 𝑀, etc. The first open index we find is index 4, so gets placed at that index.\nProbe index 9\nFULL\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nProbe index\n(9+1) mod10\nFULL\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\na few probes omitted… …\nInsert key\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9", "word_count": 245, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e9d398d2-2960-5a7b-a85c-9a5ac873e828", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 541, "real_page_number": null, "text": "17.3 Collision Resolution Techniques\n529\nTo for a key using linear probing, we first hash the key to find its intended index. Then, we probe that index to determine if the key issearch\nthere. If it is not, we would continuously probe sequential indices until we either find the key we want, or we find an empty cell (this would\nmean that the key does not exist in the table).\n\"Jelly\". \"Jelly\"For instance, suppose we wanted to find the price of We would first input into our hash function and obtain an index\n\"Jelly\" \"Jelly\"of 9. We then check index 9 to see if is there. It is not, but that does mean that does not exist in the table! Since wenot\n\"Jelly\"are using open addressing, it is possible that got placed at a different index because of a collision. Thus, we will sequentially probe\n\"Jelly\" (1.99).indices 0, 1, 2, 3, and 4. We find that is at index 4, so we return the value at that index\nProbe index 9\n\"Jelly\"Not\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nContinue probing...\n\"Jelly\" found!\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"Eggplant\". \"Eggplant\"On the other hand, suppose we wanted to find the price of a non-existent item like Passing into the hash table\n\"Jelly\" \"Eggplant\"gives us an index of 4, but occupies that index. We would then probe indices 5, 6, and 7 to make sure that was not\nplaced at a different index due to a collision. The keys in indices 5 and 6 do not match, but index 7 is empty. (noteOnce we probe an empty cell\nthat the cell must be and not — the distinction is important), This is because an empty cell hasempty deleted we can immediately stop probing.\nnever held an item before (by definition), so any key we are searching for would not have made it past a probe at this empty position when it was\n\"Eggplant\"first inserted into the table (since we stop probing as soon as we find an empty spot). In this example, cannot exist in the table\nsince it would have been placed at index 4, 5, 6, or 7 if it did exist.\nProbe index 4\n\"Eggplant\"Not\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nContinue probing...\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nEmpty cell found first...\nkey must not exist in table!\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9", "word_count": 519, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "00d0d563-9d78-5145-93d0-76e3fef309c0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 542, "real_page_number": null, "text": "530\nChapter 17. Hash Tables and Collision Resolution\n\"Tofu\"Removing elements from the table is slightly trickier. Suppose we wanted to erase the key from the table. We use our hash function to\n\"Tofu\"convert to the index 9, and then we go to index 9 to delete the value (after checking that the keys match). After the deletion, the table\nwould look like this:\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"Jelly\" \"Jelly\"However, this is actually an invalid table! Why? Suppose we wanted to retrieve the price of again. We convert to the\n\"Jelly\"index 9, but see that index 9 is empty. As a result, we would incorrectly conclude that does not exist in the table! This ended up\n\"Jelly\" \"Tofu\", \"Tofu\" \"Jelly\"happening because was added to the table after and the presence of caused to be placed at a different\n\"Tofu\", \"Tofu\"index. By removing we ended up losing access to all elements that had previously collided with when they were inserted!\nTo address this, we will need to add a special flag whenever we delete an element. This flag lets the hash table know that a specific index\npreviously held an element that is no longer there, and that elements that had previously collided with this deleted element may be elsewhere in\n\"Tofu\"the table. Using the above example of deleting and adding a special flag, the table would look like this:\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\nDELETED\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n(undefined)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"Jelly\",Now, if we want to retrieve the price of we would see that index 9 has a \"deleted\" element, and that we should continue probing for\n\"Jelly\" because it could have been sent elsewhere after a collision with the deleted element. We then probe indices 0, 1, 2, 3, and 4 before\n\"Jelly\"we find (and return its price).\nProbe index 9\nDELETED\nThis indicates you should continue probing, in case the key you want to find\ncollided with the value that was previously at index 9 and was sent elsewhere.\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\nDELETED\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n(undefined)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nContinue probing...\n\"Jelly\" found!\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Jelly\"\n\"Flour\"\n\"Ginger\"\nDELETED\nValue\n3.99\n2.69\n2.09\n4.99\n1.99\n2.29\n2.69\n(undefined)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"Jelly\"If we wanted to delete from the table, we would follow the same process as before to identify the position of this key, and then we\nwould replace it with the deleted flag so that future queries would know an element had existed at this position previously.\n\"Jelly\"...Remove\nreplace with deleted flag!\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\nDELETED\n\"Flour\"\n\"Ginger\"\nDELETED\nValue\n3.99\n2.69\n2.09\n4.99\n(undefined)\n2.29\n2.69\n(undefined)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nDeleted elements add a twist to how we conduct insertions and searches. When trying to search for an element, we treat a \"deleted\" position as\nif it were occupied and keep on looking. For insertion (assuming that duplicate keys are not allowed), we treat a \"deleted\" position as occupied\nwhile looking to see if the element already exists, but as an empty spot during the actual insertion process. For example, if we wanted to re-add\n\"Tofu\" \"Tofu\"to the hash table, we would first have to probe indices 9, 0, 1, 2, 3, 4, 5, 6, and 7 to confirm that does not already exist in the\n\"Tofu\"table. However, when we actually insert into the table, it should be written to index 9 (the first cell we encountered) and notdeleted\nindex 7 (the first cell we encountered), as positions that are deleted do not hold valid items and can be inserted into.empty", "word_count": 674, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5f550986-7e2f-5455-b7e9-7f7dd80f6722", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 543, "real_page_number": null, "text": "17.3 Collision Resolution Techniques\n531\nThe following diagrams provide a summary of insertion, search, and deletion on a hash table that uses linear probing to resolve collisions:\nKey Insertion (Linear Probing)\nKey\nHash and compress key into valid index 𝑖\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\nProbe indices 𝑖+1, 𝑖+2, 𝑖+3, (wrapping around if needed)…\nto check if the key already exists in the table\nInsert key at index 𝑖\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, so\ninsert key at the very\nfirst deleted empty cellor\nencountered while probing\nKey already exists in table, so do not insert it again\nassuming no duplicates are allowed\n(any updates should be made on this existing element)\nKey Search (Linear Probing)\nKey\nHash and compress key into valid index 𝑖\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\nProbe indices 𝑖+1, 𝑖+2, 𝑖+3, (wrapping around if needed)…\nto search for the key\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, search failed\nKey found, return its value", "word_count": 179, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2f84abb5-26ef-5f63-9a6c-b78970e753c7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 544, "real_page_number": null, "text": "532\nChapter 17. Hash Tables and Collision Resolution\nKey Deletion (Linear Probing)\nKey\nHash and compress key into valid index 𝑖\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\nProbe indices 𝑖+1, 𝑖+2, 𝑖+3, (wrapping around if needed)…\nto search for the key\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, so no need to delete anything\nKey found, erase it and mark its position as deleted\nExample 17.1 Consider a hash table of size 10 and a hash function 𝑘, where collisions are handled using linear probing. Ifℎ(𝑘) ℎ(𝑘)=\nexceeds the table size of 10, it is compressed down to a valid index by taking its modulo with 10, i.e., mod 10. After the𝑐(𝑘) ℎ(𝑘)=\nfollowing operations, what does the table look like? Duplicate keys are not allowed, and the table does not get resized.\n1. Insert 13\n2. Insert 23\n3. Insert 33\n4. Insert 14\n5. Erase 23\n6. Insert 14\n7. Erase 13\n8. Insert 43\nFirst, let’s insert 13. After compression, 13 maps to 13 mod 10 = 3, so it gets placed at index 3.\n13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNext, let’s insert 23. Like 13, 23 also maps to 23 mod 10 = 3. However, since 13 is already at index 3, we have a collision. Since we are using\nlinear probing, we would put 23 at the next available position, or index 4.\n13\n23\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n33 also maps to 3, so we place it at index 5 — the next available position — using linear probing.\n13\n23\n33\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNow, let’s insert 14. 14 maps to 14 mod 10 = 4, but there is already another value at index 4. With linear probing, we find the next open position\n(index 6) and put 14 there.\n13\n23\n33\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNext, we erase 23. Since 23’s mapped index after compression is 3, we expect it to be at index 3. However, since 13 is actually at index 3, we\nhave to probe an additional index to discover that 23 is at index 4. We cannot erase 23 completely — as mentioned earlier, we will need to mark\nindex 4 with a \"deleted\" flag for future operations.\n13\nDELETED\n33\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9", "word_count": 422, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "187ce0c9-2cfd-50a4-801b-cb616d6ee601", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 545, "real_page_number": null, "text": "17.3 Collision Resolution Techniques\n533\nNext, we will insert 14. 14 maps to 14 mod 10 = 4, so we would write 14 to index 4. However, even though index 4 does not currently store\nanything, it is marked as \"deleted\". Thus, we will have to continue probing since 14 might already exist in the table, but in a different position\nbecause of a collision with this deleted element. After probing, we notice that 14 is already at index 6, so no insertion is actually made.\n13\nDELETED\n33\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNext, we erase 13. Like before, we mark the location of 13 (index 3) with a \"deleted\" flag.\nDELETED\nDELETED\n33\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNext, we insert 43. 43 maps to 43 mod 10 = 3, so we look at index 3. Index 3 does not store a valid element, but it is marked with a \"deleted\"\nflag. Thus, we need to continue probing to determine if 43 is already in the table. The next index we probe is index 4, which is also \"deleted\", so\nwe continue on. Index 5 and index 6 do not store 43, and index 7 is empty. This empty cell indicates that 43 does not exist in the table. 43 is\nthen inserted at the first deleted or empty cell encountered during the probing process, which is index 3. It is okay to write to index 3 because\n\"deleted\" cells do not store actual data — the \"deleted\" flag is simply a marker to let the algorithm know to continue probing during a search.\n43\nDELETED\n33\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThis is the state of the hash table after all eight operations are completed.\nExample 17.2 Consider the following hash table in the previous example (which uses linear probing):\n43\nDELETED\n33\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nHow many buckets need to be probed before you can safely insert the key 53 into this table?\nBefore we can insert 53 into the table, we must first make sure that it is not in the table already. In this case, 53 hashes to 53 mod 10 = 3, so\nwe first probe index 3. Since 43 is at that spot, we then probe index 4. Index 4 contains a \"deleted\" item, which lets us know that we should\ncontinue probing. We then probe index 5, which is not equal to 53. We then probe index 6, which also is not equal to 53. We then probe index 7,\nwhich is empty. Since we reached an empty bucket without encountering 53, we know that 53 is not in the table, and that we can safely insert it\nin. The total number of buckets we probed before inserting 53 is therefore 5: indices 3, 4, 5, 6, and 7. Note that 53 would be inserted at index 4,\nsince that is the first non-occupied index we encountered while probing.\n43\n53\n33\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n¸ 17.3.3\nQuadratic Probing\nOne disadvantage of linear probing is that it is susceptible to clustering. Clustering happens when a group of adjacent indices in a hash table\nare all occupied. Because open spots are discovered and filled sequentially during a collision with linear probing, there is a tendency for many\nkeys to end up right next to each other (this is specifically known as clustering). This is not ideal, since the number of collisions wouldprimary\ngrow as more elements are added.\nTo see why this is the case, consider two different hash tables that are each half full (i.e., 𝑀∕2). In one hash table, the elements are𝑁=\nevenly distributed (alternating empty and occupied cells). In the other hash table, all the elements are clustered together. These two hash tables\nare shown below, where an X represents an occupied cell:\nX\nX\nX\nX\n…\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\n…\nWhat is the average cost of a hash table operation if the elements are evenly distributed (the first hash table)? Since the hash table is half filled\n(and assuming that our hash function is good), there is a 50% chance that a key will be hashed to an empty cell. When this happens, we only\nneed to make a single probe. The other 50% of the time, the key will be hashed to an occupied cell. However, since every occupied cell is\nfollowed by an empty one, we only need to make two probes in this situation (one for the occupied cell, one for the empty cell next to it). The\naverage cost of a hash table operation is thus 0.5 1 + 0.5 2 = 1.5 for the first hash table. This is Θ(1).× ×\nWhat about the second hash table? There is still a 50% chance that a key will be hashed to an empty cell, which will only require one probe.\nHowever, there is also a 50% chance a key hashes to a position within the cluster. When this happens, we will have to continuously probe until\nwe reach the end of the cluster. The exact number of probes we need to make depends on where we land (we only need to make two probes if a\nkey gets hashed to the end of the cluster, but we would need to make approximately 𝑛probes if a key gets hashed to the beginning of the cluster).\nHowever, on average, you would need to make probes, since that is the average distance from an index within the cluster to the end of the𝑛∕2\ncluster. We can thus compute the average cost of a hash table operation to be 0.5 1 + 0.5 𝑛∕2, which is Θ(𝑛). This is much worse than the× ×\ntime we obtained when no clustering was present!Θ(1)", "word_count": 1002, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "da8e6391-e23c-5ff9-8192-88137941f2c7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 546, "real_page_number": null, "text": "534\nChapter 17. Hash Tables and Collision Resolution\nHow can we minimize the likelihood of this happening? One possibility would be to switch from linear probing to quadratic probing when\nhandling collisions. Quadratic probing is essentially the same as linear probing, but with one key difference: rather than probing indices 𝑖+1,\n𝑖+12, 𝑖+22, 𝑖+32,𝑖+2, 𝑖+3, …, when a collision occurs at index 𝑖, quadratic probing probes indices …, instead. By squaring the distance\nwith every collision, we reduce the risk of clustering, which could bring down the efficiency of our hash table. Let’s consider the following table:\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.49\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"Avocado\" \"Avocado\"Again, let’s attempt to insert into this hash table, but with quadratic probing as our collision resolution method.\n12\"Apple\"hashes to index 0, but is already at index 0. Since index 0 is taken, we check to see if index 0 + = 1 is available. It is not. We then\n22 \"Avocado\"check to see if index 0 + = 4 is available. It is, so gets added to index 4.\nProbe index 0\nFULL\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nProbe index\n(0+12) mod10\nFULL\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nProbe index\n(0+22) mod10\nEMPTY\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nInsert key\nKey\n\"Apple\"\n\"Banana\"\n\"Milk\"\n\"Avocado\"\n\"Flour\"\n\"Ginger\"\n\"Tofu\"\nValue\n3.99\n2.69\n2.09\n4.99\n2.29\n2.69\n3.79\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThe rules for quadratic probing are shown on the next page. They are nearly identical to the rules for linear probing; the only difference is the\nsequence of positions probed upon collision.", "word_count": 345, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8de9a3fd-8a1c-5a2a-888d-3a12f83285a5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 547, "real_page_number": null, "text": "17.3 Collision Resolution Techniques\n535\nKey Insertion (Quadratic Probing)\nKey\nHash and compress key into valid index 𝑖\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\n𝑖+12, 𝑖+22, 𝑖+32,Probe indices (wrapping around if needed)…\nto check if the key already exists in the table\nInsert key at index 𝑖\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, so\ninsert key at the very\nfirst deleted empty cellor\nencountered while probing\nKey already exists in table, so do not insert it again\nassuming no duplicates are allowed\n(any updates should be made on this existing element)\nKey Search (Quadratic Probing)\nKey\nHash and compress key into valid index 𝑖\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\n𝑖+12, 𝑖+22, 𝑖+32,Probe indices (wrapping around if needed)…\nto search for the key\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, search failed\nKey found, return its value", "word_count": 157, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cb0fe595-c961-5e31-b60f-0732a9fad6a2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 548, "real_page_number": null, "text": "536\nChapter 17. Hash Tables and Collision Resolution\nKey Deletion (Quadratic Probing)\nKey\nHash and compress key into valid index 𝑖\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\n𝑖+12, 𝑖+22, 𝑖+32,Probe indices (wrapping around if needed)…\nto search for the key\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, so no need to delete anything\nKey found, erase it and mark its position as deleted\nExample 17.3 Consider a hash table with 10 buckets and a hash function 𝑘, where collisions are handled using quadratic probing.ℎ(𝑘)=\nIf exceeds the table size of 10, it is compressed down to a valid index by taking its modulo with 10, i.e., mod 10. Afterℎ(𝑘) 𝑐(𝑘) ℎ(𝑘)=\ninserting the keys 13, 23, 33, 43, and 53 into this hash table, what does the table look like?\nAfter compression, 13 maps to index 3, so it gets placed at index 3.\n13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1223 also maps to index 3, but index 3 is taken. Thus, we check if index 3 + = 4 is taken. It is not, so we place 23 at index 4.\n13\n23\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n12 2233 also maps to index 3, but index 3 is taken. We then check index 3 + = 4, but that is taken as well. We then check index 3 + = 7, which\nis available, so 33 gets placed at index 7.\n13\n23\n33\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n12 22, 3243 also maps to index 3, but index 3 is taken. We then check indices 3 + and 3 + but both are taken as well. We then check 3 + = 12\n(which wraps around to 12 mod 10 = 2). Index 2 hasn’t been taken yet, so 43 gets placed at index 2.\n43\n13\n23\n33\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n12, 22, 32, 42 42Lastly, 53 also maps to index 3. Index 3 is taken, so we check 3 + 3 + 3 + 3 + for an available position. Everything before 3 +\n42is taken. However, the hash value 3 + wraps around to index 19 mod 10 = 9, and index 9 is empty. Thus, 53 gets placed at index 9.\n43\n13\n23\n33\n53\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThis is the state of the hash table after all five keys are inserted using quadratic probing.", "word_count": 440, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6e41abd6-ee3d-5fd2-bc5f-fc00c8a98347", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 549, "real_page_number": null, "text": "17.3 Collision Resolution Techniques\n537\n¸ 17.3.4\nDouble Hashing\nCompared to linear probing, quadratic probing is better at preventing primary clustering from occurring in a hash table. However, even if\nquadratic probing is used, elements that hash the same position will still always have the same probe sequence, regardless of how far they land\nfrom their hashed position (which results in a phenomenon known as clustering). For example, if you tried to insert 63, 73, 83, …,secondary\ninto the hash table of the previous example, you will always probe index 3, then 4, then 7, then 9, and so on. To solve this problem, we can use a\ncollision resolution technique known as double hashing. With double hashing, we apply an additional hash function to the key if a collision\noccurs to determine which cells we should subsequently probe for open spots. That way, if a collision occurs, we might end up checking index\nfor one key, index for a different key, index for another key, etc. instead of checking the same sequence of 𝑖+1, 𝑖+4, 𝑖+9, etc.𝑖+3 𝑖+5 𝑖+8\nevery time. The double hashing formula is shown below:\n𝑡(𝑘𝑒𝑦)+𝑗×𝑡′(𝑘𝑒𝑦)\n𝑡′(𝑘𝑒𝑦)Here, represents a second hash that is used if there is a collision, and 𝑗represents the collision number.\nLet’s break this formula down. If double hashing is used (with modulus compression), we first probe index mod to check if it is[𝑡(𝑘𝑒𝑦) 𝑀]\nempty. If it is, we add the key to that index location. However, if we have a collision, the next index we check is\n[𝑡(𝑘𝑒𝑦)+1×𝑡′(𝑘𝑒𝑦)] 𝑀mod\nIf there is a collision at this index, the next index we would check is\n[𝑡(𝑘𝑒𝑦)+2×𝑡′(𝑘𝑒𝑦)] 𝑀mod\nIf there is still a collision, the next index we would check is\n[𝑡(𝑘𝑒𝑦)+3×𝑡′(𝑘𝑒𝑦)] 𝑀mod\nWe would continue incrementing the collision number until we get an index that is not occupied.\n𝑡′(𝑘𝑒𝑦).To summarize, in double hashing, you are given a primary hash function and a secondary hash function The primary hash𝑡(𝑘𝑒𝑦)\nfunction is applied to the key to determine the index it should go to, and the secondary hash function is applied to determine the locations to\n[𝑖+1×𝑡′(𝑘𝑒𝑦)]probe during a collision. Instead of probing indices mod 𝑀, mod 𝑀, mod 𝑀, …, we probe indices mod(𝑖+1) (𝑖+4) (𝑖+9)\n[𝑖+2×𝑡′(𝑘𝑒𝑦)] [𝑖+3×𝑡′(𝑘𝑒𝑦)] 𝑡′(𝑘𝑒𝑦)𝑀, mod 𝑀, mod 𝑀, …, and so on. For different keys, the value of may be different; this allows us to\ndiversify the probing sequence for each key, which thereby diminishes the occurrence of secondary clustering.\nKey Insertion (Double Hashing)\nKey\nHash and compress key into valid index 𝑖using primary hash function 𝑡(𝑘𝑒𝑦)\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\n𝑖+1×𝑡′(𝑘𝑒𝑦), 𝑖+2×𝑡′(𝑘𝑒𝑦), 𝑖+3×𝑡′(𝑘𝑒𝑦),Probe indices …,\n𝑡′(𝑘𝑒𝑦)using a secondary hash function (wrapping around if needed)\nto check if the key already exists in the table\nInsert key at index 𝑖\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, so\ninsert key at the very\nfirst deleted empty cellor\nencountered while probing\nKey already exists in table, so do not insert it again\nassuming no duplicates are allowed\n(any updates should be made on this existing element)", "word_count": 531, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "842d25f5-9ac1-5d93-a847-32b75c913a97", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 550, "real_page_number": null, "text": "538\nChapter 17. Hash Tables and Collision Resolution\nKey Search (Double Hashing)\nKey\nHash and compress key into valid index 𝑖using primary hash function 𝑡(𝑘𝑒𝑦)\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\n𝑖+1×𝑡′(𝑘𝑒𝑦), 𝑖+2×𝑡′(𝑘𝑒𝑦), 𝑖+3×𝑡′(𝑘𝑒𝑦),Probe indices …,\n𝑡′(𝑘𝑒𝑦)using a secondary hash function (wrapping around if needed)\nto search for the key\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, search failed\nKey found, return its value\nKey Deletion (Double Hashing)\nKey\nHash and compress key into valid index 𝑖using primary hash function 𝑡(𝑘𝑒𝑦)\nProbe index 𝑖\nEMPTY\nDELETED\nFULL\nHIT\n𝑖+1×𝑡′(𝑘𝑒𝑦), 𝑖+2×𝑡′(𝑘𝑒𝑦), 𝑖+3×𝑡′(𝑘𝑒𝑦),Probe indices …,\n𝑡′(𝑘𝑒𝑦)using a secondary hash function (wrapping around if needed)\nto search for the key\nEmpty (non-deleted) cell\nencountered before key?\nKey encountered first?\nKey not in table, so no need to delete anything\nKey found, erase it and mark its position as deleted", "word_count": 147, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9a874c2c-6641-5d05-8e5d-e54bef768831", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 551, "real_page_number": null, "text": "17.4 Load Factor and Dynamic Hashing\n539\nExample 17.4 Consider a hash table with 10 buckets and a hash function 𝑘, where collisions are handled using double hashing.ℎ(𝑘)=\nThe double hashing formula used is\n𝑡(𝑘𝑒𝑦)+𝑗(7−(𝑡(𝑘𝑒𝑦)mod7))\nwhere represents the integer value the key normally hashes to and 𝑗is the collision number. The secondary hash function is𝑡(𝑘𝑒𝑦)\n𝑡′(𝑘𝑒𝑦) mod 7). The compression function takes the modulo of with the table size 10 to return a valid index. After the7−(𝑡(𝑘𝑒𝑦) ℎ(𝑘)=\nkeys 13, 23, 33, and 25 are inserted into the table, in this order, what does the table look like?\nWe first pass 13 into our primary hash function, which returns an index of 3 after compression. Since nothing is at index 3, 13 is placed there.\n13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNext, 23 also hashes to index 3 using our primary hash function. Since 13 is already there, we have a collision. Using double hashing, the next\nindex we probe is [23 + 1 (7 - (23 mod 7))] mod 10 = 28 mod 10 = 8. Index 8 is empty, so 23 gets placed at index 8.×\n13\n23\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNext, 33 also maps to index 3 using our primary hash function. Since 13 is there, we then check index [33 + 1 (7 - (33 mod 7))] mod 10 = 35×\nmod 10 = 5. Index 5 is empty, so 33 gets placed at index 5.\n13\n33\n23\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNext, 25 maps to index 5 using our primary hash function. Since 33 is already there, we then check index [25 + 1 (7 - (25 mod 7))] mod 10 =×\n28 mod 10 = 8. We end up getting another collision, so we increment our collision number and check index [25 + 2 (7 - (25 mod 7))] mod 10×\n= 31 mod 10 = 1. Index 1 is empty, so 25 gets placed at index 1.\n25\n13\n33\n23\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThis is the state of the hash table after all four keys are inserted.\n17.4\nLoad Factor and Dynamic Hashing\n¸ 17.4.1\nLoad Factor\nThe collision resolution technique we use is just one factor that plays a role in the efficiency of a hash table. Another factor is the size. Thetable\nmore full a table becomes, the more collisions you will get, and the less efficient your hash table will be. As a result, a good hash table will need\nto be dynamically resized based on the size of the data.\nBefore we discuss table resizing, we will introduce a concept known as the load factor, denoted by 𝛼. The value of 𝛼is 𝑁∕𝑀, where 𝑁\nrepresents the number of keys in the table, and 𝑀represents the size of the underlying table. We have seen 𝑁∕𝑀before when discussing\nseparate chaining: with a good hash function, 𝛼represents the list. However, for open addressing techniques,average number of items in each\n𝛼takes on a different meaning — it represents the filled. Unlike separate chaining, each index in openpercentage of table indices that are\naddressing can hold at most one element, so 𝛼must be less than or equal to 1 if open addressing is used (since you cannot have more elements\nthan indices). Note that deleted elements do not contribute to 𝑁(since \"deleted\" is just a bookkeeping flag, not an actual key in the table).\nIn 1962, Don Knuth proved a set of equations on the expectation of linear probing. If linear probing is used and the hash function distributes\nkeys evenly, the expected number of probes required to successfully search for an existing element in a hash table with load factor 𝛼is\n1\n2\n(\n1+\n1\n1−𝛼\n)\nOn the other hand, the expected number of probes required to either (1) unsuccessfully search for a non-existent element or (2) insert an element,\nusing the same assumptions as above, is\n1\n2\n(\n1+\n1\n(1−𝛼)2\n)", "word_count": 695, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "be4de6ce-c107-59ca-bf71-0ae013d35540", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 552, "real_page_number": null, "text": "540\nChapter 17. Hash Tables and Collision Resolution\nPutting this into tabular form, we get the following:\n𝛼\nAverage Probes Needed\nfor Successful Search\nAverage Probes Needed\nfor Unsuccessful Search\n0.1\n1.1\n1.1\n0.2\n1.1\n1.3\n0.3\n1.2\n1.5\n0.4\n1.3\n1.9\n0.5\n1.5\n2.5\n0.6\n1.8\n3.6\n0.7\n2.2\n6.1\n0.8\n3.0\n13.0\n0.9\n5.5\n50.5\nNotice that the number of probes we need worsens as the table gets more and more full. In general, if we are implementing a hash table with\nlinear probing, we do not want the table to be more than half full.\nThe size of the hash table plays a greater importance if quadratic probing is used instead of linear probing. This is because of a mathematical\nproperty related to residues, or the set of possible remainders of square numbers. If we are given the equationquadratic\n(𝑐+𝑗2) mod 𝑀\nfor any constant 𝑐, collision number 𝑗, and table size 𝑀, Consider the hashwe can only produce a limited number of remainders upon division.\nfunction 𝑘for a hash table of size 7, where quadratic probing is used. We want to insert the keys 9, 16, 20, 23, and 30.ℎ(𝑘)=\n0\n1\n2\n3\n4\n5\n6\nAfter inserting 9, 16, 20, and 23 using quadratic probing, we would get the following:\n9\n16\n23\n20\n0\n1\n2\n3\n4\n5\n6\nNow, let’s try inserting 30. Upon first glance, we would initially expect 30 to fall into index 0, 1, or 5. However, 30 hashes to index 2, which is\n(2+11) (2+22)already occupied. We would then check index mod 7 = 3, which is also full. Then, we would check index mod 7 = 6, which is\nalso full. In fact, if we keep on going:\n(2+32)• mod 7 = 4 (full)\n(2+42)• mod 7 = 4 (full)\n(2+52)• mod 7 = 6 (full)\n(2+62)• mod 7 = 3 (full)\n(2+72)• mod 7 = 2 (full)\n(2+82)• mod 7 = 3 (full)\n• …\nWe have encountered a key that causes infinite collisions! This is because a modulus of 7 only supports four distinct quadratic residual values,\nso there is no way to get a new unoccupied index for the fifth key if we keep our table size at 7. In fact, for any table size 𝑀, approximately only\nhalf of the integers between 1 and 𝑀are valid quadratic residual values mod 𝑀. Thus, if quadratic probing is used, it can be dangerous to have\na table that is over half full, since you may end up with a key that cannot be inserted into the table at all.\nAs the load factor increases, the performance of open addressing techniques degrades quickly. This is one of the big disadvantages of using\nan open addressing technique instead of separate chaining to resolve collisions. For separate chaining, the search time increases gradually if you\nincrease the number of keys in the table without adjusting the table size. If you double the number of keys using separate chaining, for example,\nyou would expect the average list length at each index to double, assuming that keys are evenly distributed. However, for open addressing, the\nsearch time increases dramatically as the table fills (to a point where no more keys can be inserted when 1). This is because each cell of the𝛼=\ntable can only hold one element, and it becomes harder to find an open position as the table fills up.\n¸ 17.4.2\nDynamic Hashing\nNonetheless, a hash table’s performance degrades if we increase the number of keys without adjusting the size of the table, regardless of whether\nwe use separate chaining or open addressing. Thus, to maintain a hash table’s efficiency, we will need to dynamically resize the hash table based\non the number of keys in the table. This process is known as dynamic hashing. A common dynamic hashing procedure is to\n• Double the size of the hash table if the load factor ever exceeds 0.5.\n• Rehash every element in the old table to the new table. (Note that \"deleted\" positions don’t store valid elements, so they aren’t rehashed.)", "word_count": 697, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4f972b9e-759f-5b06-9db9-93fb0e0e9d17", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 553, "real_page_number": null, "text": "17.4 Load Factor and Dynamic Hashing\n541\nThe rehashing step is important because elements may not have the same position when they are moved to a new table. For example, if we had a\nhash table of size 4 that compresses keys using 𝑘mod 𝑀, the key 37 would go into index 37 mod 4 = 1.𝑐(𝑘)=\n37\n0\n1\n2\n3\nHowever, if we double the hash table to size 8, the key 37 would instead fall into index 37 mod 8 = 5 instead.\n37\n0\n1\n2\n3\n4\n5\n6\n7\nAs a result, we cannot directly copy 37 from index 1 of the old table to index 1 of the new table, since 37 would no longer hash to that position.\nInstead, we need to rehash 37 to obtain its position in the new table before adding it in.\nThis process is expensive, but it is also infrequent. Using amortized analysis, we can actually prove that insertion into a hash table takes\namortized constant time, even if a single insertion may require elements to be rehashed into a larger table. Suppose we have a hash table that\ndoubles its size if and rehashes all existing elements over. Assuming the cost of inserting an element is 1, the following table shows the𝛼>0.5\nwork associated with each insertion, starting from an empty table:\nItem Number\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nTable Size\n2\n4\n8\n8\n16\n16\n16\n16\n32\n32\n32\n32\n32\n32\n32\n32\nCost\n1\n1+1\n1+2\n1\n1+4\n1\n1\n1\n1+8\n1\n1\n1\n1\n1\n1\n1\nThe load factor exceeds 0.5 on insertions 2, 3, 5, and 9, which require us to copy the existing elements over to a new, larger table. This is why\n9ththe shaded insertions require a cost larger than 1 (e.g., on insertion 9, we have to rehash the original 8 elements before adding the element,\nfor a total cost of 9). The total work required to insert 𝑛elements can thus be expressed as\n𝑇(𝑛)=\n(1+1+1+1+…)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncostofinsertingnewelements\n(1+2+4+8+…+𝑛′)+\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncostofreallocationtonewtable\n𝑛′where is done 𝑛times, and represents the largest power of two that is smaller than 𝑛. Since 1 is added 𝑛times, the total cost of1+1+…\nappending new elements is 𝑛. To determine the cost of copying during reallocation, we can use the following identity\n𝑛−1\n∑\n𝑘=0\n(𝑎𝑟𝑘) 𝑎=\n(1−𝑟𝑛\n1−𝑟\n)\n1+2+4+8+…+𝑛′where 𝑎is the first term, 𝑟is the common ratio, and 𝑛is the number of terms. We can express as\n𝑛+1)−1(log2\n∑\n𝑘=0\n𝑛+11−2log22𝑘=\n1−2\n𝑛×21−2log2=\n1−2\n1−2𝑛=\n2𝑛−1=1−2\n2𝑛−1=2−1\nPutting it all together, the total work required to push 𝑛elements in the worst case is bounded by\n𝑇(𝑛) 𝑛+2𝑛−1 Θ(𝑛)= =\nThe amortized cost of inserting into this hash table is therefore\n𝑇(𝑛)Amortized Complexity=\n𝑛\nΘ(𝑛)=\n𝑛\n=Θ(1)\n¸ 17.4.3\nSummary of Collision Resolution Techniques, Load Factor, and Dynamic Hashing\nIn summary, there are four different ways to resolve a collision if two keys end up hashing to the same position. One method, separate chaining,\ncreates a linked list for each index of the table. The other three methods — linear probing, quadratic probing, and double hashing — utilize a\nprocess known as open addressing, which looks for another empty index in the table to insert the new value. Assuming the collision occurred at\nindex 𝑖, linear probing probes for open positions sequentially starting from the collision index (i.e., 𝑖+1, 𝑖+2, …), quadratic probing probes for\n𝑖+12, 𝑖+22, 𝑡′open positions at square distances from the collision index (i.e., …), and double hashing applies an additional hash function to\n𝑖+1×𝑡′(𝑘𝑒𝑦), 𝑖+2×𝑡′(𝑘𝑒𝑦),determine how far from the original hash value to probe (i.e., …). In addition, the load factor of a hash table,\ndenoted by 𝑁∕𝑀, is a value that can be used to reflect a table’s efficiency. The larger the load factor, the less efficient a hash table is. Thus,𝛼=\nto ensure that the performance of a hash table remains optimal at all times, we utilize a process known as dynamic hashing, which increases the\ntable size whenever the load factor exceeds a certain threshold.", "word_count": 731, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3b2a4b51-2c46-54fb-bc43-b544eebcedca", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 554, "real_page_number": null, "text": "542\nChapter 17. Hash Tables and Collision Resolution\n17.5\nThe STL Unordered Map Container\nstd::unordered_map<> std::unordered_set<>.The STL implements two important containers using hashing: the and the The\nstd::unordered_map<>, <unordered_map>in the library, is an associative container that stores key-value pairs. The key is used to\nidentify an element, and the value is used to store information associated with each key. Unordered maps are useful for problems thatuniquely\nrequire fast lookup of data given a key (e.g., looking up a student’s records given their ID). Consider the food prices that we introduced earlier:\nFood Item\nPrice\nApple\n$3.99\nAvocado\n$4.99\nBanana\n$2.49\nFlour\n$2.29\nGinger\n$2.69\nMilk\n$2.09\nTofu\n$3.79\nIf we wanted to store this data in a way that allows us to efficiently retrieve the price of any food item given its name, we could put this in an\nstd::unordered_map<>. To create one, we must pass in the types of the key and mapped value, as follows:\nstd::unordered_map<KEY_TYPE, VALUE_TYPE> map_name;\nIn this problem, we are given the name of a food as a string, and we want to use this name to look up its price. Thus, the key has a type of\nstd::string, double.and the mapped value has a type of\ndouble>std::unordered_map<std::string, food_prices;\nstd::unordered_map<>:We can then go through and insert our data into the\nfood_prices.insert({\"Apple\", 3.99});\nfood_prices.insert({\"Avocado\", 4.99});\nfood_prices.insert({\"Banana\", 2.49});\nfood_prices.insert({\"Flour\", 2.29});\nfood_prices.insert({\"Ginger\", 2.69});\nfood_prices.insert({\"Milk\", 2.09});\nfood_prices.insert({\"Tofu\", 3.79});\noperator[]After inserting the key-value pairs, can be used to retrieve the value associated with each key. For instance, if you wanted to get\nfood_prices[\"Tofu\"]the price of \"Tofu\", you can just call (you can think of this as \"indexing\" into the unordered map using the key).\ndouble price_tofu = food_prices[\"Tofu\"];\nstd::cout << price_tofu << '\\n';\n// prints 3.79\nstd::unordered_map<>:The following lists some common member functions that can be used with an\ntemplate <typename typenameK, V>\nbool> V>::insert(conststd::pair<iterator, std::unordered_map<K, std::pair<K, V>& p);\nstd::unordered_map<>Attempts to insert a key-value pair into the container. Since keys in an must be unique, an insertion is done\nonly if the key does not already exist in the container. Returns a pair consisting of an iterator to the inserted element (or the element that\nboolprevented insertion) and a for whether the insertion took place.\ntemplate <typename typename typename...K, V, Args>\nbool>std::pair<iterator, std::unordered_map<K, V>::emplace(Args&&... args);\nargs. std::unordered_map<>Attempts to insert a key-value pair into the container, constructed in-place using Since keys in an\nmust be unique, an insertion is done only if the key does not already exist in the container. Returns a pair consisting of an iterator to the\nboolinserted element (or the element that prevented insertion) and a for whether the insertion took place.\n1\ndouble>std::unordered_map<std::string, food_prices;\n2\nauto item1 = food_prices.insert({\"Apple\", 3.99});\n3\nstd::cout << item1.second << '\\n';\n// prints 1 for true\n4\nauto item2 = food_prices.insert({\"Apple\", 3.89});\n5\nstd::cout << item2.second << '\\n';\n// prints 0 for false (key \"Apple\" exists)\n6\nstd::cout << food_prices[\"Apple\"] << '\\n';\n// prints 3.99 (first value inserted)\n7\n8\n// emplace does not require you to pass in a pair, you can pass in constructor args instead\n9\nauto item3 = food_prices.emplace(\"Banana\", 2.49);\n10\nstd::cout << item3.second << '\\n';\n// prints 1 for true\nemplace(), std::pair<>Remark: When you insert an element into a map using the key-value associated with the new element is\nconstructed in place. However, although the pair itself is not copied into the map after it is created, the key and value could still be copied to\nstd::pair<>construct the pair. To avoid all potential cases of unnecessary copying, you can use the overload of thepiecewise construct\nconstructor. The details about this are a bit out of scope for this chapter, but the following is an example of how this feature can be used. In\ntry_emplace()C++17 and beyond, you can also use as a similar alternative, which is covered on the next page.\n1\n// class Course(const std::string& department, int32_t id);\n2\n// class Student(int32_t id, const std::string& first_name, const std::string& last_name);\n3\nstd::unordered_map<Course, Student> course_map;", "word_count": 680, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4d6456fa-9eb4-5985-9d0f-8fe5a555f754", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 555, "real_page_number": null, "text": "17.5 The STL Unordered Map Container\n543\n4\n// This emplaces a course/student pair without piecewise construct\n5\ncourse_map.emplace(Course{\"EECS\", 281}, Student{12345678, \"Bob\", \"Smith\"});\n6\n7\n// This uses piecewise construct to emplace the same course/student pair\n8\ncourse_map.emplace(std::piecewise_construct,\n9\nstd::forward_as_tuple(\"EECS\", 281),\n10\nstd::forward_as_tuple(12345678, \"Bob\", \"Smith\"));\nstd::piecewise_constructYou can think of as a special marker to indicate that multiple objects are to be created, and that the\nconstructor parameters for these objects will be passed in as tuples that need to be delineated.\ntry_emplace()C++17 introduced the member method, which can be used to perform insertions more efficiently for certain key-value\ninsert() emplace()pairs. As an example, the and methods covered above may involve the construction of a key-value pair when invoked,\ntry_emplace()even if doing so is not necessary. In contrast, the method does touch its provided arguments if the key already exists innot\nthe table. This optimization is particularly useful if the value object is expensive to construct.\ntemplate <typename typename typename...K, V, Args>\nbool>std::pair<iterator, std::unordered_map<K, V>::try_emplace(K&& k, Args&&... args);\nbool> V>::try_emplace(conststd::pair<iterator, std::unordered_map<K, K& k, Args&&... args);\nk args, kInserts a new element into the container with key and value constructed using but only if does not exist as a key in the table\ninsert() emplace(), try_emplace()already. Unlike and does not move from rvalue arguments if an insertion does not happen,\nstd::unique_ptr<>which makes them useful when working with maps whose values are move-only types such as (covered in chapter\nemplace(), try_emplace()27). Also, unlike treats the arguments of the key and mapped value separately, so you do not need\nstd::pair<> try_emplace(key, value_arg1, value_arg2, ...)the constructor arguments to directly construct a (i.e.,\nemplace(key, value) value_arg...works instead of which passes in an instance of the value directly, while can be used to\nvalue emplace().construct in-place). The return value is the same as that of\n1\nstd::unordered_map<std::string, BigObject> big_objects;\n2\nstd::string k = \"key\";\n3\n// insert key k, so k is now in the hash table with a BigObject value\n4\n// all subsequent insertions with key k should fail to insert a new key-value pair\n5\nbig_objects[k] = BigObject(arg1, arg2, arg3, ...);\n6\n// constructs a pair with BigObject that ends up being immediately destructed since k in table\n7\nbig_objects.insert({k, BigObject(arg1, arg2, arg3, ...)});\n8\n// constructs a pair with BigObject that ends up being immediately destructed since k in table\n9\nbig_objects.emplace(k, BigObject(arg1, arg2, arg3, ...));\n10\n// does not construct a BigObject at all since k is already in the table (optimal)\n11\nbig_objects.try_emplace(k, arg1, arg2, arg3, ...);\nstd::unordered_map<>Remark: Because the insert operations for an return a pair containing an iterator and a Boolean indicating if\nthe insertion was successful, C++17’s structured bindings can be useful in extracting this return value in a clean manner (if you want a\nrefresher on structured bindings, see section 11.13.1). An example is shown below:\n1\ndouble>std::unordered_map<std::string, food_prices;\n2\nauto [it, success] = food_prices.try_emplace(\"Apple\", 3.99);\n3\nstd::cout << it->first << '\\n';\n// prints \"Apple\"\n4\nstd::cout << it->second << '\\n';\n// prints \"3.99\"\n5\nit->second = 4.99;\n// change price of \"Apple\" to 4.99\n6\nstd::cout << success << '\\n';\n// prints \"1\" for true\n7\n8\n// this insertion does not do anything since \"Apple\" is already in table\n9\nauto [it2, success2] = food_prices.try_emplace(\"Apple\", 3.99);\n10\nstd::cout << it2->first << '\\n';\n// prints \"Apple\"\n11\nstd::cout << it2->second << '\\n';\n// prints \"4.99\" (from previous update)\n12\nstd::cout << success2 << '\\n';\n// prints \"0\" for false\ntemplate <typename typenameK, V>\nbool std::unordered_map<K, V>::empty();\nboolReturns a indicating whether the unordered map is empty.\n1\ndouble>std::unordered_map<std::string, food_prices;\n2\nstd::cout << food_prices.empty() << '\\n';\n// prints true\ntemplate <typename typenameK, V>\nsize_t std::unordered_map<K, V>::size();\nReturns the number of elements in the container.\n1\ndouble>std::unordered_map<std::string, food_prices;\n2\nfood_prices.insert({\"Apple\", 3.99});\n3\nfood_prices.insert({\"Avocado\", 4.99});\n4\nstd::cout << food_prices.size() << '\\n';\n// prints 2", "word_count": 656, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f1b02d1d-9d6c-5bb2-b0bb-4a9dae7fce09", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 556, "real_page_number": null, "text": "544\nChapter 17. Hash Tables and Collision Resolution\ntemplate <typename typenameK, V>\niterator std::unordered_map<K, V>::begin();\nstd::unordered_map<> (.cbegin() constReturns an iterator pointing to the first element in the container returns a version of\nthis iterator).\ntemplate <typename typenameK, V>\niterator std::unordered_map<K, V>::end();\nstd::unordered_map<> (.cend() constReturns an iterator pointing to one past the last element in the container returns a version\nof this iterator).\nbegin() end() std::unordered_map<>Theiteratorsreturnedby and pointtoakey-valuepair. Notethatan isunordered,andthusdoes\nbegin() end()first. However, if you iterate from the iterator to the iterator, you arenot provide a guarantee on which element is considered\nstd::unordered_map<>guaranteed to visit every element in the (however, the order in which elements are visited is non-deterministic).\n1\ndouble>std::unordered_map<std::string, food_prices;\n2\nfood_prices.insert({\"Apple\", 3.99});\n3\nauto it = food_prices.begin();\n4\nstd::cout << it->first << '\\n';\n// prints Apple\n5\nstd::cout << it->second << '\\n';\n// prints 3.99\ntemplate <typename typenameK, V>\nvoid std::unordered_map<K, V>::clear();\nErases all elements in the container, dropping size to 0.\ntemplate <typename typenameK, V>\nV>::erase(constiterator std::unordered_map<K, iterator pos);\nposErases the element pointed to by and returns an iterator to the element following the one that was removed.\ntemplate <typename typenameK, V>\nV>::erase(const constiterator std::unordered_map<K, iterator first, iterator last);\n[first, last)Erases all elements in the iterator range and returns an iterator to the element following the last element removed.\ntemplate <typename typenameK, V>\nsize_t V>::erase(conststd::unordered_map<K, K& key);\nkeyErases the element with the key equivalent to and returns the number of elements removed (which should always be 1 for an\nstd::unordered_map<> since keys are unique).\n1\ndouble>std::unordered_map<std::string, food_prices;\n2\nfood_prices.insert({\"Apple\", 3.99});\n3\nfood_prices.insert({\"Avocado\", 4.99});\n4\nfood_prices.erase(\"Apple\");\n5\nstd::cout << food_prices.size() << '\\n';\n// prints 1\ntemplate <typename typenameK, V>\nV>::operator[](constV& std::unordered_map<K, K& key);\nReturns a reference to the value associated with the given key. If the key does not already exist, it will be inserted into the container.\nWhen this happens, the value associated with this key is value-initialized.\noperator[key] operator[KEY] = VALUEIt is important to remember that creates the key if it does not already exist! As a result, can\n.insert() std::unordered_map<>:be used as an alternative to to add values into an\n1\ndouble>std::unordered_map<std::string, food_prices;\n2\nfood_prices[\"Apple\"] = 3.99;\n3\nfood_prices[\"Avocado\"] = 4.99;\n4\nfood_prices[\"Banana\"] = 2.49;\n5\nfood_prices[\"Flour\"] = 2.29;\n6\nfood_prices[\"Ginger\"] = 2.69;\n7\nfood_prices[\"Milk\"] = 2.09;\n8\nfood_prices[\"Tofu\"] = 3.79;\nstd::unordered_map<>However, this behavior can also lead to some unexpected consequences. Consider the following that stores the\nnumber of days in a month.\n1\nint32_t>std::unordered_map<std::string, days_in_month;\n2\ndays_in_month[\"January\"] = 31;\n3\ndays_in_month[\"February\"] = 28;\n4\ndays_in_month[\"March\"] = 31;\n5\ndays_in_month[\"April\"] = 30;\n6\ndays_in_month[\"May\"] = 31;\n7\ndays_in_month[\"June\"] = 30;\n8\ndays_in_month[\"July\"] = 31;\n9\ndays_in_month[\"August\"] = 31;\n10\ndays_in_month[\"September\"] = 30;\n11\ndays_in_month[\"October\"] = 31;\n12\ndays_in_month[\"November\"] = 30;\n13\ndays_in_month[\"December\"] = 31;", "word_count": 489, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "66edadbc-a7fe-54b2-98c3-80efe6bc7ab4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 557, "real_page_number": null, "text": "17.5 The STL Unordered Map Container\n545\nstd::unordered_map<>Suppose this is used in a program that takes in a month from the user and prints out the number of days in that\nmonth. The following implementation would be fully correct:not\n1\nstd::string input_month;\n2\nstd::cout << \"Enter a month name: \";\n3\nstd::cin >> input_month;\n4\n5\n// print out the number of days\n6\nstd::cout << input_month << \" has \" << days_in_month[input_month] << \" days\\n\";\n\"Feb\"If the user correctly inputs a month, this would run with no errors. However, what if the user entered a bad month name, like or\n\"Banana\"? operator[] would attempt to look up those keys, but they do not exist in the table. As a result, the operator would create an\n\"Feb\" \"Banana\"element in the table with the key or with a value that is value-initialized to zero — this is not something you want! Instead,\n.find()if you want to retrieve the value of a key that you are not sure actually exists, you will need to use the member function first.\ntemplate <typename typenameK, V>\nV>::find(constiterator std::unordered_map<K, K& key);\nkeyChecks if exists in the container. If it exists, the function returns an iterator to the element with that key. If it does not exist, the function\nendreturns an iterator that points one past the end (i.e., the iterator).\n.find() endThe correct way to look up an item is to first check if it exists by making sure that does not return the iterator. You\noperator[]should only use if you are certain that a key exists in the table. This prevents non-existent keys from being added to the\nstd::unordered_map<> without your awareness.\n1\nauto it = months.find(input_month);\n2\nif (it == months.end()) {\n3\nstd::cout << input_month << \" not found\\n\";\n4\n} // if\n5\nelse {\n6\nstd::cout << it->first << \" has \" << it->second << \" days\\n\";\n7\n} // else\nstd::unordered_map<> it->first it->secondSince iterators to an element in the point to a key-value pair, represents the key and\n.find()represents the value stored at that key. Furthermore, because returns an iterator to a key if it exists, you do need to make anothernot\nlookup to obtain the value of that key!\n.find() .count(), std::unordered_map<>An alternative to is which returns the number of elements with a given key. Since an\ndoes not allow duplicate keys, this function will always return either 0 or 1 (0 if the key does not exist, 1 if it does).\ntemplate <typename typenameK, V>\nsize_t V>::count(conststd::unordered_map<K, K& key);\nkey. std::unordered_map<>,Returns the number of elements whose key compares equal to For an this function returns 0 if the key\ndoes not exist, and 1 if it does.\n1\nif (months.count(input_month)) {\n2\nstd::cout << input_month << \" has \" << months[input_month] << \" days\\n\";\n3\n} // if\n4\nelse {\n5\nstd::cout << input_month << \" not found\\n\";\n6\n} // else\n.find()However, is preferred since it returns an iterator to the actual element that matches the given key. As a result, you can just use the\n.count(),iterator to access the value without having to make a second lookup. With you may need to make two lookups if the key exists: one\n.count() operator[]during the call to determine if the key exists, and one using to actually retrieve the value associated with that key.\nstd::unordered_map<>Behind the scenes, the STL is implemented as a hash table that uses separate chaining (with linked lists) to\nresolve collisions. This is due to the requirements of the C++ standard, which make it difficult to implement collision resolution in any other\nway. However, you may encounter different implementations of hash tables that use other collision resolution methods if you ever work for a\ncompany that has their own custom implementation.\nstd::unordered_map<>Since an relies on hashing for performance, the key of a hash table must be hashable (i.e., there must exist a\nkey_type int,double, std::string,hashfunctionthatcanconvertanobjectoftype intoanindex). Formanypre-definedtypeslike and\nstd::unordered_map<>a hash function is provided for you. However, if you want to create an where your key has a custom type, you\nwill need to define a custom hash function for that type (which will be discussed in more detail in section 17.8).\nstd::unordered_map<>Complexity of Operations\nInsert Element\nAccess Element\nErase Element\nFind Element\naverage-case Θ(1)\naverage-case Θ(1)\naverage-case Θ(1)\naverage-case Θ(1)\nworst-case Θ(𝑛)\nworst-case Θ(𝑛)\nworst-case Θ(𝑛)\nworst-case Θ(𝑛)", "word_count": 766, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "336311e4-cf92-554b-a74f-ec12c51e4008", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 558, "real_page_number": null, "text": "546\nChapter 17. Hash Tables and Collision Resolution\nExample 17.5 Write a function that takes in a text file in the form of a stream and prints out the word count for each word in the stream.\nFor example, given a stream with the contents:\neecs is fun is it\nYou would print out the following:\neecs 1\nis 2\nfun 1\nit 1\nThe words may be printed out in any order. You may assume that all the words in the stream are in lower case, and there is no punctuation\npresent. The function header is shown below:\nvoid print_word_count(std::istream& words);\nFor this problem, we are given a collection of words, and we want to identify the word count for each word. This fits perfectly with the key-value\nstd::unordered_map<>,structure that is ideal for a since we are given a key (a word) and an associated value that we need to lookup for\nstd::unordered_map<>that key (the word count). To solve this problem, we will create a that maps each word to its word count. Then,\nwe will read in each of the words and increment its count in the hash table. The code is shown below:\n1\nvoid print_word_count(std::istream& words) {\n2\nint32_t>std::unordered_map<std::string, word_count_map;\n3\nstd::string current_word;\n4\nwhile (words >> current_word) {\n5\n++word_count_map[current_word];\n6\n} // while\n7\nfor (auto& word_pair : word_count_map) {\n8\nstd::cout << word_pair.first << \" \" << word_pair.second << '\\n';\n9\n} // for word_pair\n10\n} // print_word_count()\nstd::unordered_map<> operator[];On line 5, we do not check if the word already exists in the before we use as a result, if\ncurrent_word does not already exist in the hash table, it gets added automatically (with a word count that is value-initialized to 0). For this\noperator[]specific problem, this is intended behavior. However, this is not always the case, and using on non-existent keys is dangerous if\nyou do not want to automatically add these keys to the table.\nstd::unordered_map<>,If you iterate through an each element that you visit is actually a key-value pair , so you will have to call\n.first .secondto obtain the key, and to obtain the value (or use a structured binding). This is shown on line 7 (although you can just use\nauto std::unordered_map<>if you do not want to explicitly write out the type). In addition, because keys in an are unordered, you\ncannot guarantee the order in which words are printed out. If you want to print out the words in a predetermined order, you will have to rely on a\nstd::map<>,different data structure that supports this functionality (one possible alternative is a which will be covered in the next chapter).\noperator[]Because returns a reference to the value associated with a given key, you are allowed to do things like this:\n1\nstruct Student {\n2\nstd::string uniqname;\n3\nstd::string full_name;\n4\nstd::vector<double> grades;\n5\n};\n6\n7\nstd::unordered_map<std::string, Student> all_students;\n8\n9\nvoid add_grade(const doublestd::string& uniqname, grade) {\n10\nall_students[uniqname].grades.push_back(grade);\n11\n} // add_grade()\nall_students[uniqname] Student uniqname,On line 10, is a reference to the object associated with the key which is why we were\nall_students[uniqname].grades,able to directly access which is a vector. This makes it easy to retrieve and work with the value of\noperator[]a key without having to explicitly store it somewhere else. However, you have to be careful not to call on the same key over and\noperator[]over again if you do not need to, since every call to requires a lookup (which hashes the key). Consider the following code:\n1\nstruct Employee {\n2\nstd::string title;\n3\ndouble salary;\n4\nint32_t years_working;\n5\n};\n6\n7\nstd::unordered_map<std::string, Employee> employee_map;\n8\n9\nvoid update_employee_info(const conststd::string& name, std::string& title,\n10\ndouble int32_tsalary, years_working) {\n11\nemployee_map[name].title = title;\n12\nemployee_map[name].salary = salary;\n13\nemployee_map[name].years_working = years_working;\n14\n} // update_employee_info()", "word_count": 651, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ad11432a-508c-555e-b554-8801d0da7523", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 559, "real_page_number": null, "text": "17.6 The STL Unordered Set Container\n547\nThis code ends up making three lookups (on lines 11, 12, and 13), when only one lookup is necessary. Although each lookup takes constant\ntime, the work required to hash and look up each key is non-negligible and can quickly accumulate. The alternative is to only make one lookup\nnameand store an iterator or reference to the value you want to use (the following code assumes that is guaranteed to be a key in the table).\n1\n// Method 1: store an iterator (only one lookup needed)\n2\nauto it = employee_map.find(name);\n3\nit->second.title = title;\n4\nit->second.salary = salary;\n5\nit->second.years_working = years_working;\n6\n7\n// Method 2: store a reference (only one lookup needed)\n8\nauto& employee = employee_map[name]; // make sure to include the ampersand\n9\nemployee.title = title;\n10\nemployee.salary = salary;\n11\nemployee.years_working = years_working;\nThis is particularly true if you are trying to check the existence of a key before you use it. From a performance standpoint, you should avoid\ndoing something like this:\n1\nif (my_map.find(key) != my_map.end()) {\n2\nstd::cout << \"Found: \" << my_map[key] << '\\n';\n3\n} // if\n.find() operator[]Notice that the code above makes two lookups instead of one: once with the call on line 1 and once with the call on\n.find() operator[]line 2. To fix this, use the iterator returned by instead of calling again:\n1\nauto it = my_map.find(key);\n2\nif (it != my_map.end()) {\n3\nstd::cout << \"Found: \" << iter->second << '\\n';\n4\n} // if\nWith this change, only a single lookup is made.\nstd::unordered_map<>Remark: The is a useful data structure that can be used to solve many different types of problems. However,\na hash table may not be the best container to use depending on the problem at hand. If your keys are small integers, it may be better to use an\nvector and use direct addressing to look up values associated with each key. In general, if a problem can be efficiently solved without a hash\ntable, then it may make sense not to use one. Hash tables are extremely versatile, but they also require significantly higher memory overhead\nand sophisticated operations to support their functionality (e.g., computing a hash for every key). Depending on the problem you are trying\nto solve, an overreliance on hash tables may end up making your time and memory performance worse!\n17.6\nThe STL Unordered Set Container\nstd::unordered_set<> <unordered_set>The STL also provides the container, which can be found in the library. Like an\nstd::unordered_map<>, std::unordered_set<>an stores elements in no particular order. However, one major differ-unique\nstd::unordered_set<>ence with a is that the value of an element is at the same time treated as its key. Much like its name implies, an\nstd::unordered_set<> is a good container to use if you want a way to efficiently keep track of whether an item exists in a set.\nstd::unordered_map<>, std::unordered_set<>Similar to the STL’s the is implemented using a hash table as its underlying\nstd::unordered_set<>structure, with separate chaining as the collision resolution technique. An also supports many of the same\nstd::unordered_map<>, operator[]operations as an with the exception of (as there is no mapping from a key to a separate value).\nstd::unordered_set<>The complexities of operations are summarized below:\nInsert Element\nAccess Element\nErase Element\nFind Element\naverage-case Θ(1)\naverage-case Θ(1)\naverage-case Θ(1)\naverage-case Θ(1)\nworst-case Θ(𝑛)\nworst-case Θ(𝑛)\nworst-case Θ(𝑛)\nworst-case Θ(𝑛)\nstd::unordered_set<>A few common operations are summarized below.\ntemplate <typename K>\nbool> std::unordered_set<K>::insert(conststd::pair<iterator, K& key);\nstd::unordered_set<>Attempts to insert a key into the container. Since keys in an must be unique, an insertion is done only if the\nkey does not already exist in the container. Returns a pair consisting of an iterator to the inserted element (or the element that prevented\nboolinsertion) and a for whether the insertion took place.\ntemplate <typename typenameK, InputIterator>\nvoid std::unordered_set<K>::insert(InputIterator first, InputIterator last);\n[first, last) std::unordered_set<>.Attempts to insert all elements from the range into the\ntemplate <typename K>\nvoid std::unordered_set<K>::insert(std::initializer_list<K> ilist);\nilist std::unordered_set<>Attempts to insert all the elements from the initializer list into the (for example, the line\nmy_set.insert({1, 3, 5, 7}) inserts the elements 1, 3, 5, and 7).", "word_count": 713, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0dba8d1c-4ed3-5d8e-b54e-a1fadb568ce8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 560, "real_page_number": null, "text": "548\nChapter 17. Hash Tables and Collision Resolution\n1\nstd::unordered_set<int32_t> my_set;\n2\nmy_set.insert(280);\n3\nmy_set.insert(281);\n4\nmy_set.insert(370);\n5\nmy_set.insert(376);\n6\n7\nstd::vector<int32_t> vec = {481, 482, 483, 484, 485, 486};\n8\nmy_set.insert(vec.begin(), vec.end());\n9\n// my_set now stores {280, 281, 370, 376, 481, 482, 483, 484, 485, 486} (may NOT be in this order)\ntemplate <typename K>\nbool std::unordered_set<K>::empty();\nboolReturns a indicating whether the unordered set is empty.\ntemplate <typename K>\nsize_t std::unordered_set<K>::size();\nReturns the number of elements in the container.\ntemplate <typename K>\niterator std::unordered_set<K>::begin();\nstd::unordered_set<> (.cbegin() constReturns an iterator pointing to the first element in the container returns a version of\nthis iterator).\ntemplate <typename K>\niterator std::unordered_set<K>::end();\nstd::unordered_set<> (.cend() constReturns an iterator pointing to one past the last element in the container returns a version\nof this iterator).\ntemplate <typename K>\nvoid std::unordered_set<K>::clear();\nErases all elements in the container, dropping size to 0.\ntemplate <typename K>\nstd::unordered_set<K>::erase(constiterator iterator pos);\nposErases the element pointed to by and returns an iterator to the element following the one that was removed.\ntemplate <typename K>\nstd::unordered_set<K>::erase(const constiterator iterator first, iterator last);\n[first, last)Erases all elements in the iterator range and returns an iterator to the element following the last element removed.\ntemplate <typename K>\nsize_t std::unordered_set<K>::erase(const K& key);\nkeyErases the element with the key equivalent to and returns the number of elements removed (which is always 1 since keys are unique).\ntemplate <typename K>\nstd::unordered_set<K>::find(constiterator K& key);\nkeyChecks if exists in the container. If it exists, the function returns an iterator to the element with that key. If it does not exist, the function\nendreturns an iterator that points one past the end (i.e., the iterator).\ntemplate <typename K>\nsize_t std::unordered_set<K>::count(const K& key);\nkey. std::unordered_set<>,Returns the number of elements whose key compares equal to For an this function returns 0 if the key\ndoes not exist, and 1 if it does.\nExample 17.6 Given a vector of integers, write a program that finds the first repeated integer in the container. If there are no repeated\n-1.integers in the vector, return\nstd::unordered_set<>Since this problem is asking us to determine if we have seen an element before, an would be an appropriate con-\nstd::unordered_set<>tainertouse(forthisexample,thereisnoneedtomapakeytoavalue). Tosolvethisproblem,wewouldinitializean\nand iterate through the vector. For each element, we would first check if it already exists in our set. If it exists, we have found the first repeated\ninteger and would return that value. Otherwise, we would add the value into the set and continue looking. The solution code is shown below:\n1\nint32_t first_repeated_element(const std::vector<int32_t>& vec) {\n2\nstd::unordered_set<int32_t> seen;\n3\nfor (int32_t val : vec) {\n4\nauto it = seen.find(val);\n5\nif (it == seen.end()) {\n// element not found\n6\nseen.insert(val);\n7\n} // if\n8\nelse {\n// element seen before\n9\nreturn *it;\n10\n} // else\n11\n} // for val\n12\nreturn -1;\n13\n} // first_repeated_element()\nThe worst-case time complexity of this solution is Θ(𝑛), where 𝑛is the size of the vector of integers.", "word_count": 533, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3822ea6c-fef4-538e-a3e9-fd225ce0adf1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 561, "real_page_number": null, "text": "17.7 The STL Unordered Multimap and Unordered Multiset Containers\n549\n17.7\nThe STL Unordered Multimap and Unordered Multiset Containers (✽)\nstd::unordered_multimap<>The STL also provides an container, which is an unordered map that supports duplicate keys. Since the\nsame key can exist more than once in these containers, a single key may be mapped to multiple values. Thus, to retrieve the value of a key in an\nstd::unordered_multimap<>, you have to iterate through all elements in the multimap that share that key. To retrieve all the elements\n.equal_range()that share a given key, you can use the member function:\ntemplate <typename typenameK, V>\nV>::equal_range(conststd::pair<iterator, iterator> std::unordered_multimap<K, K& key);\nReturns an iterator range containing all elements in the multimap with the given key. Like with other STL algorithms, the first iterator\nreturned is inclusive, and the second iterator returned is exclusive.\nFor example, the following multimap maps department names to the classes provided under that department.\n1\nint32_t>std::unordered_multimap<std::string, classes;\n2\nclasses.insert({\"EECS\", 280});\n3\nclasses.insert({\"MATH\", 217});\n4\nclasses.insert({\"EECS\", 183});\n5\nclasses.insert({\"ENGR\", 101});\n6\nclasses.insert({\"EECS\", 281});\n\"EECS\", .equal_range(\"EECS\")If you want to retrieve all values in the multimap that have the key you can call to get an iterator range\nthat includes all elements in the multimap with that key. You would then have to iterate through the range and visit each value individually.\n7\nauto iter_range = classes.equal_range(\"EECS\");\n8\nfor (auto it = iter_range.first; it != iter_range.second; ++it) {\n9\nstd::cout << it->first << \" \" << it->second << '\\n';\n10\n} // for it\nstd::unordered_multimap<>Since the is unordered, you cannot guarantee the order of elements in the iterator range. The output of\nthe above code could be the following (these three lines need not be printed in this order):\nEECS 281\nEECS 183\nEECS 280\nHowever, for this course, there will never be a need to use this container. If you want a key to map to multiple values, you can just use a standard\nstd::unordered_map<> and map each key to a container of values. This gives you greater freedom and flexibility when working with the\nvalues associated with each key and removes the risk of non-deterministic behavior (like the scenario above). For example, the following code\ndoes the same thing as a the code above, but without the need for a multimap:\n1\n// map each string to a vector of ints\n2\nstd::vector<int32_t>>std::unordered_map<std::string, classes;\n3\nclasses[\"EECS\"].push_back(280);\n4\nclasses[\"MATH\"].push_back(217);\n5\nclasses[\"EECS\"].push_back(183);\n6\nclasses[\"ENGR\"].push_back(101);\n7\nclasses[\"EECS\"].push_back(281);\n8\n// print out all values associated with \"EECS\"\n9\nauto it = classes.find(\"EECS\");\n10\nif (it != classes.end()) {\n11\nfor (int32_t class_num : it->second) {\n12\nstd::cout << it->first << \" \" << class_num << '\\n';\n13\n} // for class_num\n14\n} // if\nstd::unordered_multiset<>The is another container that is provided by the STL. As its name implies, this container behaves like\nstd::unordered_set<> std::unordered_multiset<>an but allows for duplicate keys. Much like before, the relies on the\n.equal_range() member function to return an iterator range to all instances of a key that exists within the multiset.\nstd::unordered_multimap<> std::unordered_multiset<>Remark: If you are curious about the STL’s and containers,\nyou can read their documentation. However, you will be responsible for knowing how to use these containers for this class. This sectionnot\nsimply provides a brief introduction to these containers to let you know they exist, but anything you can do with an unordered multimap or\nunordered multiset for this class can also be done with another container class.", "word_count": 582, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "adae3c30-466b-539e-8710-10e9f78d4501", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 562, "real_page_number": null, "text": "550\nChapter 17. Hash Tables and Collision Resolution\n17.8\nSTL Hashing and Composite Hash Functions\n¸ 17.8.1\nstd::hash\nint,double, std::string,Asmentionedearlier,hashfunctionsformanypre-definedtypes,suchas and areprovidedbytheC++standard\nstd::hash<>,library. The default hash function used by the standard library can be accessed using a unary function object defined in the\n<functional> std::hash<>library. If you invoke the functor on an argument, you would get the hash value of that argument, using a\ndefault hash function provided by the standard library.\n1\nstd::hash<std::string> hasher;\n// hash function object on strings\n2\nstd::cout << hasher(\"EECS281\") << '\\n';\n// prints hash value of \"EECS281\"\n\"EECS281\"Theactualhashfunctionitselfisimplementation-dependent. Forinstance, whentheabovehasherwasrunonthestring inaCAEN\nenvironment, the hash value was 4147570959360960813. However, when the same code was run on Microsoft Visual Studio, the hash value\nstd::hash<>was 3475765778. Nonetheless, these differences should not matter, since the hash function implemented by always hashes the\n\"EECS281\"same key to the same hash value regardless of what environment you are using (i.e., should always hash to 4147570959360960813\non CAEN using this hasher), and it also ensures that the probability of two different keys hashing to the same hash value is near zero.\n¸ 17.8.2\n(✽)Composite Hash Functions\nHashing with pre-defined types is easy, since you can rely on hash functions that are provided by the standard library. However, what if you\nwanted to hash a custom object type? For instance, suppose you wanted to construct a hash table that maps coordinate points to location data,\nwhere the coordinates are defined as a custom object:\n1\nstruct Coordinate {\n2\nint32_t x;\n3\nint32_t y;\n4\n};\n5\n6\nstd::unordered_map<Coordinate, std::string> location_map;\nCoordinateThe above code would not compile, since does not support a hash function, and thus cannot be the type of the key. If you wanted\nCoordinate Coordinateto use a as the type of the key, you will have to define a custom hash function for the object. One way to do so is\nto use a function, which combines the hash values of an object’s components to calculate the hash value of the entire objectcomposite hash\nitself. One example of a composite hash function is\n𝐻({𝑥,𝑦}) ℎ(𝑥)+𝑝×ℎ(𝑦)=\nwhere ℎis the hash function for each coordinate value, 𝐻is the composite hash function for the entire coordinate object, and 𝑝is an arbitrarily\nchosen integer. For instance, if 10, 20, and 5, the hash value of the coordinate would be 110.ℎ(15) ℎ(−27) 𝑝= (15,−27)= = 10+5×20=\nHowever, a hash function that linearly combines the individual hash values of its components is not entirely ideal. This is because collisions\nusing this hash function are easily predictable. If you want to write a hash function for a custom object, it is much better to combine the hash\ncombination.2values of its components using the following equation instead of a linear Here, is the hash value of component 𝑖of theℎ(𝑘𝑖)\ncustom object, and 𝐶is a constant (in the Boost library, 𝐶is the hexadecimal constant 0x9e3779b9, but you do not need to know why this\nnumber was chosen). (You are not required to know the following material for this class!)\nseed ^= ℎ(𝑘𝑖)+𝐶+(seed << 6)+(seed >> 2)\nCoordinateFor example, if we started with an initial seed of 0, and and 20, the object would have a hashℎ(15) ℎ(−27) (15,−27)=10 =\nvalue of 175247765808:\n1. Combine the hash value of with seed:𝑘1\n0 ^= 10 + 0x9e3779b9 + (0 << 6) + (0 >> 2) = 2654435779\n2. Combine the hash value of with seed:𝑘2\n2654435779 ^= 20 + 0x9e3779b9 + (2654435779 << 6) + (2654435779 >> 2) = 175247765808\n2This boost::hash_combine.is the implementation of the hash combiner used by the Boost library, However, Boost is prohibited in this class, so the\nexamplesonthefollowingfewpagesimplementthishashcombinerasaseparatefunction(insteadofimportingitfromBoost).", "word_count": 677, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6e6dc028-07e1-5a15-ad26-2a3fe442672c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 563, "real_page_number": null, "text": "17.8 STL Hashing and Composite Hash Functions\n551\nRemark: What do the symbols ^=, << , and >> mean in the hash combiner equation? If you have never seen them before, don’t worry!\nThese are bit manipulation operators, which are covered in EECS 370 and not in this class. ^= is the operator,bitwise XOR assignment\na ^= b a a b.where assigns with the result of taking the XOR of and To take the XOR (short for eXclusive OR) of two numbers, traverse\nthe binary representation of the two numbers, compare the digits at each position, and return 1 if two bits are different and 0 if they are the\nsame. For instance, the XOR of 281 (100011001 in binary) and 370 (101110010 in binary) is 107 (001101011 in binary):\n100011001\n101110010\n001101011\nThe << and >> might look like the insertion and extraction operators for streams, but in this case, they are operators. If youbit shifting\napply a left shift (<<) on a number, you shift its underlying binary bit pattern to the left by one.\n00000011\n(3 in decimal)\n00000110\n(left shift, 6 in decimal)\n00001100\n(left shift, 12 in decimal)\n00011000\n(left shift, 24 in decimal)\n00110000\n(left shift, 48 in decimal)\n01100000\n(left shift, 96 in decimal)\nSimilarly, applying a right shift (>>) on a number shifts its underlying binary bit pattern to the right by one.\n01011001\n(89 in decimal)\n00101100\n(right shift, 44 in decimal)\n00010110\n(right shift, 22 in decimal)\n00001011\n(right shift, 11 in decimal)\n00000101\n(right shift, 5 in decimal)\n00000010\n(right shift, 2 in decimal)\n00000001\n(right shift, 1 in decimal)\nIn the previous formula, the term (seed << 6) is the value obtained after performing 6 left shifts on the value of seed, and the term (seed >>\n2) is the value obtained after performing 2 right shifts on the value of seed.\nCoordinatePutting this all together, we can write the following code to generate the hash value of a object:\n1\nvoid hash_combine(size_t& const int32_tseed, v) {\n2\nstd::hash<int32_t> hasher;\n3\nseed ^= hasher(v) + 0x9e3779b9 + (seed << 6) + (seed >> 2);\n4\n} // hash_combine()\n5\n6\nsize_t hash_coordinate(const Coordinate& coord) {\n7\nsize_t hash_value = 0;\n8\nhash_combine(hash_value, coord.x);\n9\nhash_combine(hash_value, coord.y);\n10\nreturn hash_value;\n11\n} // hash_coordinate()\n¸ 17.8.3\n(✽)Hashing a Custom Type\nCoordinateWith this information, we can actually define so that it can be accepted as the key-type of STL containers that utilize hashing,\nstd::unordered_map<> std::unordered_set<>.such as the and To be able to use these containers with a custom key-type, we\nmust define two things:\noperator()1. A hash function functor that overrides and calculates the hash value of an object with the type of the key.\n2. A comparison function for equality so that the hash table can compare instances of the custom-defined key in the case of a collision.\noperator() operator==This can be done by either (1) defining an equality predicate that overrides or by (2) overloading for the\noperator==custom type (overloading is typically the simpler method of the two).\nCoordinateUsing the object as an example, let’s start by defining the comparison function for equality. We can do this by overloading\noperator== Coordinate Coordinatein the definition of the type so that different objects can be compared.\n1\nstruct Coordinate {\n2\nint32_t x;\n3\nint32_t y;\n4\nbool operator==(const constCoordinate& other) {\n5\nreturn (x == other.x && y == other.y);\n6\n} // operator==()\n7\n};", "word_count": 706, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "eeb4da5c-d816-575d-b70f-ff152031afa5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 564, "real_page_number": null, "text": "552\nChapter 17. Hash Tables and Collision Resolution\nCoordinateNow, we will need to define a hash function for the type. Using the composite hash function discussed earlier, we can write the\nCoordinateHasher Coordinatefollowing functor, which can be used to calculate the hash value of a object by combining the hash\nvalues of its individual components.\n9\nstruct CoordinateHasher {\n10\nvoid hash_combine(size_t& const int32_t constseed, v) {\n11\nstd::hash<int32_t> hasher;\n12\nseed ^= hasher(v) + 0x9e3779b9 + (seed << 6) + (seed >> 2);\n13\n} // hash_combine()\n14\n15\nsize_t operator()(const constCoordinate& coord) {\n16\nsize_t hash_value = 0;\n17\nhash_combine(hash_value, coord.x);\n18\nhash_combine(hash_value, coord.y);\n19\nreturn hash_value;\n20\n} // operator()\n21\n};\nCoordinate std::unordered_map<> std::unordered_set<>.Aftercompletingthesesteps,wecannowuse asthekey-typeofan or\nCoordinateHasher std::unordered_map<>The following code will now compile (notice that we pass in as the third argument of the\nCoordinateHashertemplate on line 24, which lets the map know to use to hash each coordinate):\n23\nint main() {\n24\nstd::unordered_map<Coordinate, std::string, CoordinateHasher> location_map = {\n25\n{{-738, 277}, \"Diag\"},\n26\n{{-717, 291}, \"Pierpont\"},\n27\n{{-721, 294}, \"Bursley\"},\n28\n{{-716, 293}, \"BBB\"},\n29\n{{-738, 273}, \"Ross\"}\n30\n};\n31\n32\nCoordinate c = {-717, 291};\n33\nauto it = location_map.find(c);\n34\nif (it != location_map.end()) {\n35\nstd::cout << \"The landmark at coordinates (\" << c.x << \", \" << c.y << \") is \"\n36\n<< it->second << '\\n';\n37\n} // if\n38\n} // main()\nThe output of this code is:\nThe landmark at coordinates (-717, 291) is Pierpont\nstd::unordered_set<>;We can also apply a composite hash function in an this can be done by passing in the hash combiner functor as\nthe second term of the container’s initialization template. An example is shown below:\n23\nint main() {\n24\nstd::unordered_set<Coordinate, CoordinateHasher> location_set;\n25\nCoordinate c = {-717, 291};\n26\nlocation_set.insert(c);\n27\nstd::cout << location_set.count(c) << '\\n';\n// prints 1\n28\n} // main()\n17.9\nSolving Problems Using Hash Tables\nBecause they provide fast key-value lookup, hash tables are quite versatile and can be used to solve many different types of problems. In this\nsection, we will go over a few problems that can be solved using a hash table container.\nExample 17.7 Back in chapter 15, we introduced a problem where you are given an array of 𝑛positive integers that are sorted in ascending\norder, and you want to find a pair of two numbers in the array that sums to a given target number. We then introduced a time solutionΘ(𝑛)\nusing the two pointer technique, where two indices are selectively incremented toward each other until the target sum is found.\n[7,Consider the same problem, but this time For example, given the arraythe array of integers is no longer sorted in ascending order.\n10, 3, 9, 4] 11, [4, 7]. [-1, -1]and a target of you would return Return if there is no way to sum to the target number. Is it still\npossible to solve this problem in linear time, and what is the approach if it can?\nWithout the numbers being sorted, the two pointer technique can no longer be used, since it relies on knowledge on whether the pointers are\nmoving toward a smaller or larger number. Because of this, we will need to find an alternative solution for this problem.\nNotice that we want to find the solution to this problem in average-case linear time. This means that, if we iterate over the given array, the\namount of work done at each element toward solving the problem must be constant on average. Therefore, if you want to store any information\nfor each element, you would preferably want to store it in a constant-time lookup container like a hash table.\nWhat information do we need to know at each element? Using our target value of 11, suppose we encountered 7 while iterating over our\narray. In this case, we would know that 7 is in the solution if and only if its complement of 11 - 7 = 4 is also in the array. However, we do\n[4, 7]not yet know if 4 is in the array or not, so we will need to store this information somewhere so that we can return the solution of if\nwe ever encounter 4 in the array. This can be done using a hashing container that supports constant-time insertion and lookup, such as the\nstd::unordered_set<>.", "word_count": 750, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "02581c44-06c0-5add-bc63-e2c8ecb9af41", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 565, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n553\nThis forms the basis of our linear-time solution. We would iterate over the array one element at a time. At each element, we would push its\nstd::unordered_set<>.complement (the value you get after subtracting the element from the target value) into an Then, if you ever\nstd::unordered_set<>,encounter an element in the array that is in the you can simply return the corresponding pair. An illustration of\nthis process is shown below using the example input array.\n7\n10\n3\n9\n4\nstd::unordered_set<>\n{}\nThe first element we consider in the array is 7. Since our target is 11, the complement of 7 is 11 - 7 = 4, so we insert 4 into our set.\n7\n10\n3\n9\n4\nstd::unordered_set<>\n{4}\nThe next element in the array is 10. 10 is not in our set, so it is not part of a solution yet. We then insert 10’s complement of 1 into the set.\n7\n10\n3\n9\n4\nstd::unordered_set<>\n{4, 1}\nThe next element in the array is 3. 3 is not in our set, so it is not part of a solution yet. We then insert 3’s complement of 8 into the set.\n7\n10\n3\n9\n4\nstd::unordered_set<>\n{4, 1, 8}\nThe next element in the array is 9. 9 is not in our set, so it is not part of a solution yet. We then insert 9’s complement of 2 into the set.\n7\n10\n3\n9\n4\nstd::unordered_set<>\n{4, 1, 8, 2}\nThe next element in the array is 4. 4 is in our set, which means that we had encountered its complement in the array before. Thus, we know that\n[4, 7].4 (along with its complement of 7) must be a valid pair that sums to our target, so we return\n7\n10\n3\n9\n4\nstd::unordered_set<>\n{4, 1, 8, 2}\nIf we manage to iterate over the array without ever encountering a value that was already in the set, then there would be no solution.\nRemark: If we had been asked to return the of the two values that sum to the target (instead of the values themselves), thenindices\nwe would need a way to keep track of the index of each value in the input array. This additional bit of information, however, does not\nstd::unordered_map<> std::unordered_set<>,alter our solution much; we can address this by using an instead of an where\neach key is mapped to the index of its complement. Once we encounter a value in our map, we can also look up its complement’s index\nin the map, thereby allowing us to easily return both indices in our solution. Our solution still runs in average-case linear time, as the\nlookup.3std::unordered_map<> also supports average-case constant time insertion and\nAn implementation of this solution is shown below:\n1\nstd::pair<int32_t, int32_t> target_sum(const std::vector<int32_t>& int32_tvec, target) {\n2\nstd::unordered_set<int32_t> complements;\n3\nfor (int32_t val : vec) {\n4\nauto it = complements.find(val);\n5\nif (it == complements.end()) {\n6\ncomplements.insert(target - val);\n7\n} // if\n8\nelse {\n9\nreturn {val, target - val};\n10\n} // else\n11\n} // for\n12\nreturn {-1, -1};\n// no solution\n13\n} // target_sum()\nThe average-case time complexity of this solution is Θ(𝑛), where 𝑛is the number of values in the input vector. This is because we are looping\nthrough the array once and then performing a lookup and/or an insertion on each item in the array (both of which take average-case constant\ntime with a hash table container). The auxiliary space used by this solution is also Θ(𝑛), since we are inserting complements into a separate\ncontainer as we iterate over the array.\n3Thisvariationisactuallyafamousinterviewproblemknownasthe problem,andyoumayseeitmentionedassuchfromtimetotime.two-sum", "word_count": 648, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c156b217-4302-5970-b565-c216b912aea5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 566, "real_page_number": null, "text": "554\nChapter 17. Hash Tables and Collision Resolution\nExample 17.8 EECS 281 has had difficulty keeping up with the office hours demand this semester and has decided to limit the frequency\nthat students can join the queue to minutes. Unfortunately, we cannot code, so that is up to you! You are given a class object,once every 60\nallow_to_join()YamBot, that handles all requests to join the queue. You must implement the class function that takes in a timestamp\nfalse.(an integer in minutes) and a uniqname. If the student with that uniqname has joined the queue within the last 60 minutes, return\ntrue.Otherwise, return (Note: exam.)This was a proposed written exam question for the Fall 2020 final\nTime Complexity: per call on averageΘ(1)\nSpace Complexity: on average for making any number of calls with 𝑛different uniqnamesΘ(𝑛)\nExample:\nUniqname\nTimestamp\nExplanation\njoericha\n0\ntrueJoe hasn’t been on the queue yet, so return\ndanlliu\n35\ntrueDaniel hasn’t been on the queue yet, so return\njoericha\n40\nfalseJoe previously joined the queue at time 0, so return\nrushilk\n50\ntrueRushil hasn’t been on the queue yet, so return\njoericha\n70\ntrueJoe previously joined the queue at time 0, so his cooldown has finished, return\ndanlliu\n75\nfalseDaniel previously joined the queue at time 35, so return\nrushilk\n110\ntrueRushil previously joined the queue at time 50, so his cooldown has finished, return\nStarter Code:\n1\nclass YamBot {\n2\nprivate:\n3\n// TODO: Add any data structures here!\n4\npublic:\n5\n// Returns true if the student should be allowed to join the OH queue at\n6\n// the given timestamp. Otherwise, the student will not be allowed to\n7\n// join the queue, and you should return false. The timestamp is in minutes.\n8\nbool allow_to_join(const int32_tstd::string& uniqname, timestamp) {\n9\n// TODO: Implement code here\n10\n} // allow_to_join()\n11\n};\nTo successfully solve this problem, we would need to know the last timestamp at which each student joined the queue. Every time a student\ntries to join the queue, we would then compare the current timestamp with this most recent timestamp to determine if the student should be\nstd::unordered_map<>,allowed into the queue again or not. This behavior fits well with the key-value lookup behavior supported by an\nespecially since lookups can be done in average-case (which is required by the problem).Θ(1)\nstd::unordered_map<>We can therefore implement this solution by using a that maps each student’s uniqname to the last time they\nallow_to_join()joined the queue. Every time is called on a student, we look up this student in the map to identify when they last joined\nthe queue. There are three outcomes that may arise from this lookup:\n• If the lookup fails to find the student’s name, then the student never joined the queue before. We would therefore add their uniqname to\ntrue.the map with the current timestamp as the value and return\n• If we find the student in the map, and their most recent timestamp was 60+ minutes ago, then their cooldown has finished and they can\njoin the queue again. We would therefore update their most recent timestamp to the current time (since they joined the queue again) and\ntrue.return\nIf we find the student in the map, and their most recent timestamp was less than 60 minutes ago, then they joined the queue too recently•\nfalse.and should not be allowed to join again. Therefore, we return\nAn implementation of this solution is shown below:\n1\nclass YamBot {\n2\nprivate:\n3\nint32_t>std::unordered_map<std::string, oh_map;\n4\npublic:\n5\nbool allow_to_join(const int32_tstd::string& uniqname, timestamp) {\n6\nauto it = oh_map.find(uniqname);\n7\n// first time joining the queue, add to map and return true\n8\nif (it == oh_map.end()) {\n9\noh_map[uniqname] = timestamp;\n10\nreturn true;\n11\n} // if\n12\n// cooldown finished, update timestamp and return true\n13\nelse if (timestamp - it->second >= 60) {\n14\nit->second = timestamp;\n15\nreturn true;\n16\n} // else if\n17\n// otherwise, joined less than 60 minutes ago, return false\n18\nreturn false;\n19\n} // allow_to_join()\n20\n};", "word_count": 703, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "98d99ad0-02b6-59bc-a7da-262af9bc640c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 567, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n555\nExample 17.9 A is a temporary data storage location that holds frequently accessed data, allowing applications to speed up datacache\nretrieval. As an example, your browser may use a cache to store frequently accessed websites so that you do not have to refetch the contents\nof a website whenever you visit it; you can simply pull the data from a cache. This is beneficial because the contents of a cache are stored in\nmemory that supports high-speed access (e.g., static random-access memory (SRAM), but this is an EECS 370 concept), thereby allowing\nyou to access memory in a cache much faster than from anywhere else. Unfortunately, this type of memory is also expensive, so the capacity\nof the cache is bounded; you can only keep a limited amount of data in a cache at any point in time. Since the data that you want to work\nwith may exceed the storage size of the cache, you will need a way to determine what data should be kept in the cache and what data should\nbe removed if the cache capacity is reached. (Note: None of the information is class material, just background for this problem.)\nOne such method is to use a cache, which uses a policy. In a LRU cache, the least recently used item isLRU least-recently used (LRU)\nevicted from the cache first if the cache ever goes above its capacity. As an example, consider a cache with a capacity of 4 that stores the\nprices of items at a store (similar to the first example in this chapter). The following queries are made:\n• Update price of apple to $3.99.\n• Update price of banana to $2.49.\n• Update price of ginger to $2.69.\n• Update price of apple to $3.79.\n• Update price of milk to $2.09.\n• Update price of tofu to $3.79.\n• Update price of ginger to $2.79.\n• Update price of banana to $2.59.\n{\"Apple\", 3.99}We first update the price of apple. Since there is nothing in the cache yet, the key-value pair of is added to the cache.\nBecause it is the only value in the cache, it is also the least recently used value by default.\nLRU Cache (Capacity 4)\nItem\nPrice\nLRU?\n\"Apple\"\n3.99\n✓\nNext, we update the price of banana. Since banana is not in the cache, we add it. The same applies for ginger, which is added next.\nLRU Cache (Capacity 4)\nItem\nPrice\nLRU?\n\"Apple\"\n3.99\n✓\n\"Banana\"\n2.49\n\"Ginger\"\n2.69\n\"Apple\"Next, we update the price of apple to $3.79. Notice that is currently the least recently used element in the cache, but that would\n\"Banana\".no longer be the case after the update. Instead, the least recently used element would become\nLRU Cache (Capacity 4)\nItem\nPrice\nLRU?\n\"Apple\"\n3.79\n\"Banana\"\n2.49\n✓\n\"Ginger\"\n2.69\nNext, we update the price of milk to $2.09. Milk is not currently in the cache, so we add it.\nLRU Cache (Capacity 4)\nItem\nPrice\nLRU?\n\"Apple\"\n3.79\n\"Banana\"\n2.49\n✓\n\"Ginger\"\n2.69\n\"Milk\"\n2.09\nNext, we update the price of tofu to $3.79. Tofu is not in the cache, so we need to add it. However, our cache is already filled to capacity, so\n\"Banana\". \"Banana\" \"Tofu\".we will need to evict the least recently used element, which is After is evicted, we add in Note that the\n\"Ginger\".least recently used element now becomes\nLRU Cache (Capacity 4)\nItem\nPrice\nLRU?\n\"Apple\"\n3.79\n\"Tofu\"\n3.79\n\"Ginger\"\n2.69\n✓\n\"Milk\"\n2.09", "word_count": 595, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0807006e-edd6-51a7-b261-69b79ce0b484", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 568, "real_page_number": null, "text": "556\nChapter 17. Hash Tables and Collision Resolution\n\"Apple\".Next, we update the price of ginger to $2.79. Ginger is in the cache, so we update its value. The LRU element now becomes\nLRU Cache (Capacity 4)\nItem\nPrice\nLRU?\n\"Apple\"\n3.79\n✓\n\"Tofu\"\n3.79\n\"Ginger\"\n2.79\n\"Milk\"\n2.09\n\"Apple\" \"Banana\"Next, we update the price of banana to $2.59. Banana is not in the cache, so we evict the LRU of and add back in.\nLRU Cache (Capacity 4)\nItem\nPrice\nLRU?\n\"Banana\"\n2.59\n\"Tofu\"\n3.79\n\"Ginger\"\n2.79\n\"Milk\"\n2.09\n✓\nLRUCacheAn outline of an class for the above cache is shown below:\n1\nclass LRUCache {\n2\nprivate:\n3\nsize_t capacity;\n4\n// TODO: Add any data structures here!\n5\npublic:\n6\nLRUCache(size_t capacity_in) : capacity{capacity_in} {}\n7\n8\n// Gets the value of the key if it exists in the cache\n9\n// Time complexity: O(1) on average\n10\nstd::optional<double> get(const std::string& key) {\n11\n// TODO: Implement code here\n12\n} // get()\n13\n14\n// Updates the value of the key, inserting it if necessary\n15\n// If the cache is already at capacity, evict the least-recently used key\n16\n// Time complexity: O(1) on average\n17\nvoid set(const doublestd::string& key, value) {\n18\n// TODO: Implement code here\n19\n} // set()\n20\n};\nYour goal is to implement the following two methods:\nget(): std::nullopt.• Returns the value of the given key if it exists in the cache; otherwise, return\nset():• Updates the value of the given key to the given value if the key exists in the cache. Otherwise, add the key-value pair to the\ncache, evicting the least recently used key if the insertion would cause the cache to go over capacity.\nget() set() std::stringBoth and should run in time on average. For simplicity, assume the key is a and the value is aΘ(1)\ndouble (like with the shopping items example above).\nstd::unordered_map<>Since we want to store our data in a way that supports efficient key-value lookup, our solution should utilize an\nthat maps each key to its corresponding value. However, the tricky part of this problem is to devise a method that allows you to evict the least\nrecently used key from the cache. A potential naïve solution would be to associate a \"timestamp\" with each key to identify when it was most\nrecently used. From an efficiently standpoint, though, this approach is not ideal, since this would require us to complete a linear pass over all the\ntimestamps to identify which item was least recently used (as well as to update the least recently used item after an eviction).\nstd::priority_queue<>Another solution would be to maintain a of key-value pairs with priority based on a timestamp. However,\nget() set()the use of a priority queue would make and run in time. Is there another data structure that allows us to evict keysΘ(log(𝑛))\nfrom our cache in constant time? The answer turns out to be a list. A doubly-linked list is useful here for two reasons:doubly-linked\n1. If you have access to a node of a doubly-linked list, you can remove it in time.constant\n2. Lists do not have to worry about reallocation, and a node in a list is not invalidated until the node itself is removed.\nWhen an item is inserted into the cache, we can insert it to the front of the list in constant time. Additionally, when an item is accessed using\nget() set(),or modified using we can also move it to the front of the list in constant time. This causes the least recently used elements to\nmove toward the back of the list, allowing us to easily identify which keys should be evicted when the cache becomes full.\nLet’s look at how a doubly-linked list can be used to solve this problem using the previous example. We first insert the key-value pair of\n{\"Apple\", 3.99} to the front of our list, as shown.\nhead\n\"Apple\"\n3.99\nnullptr", "word_count": 673, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ee094e79-fffa-5caf-ae22-b16a79138c28", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 569, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n557\n{\"Banana\", 2.49} {\"Ginger\", 2.69}Next, we insert the key-value pairs of and to the front of the list.\nhead\n\"Ginger\"\n2.69\n\"Banana\"\n2.49\n\"Apple\"\n3.99\nnullptr\n\"Apple\". \"Apple\"Next, we update the value of Since was accessed, we move it to the front of the list, which can be done in constant time\n\"Apple\"if we have access to the node of (we will go over how to access this node in constant time later, but it has to do with the hash table\n\"Banana\",we mentioned earlier). Notice that the item at the back of our list, is now our new least recently used element.\nhead\n\"Ginger\"\n2.69\n\"Banana\"\n2.49\n\"Apple\"\n3.99\nnullptr\nhead\n\"Apple\"\n3.79\n\"Ginger\"\n2.69\n\"Banana\"\n2.49\nnullptr\n{\"Milk\", 2.09}Next, we insert the key-value pair of to the front of the list.\nhead\n\"Milk\"\n2.09\n\"Apple\"\n3.79\n\"Ginger\"\n2.69\n\"Banana\"\n2.49\nnullptr\n{\"Tofu\", 3.79}.Next, we insert the key-value pair of However, our cache is already at its capacity of 4, so we cannot add this new element\n\"Banana\"without removing something in the cache first. Since the least recently used element is at the back of the list, we will remove from\n\"Tofu\".the cache before adding in\nhead\n\"Milk\"\n2.09\n\"Apple\"\n3.79\n\"Ginger\"\n2.69\n\"Banana\"\n2.49\nnullptr\nhead\n\"Tofu\"\n3.79\n\"Milk\"\n2.09\n\"Apple\"\n3.79\n\"Ginger\"\n2.69\nnullptr\n\"Ginger\". \"Ginger\" \"Apple\"Next, we update the value of Since was accessed, we move it to the front of the list, which leaves as the\ncurrent least recently used element.\nhead\n\"Tofu\"\n3.79\n\"Milk\"\n2.09\n\"Apple\"\n3.79\n\"Ginger\"\n2.69\nnullptr\nhead\n\"Ginger\"\n2.79\n\"Tofu\"\n3.79\n\"Milk\"\n2.09\n\"Apple\"\n3.79\nnullptr\n\"Banana\" \"Apple\"Next, we add back into the cache. Since our cache is full, we will evict the least recently used item — which is since it\n\"Banana\".is at the back of the list — before adding in\nhead\n\"Ginger\"\n2.79\n\"Tofu\"\n3.79\n\"Milk\"\n2.09\n\"Apple\"\n3.79\nnullptr\nhead\n\"Banana\"\n2.59\n\"Ginger\"\n2.79\n\"Tofu\"\n3.79\n\"Milk\"\n2.09\nnullptr\nTo perform this procedure efficiently, we would need a way to access the elements in the list in constant time on average (i.e., given the string\n\"Tofu\", \"Tofu\").we need to be able to return the node corresponding to However, if we just had a list on its own, we would need to iterate\nover the list to find the node associated with any given key. How can we support constant time access into the nodes of the list?", "word_count": 419, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "81bb7708-6466-5069-8141-e1f91cde5567", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 570, "real_page_number": null, "text": "558\nChapter 17. Hash Tables and Collision Resolution\nThe solution is to that maps each item to an iterator that points to its node in the list (this is okay sincesupplement the list with a hash table\niterators in a list are not invalidated until their corresponding items are removed). An illustration is shown below:\nKey\nValue\n\"Banana\"\n\"Ginger\"\n\"Tofu\"\n\"Milk\"\nhead\n\"Banana\"\n2.59\n\"Ginger\"\n2.79\n\"Tofu\"\n3.79\n\"Milk\"\n2.09\nnullptr\n\"Tofu\",For example, if you wanted to update the value of you can use the hash table to look up its node in the list in constant time.\nKey\nValue\n\"Banana\"\n\"Ginger\"\n\"Tofu\"\n\"Milk\"\nhead\n\"Banana\"\n2.59\n\"Ginger\"\n2.79\n\"Tofu\"\n3.79\n\"Milk\"\n2.09\nnullptr\nstd::list<> std::unordered_map<>Asolutiontotheproblemisimplementedbelow. A isusedtostoretheitemsinthecache, andan\nis used to map items to their corresponding node in the list. Whenever an item is updated or added, it is moved to the front of the list; whenever\nan item needs to be evicted from the cache, it is removed from the back of the list. Because unordered maps support time lookup onΘ(1)\nget() set()average, and lists support time insertions and deletions when given a node, the overall time complexities of and are Θ(1).Θ(1)\n1\nclass LRUCache {\n2\nprivate:\n3\nsize_t capacity;\n4\ndouble>>std::list<std::pair<std::string, cache_list;\n5\ndouble>>::iterator>std::unordered_map<std::string, std::list<std::pair<std::string, cache_map;\n6\npublic:\n7\nLRUCache(size_t capacity_in) : capacity{capacity_in} {}\n8\n9\nstd::optional<double> get(const std::string& key) {\n10\nauto it = cache_map.find(key);\n11\nif (it == cache_map.end()) {\n12\nreturn std::nullopt;\n13\n} // if\n14\n// the std::list::splice() method can be used to move the accessed node to the the front\n15\n// of the list, but you can also do a standard erase() and push_front() instead\n16\ncache_list.splice(cache_list.begin(), cache_list, it->second);\n17\nreturn it->second->second;\n18\n} // get()\n19\n20\nvoid set(const doublestd::string& key, value) {\n21\nauto it = cache_map.find(key);\n22\nif (it != cache_map.end()) {\n23\ncache_list.splice(cache_list.begin(), cache_list, it->second);\n24\nit->second->second = value;\n25\n} // if\n26\nelse {\n27\n// if cache is at capacity, evict element at back of list and remove it from map\n28\nif (cache_map.size() == capacity) {\n29\nstd::string item_to_evict = cache_list.back().first;\n30\ncache_list.pop_back();\n31\ncache_map.erase(item_to_evict);\n32\n} // if\n33\ncache_list.emplace_front(key, value);\n34\ncache_map[key] = cache_list.begin();\n35\n} // else\n36\n} // set()\n37\n};", "word_count": 400, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e111ee85-2860-5b26-9368-4694634668b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 571, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n559\nExample 17.10 You are given two vectors containing strings. Write a function that returns the strings that with theare in both vectors\ni j i + j.sum. The index sum of a string that exists at index of the first vector and index of the second vector is You maysmallest index\nreturn the strings in any order. There are no duplicates in each of the vectors, and the vectors are small enough that you do not need to worry\nabout integer overflow when calculating the index sum. The function declaration is as follows:\nminimum_index_sum(conststd::vector<std::string> std::vector<std::string>& vec1,\nconst std::vector<std::string>& vec2);\nExample: Given the following two vectors:\nvec1 = [\"Nikhil\", \"Daniel\", \"Aray\", \"Reyna\", \"Brian\", \"James\", \"Gavin\"]\nvec2 = [\"Gavin\", \"Mert\", \"Reyna\", \"Brian\", \"Daniel\", \"Zach\", \"Aasher\", \"Nikhil\"]\nOf the strings that exist in both vectors, these are their index sums:\nString\nvec1Index in\nvec2Index in\nIndex Sum\n\"Brian\"\n4\n3\n4 + 3 = 7\n\"Daniel\"\n1\n4\n51 + 4 =\n\"Gavin\"\n6\n0\n6 + 0 = 6\n\"Nikhil\"\n0\n7\n0 + 7 = 7\n\"Reyna\"\n3\n2\n53 + 2 =\n[\"Daniel\", \"Reyna\"]The smallest index sum among the strings is 5, so the function should return the vector (in any order).\nforA naïve solution would be to iterate over the two vectors using a nested loop. For each element we visit in the outer loop, we iterate over\nthe other vector to identify the index sum of that element if it exists. Along the way, we keep track of the best solution(s) we have encountered\nso far, updating it whenever we encounter an element with a better index sum. An implementation of this naïve solution is shown below:\n1\nminimum_index_sum(conststd::vector<std::string> std::vector<std::string>& vec1,\n2\nconst std::vector<std::string>& vec2) {\n3\nstd::vector<std::string> solution;\n4\nsize_t std::numeric_limits<size_t>::max();best_idx_sum =\n5\n6\nfor (size_t idx1 = 0; idx1 < vec1.size(); ++idx1) {\n7\nstd::string& curr = vec1[idx1];\n8\nfor (size_t idx2 = 0; idx2 < vec2.size(); ++idx2) {\n9\nif (vec2[idx2] == curr) {\n10\nsize_t idx_sum = idx1 + idx2;\n11\nif (idx_sum < best_idx_sum) {\n12\nsolution.clear();\n13\nsolution.push_back(curr);\n14\nbest_idx_sum = idx_sum;\n15\n} // if\n16\nelse if (idx_sum == best_idx_sum) {\n17\nsolution.push_back(curr);\n18\n} // else if\n19\n} // if\n20\n} // for idx2\n21\n} // for idx1\n22\n23\nreturn solution;\n24\n} // minimum_index_sum()\nThe time complexity of this solution is Θ(𝑚𝑛), where 𝑚and 𝑛are the lengths of the two vectors. How can we improve this solution? Notice\nthat the inefficiency is caused by the nested loop, since we have to repeatedly iterate over one of the vectors once for each element in the other\nvector. However, the only reason we are performing this nested loop is to complete a lookup: given a string in one vector, we want to find\nits index (if it exists) in the other vector. Therefore, we can circumvent the need for a nested loop by inserting the contents of one vector into\nstd::unordered_map<>an that maps each element to its index. Then, when iterating over the other list, we can query the hash table in\ntime to find the index of an element, instead of having to perform a linear time traversal.constant\n\"Gavin\"\n\"Mert\"\n\"Reyna\"\n\"Brian\" \"Daniel\"\n\"Zach\"\n\"Aasher\" \"Nikhil\"\n0\n1\n2\n3\n4\n5\n6\n7\nKey\nValue\n\"Nikhil\"\n0\n\"Daniel\"\n1\n\"Aray\"\n2\n\"Reyna\"\n3\n\"Brian\"\n4\n\"James\"\n5\n\"Gavin\"\n6\n5Index Sum: 2 + 3 =\nlookupΘ(1)", "word_count": 595, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d557a620-77e9-5608-923c-85179dab7801", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 572, "real_page_number": null, "text": "560\nChapter 17. Hash Tables and Collision Resolution\nThis improved solution is implemented below:\n1\nminimum_index_sum(conststd::vector<std::string> std::vector<std::string>& vec1,\n2\nconst std::vector<std::string>& vec2) {\n3\nstd::vector<std::string> solution;\n4\nsize_t std::numeric_limits<size_t>::max();best_idx_sum =\n5\n6\n// insert all the strings in one vector into a hash table that maps string -> index\n7\nsize_t>std::unordered_map<std::string, idx_map;\n8\nfor (size_t idx1 = 0; idx1 < vec1.size(); ++idx1) {\n9\nidx_map.emplace(vec1[idx1], idx1);\n10\n} // for idx1\n11\n12\n// iterate over the other vector, querying the hash table to identify index in first vector\n13\nfor (size_t idx2 = 0; idx2 < vec2.size(); ++idx2) {\n14\nstd::string& curr = vec2[idx2];\n15\nauto it = idx_map.find(curr);\n16\nif (it != idx_map.end()) {\n17\nsize_t idx_sum = it->second + idx2;\n18\nif (idx_sum < best_idx_sum) {\n19\nsolution.clear();\n20\nsolution.push_back(curr);\n21\nbest_idx_sum = idx_sum;\n22\n} // if\n23\nelse if (idx_sum == best_idx_sum) {\n24\nsolution.push_back(curr);\n25\n} // else if\n26\n} // if\n27\n} // for idx2\n28\n29\nreturn solution;\n30\n} // minimum_index_sum()\nSince our loops are no longer nested and are instead performed sequentially, the time complexity now becomes Θ(𝑚+𝑛), where 𝑚and 𝑛are the\nlengths of the two vectors. This is because the bodies of both loops can be completed in average-case constant time (since insertion and lookup\nin a hash table can both be done in time on average). Notice that an additional optimization we can make is to add the of the twosmallerΘ(1)\nvectors to the unordered map — this allows us to reduce our memory usage, as we only need to store the contents of one of the vectors.\nChapter 17 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n(std::unordered_map<>)1. Givenahashtable thatmapsalltheitemsinagrocerystoretotheirprices, whichofthefollowingoperations\ncan be performed in time on average?Θ(1)\nI. Finding the price of a given item\nII. Finding the cheapest item in the store\nIII. Updating the price of a given item\nA) I only\nB) III only\nC) I and II only\nD) I and III only\nE) I, II, and III\n2. Which of the following statements is/are TRUE?\noperator[] std::unordered_map<>I. Using on a non-existent key in a will insert the key into the container.\nstd::unordered_map<>II. The worst-case time complexity of finding an element in a of size 𝑛is Θ(1).\nrand()III. The function, which generates random numbers, is great for hashing since it greatly reduces the chances of collision.\nA) I only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n3. Which of the following is NOT a characteristic of a good hash function?\nA) The capability to distribute keys evenly in a hash table\nB) The capability to keep similar keys close together in a hash table\nC) The capability to compute a hash for every possible key\nD) The capability to compute the same hash for the same key\nE) All of the above are characteristics of a good hash function", "word_count": 560, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "b4849c89-8efb-5563-8c58-bea2166d2717", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 573, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n561\n4. The scores for the Winter 2018 EECS 281 midterm exam fall into the range [17, 99). If we were to input these scores into a hash table of size\n41 using the following hash function:\n⌊𝑘𝑒𝑦−𝑠ℎ(𝑘𝑒𝑦)=\n𝑡−𝑠×𝑀⌋\nwhere the relevant range is and the hash table size is 𝑀, a score of 83 would end up at which index of the hash table?[𝑠,𝑡)\nA) 31\nB) 32\nC) 33\nD) 34\nE) 35\nkey5. An easy way to compress an integer key into a hash table of size 𝑀is to take its modulus with 𝑀(i.e., mod 𝑀). For which of the\nfollowing collection of keys is this compression method the most ideal?\nA) A list of all scores on a 20-question multiple choice exam, where each question is worth 5 points\nB) The average number of hours of sleep each students gets on the weekend, rounded up\nC) The number of credits each full time student at the university is taking this semester\nD) The collection of student IDs of all students currently enrolled in EECS 281\nE) All of the above are equally ideal use cases\nFor questions 6-8, consider the following code:\n1\nint main() {\n2\nstd::unordered_map<std::string, std::string> my_map;\n3\nmy_map.insert(std::make_pair(\"Paoletti\",\"Darden\"));\n4\nmy_map.insert(std::make_pair(\"Angstadt\",\"Darden\"));\n5\nmy_map.insert(std::make_pair(\"Paoletti\",\"Angstadt\"));\n6\nmy_map[\"Angstadt\"] = \"Paoletti\";\n7\nmy_map.insert(std::make_pair(\"Paoletti\", \"Garcia\"));\n8\nstd::cout << my_map[\"Paoletti\"] << std::endl;\n9\nstd::cout << my_map[\"Darden\"] << std::endl;\n10\nmy_map.erase(\"Paoletti\");\n11\nstd::cout << my_map[\"Angstadt\"] << std::endl;\n12\nstd::cout << my_map.size() << std::endl;\n13\n} // main()\n6. What does line 8 print?\nPaolettiA)\nDardenB)\nAngstadtC)\nGarciaD)\nE) Any empty string\n7. What does line 11 print?\nPaolettiA)\nDardenB)\nAngstadtC)\nGarciaD)\nE) Any empty string\n8. What does line 12 print?\n1A)\n2B)\n3C)\n4D)\n7E)\n9. Consider the following snippet of code:\n1\nint main() {\n2\nstd::unordered_map<int32_t, double> umap;\n3\nm[280] = 281;\n4\nm[203] = 376;\n5\nfor (auto x : umap) {\n6\n// do stuff\n7\n} // for\n8\n} // main()\nxWhat is the type of on line 5?\nint32_tA)\ndoubleB)\nstd::pair<int32_t, double>C)\nstd::unordered_map<int32_t, double>D)\nE) None of the above", "word_count": 375, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d9b8a0b8-94ed-5df8-9ef7-7c2c11006c07", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 574, "real_page_number": null, "text": "562\nChapter 17. Hash Tables and Collision Resolution\n10. OntheWinter2018EECS281Piazzapage,postscanbecategorizedintothreeuniquegroups: questions,notes,andmemes. Arepresentation\nof this is shown below:\nSuppose you implemented the following unordered map that maps the type of each post to the post IDs that correspond to each of these\nnotes [3450,3983,4343]):types (for instance, the key maps to\nstd::vector<int>>std::unordered_map<std::string, posts;\nWhile searching through Piazza, you discover that post @4753 is a meme. Which of the following successfully appends 4753 to the vector\nmemes?associated with the key\nposts[\"memes\"] = 4753;A)\nposts[\"memes\"] = memes.push_back(4753);B)\nposts[\"memes\"] = posts.push_back(4753);C)\nposts[\"memes\"].push_back(4753);D)\nposts[\"memes\"].second.push_back(4753);E)\nFor questions 11-12, consider the following code:\n1\nint main() {\n2\nint32_t>>std::unordered_map<std::string, std::pair<std::string, foods;\n3\nfoods[\"cabbage\"] = std::make_pair(\"vegetable\", 1);\n4\nfoods[\"banana\"] = std::make_pair(\"fruit\", 2);\n5\nfoods[\"donut\"] = std::make_pair(\"dessert\", 3);\n6\nfoods[\"apple\"] = std::make_pair(\"fruit\", 4);\n7\nfoods[\"eggplant\"] = std::make_pair(\"vegetable\", 5);\n8\nstd::vector<std::string> vec;\n9\nfor (const auto& x : foods) {\n10\nvec.push_back(x.first);\n11\n} // for\n12\nstd::cout << vec.front() << std::endl;\n13\nreturn 0;\n14\n} // main()\n11. What does line 12 print?\nappleA)\nfruitB)\ncabbageC)\nvegetableD)\nE) Impossible to determine\n1?12. Which of the following lines of code prints out\nstd::cout << foods[0] << std::endl;A)\nstd::cout << foods[\"cabbage\"] << std::endl;B)\nstd::cout << foods[\"cabbage\"].second << std::endl;C)\nstd::cout << foods[\"cabbage\"].second.second << std::endlD)\nE) More than one of the above", "word_count": 246, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "41d85db0-d654-5386-b4c4-e31fb42f698a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 575, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n563\n13. Given two unsorted arrays of distinct numbers of size 𝑛, you want to find all pairs of numbers, one from array 1 and one from array 2, that\nsum to a given value. For instance, if you are given\narr1[] = {1, 2, 3, 4, 5, 7, 11}\narr2[] = {2, 3, 4, 5, 6, 8, 12}\n(1, 8), (3, 6), (4, 5), (5, 4), (7, 2).you would return and What is the average-case time complexity of doing this if you use\nthe most efficient algorithm?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n14. Which of the following collision resolution methods is NOT a form of open addressing?\nA) Separate chaining\nB) Linear probing\nC) Quadratic probing\nD) Double hashing\nE) All of the above use open addressing\n15. Which of the following statements is FALSE?\nA) The load factor 𝛼of a hash table can exceed 1\nB) As 𝛼increases, the performance of separate chaining does not deteriorate as quickly as the performances of open addressing methods\nC) The time complexities of searching and removing are both on average for a hash table that uses separate chaining to resolveΘ(𝛼)\ncollisions\nD) If 𝛼is less than 0.5, linear probing is better than double hashing at preventing keys in a hash table from clustering together\nE) More than one of the above\n16. Which of the following is NOT a disadvantage of using quadratic probing to resolve collisions?\nA) Two elements that hash to the same position will still have the same probe sequence, regardless of how far they land from their\nhashed location\nQuadratic probing is susceptible to primary clustering, where keys typically end up next to each other rather than distributedB)\nthroughout the entire hash table\nDependingonthesizeofthehashtableinvolved,itispossibleforquadraticprobingtoneverconsiderspecificindiceswhilesearchingC)\nfor an open position\nD) The performance of quadratic probing can deteriorate dramatically as the load factor increases\nE) None of the above\n17. A hash table of size 100 has 40 empty positions and 25 deleted positions. What is its load factor?\nA) 0.25\nB) 0.35\nC) 0.60\nD) 0.65\nE) 0.75\n≥0.6.18. You are given a hash table that immediately doubles its size whenever the load factor becomes You notice that, immediately after\ninserting an element into this hash table, its size doubled from 75 to 150. How many elements must be in the hash table after your insertion\n(assuming that no other elements are added or removed)?\nA) 30\nB) 45\nC) 60\nD) 75\nE) 90\n19. Suppose you have a hash table of size that uses the hash function and the compression function 𝑀.𝑀= 𝐻(𝑛) 3𝑛+7 𝐶(𝑛) 𝑛mod10 = =\n{10, 8,Linear probing is used to resolve collisions. You enter the following nine elements into this hash table in the following order:\n18, 17, 4, 20, 6, 3, 16}. No resizing is done. After all collisions are resolved, which index of the hash table remains empty?\nA) 3\nB) 4\nC) 5\nD) 6\nE) None of the above", "word_count": 531, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "806d9e75-c467-503d-9d0f-3cf229f7ed47", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 576, "real_page_number": null, "text": "564\nChapter 17. Hash Tables and Collision Resolution\n20. Suppose you have a hash table of size that uses the hash function and the compression function 𝑀.𝑀= 𝐻(𝑛) 5𝑛+7 𝐶(𝑛) 𝑛mod10 = =\n{1, 2,Linear probing is used to resolve collisions. You enter the following nine elements into this hash table in the following order:\n3, 4, 5, 6, 7, 8, 9}. No resizing is done. After all collisions are resolved, which index of the hash table remains empty?\nA) 1\nB) 2\nC) 3\nD) 9\nE) None of the above\n21. Suppose you have a hash table of size that uses the hash function 𝑛and the compression function 𝑀.𝑀= 𝐻(𝑛) 𝐶(𝑛) 𝑛mod7 = =\nQuadratic probing is used to resolve collisions. You enter the following six elements into this hash table in the following order:\n{24, 11, 17, 21, 10, 4}. No resizing is done. After all collisions are resolved, which index of the hash table remains empty?\nA) 1\nB) 2\nC) 5\nD) 6\nE) None of the above\n22. Suppose you have a hash table of size that uses the hash function and the compression function 𝑀.𝑀= 𝐻(𝑛) 2𝑛+3 𝐶(𝑛) 𝑛mod13 = =\nQuadratic probing is used to resolve collisions. You enter the following twelve elements into this hash table in the following order:\n{9, 22, 7, 10, 3, 1, 20, 13, 18, 5, 0, 11}. No resizing is done. After all collisions are resolved, which index of the\nhash table remains empty? Note: the following are the first five multiples of 13, in case you find them useful: 13, 26, 39, 52, 65.\nA) 2\nB) 6\nC) 11\nD) 12\nE) None of the above\n23. A hash table of size 10 uses open addressing with a hash function 𝑘, compression function 10, and linear probing.𝐻(𝑘) 𝐶(𝑘) 𝑘mod= =\nAfter entering six values into an empty hash table, its state is as shown below. Which of the following insertion orders is possible?\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n62\n43\n24\n82\n76\n53\nA) 76, 62, 24, 82, 43, 53\nB) 24, 62, 43, 82, 53, 76\nC) 76, 24, 62, 43, 82, 53\nD) 62, 76, 53, 43, 24, 82\nE) None of the above\n24. Suppose you are using a hash function where each string is hashed to an integer representing its first letter. The integer each letter hashes to\nis shown in the table below:\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\no\np\nq\nr\ns\nt\nu\nv\nw\nx\ny\nz\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nThe compression function 𝑀is used, where 𝑀represents the size of the hash table. You are using a hash table of size 𝑀.𝐶(𝑥) 𝑥mod=\nIf the strings \"paoletti\" and \"darden\" collide in this hash table, which of the following is not a possible value of 𝑀?\nA) 3\nB) 6\nC) 9\nD) 12\nE) None of the above\n25. Suppose you have a hash table of size that uses the hash function and the compression function 𝑀.𝑀= 𝐻(𝑛) 2𝑛+1 𝐶(𝑛) 𝑛mod7 = =\nDouble hashing is used to resolve collisions, with the following double hashing formula:\nmod𝐻(𝑛)+𝑗(5−(𝐻(𝑛) 5))\nwhere represents the integer value the key normally hashes to without compression, mod represents the secondary𝐻(𝑛) (5−(𝐻(𝑛) 5))\n{6, 10, 4,hash function, and 𝑗is the collision number. You enter the following six elements into this hash table in the following order:\n13, 11, 12}. No resizing is done. After all collisions are resolved, which index of the hash table remains empty?\nA) 1\nB) 3\nC) 4\nD) 5\nE) None of the above", "word_count": 650, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2e5a41a5-6264-54eb-a616-bf46a0e5b31a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 577, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n565\n26. Suppose you are using a hash function where each string is hashed to an integer representing its first letter. The integer each letter hashes to\nis shown in the table below:\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\no\np\nq\nr\ns\nt\nu\nv\nw\nx\ny\nz\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nYou have a hash table of size 10, and you insert the planets of the solar system (with the sun) into this hash table in the following order:\n1. \"sun\"\n2. \"mercury\"\n3. \"venus\"\n4. \"earth\"\n5. \"mars\"\n6. \"jupiter\"\n7. \"saturn\"\n8. \"uranus\"\n9. \"neptune\"\nCollisions are resolved using the double hashing formula\n𝑡(𝑘𝑒𝑦)+𝑗×(𝑞−(𝑡(𝑘𝑒𝑦) 𝑞))mod\nwhere is the integer that a key hashes to, 𝑗is the number of collisions that have occurred so far with that key, and 𝑞is 7. Compression𝑡(𝑘𝑒𝑦)\nis done using the formula 𝑀, where 𝑀is 10 (the size of the hash table). No resizing is done. After all collisions are resolved,𝐶(𝑛) 𝑛mod=\nwhich index of the hash table remains empty?\nA) 2\nB) 5\nC) 6\nD) 7\nE) None of the above\n27. Suppose you have a hash table of size that uses the same hash function as mentioned previously. You enter the following foods as𝑀=13\nkeys into this hash table in the following order:\n1. \"apples\"\n2. \"almonds\"\n3. \"avocados\"\n4. \"asparagus\"\nCollisions are resolved using the double hashing formula:\n𝑡(𝑘𝑒𝑦)+𝑗×(𝑞−(𝑡(𝑘𝑒𝑦) 𝑞))mod\nAfter all collisions are resolved, you notice that the key \"asparagus\" ended up at index 7. Knowing this, what is a possible value of 𝑞?\nA) 3\nB) 5\nC) 7\nD) 9\nE) 11\n28. Suppose you attempted to insert the same four keys into a different hash table, in the same order. Collisions are still resolved using the\ndouble hashing formula:\n𝑡(𝑘𝑒𝑦)+𝑗×(𝑞−(𝑡(𝑘𝑒𝑦) 𝑞))mod\nYou do not know the size 𝑀of this hash table. However, you do know that the value of 𝑞is 7 and that the key \"asparagus\" ended up at\nindex 4. With this information, what is a possible value of 𝑀?\nA) 11\nB) 13\nC) 15\nD) 17\nE) 19\n29. Which of the following statements is TRUE?\nI. As the load factor of a hash table increases, the average-case performance of finding a key deteriorates.\nII. A hash table that uses open addressing can still perform reasonably well with a load factor of 1.1.𝛼=\nIII. A hash table that uses separate chaining can still perform reasonably well with a load factor of 1.1.𝛼=\nA) I only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III", "word_count": 482, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f037ae45-8923-53e8-ae80-4f091dc81829", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 578, "real_page_number": null, "text": "566\nChapter 17. Hash Tables and Collision Resolution\n30. If the universe of possible keys we want to store in a hash table can be drawn from the set {0, 1, …, 𝑚−1}, where 𝑚is small, we can use\nthe value of each key to directly access the underlying table, as shown in the figure below.\n∙\n0\n∙\n1\n∙\n4\n∙\n6\n∙\n3\n∙\n2\n∙\n5\n∙\n7\n0\n1\n2\n3\n4\n5\n6\n7\nUniverse of possible keys\nActual keys in table\nThis addressing method is known as\nA) Direct addressing\nB) Hashed addressing\nC) Dynamic addressing\nD) Dynamic hashing\nE) Collision resolution\n31. Consider the following snippet of code:\n1\nstruct Thing {\n2\nint32_t x;\n3\nint32_t y;\n4\n5\nThing(int32_t int32_tx_in, y_in)\n6\n: x{x_in}, y{y_in} {}\n7\n};\n8\n9\nint main() {\n10\nstd::unordered_map<int32_t, Thing> thing_map;\n11\nThing my_thing{280, 281};\n12\n13\n} // main()\nWhich of the following statements, when added on line 12, would run without any compilation issues?\nthing_map[183] = my_thing;I.\nthing_map.insert(183, my_thing);II.\nthing_map.emplace(183, my_thing);III.\nA) I only\nB) III only\nC) I and III only\nD) II and III only\nE) I, II, and III\n32. For which of the following applications would a hash table be least efficient?\nA) Filtering duplicates from a list of elements\nB) Identifying all unique values from a list of elements\nC) Printing out values from a list of elements in sorted order\nD) Counting the frequencies of values in a list of elements\nE) All of the above can be efficiently solved with a hash table\n33. Which of the following collision resolution techniques does NOT need to keep track of a special marker for deleted items?\nA) Separate chaining\nB) Linear probing\nC) Quadratic probing\nD) Double hashing\nE) None of the above", "word_count": 310, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d8e839b6-9cc4-52fe-96f9-3c0478aa30af", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 579, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n567\nstd::string?34. Which one of the following four hash functions is most ideal for a\nHash 1\nHash 2\nint32_t hash1(const std::string& s) {\nint32_t hash = 7, n = s.size();\nfor (int32_t i = 0; i < n; ++i)\nhash += rand();\nreturn hash;\n} // hash1()\nint32_t hash2(const std::string& s) {\nint32_t hash = 7, n = s.size();\nfor (int32_t i = 0; i < n; ++i)\nhash += s[i];\nreturn hash;\n} // hash2()\nHash 3\nHash 4\nint32_t hash3(const std::string& s) {\nint32_t hash = 7, n = s.size();\nfor (int32_t i = 0; i < n; ++i)\nhash += s[i] i;*\nreturn hash;\n} // hash3()\nint32_t hash4(const std::string& s) {\nint32_t hash = 7, n = s.size();\nfor (int32_t i = 0; i < n; ++i)\nhash += s[i] rand();*\nreturn hash;\n} // hash4()\nA) Hash 1\nB) Hash 2\nC) Hash 3\nD) Hash 4\nE) All of the above work equally well\n35. Suppose you had a hash function that you want to use to hash items into a hash table of size 10. The state of the hash table is shown𝐻(𝑘)\nX DELETEDbelow, where represents an occupied position and represents a position that previously stored an element. Suppose you want\nto search for an element 𝐾that does not exist in the table, and 5. If the hash table uses quadratic probing as its collision resolution𝐻(𝐾)=\nmethod, which positions of the hash table must be probed before you can safely conclude that 𝐾is not in the table?\nX\nX\nX\nDELETED\nX\nDELETED\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nA) 5, 6\nB) 5, 6, 0\nC) 5, 6, 7\nD) 5, 6, 9, 3, 0\nE) 5, 6, 9, 4, 1\n36. Which of the following is the most appropriate situation to use a hash table?\nA) Keeping track of the top three values from a stream of integers\nB) Finding the highest priority thread to execute in a program, where each thread is assigned a priority value\nC) Keeping track of the number of students who have birthdays on each of the 31 days in January\nD) Finding information on a vehicle before a used car purchase, when given a 17-character vehicle identification number (VIN)\nE) Printing out a list of graduates in descending order of cumulative GPA\n37. You want to write a hash function that will distribute keys over 10 buckets, number 0 to 9, for integer keys 𝑖ranging from 0 to 2019.\nThe hash table is implemented using separate chaining. Of the following hash and compression functions, which one would be best at\ndistributing the keys uniformly over all 10 buckets?\n3𝑖2A) mod𝐻(𝑖)= 10\n6𝑖2B) mod𝐻(𝑖)= 10\n9𝑖3C) mod𝐻(𝑖)= 10\n12𝑖3D) mod𝐻(𝑖)= 10\n15𝑖4E) mod𝐻(𝑖)= 10", "word_count": 492, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b22cc917-2c0b-5b09-b57a-37c0b74fd655", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 580, "real_page_number": null, "text": "568\nChapter 17. Hash Tables and Collision Resolution\n𝑛238. Supposeyouhaveahashtableofsize thatusesthehashfunction andthecompressionfunction 𝑀. Double𝑀= 𝐻(𝑛) 𝐶(𝑛) 𝑛mod7 = =\n𝐻′(𝑛) 𝑛3.hashing is used to resolve collisions, with a secondary hash function of The full double hashing equation is shown below:=\n𝐻(𝑛)+𝑗×𝐻′(𝑛)\n𝐻′(𝑛)where represents the integer value the key normally hashes to without compression, represents the secondary hash function,𝐻(𝑛)\n{7, 6, 5, 4, 3}.and 𝑗is the collision number. You enter the following five elements into this hash table in the following order: No\n3resizing is done. After all collisions are resolved, which index of the hash table does end up at? Note: the following are the first twenty\nmultiples of 7, in case you find them useful: 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98, 105, 112, 119, 126, 133, 140.\nA) 0\nB) 1\nC) 2\nD) 6\nE) None of the above\n39. You are given the following hash table of initial size 7, which doubles in size and rehashes all its elements when its load factor𝑀=\n≥0.5.becomes Duplicates are not allowed. The current state of the hash table is shown below.\naardvark\nhamster\ngiraffe\n0\n1\n2\n3\n4\n5\n6\nThehashfunctionusedis 𝑘[0]−‘𝑎’, orthedistanceofthefirstcharacterofthestringfrom‘a’(wordsthatstartwith‘a’arehashedto𝐻(𝑘)=\n0, words that start with ‘b’ are hashed to 1, etc.). The integer each letter hashes to is shown in the table below for convenience. Compression\nis done using the formula 𝑛mod 𝑀, where 𝑀is the size of the table. Linear probing is used as the collision resolution technique.𝐶(𝑛)=\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\no\np\nq\nr\ns\nt\nu\nv\nw\nx\ny\nz\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nYou perform the following operations on the given hash table:\n\"giraffe\"1. Delete\n\"fish\"2. Insert\n\"lion\"3. Insert\n\"frog\"4. Insert\n\"ferret\"5. Insert\n\"hamster\"6. Delete\n\"fox\"7. Insert\n\"fox\"How many positions of the hash table must be probed before you can insert into the table?\nA) 2\nB) 3\nC) 4\nD) 5\nE) 6\n\"fox\"40. What index of the final hash table does end up in after the insertion?\nA) 1\nB) 3\nC) 5\nD) 7\nE) 9\n41. Suppose you have an empty hash table of size 𝑀, where each string is hashed to an integer representing its first letter (𝑎=0, 1, 2,𝑏= 𝑐=\n…, see table from question 39 for the full alphabet). You enter the following strings as keys into this hash table in the following order:\n1. \"bear\"\n2. \"badger\"\n3. \"bat\"\n4. \"buffalo\"\nCollisions are resolved using the double hashing formula:\n𝑡(𝑘𝑒𝑦)+𝑗×(𝑞−(𝑡(𝑘𝑒𝑦) 𝑞))mod\nThe value of 𝑞is 7, and compression is performed using the formula 𝑛mod 𝑀, where 𝑀is the size of the table. No resizing is𝐶(𝑛)=\ndone. After all four strings are inserted, you iterate through the underlying vector from beginning to end and print out the keys in the order\n\"buffalo\"in which they are encountered. If is the first of the four keys to be printed out, what is a potential value of 𝑀?\nA) 13\nB) 17\nC) 19\nD) 25\nE) There exists no such 𝑀that satisfies this condition", "word_count": 606, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9c7e7b17-eba2-5ff1-ac58-7f14cd3f32c4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 581, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n569\n42. You are tasked to build a hash table, but you are only allowed to use hash functions that take time on the size of the input. If youlinear\nknow that your input will be substantially large, which of the following collision resolution methods is the least appropriate?\nA) Separate chaining\nB) Linear probing\nC) Quadratic probing\nD) Double hashing\nE) None of the above\n43. Which of the following properties is required for a valid hash function ℎ(𝑘)?\n≠ℎ(𝑘2) ≠𝑘2I. A hash function must produce unique hash values for unique inputs (i.e., if for all 𝑘).ℎ(𝑘1) 𝑘1\nII. A hash function must produce the same hash value for the same inputs (i.e., if for all 𝑘).ℎ(𝑘1) ℎ(𝑘2) 𝑘1 𝑘2= =\nIII. A hash function must produce similar hash values for similar (but distinct) inputs.\nA) I only\nB) II only\nC) I and II only\nD) II and III only\nE) I, II, and III\n≥0.5.44. You are given an open addressing hash table with capacity 31 that doubles whenever its load factor becomes There are currently\n12 positions in the table marked as deleted. After inserting an element into this hash table, you notice that the capacity of the hash table\nimmediately doubled from 31 to 62. After this insertion and reallocation, how many elements in this hash table are marked as empty,\ndeleted, and occupied (assuming that no other elements are added or removed)?\nA) Empty: 35, Deleted: 12, Occupied: 15\nB) Empty: 23, Deleted: 24, Occupied: 15\nC) Empty: 34, Deleted: 12, Occupied: 16\nD) Empty: 22, Deleted: 24, Occupied: 16\nE) None of the above\n45. Which of the following statements is FALSE?\nA) Deleting items from a hash table that uses open addressing will always improve insertion speed\nB) If no resizing is done and no elements are deleted, an open addressing hash table’s lookup performance will deteriorate as the number\nof elements inserted increases\nC) The performance of a hash table that uses separate chaining will be less sensitive to the load factor than the performance of a hash\ntable that uses open addressing\nD) A poor hash function can directly worsen the performance of hash table operations, such as lookup, insertion, and removal\nE) None of the above (i.e., all of the above statements are true)\nboost::unordered_flat_map<>, std::unordered_map<>46. In 2022, the Boost library introduced the a fast alternative to that\nboost::unordered_flat_map<>,uses open addressing rather than separate chaining. In the an underlying array is split into 𝑚\ngroups of size 15, where 𝑚is always a power of two. A hash function determines which group a key falls into, and each key is inserted\ninto the first available position of that group — if the group to insert into becomes full, future insertions that hash to that group will use\nquadratic probing to identify another unused group to support the insertion. A rough depiction of this layout is shown below:\n2𝑛groups\n15 buckets\n15 buckets\n15 buckets\nboost::unordered_flat_map<>Which of the following could be potential explanations as to why the is so performant?\nA) Because open addressing is used instead of separate chaining with linked lists, there is no indirection required to go from the bucket\nthat an element hashes to to the actual memory location of that element (i.e., less dereferencing overhead)\nB) Because groups of elements are stored contiguously in memory, the Boost hash map is able to take advantage of faster data retrieval\nthat occurs when elements are stored close together in memory (a process known as caching)\nC) By utilizing groups of buckets of size 15, the Boost hash map is able to support up to 15 elements that hash to the same group before\na collision resolution technique needs to be used to identify a different group to insert into\nD) The Boost hash map is more lightweight and requires fewer memory allocations compared to a separate chaining approach that uses\nlinked lists, as it does not need to store additional pointers to link the elements that hash to the same group\nE) All of the above", "word_count": 693, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d9e931a2-e989-50bb-9f86-8f63a93c8581", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 582, "real_page_number": null, "text": "570\nChapter 17. Hash Tables and Collision Resolution\n47. Consider the following snippet of code:\n1\ndouble get_student_gpa(const std::unordered_map<int32_t, double>& gpa_map,\n2\nint32_t doublestudent_id, gpa) {\n3\nauto gpa_it = gpa_map.find(student_id);\n4\nif (gpa_it != gpa_map.end()) {\n5\nreturn gpa_map[student_id];\n6\n} // if\n7\n8\nreturn 0.0;\n9\n} // add_student_gpa()\n10\n11\nint main() {\n12\nstd::unordered_map<int32_t, double> gpa_map;\n13\n14\nint32_t student_id;\n15\ndouble gpa;\n16\nwhile (std::cin >> student_id >> gpa) {\n17\ngpa_map[student_id] = gpa;\n18\n} // while\n19\n20\nfor (const auto& entry : gpa_map) {\n21\nstd::cout << entry.first << \" \"\n22\n<< get_student_gpa(gpa_map, entry.first, entry.second) << std::endl;\n23\n} // for\n24\n} // main()\nDoes this code compile? If not, which line causes an issue?\nA) This code compiles without issue\nB) This code does not compile due to an issue on line 3\nC) This code does not compile due to an issue on line 5\nD) This code does not compile due to an issue on line 17\nE) This code does not compile due to an issue on line 22\nstd::unordered_map<>, std::unordered_set<>?48. Which of the following is supported by an but NOT an\noperator[]A)\noperator==B)\noperator<C)\n.insert()D)\n.find()E)\nA trusted colleague of yours has developed a method for hash collision resolution that they claim runs in only 𝚯(1) time. This method uses49.\nand is almost identical to the method described in this chapter. The only difference is that, when a collision occurs, theseparate chaining\ngiven element is inserted as the head node of the linked list at a particular index in time, instead of as the tail node in time, whereΘ(𝑚)Θ(1)\n𝑚is the size of the list at that index. Did your colleague actually develop a valid collision resolution technique that is guaranteed toΘ(1)\nwork in all cases? Why or why not?\nstd::string50. Consider the following empty hash table of size 10, which stores keys.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThe hash function used translates the first letter of each string to its distance from the character ‘a’. The integer each letter hashes to is\nshown in the table below. Compression is done using the formula 𝑛mod 𝑀, where 𝑀is the size of the table. No resizing is done.𝐶(𝑛)=\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\no\np\nq\nr\ns\nt\nu\nv\nw\nx\ny\nz\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nForthisquestion, youmaychoosewhetheryouwantthistabletohandlecollisionsusing or probing. Afterdecidinglinearprobing quadratic\nthe collision resolution method, provide a sequence of strings that you can insert into this table so that a subsequent insertion oflowercase\n\"slime\"the string would cause it to end up at index 7 of the hash table. This is an open ended question, so there may be multiple answers!", "word_count": 518, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e83db404-b686-5b2e-a68a-5bad0540e54e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 583, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n571\nDoubleHashTable51. You are given the following definition of the class:\n1\nenum class uint8_tStatus : { Empty, Occupied, Deleted };\n2\n3\nstruct Bucket {\n4\nStatus status = Status::Empty;\n5\nint32_t key;\n6\n};\n7\n8\nclass DoubleHashTable {\n9\nstd::vector<Bucket> hash_table;\n10\nsize_t num_elements = 0;\n11\nstatic constexpr int32_t table_size = 13;\n12\nstatic constexpr int32_t prime = 7;\n13\n14\npublic:\n15\nDoubleHashTable() {\n16\nhash_table.resize(table_size);\n17\n} // DoubleHashTable()\n18\n19\nbool is_full() {\n20\nreturn num_elements == table_size;\n21\n} // is_full()\n22\n23\nsize_t hash1(int32_t key) {\n24\nreturn key;\n25\n} // hash1()\n26\n27\nsize_t hash2(int32_t key) {\n28\nreturn prime - (key % prime);\n29\n} // hash2()\n30\n31\nbool insert_key(int32_t key) {\n32\n// TODO: Implement this function\n33\n} // insert_key()\n34\n};\nhash1() hash2()This hash table uses double hashing to resolve collisions, where is the primary hash function and is the(a)\ninsert_key()secondary hash function. Implement the function (as shown by the TODO above), which inserts the provided\nkey into the hash table and returns whether the key was successfully inserted. If the hash table is full, do not insert anything and\nfalse.return You may NOT use anything from the STL.\nbool DoubleHashTable::insert_key(int32_t key);\ninsert_key()):(b) Now, suppose you ran the following snippet of code (with a working implementation of\n1\nint main() {\n2\nint32_t arr[] = {11, 24, 25, 41, 45, 48};\n3\nDoubleHashTable table;\n4\nfor (int32_t val : arr) {\n5\ntable.insert_key(val);\n6\n} // for val\n7\n} // main()\nWhat does the internal state of the hash table look like? Fill out the empty table below with the values in their correct positions.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n52. You are currently waiting for the Northwood bus at Pierpont, which you know will arrive after a certain amount of time thanks to the M-Bus\napp. Since you do not want to wait in silence, you decide to take out a mobile device and listen to some music. You have many songs to\nchoose from on your device, and you know that you have enough time to finish two tracks before the next bus arrives. However, since you\nwant to maximize your listening time without having to stop a song before it finishes, you want to select two songs such that their combined\nruntime equals the exact wait time required before the bus’s arrival.\nImplement a function that takes in an integer wait time (in seconds) and a vector of music tracks (in seconds) and determines if there exist\ntwo tracks whose combined length equals the wait time. You may assume that the wait time is guaranteed to be exact to the second. One\nnotable detail is that you do want to listen to the same song twice, even if the track length is exactly half of the total wait time, so suchnot\ntracks should not count as a solution for this problem.\ntracks = [300, 240, 180, 200] 540, true 300+240=540.Example 1: Given and a wait time of you would return since\ntracks = [300, 240, 180, 200] 600, falseExample 2: Given and a wait time of you would return since there are no two\n300tracks that sum to this value. Note that the track of length cannot be listened to twice.\nbool identify_tracks(const std::vector<int32_t>& int32_ttracks, wait_time);\nYour solution should take linear time on the length of the tracks vector in the average case.", "word_count": 599, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c721ec25-1c76-5a11-ac84-a4f281d0807a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 584, "real_page_number": null, "text": "572\nChapter 17. Hash Tables and Collision Resolution\nMessage53. Consider the following object, which represents a message that is sent in a chat server:\nstruct Message {\nstd::string sender;\nstd::string text;\n};\nsender textThe member stores the username of the person who sent a message as a lowercase string, and the member stores the\nMessagecontents of the actual message itself. Given a vector of objects, implement a function that returns the user with the highest word\ncount among all their messages. If there is a tie, return the user whose username is lexicographically. You may assume that everylargest\nstd::count()word in each message will be separated by a single space. Hint: the method could be helpful here.\nFor example, given a vector of messages that correspond to the following conversation in the EECS 281 Discord server:\n8/19/20225:22:56PM\nslime: did anyone new get 281\n8/19/20225:55:16PM\nslime: @everyone\n8/19/20225:55:29PM\niteemhe: emmm why\n8/19/20225:55:34PM\ndoubledelete: i didn’t expect you to do it\n8/19/20225:55:35PM\niteemhe: why allow everyone\n8/19/20225:55:40PM\ntoafu: Oh he actually did it\n8/19/20225:55:50PM\ndenalz: slime moment\n8/19/20225:56:06PM\ndenalz: that was funny tho\n8/19/20225:56:12PM\ndenalz: im not bothered\n8/19/20225:56:28PM\ndeebs: unacceptable behavior\n8/19/20225:58:16PM\namadeus: We have never had this many concurrent users online\n8/19/20226:00:25PM\nslime: server’s active today\n8/19/20226:01:46PM\nkryptof: I wonder why\nthere are three users with a tie for the highest word count (9): \"amadeus\", \"denalz\", and \"slime\". In this case, \"slime\" is returned because it\nis the largest username lexicographically (since ‘s’ comes after ‘a’ and ‘d’).\nuser_with_highest_word_count(conststd::string std::vector<Message>& messages);\nYou solution should take time in the average case, where 𝑛is the number of messages and 𝑑is the maximum length of a message.𝑂(𝑛𝑑)\n54. Two pairs and are symmetric if 𝑐and 𝑑. Suppose you are given a vector of pairs, and you want to find all symmetric(𝑎,𝑏) (𝑐,𝑑) 𝑏= 𝑎=\npairs on the vector. The first element of all pairs is distinct. For instance, if you are given the following vector:\nvec = [(14, 23), (11, 2), (52, 83), (49, 38), (38, 49), (2, 11)]\n[(11, 2), (2, 11)] [(49, 38), (38, 49)]you would return and (in any order).\nstd::vector<std::vector<std::pair<int32_t, int32_t>>> find_symmetric_pairs(\nconst std::vector<std::pair<int32_t, int32_t>>& vec);\nYour solution should take linear time on the length of the vector in the average case.\nGiven a vector of 𝑛elements where elements may be repeated, implement a function that returns the minimum number of elements that55.\nneed to be deleted from the vector so that all elements in the vector are equal. For instance, if you are given the following vector:\nvec = [3, 6, 8, 6, 2, 7, 6, 3, 1, 3, 6]\n7,you would return as this is the minimum number of deletions required to obtain a vector where all the elements are the same (in this case,\nyou would get a vector with all 6’s).\nint32_t min_deletions_for_all_equal(const std::vector<int32_t>& vec);\nYour solution should take linear time on the length of the vector in the average case.\n56. You are given an array with repeated elements. Implement a function that returns the maximum distance between any two occurrences of a\nrepeated element. For example, given the following input:\nvec = [1, 2, 3, 2, 2, 1, 3, 3, 2]\n7,you would return since the maximum distance between any two repeated elements in the distance between the 2 at index 1 and the 2 at\nindex 8, or 8 - 1 = 7.\nint32_t max_repeated_distance(const std::vector<int32_t>& vec);\nYour solution should take linear time on the length of the vector in the average case.\n57. You are given an array and a target value 𝑘. Implement a function that returns the number of subarrays in the input array that have a sum of\n𝑘. A subarray is a contiguous non-empty sequence of values in an array. For example, given the following input and a target of −1:𝑘=\nvec = [2, -3, 4, -2, 1, -1]\n5,you would return since there are five subarrays that sum up to -1:\n[2, -3, 4, -2, 1, -1]\n-3, 4, -2,[2, 1, -1]\n-2, 1,[2, -3, 4, -1]\n-1][2, -3, 4, -2, 1,\n-3, 4, -2, 1, -1][2,\nint32_t num_subarrays_with_sum_k(const std::vector<int32_t>& int32_tvec, k);\nYour solution should take linear time on the length of the vector in the average case.", "word_count": 746, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d530455f-ffb5-501b-81f1-080371d77b6c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 585, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n573\n58. You are given a singly-linked list where each node contains an additional random pointer that could point to any node in the list (or\nnullptr). An example of one possible list is shown below:\n1\nnext\nrand\n2\nnext\nrand\n3\nnext\nrand\n4\nnext\nrand\n5\nnext\nrand\nnullptr\nEach node is represented as follows:\nstruct Node {\nint32_t val;\nNode* next;\nNode* random;\nNode(int32_t next{nullptr}, random{nullptr}val_in) : val{val_in}, {}\n};\nImplement a function that returns a deep copy of the list.\nNode* copy_random_list(Node* head);\nYour solution should take linear time on the length of the list in the average case.\nTransitManagerYou are in charge of a transit system for a large city, and you are given the following class, which handles data on59.\nTransitManagerridership. Implement the following methods for the class:\nvoid check_in(int64_t const int64_trider_id, std::string& station_name, timestamp);•\nrider_id station_name timestamp.– This method is called when the rider with ID checks into the station at timestamp\n– Oncea riderchecksintoa station, they cannotcheck inagainuntilaftertheycheck out. Youmayassumethatthiserrorcondition\nwill never happen for this problem.\nvoid check_out(int64_t const int64_trider_id, std::string& station_name, timestamp);•\nrider_id station_name timestamp.– This method is called when the rider with ID checks out from station at timestamp\ndouble get_avg_time(const conststd::string& start_station, std::string& end_station);•\nstart_station end_station.– This method returns the average time it takes to travel from to This value is calculated\nfrom all previous riders who directly traveled between the two stations (where start is the check in station and end is the check\nout station). For example, if rider 1 traveled from start to end in 5 minutes, and rider 2 traveled from start to end in 7 minutes, the\naverage travel time between start and end would be 6 minutes (assuming no other travelers have the same start and end stations).\nint64_t get_num_rides(int64_t rider_id);•\nrider_id.– This method returns the number of rides initiated with the rider with ID The number of rides is increased when a\nrider checks in (so if a rider checks into their 5th ride but has not checked out yet, this function would still return 5).\nint64_t get_num_route(const conststd::string& start_station, std::string& end_station);•\nstart_station end_station– This method returns the number of riders who have traveled the route between and (i.e.,\nthe count is only increased for the route 𝐴→𝐵if a rider checks in at the start station 𝐴and checks out at the end station 𝐵).\ncheck_in() check_out()You may assume that calls to and are consistent, and that timestamps are provided in chronological order.\n1\nclass TransitManager {\n2\nprivate:\n3\n// TODO: Add any data structures here!\n4\npublic:\n5\nvoid check_in(int64_t const int64_trider_id, std::string& station_name, timestamp) {\n6\n// TODO: Implement this\n7\n} // check_in()\n8\n9\nvoid check_out(int64_t const int64_trider_id, std::string& station_name, timestamp) {\n10\n// TODO: Implement this\n11\n} // check_out()\n12\n13\ndouble get_avg_time(const conststd::string& start_station, std::string& end_station) {\n14\n// TODO: Implement this\n15\n} // get_avg_time()\n16\n17\nint64_t get_num_rides(int64_t rider_id) {\n18\n// TODO: Implement this\n19\n} // get_num_rides()\n20\n21\nint64_t get_num_route(const conststd::string& start_station, std::string& end_station) {\n22\n// TODO: Implement this\n23\n} // get_num_route()\n24\n};", "word_count": 562, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "02888be9-a6ca-5344-9cad-529cb52f43d3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 586, "real_page_number": null, "text": "574\nChapter 17. Hash Tables and Collision Resolution\nTransactionManager60. Your local bank is designing a class that can be used to automate all transactions. This class will need to support\nfour operations, which you are tasked to implement:\nbool open(const std::string& account_name);•\naccount_name– Opens an account with the account name and return whether this was successful. If an account already exists\nfalse.with the requested name, return\nbool deposit(const doublestd::string& account_name, amount);•\namount account_name,– Deposits the amount into the account and returns a Boolean specifying whether the transaction\naccount_name false.was successful. If does not exist, a deposit should not happen and this method should return\nbool withdraw(const doublestd::string& account_name, amount);•\namount account_name,– Withdraws the amount from the account and returns a Boolean specifying whether the transaction\naccount_namewas successful. If does not exist, or if the account is trying to withdraw more money than they have, a\nfalse.withdrawal should not happen and this method should return\nbool transfer(const const double• std::string& src_account, std::string& dest_account, amount);\namount src_account dest_account,– Transfers the amount from the account of into the account of and returns whether\nsrc_account,the transaction was successful. If either account does not exist, or if there is not enough money in a transfer\nfalse.should not happen and this method should return\nAn outline of this class is provided below:\n1\nclass TransactionManager {\n2\nprivate:\n3\n// TODO: Add any data structures here!\n4\npublic:\n5\nbool open(const std::string& account_name) {\n6\n// TODO: Implement this\n7\n} // open()\n8\n9\nbool deposit(const doublestd::string& account_name, amount) {\n10\n// TODO: Implement this\n11\n} // deposit()\n12\n13\nbool withdraw(const doublestd::string& account_name, amount) {\n14\n// TODO: Implement this\n15\n} // withdraw()\n16\n17\nbool transfer(const const doublestd::string& src_account, std::string& dest_account, amount) {\n18\n// TODO: Implement this\n19\n} // transfer()\n20\n};\nDatabase61. In this problem, you will be designing a class that represents a simplified database. You are given 𝑛tables that you need to\nTableInfocreate in this database. Each table is given a name and a list of columns, which is provided to you in the following object:\nstruct TableInfo {\nstd::string table_name;\nstd::vector<std::string> column_names;\n};\nTableInfo table_name \"Students\"Thelistoftableandcolumnnamesareguaranteedtobeunique. Forexample,givena where is\ncolumn_names [\"Name\", \"Year\", \"Major\", \"GPA\"], \"Students\"and is youwouldcreateatablenamed inyourdatabase\n\"Name\", \"Year\", \"Major\", \"GPA\".with the columns of and For simplicity, you may assume that all values are stored in the database\nas strings, even if they are numerical values (such as GPA in the example above).\nDatabaseYour class will need to support the following operations, which you are tasked to implement:\nbool create(const TableInfo& table_info);•\n– Creates a table with the settings described in the table info, and return whether this was successful. If the table name exists\nfalse.already, return\nint64_t insert(const conststd::string& table_name, std::vector<std::string>& row_data);•\nrow_data table_name,– Inserts the contents of into the table with name and assigns this row with an ID that is returned.\ntable_name row_dataYou are guaranteed that the table will exist, and the contents of will correspond to the columns that\n\"Students\" row_datathe table was initialized with. For example, with our table, you could be given the following input:\nrow_data = [\"Alice\", \"3\", \"Computer Science\", \"3.9\"]\n\"Name\", \"Year\", \"Major\",In this case, you would insert a row into the table with these values, which correspond to and\n\"GPA\", respectively.\n– Each row is assigned an auto-incremented ID value that starts from 1. For instance, the first row inserted is assigned an ID of 1,\nthe second row inserted is assigned an ID of 2, and so on. Deleted rows do not impact the next assignable ID — if the row with\nID 2 is deleted, and another row is inserted, that new row would get assigned an ID of 3 instead of 2.\nbool remove(const int64_tstd::string& table_name, row_id);•\ntable_name row_id,– Deletes the row in the table with the ID if it exists, and returns whether a deletion was performed.\ntable_nameYou are guaranteed that the table will exist.", "word_count": 713, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab6f0be0-eaeb-57cf-a69e-4d49ff94b0dc", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 587, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n575\nselect(const int64_tstd::optional<std::string> std::string& table_name, row_id,•\nconst std::string& column_name);\ntable_name row_id column_name.– Returnsthevalueofthecellinthetable associatedwiththerowwithID andcolumnname\nstd::nullopt. select() \"Students\",If no such value exists, return For example, if is called with a table name of a\n1, \"GPA\", \"3.9\".row ID of and a column name of you would return\nAn outline of this class is provided below:\n1\nclass Database {\n2\nprivate:\n3\n// TODO: Add any data structures here!\n4\npublic:\n5\nbool create(const TableInfo& table_info) {\n6\n// TODO: Implement this\n7\n} // create()\n8\n9\nint64_t insert(const conststd::string& table_name, std::vector<std::string>& row_data) {\n10\n// TODO: Implement this\n11\n} // insert()\n12\n13\nbool remove(const int64_tstd::string& table_name, row_id) {\n14\n// TODO: Implement this\n15\n} // remove()\n16\n17\nselect(const int64_tstd::optional<std::string> std::string& table_name, row_id,\n18\nconst std::string& column_name) {\n19\n// TODO: Implement this\n20\n} // select()\n21\n};\nChapter 17 Exercise Solutions\n1. The correct answer is (D). Hash tables provide constant time access to a value given its key, so both I and III can be performed in Θ(1)\ntime. However, they are not sorted, so finding the cheapest item in the store would not be doable in time.Θ(1)\noperator[]2. The correct answer is (A). Only statement I is true: inserts a key if it does not exist. Statement II is false because the\nworst-case is Θ(𝑛). Statement III is false because a requirement of hash functions is that they must always hash the same key to the same\nhash value, which is not guaranteed if you use a random number generator to determine your hash value.\n3. The correct answer is (B). A good hash function evenly distributes keys in a hash table; there is no requirement to group similar elements\ntogether. In fact, you would likely want the keys in a hash table to be evenly spread out instead of clustered together, which may be a\nhindrance to performance for open addressing.\n4. The correct answer is (C). Using 17, 99, 83, and 41, we get the following:𝑡= 𝑘𝑒𝑦= 𝑀==\n⌊83−17ℎ(𝑘𝑒𝑦)=\n⌊66×41⌋=99−17\n41×41⌋=⌊66×82\n182⌋=⌊66×\n2⌋=33\n5. The correct answer is (D). The compression method provided works best when the keys are randomly distributed. The test scores are not\nevenly distributed; since each question is worth 5 points, scores can only take on the values 0, 5, 10, 15, …, 90, 95, 100. The number of\nhours of sleep and number of credits taken by each student are also not evenly distributed and take on a rather restricted range of values.\nOnly the collection of student IDS at the university is random as the ID of any particular student in EECS 281 could be any 8-digit number.\ninsert()6. The correct answer is (B). The method does not do anything if the key already exists in the unordered map, as unordered maps\n\"Paoletti\" \"Darden\".cannot have duplicate keys. On line 3, the key is initialized with the value Lines 5 and 7 do not do anything\n\"Paoletti\" \"Darden\"since the key already exists in the table. Thus, the value of remains unchanged and is printed out on line 8.\n\"Angstadt\" \"Darden\"7. The correct answer is (A). Even though the key is initially created with the value on line 4, it is updated on\n\"Paoletti\". \"Paoletti\"line 6 to a value of Thus, is printed out on line 11.\n\"Paoletti\" \"Angstadt\"8. Thecorrectansweris(B).Priortoline10,therearethreekeysthatexistinthehashtable: (createdonline3),\n\"Darden\" operator[](created on line 4), and (created on line 9 – recall that automatically inserts a key if it does not already exist).\n\"Paoletti\" 2.After is removed from the hash table on line 10, there are only two keys remaining, so line 12 prints out\n9. The correct answer is (C). The type of an element in an unordered map is a key-value pair.\nposts[\"memes\"] \"memes\",10. The correct answer is (D). The expression is a reference for the value associated with the key or the\nposts[\"memes\"]vector of post IDs associated with this key. As a result, you can treat as the type of the value (a vector) and simply\npush back the new ID.\n11. The correct answer is (E). An unordered map is, by definition, unordered. Thus, you cannot determine which key was encountered first\nwhile iterating over the map.\nfoods[\"cabbage\"] std::pair<std::string,12. The correct answer is (C). is the value type of the unordered map, which is a\nint32_t>. 1 \"cabbage\"), .secondThus, to get the integer value of this pair (which is for the key you would need to reference the\nmember of the pair.", "word_count": 818, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "8e14a01d-590a-52e9-9c6b-e6024f4f2259", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 588, "real_page_number": null, "text": "576\nChapter 17. Hash Tables and Collision Resolution\nstd::unordered_set<>.13. Thecorrectansweris(C).Tosolvethisproblem,firstiterateoverthefirstarrayandstoreallofitselementsinan\nThen, iterate over the second array and check if the difference between each element in this array and the target sum is in the unordered set.\nIf it is, you have found a pair that sums to the target value. Since you are simply iterating over the arrays of length 𝑛, and inserting into an\nunordered set takes constant time on average, the average-case time complexity of the overall solution is Θ(𝑛).\n14. The correct answer is (A). Open addressing is a class of collision resolution methods in which open or empty hash table positions are used\nto resolve collisions. This is not the case for separate chaining, which uses closed addressing to resolve collisions (e.g., if a key hashes to\nposition 𝑥, you would know that this key can be found at position 𝑥, even if it might not be alone at that location).\n15. The correct answer is (D). Option (A) is true because load factor can be greater than 1 if separate chaining is used. Similarly, option (B) is\ntrue because open addressing methods need to search for open positions in the case of a collision, which are harder to find as the table fills\nup. Option (C) is true because the cost of search and erase in the average case is for a hash table that uses separate chaining, as theΘ(𝛼)\naverage linked list in the table would have a length of 𝑁∕𝑀if the 𝑁keys are evenly distributed across all 𝑀indices. Option (D) is𝛼=\nfalse because in linear probing, collisions often result in keys being placed close to their hashed location (since we always incrementing the\nindex by one when checking for the next available position), while in double hashing, keys that result in collisions are more evenly spread\nthroughout the hash table (since we use a separate hash function to determine our increment).\n16. The correct answer is (B). Options (A) and (C) are related to the concept of quadratic residuals, which is a disadvantage of quadratic\nprobing: depending on the table size, certain indices of the table may never be visited during the collision resolution process (see section\n17.4). Option (D) is also a disadvantage this is characteristic of open addressing techniques. Option (B) is not a disadvantage, since open\npositions are not searched for consecutively (instead, quadratic probing uses increments of 1, 4, 9, …, which is less susceptible to primary\nclustering). Primary clustering is mainly a concern of linear probing, where there is a tendency for many keys to end up next to each other.\n17. The correct answer is (B). If a hash table of size 100 has 40 empty positions and 25 deleted positions, then there must be 100 - 40 - 25 =\n35 positions that are occupied by existing values. Thus, the load factor is 35/100 = 0.35.\nThe correct answer is (B). Your insertion must have caused the hash table to reach a load factor of 0.6, as it doubled and rehashed after the18.\ninsertion. Since the original size of the hash table was 75, your insertion must have added the 0.6 * 75 = 45th value into the hash table.\n19. The correct answer is (B). Here is how the elements are placed into this hash table:\n• 10 is placed at index (3(10) + 7) mod 10 = 37 mod 10 = 7\n• 8 is placed at index (3(8) + 7) mod 10 = 31 mod 10 = 1\n• 18 is placed at index (3(18) + 7) mod 10 = 61 mod 10 = 1 (collision!)\n– Linear probing sends 18 to the next open index after index 1, or index 2\n• 17 is placed at index (3(17) + 7) mod 10 = 58 mod 10 = 8\n• 4 is placed at index (3(4) + 7) mod 10 = 19 mod 10 = 9\n• 20 is placed at index (3(20) + 7) mod 10 = 67 mod 10 = 7 (collision!)\n– Linear probing sends 20 to the next open index after index 7\n– Indices 8 and 9 are occupied, so the search is looped to the beginning, and 20 is placed at index 0, which is open\n• 6 is placed at (3(6) + 7) mod 10 = 25 mod 10 = 5\n• 3 is placed at (3(3) + 7) mod 10 = 16 mod 10 = 6\n• 16 is placed at index (3(16) + 7) mod 10 = 55 mod 10 = 5 (collision!)\n– Linear probing sends 16 to the next open index after index 5\n– This forces 16 to index 3 (since the search loops around to the beginning)\nThis is what the final table looks like:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n20\n8\n18\n16\n6\n3\n10\n17\n4\n20. The correct answer is (A). In this case, the odd numbers hash to index 2 and the even numbers hash to index 7. Thus, 1 hashes to 2, 2\nhashes to 7, 3 hashes to 3, 4 hashes to 8, 5 hashes to 4, 6 hashes to 9, 7 hashes to 5, 8 hashes to 0, and 9 hashes to 6.\nThis is what the final table looks like:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n8\n1\n3\n5\n7\n9\n2\n4\n6\n21. The correct answer is (B). Here is how the elements are placed into this hash table:\n• 24 is placed at index 24 mod 7 = 3\n• 11 is placed at index 11 mod 7 = 4\n• 17 is placed at index 17 mod 7 = 3 (collision!)\n12)– Quadratic probing sends 17 to index (3 + mod 7 = 4 (collision!)\n22)– Quadratic probing sends 17 to index (3 + mod 7 = 7 mod 7 = 0\n• 21 is placed at index 21 mod 7 = 0 (collision!)\n12)– Quadratic probing sends 21 to index (0 + mod 7 = 1\n• 10 is placed at index 10 mod 7 = 3 (collision!)\n12)– Quadratic probing sends 10 to index (3 + mod 7 = 4 (collision!)\n22)– Quadratic probing sends 10 to index (3 + mod 7 = 7 mod 7 = 0 (collision!)\n32)– Quadratic probing sends 10 to index (3 + mod 7 = 12 mod 7 = 5", "word_count": 1103, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6ce24ca6-02dd-5fdf-a4f6-a7f9063ac1c8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 589, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n577\n• 4 is placed at index 4 mod 7 = 4 (collision!)\n12)– Quadratic probing sends 4 to index (4 + mod 7 = 5 (collision!)\n22)– Quadratic probing sends 4 to index (4 + mod 7 = 8 mod 7 = 1 (collision!)\n32)– Quadratic probing sends 4 to index (4 + mod 7 = 13 mod 7 = 6\nThis is what the final table looks like:\n0\n1\n2\n3\n4\n5\n6\n17\n21\n24\n11\n10\n4\n22. The correct answer is (C). Here is how the elements are placed into this hash table:\n• 9 is placed at index (2(9) + 3) mod 13 = 21 mod 13 = 8\n• 22 is placed at index (2(22) + 3) mod 13 = 47 mod 13 = 8 (collision!)\n12)– Quadratic probing sends 22 to index (8 + mod 13 = 9\n• 7 is placed at index (2(7) + 3) mod 13 = 17 mod 13 = 4\n• 10 is placed at index (2(10) + 3) mod 13 = 10 mod 13 = 10\n• 3 is placed at index (2(3) + 3) mod 13 = 9 mod 13 = 9 (collision!)\n12)– Quadratic probing sends 3 to index (9 + mod 13 = 10 (collision!)\n22)– Quadratic probing sends 3 to index (9 + mod 13 = 13 mod 13 = 0\n• 1 is placed at index (2(1) + 3) mod 13 = 6 mod 13 = 13\n• 20 is placed at index (2(20) + 3) mod 13 = 43 mod 13 = 4 (collision!)\n12)– Quadratic probing sends 20 to index (4 + mod 13 = 5\n• 13 is placed at index (2(13) + 3) mod 13 = 29 mod 13 = 3\n• 18 is placed at index (2(18) + 3) mod 13 = 39 mod 13 = 0 (collision!)\n12)– Quadratic probing sends 18 to index (0 + mod 13 = 1\n• 5 is placed at index (2(5) + 3) mod 13 = 13 mod 13 = 0 (collision!)\n12)– Quadratic probing sends 5 to index (0 + mod 13 = 1 (collision!)\n22)– Quadratic probing sends 5 to index (0 + mod 13 = 4 (collision!)\n32)– Quadratic probing sends 5 to index (0 + mod 13 = 9 (collision!)\n42)– Quadratic probing sends 5 to index (0 + mod 13 = 3 (collision!)\n52)– Quadratic probing sends 5 to index (0 + mod 13 = 12\n• 0 is placed at index (2(0) + 3) mod 13 = 3 mod 13 = 3 (collision!)\n12)– Quadratic probing sends 0 to index (3 + mod 13 = 4 (collision!)\n22)– Quadratic probing sends 0 to index (3 + mod 13 = 7\n• 11 is placed at index (2(11) + 3) mod 13 = 25 mod 13 = 12 (collision!)\n12)– Quadratic probing sends 11 to index (12 + mod 13 = 0 (collision!)\n22)– Quadratic probing sends 11 to index (12 + mod 13 = 3 (collision!)\n32)– Quadratic probing sends 11 to index (12 + mod 13 = 8 (collision!)\n42)– Quadratic probing sends 11 to index (12 + mod 13 = 2\nThis is what the final table looks like:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n3\n18\n11\n13\n7\n20\n1\n0\n9\n22\n10\n5\n23. The correct answer is (C). Note that all the elements are in their proper positions except for 82 and 53. For 82 to end up at index 5, all\nindices from 2 to 4 must have been filled when 82 was inserted. Thus, 82 must have been inserted after 62, 43, and 24, eliminating choice\n(A). For 53 to end up at index 7, all indices from 3 to 6 must have been filled when 53 was inserted. Thus, 53 must have been inserted after\n43, 24, 82, and 76. This eliminates choices (B) and (D). Only choice (C) remains, and it does in fact work.\n24. The correct answer is (C). For the strings \"paoletti\" and \"darden\" to collide, 15 mod 𝑀must be the same as 3 mod 𝑀. This is true for all\nof the above except for (15 mod 9 = 6 while 3 mod 9 = 3).𝑀=9\n25. The correct answer is (A). Here is how the elements are placed into this hash table:\n• 6 is placed at index (2(6) + 1) mod 7 = 13 mod 7 = 6\n• 10 is placed at index (2(10) + 1) mod 7 = 21 mod 7 = 0\n• 4 is placed at index (2(4) + 1) mod 7 = 9 mod 7 = 2\n• 13 is placed at index (2(13) + 1) mod 7 = 27 mod 7 = 6 (collision!)\n– The secondary hash function produces a value of (5 - (27 mod 5)) = 3, so we will probe open positions a distance of 3 apart\n– Double hashing sends 13 to index (6 + 1(3)) mod 7 = 9 mod 7 = 2 (collision!)\n– Double hashing sends 13 to index (6 + 2(3)) mod 7 = 12 mod 7 = 5\n• 11 is placed at index (2(11) + 1) mod 7 = 23 mod 7 = 2 (collision!)\n– The secondary hash function produces a value of (5 - (23 mod 5)) = 2, so we will probe open positions a distance of 2 apart\n– Double hashing sends 11 to index (2 + 1(2)) mod 7 = 4 mod 7 = 4 (collision!)\n• 12 is placed at index (2(12) + 1) mod 7 = 25 mod 7 = 4 (collision!)\n– The secondary hash function produces a value of (5 - (25 mod 5)) = 5, so we will probe open positions a distance of 5 apart\n– Double hashing sends 12 to index (4 + 1(5)) mod 7 = 9 mod 7 = 2 (collision!)\n– Double hashing sends 12 to index (4 + 2(5)) mod 7 = 14 mod 7 = 0 (collision!)\n– Double hashing sends 12 to index (4 + 3(5)) mod 7 = 19 mod 7 = 5 (collision!)\n– Double hashing sends 12 to index (4 + 4(5)) mod 7 = 24 mod 7 = 3", "word_count": 1078, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6b55bb41-9311-5cce-bb1b-1971abf4f313", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 590, "real_page_number": null, "text": "578\nChapter 17. Hash Tables and Collision Resolution\nThis is what the final table looks like:\n0\n1\n2\n3\n4\n5\n6\n10\n4\n12\n11\n13\n6\n26. The correct answer is (B). Here is how the elements are placed into this hash table:\n• \"sun\" is placed at index 18 mod 10 = 8\n• \"mercury\" is placed at index 12 mod 10 = 2\n• \"venus\" is placed at index 21 mod 10 = 1\n• \"earth\" is placed at index 4 mod 10 = 4\n• \"mars\" is placed at index 12 mod 10 = 2 (collision!)\n– Double hashing sends \"mars\" to index 2 + 1(7 - 12 mod 7) mod 10 = 2 + 2 = 4 (collision!)\n– Double hashing sends \"mars\" to index 2 + 2(7 - 12 mod 7) mod 10 = 2 + 4 = 6\n• \"jupiter\" is placed at index 9 mod 10 = 9\n• \"saturn\" is placed at index 18 mod 10 = 8 (collision!)\n– Double hashing sends \"saturn\" to index 8 + 1(7 - 18 mod 7) mod 10 = 11 mod 10 = 1 (collision!)\n– Double hashing sends \"saturn\" to index 8 + 2(7 - 18 mod 7) mod 10 = 14 mod 10 = 4 (collision!)\n– Double hashing sends \"saturn\" to index 8 + 3(7 - 18 mod 7) mod 10 = 17 mod 10 = 7\n• \"uranus\" is placed at index 20 mod 10 = 0\n• \"neptune\" is placed at index 13 mod 10 = 3\nThis is what the final table looks like:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\"uranus\"\n\"venus\"\n\"mercury\"\n\"neptune\"\n\"earth\"\n\"mars\"\n\"saturn\"\n\"sun\"\n\"jupiter\"\n27. The correct answer is (E). All four strings hash to the same location in a hash table of size 13. Thus, we know the following:𝑀=\n• \"apples\" is hashed to index 0\n• \"almonds\" is hashed to index 0 + 1(𝑞- 0 mod 𝑞) mod 13 = 1𝑞mod 13\n• \"avocados\" is hashed to index 0 + 2(𝑞- 0 mod 𝑞) mod 13 = 2𝑞mod 13\n• \"asparagus\" is hashed to index 0 + 3(𝑞- 0 mod 𝑞) mod 13 = 3𝑞mod 13\nSince \"asparagus\" ended up at index 7, we know that 3𝑞has a remainder of 7 when divided by 13 (e.g., 3𝑞mod 13 = 7). This is only true in\nthe case of 11, since 33 mod 13 = 7.𝑞=\n28. The correct answer is (D). We can solve this problem by using the same process as in the previous question, but solving for 𝑀instead of 𝑞:\n• \"apples\" is hashed to index 0\n• \"almonds\" is hashed to index 0 + 1(7 - 0 mod 7) mod 𝑀= 7 mod 𝑀\n• \"avocados\" is hashed to index 0 + 2(7 - 0 mod 7) mod 𝑀= 14 mod 𝑀\n• \"asparagus\" is hashed to index 0 + 3(7 - 0 mod 7) mod 𝑀= 21 mod 𝑀\nSince \"asparagus\" ended up at index 4, we know that 21 mod 𝑀is equal to 4. This works for 17.𝑀=\n29. The correct answer is (C). Only statements I and III are true. Statement II is false because, unlike separate chaining, you cannot have more\nthan one element at each index of the hash table if open addressing is used.\n30. The correct answer is (A). Direct addressing is the name for this addressing method, where the key is directly mapped to a position in the\nhash table without any hashing.\noperator[]31. The correct answer is (B). Only statement III compiles. Statement I does not work because requires the value type to be\nThingdefault constructible (since it needs to create an object of the value type), and does not have a default constructor. Statement II\ndoes not work because a pair needs to be inserted into an unordered map as a single entity, and not just its constructor arguments (e.g.,\nthing_map.insert({183, my_thing}) thing_map.insert(std::make_pair(183, my_thing))or would work).\n32. The correct answer is (C). To support efficient operations, hash tables do not store their elements in sorted order, so a different data\nstructure will be preferable if you want to print a given collection of elements in sorted order.\n33. The correct answer is (A). With separate chaining, each element will always end up at the bucket located at the position it hashes to, so no\ndeleted marked is necessary. The special deleted marker is only needed for open addressing since the position of elements may be different\nthan their hashed positions due to the collisions that are encountered, and we need a way to find these elements even if the values they\ncollided with upon insertion have been removed.\n34. The correct answer is (C). Hashes 1 and 4 are non-viable since they use a random number generator, which prevents the same string from\nbeing hashed to the same hash value every time (a requirement for hash functions). Between hashes 2 and 3, hash 3 is better since the hash\nvalue also depends on a character’s positioning, which reduces the likelihood of collisions (e.g., for hash 2, \"act\" and \"cat\" would have the\nsame hash value, but would not for hash 3).\n35. The correct answer is (E). The indices probed are 5, (5 + 1) mod 10 = 6, (5 + 4) mod 10 = 9, (5 + 9) mod 10 = 4, and (5 + 16) mod 10 =\n1 before we can conclude that 𝐾is not in the table. Note that index 1 needs to be probed because we need to confirm that it is empty before\nwe can make our conclusion.", "word_count": 948, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f96f5a82-7839-5a2a-ad93-42815f5f1981", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 591, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n579\n36. The correct answer is (D). Options (A) and (E) require knowledge of the ordering of elements, which is not well-suited for the intended\nuse case of a hash table. Option (B) needs to keep track of a priority value for each value, which is not ideal either (a priority queue would\nbe better for this). Hash tables are best designed for data that require fast lookups, of which options (C) and (D) fit the bill. However, option\n(C) involves keys that fall into a small universe of keys from 1 to 31, so a hash table is not necessary (we could just use an array of size\n31 and index into it based on the day of the month). Option (D) is the best use case for a hash table, since it involves key-value lookups\ninvolving a string that cannot easily take advantage of direct addressing.\n37. The correct answer is (C). Options (B) and (D) would only place elements at even number indices, leaving half the hash table with no\nelements. Option (E) would only place elements at indices 0 and 5, leaving eight other positions empty. This leaves us with (A) and (C).\nHowever, if you write out the indices for the first few keys using both of these hash functions, you would notice that the value of a perfect\nsquare can only end in 0, 1, 4, 5, 6, and 9 (e.g., 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441,\n3𝑖2etc.). Because of this, the value of can only end with the digit 0, 3, 2, 5, 8, or 7, which would always leave indices 1, 4, 6, and 9 empty.\nOn the other hand, a cube number can end with any digit, a pattern that repeats based on the final digit of the number that is being cubed\n(e.g., 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, etc.). This also applies to multiples of 9, which can also end with any digit: 9, 18, 27, 36,\n9𝑖345, 54, 63, 72, 81, 90, etc.). Because of this, would evenly distribute the numbers from 0 to 2019 among all 10 buckets of the hash table.\n38. The correct answer is (D). Here is how the elements are placed into this hash table:\n72• 7 is placed at index mod 7 = 0\n62• 6 is placed at index mod 7 = 1\n52• 5 is placed at index mod 7 = 4\n42• 4 is placed at index mod 7 = 2\n32• 3 is placed at index mod 7 = 2 (collision!)\n33– The secondary hash function produces a value of = 27, so we will probe open positions a distance of 27 apart\n– Double hashing sends 3 to index (2 + 1(27)) mod 7 = 29 mod 7 = 1 (collision!)\n– Double hashing sends 3 to index (2 + 2(27)) mod 7 = 56 mod 7 = 0 (collision!)\n– Double hashing sends 3 to index (2 + 3(27)) mod 7 = 83 mod 7 = 6\nThis is what the final table looks like:\n0\n1\n2\n3\n4\n5\n6\n7\n6\n4\n5\n3\n\"giraffe\"39. The correct answer is (D). Following the given operations, this is what happens to the hash table. First, we delete and mark\nit with a deleted flag.\naardvark\nhamster\nDELETED\n0\n1\n2\n3\n4\n5\n6\n\"fish\",Then, we insert which hashes to index 5.\naardvark\nhamster\nfish\nDELETED\n0\n1\n2\n3\n4\n5\n6\n\"lion\",Then, we insert which hashes to index 11 mod 7 = 4.\naardvark\nhamster\nlion\nfish\nDELETED\n0\n1\n2\n3\n4\n5\n6\nThe number of elements in our table is now 4, which causes the load factor to exceed 0.5. As a result, the table doubles in size and rehashes\nall the elements to their correct positions in the new table. Deleted markers are not carried over to the new table.\naardvark\nfish\nhamster\nlion\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\"frog\", \"frog\"Then, we insert which hashes to index 5. There is a collision, so is sent to index 6.\naardvark\nfish\nfrog\nhamster\nlion\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\"ferret\", \"ferret\"Then, we insert which collides at index 5. Using linear probing, is sent to index 8.\naardvark\nfish\nfrog\nhamster\nferret\nlion\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\"hamster\",Then, we delete so we mark it with a deleted flag.\naardvark\nfish\nfrog\nDELETED\nferret\nlion\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\"fox\",To insert we would need to probe indices 5, 6, 7, 8, and 9 before you can safely insert it into the table.", "word_count": 838, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ffddd316-32be-5369-bdb1-d8baa9a03732", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 592, "real_page_number": null, "text": "580\nChapter 17. Hash Tables and Collision Resolution\n\"fox\"40. The correct answer is (D). would be placed at the first index probed that is not occupied, which is index 7. Notice that deleted\n\"fox\"elements do not actually hold any value, so can still be inserted there.\n\"bear\" \"buffalo\"41. The correct answer is (C). The string hashes to index 1, so for to be printed out first, it must have been sent to\nindex 0 after collision resolution. All four strings collide on index 1, so the index each string gets sent to is:\n\"badger\":• 1 + 1(7 - (1 mod 7)) mod 𝑀= 1 + 1(6) mod 𝑀= 7 mod 𝑀\n\"bat\":• 1 + 2(7 - (1 mod 7)) mod 𝑀= 1 + 2(6) mod 𝑀= 13 mod 𝑀\n\"buffalo\":• 1 + 3(7 - (1 mod 7)) mod 𝑀= 1 + 3(6) mod 𝑀= 19 mod 𝑀\n\"buffalo\"For to be sent to index 0, 𝑀can only be 19 from the provided options.\n42. The correct answer is (D). Double hashing uses a secondary hash function to determine the collision probing interval, so it would be\naffected the most if the hash functions you use have to take linear time.\n43. The correct answer is (B). Only II is a requirement of hash functions. Hash functions do not need to be unique for each input (which is\nresolved using collision resolution). There is also no need for hash functions to produce similar hash values for similar inputs.\n44. The correct answer is (E). Deleted markers are not copied over during reallocation, so the number of deleted elements is 0. The number of\noccupied positions would be 16 (since the 16th element would cause the load factor to exceed 0.5 if the original hash table size was 31), and\nthe number of empty positions would be 62 - 16 = 46.\n45. The correct answer is (A). Deleting items from a hash table that uses open addressing does not guarantee improved insertion speed. This\nis because we still have to check if an element we want to insert exists, which involves probing deleted positions in the hash table as if an\nactual value existed at that position.\n46. The correct answer is (E). All of the options are true and could be valid explanations.\noperator[] const const47. The correct answer is (C). cannot be used on a unordered map (since a implementation of this operator is\nnot defined), so this code does not compile. Even though this use case is valid (since we check if the key exists first), the compiler still\ngpa_it->secondchecks for this to safeguard against a potential runtime error. One potential fix for this error is to return on line 5,\n.find()since the iterator returned by points to the key-value pair containing the element, if found.\noperator[] std::unordered_map<>, std::unordered_set<>,48. The correct answer is (A). exists in an but not an since\noperator== operator<unordered sets do not associate a value with each key. is defined for both containers, is defined for neither,\n.insert() .find()and and are defined for both.\n49. No, you would still have to iterate over the list to check if an element already exists before you insert it. This would take up to time.Θ(𝑚)\n50. There are multiple ways to solve this problem. If linear probing is chosen, you would want to fill the entire table except index 7, since\n\"slime\" normally hashes to index 8 and would have to collide and wrap around the entire table before it reaches index 7. If quadratic\n\"slime\"probing is used (the easier approach), you can insert three strings that hash to index 8, so that inserting would cause it to end up\n32)at index (8 + mod 10 = 17 mod 10 = 7.\n51. (a) One possible solution is as follows. We use the first hash to generate a hash for the given key. If a collision occurs, we use the second\nhash to generate another key, using the double hashing method until the key is successfully inserted into the table.\n1\nbool DoubleHashTable::insert_key(int32_t key) {\n2\nif (is_full()) {\n3\nreturn false;\n4\n} // if\n5\nsize_t index = hash1(key) % table_size;\n6\nif (hash_table[index].status == Status::Empty) {\n7\nhash_table[index].status = Status::Occupied;\n8\nhash_table[index].key = key;\n9\n} // if\n10\nelse {\n11\nsize_t increment = hash2(key);\n12\nsize_t j = 1;\n13\nwhile (true) {\n14\nsize_t new_index = (index + j increment) % table_size;*\n15\nif (hash_table[new_index].status == Status::Empty) {\n16\nhash_table[new_index].status = Status::Occupied;\n17\nhash_table[new_index].key = key;\n18\nbreak;\n19\n} // if\n20\n++j;\n21\n} // while\n22\n} // else\n23\n++num_elements;\n24\nreturn true;\n25\n} // insert_key()\n(b) Here is how the elements are placed into this hash table:\n• 11 is placed at index 11 mod 13 = 11\n• 24 is placed at index 24 mod 13 = 11 (collision!)\n– 24 is placed at index (11 + 1(7 - (24 mod 7))) mod 13 = 15 mod 13 = 2\n• 25 is hashed to index 25 mod 13 = 12", "word_count": 863, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "40e0d5db-ba4a-5699-8b63-5e1399fd98bf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 593, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n581\n• 41 is hashed to index 41 mod 13 = 2 (collision!)\n– 24 is placed at index (2 + 1(7 - (41 mod 7))) mod 13 = 3 mod 13 = 3\n• 45 is hashed to index 45 mod 13 = 6\n• 48 is hashed to index 48 mod 13 = 9\nThis is what the final table looks like:\n24\n41\n45\n48\n11\n25\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n52. One possible solution is as follows: we can create an unordered set that stores the lengths of the tracks and check if a track exists in the\nunordered set such that two tracks are able to sum to the wait time. To prevent listening to the same track twice, we check the unordered set\nfor a value before inserting into it.\n1\nbool identify_tracks(const std::vector<int32_t>& int32_ttracks, wait_time) {\n2\nstd::unordered_set<int32_t> track_lengths;\n3\nfor (int32_t curr_length : tracks) {\n4\nif (track_lengths.find(wait_time - curr_length) != track_lengths.end()) {\n5\nreturn true;\n6\n} // if\n7\ntrack_lengths.insert(curr_length);\n8\n} // for\n9\nreturn false;\n10\n} // identify_tracks()\nThis is a fairly standard hash table problem, where you are tasked to solve a problem that involves key-value lookups (where the key is a53.\nuser and the value is the number of words they have sent). One such implementation is shown below. Here, we initialize an unordered map\nthat maps from username to word count. We iterate over the list of messages and count the number of spaces (and add one) to get the word\ncount of each message. We then add it to the value in the map associated with the user that sent the message. After all the messages are\nprocessed, we iterate over the map and return the user we encounter with the highest word count.\n1\nuser_with_highest_word_count(conststd::string std::vector<Message>& messages) {\n2\nint32_t>std::unordered_map<std::string, word_count_by_user;\n3\nfor (const Message& message : messages) {\n4\n// can also iterate over string and count spaces instead of std::count()\n5\nword_count_by_user[message.sender] +=\n6\n(std::count(message.text.begin(), message.text.end(), ' ') + 1);\n7\n} // for\n8\n9\nstd::string max_word_count_user;\n10\nint32_t max_encountered = 0;\n11\nfor (auto [user, word_count] : word_count_by_user) {\n12\nif (word_count == max_encountered) {\n13\nmax_word_count_user = std::max(max_word_count_user, user);\n14\n} // if\n15\nelse if (word_count > max_encountered) {\n16\nmax_word_count_user = user;\n17\nmax_encountered = word_count;\n18\n} // else if\n19\n} // for\n20\n21\nreturn max_word_count_user;\n22\n} // user_with_highest_word_count()\n54. One potential solution to this problem is to traverse over the vector of pairs and store the points into a hash table where the first values\n(𝑥-coordinates) are keys and the second values (𝑦-coordinates) are values. For each coordinate, we check if its 𝑦-coordinate exists as a key\nin this hash table. If so, compare the other coordinate and add to the result if equal.\n1\nstd::vector<std::vector<std::pair<int32_t, int32_t>>> find_symmetric_pairs(\n2\nconst std::vector<std::pair<int32_t, int32_t>>& vec) {\n3\nstd::vector<std::vector<std::pair<int32_t, int32_t>>> result;\n4\nstd::unordered_map<int32_t, int32_t> xy_map;\n5\nfor (auto [x, y] : vec) {\n6\nauto y_it = xy_map.find(y);\n7\nif (y_it != xy_map.end() && y_it->second == x) {\n8\nresult.push_back({{x, y}, {y, x}});\n9\n} // if\n10\nelse {\n11\nxy_map[x] = y;\n12\n} // else\n13\n} // for\n14\n15\nreturn result;\n16\n} // find_symmetric_pairs()", "word_count": 566, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d869055d-ba37-5cbb-94c7-6127d268d37a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 594, "real_page_number": null, "text": "582\nChapter 17. Hash Tables and Collision Resolution\n55. One potential solution to this problem is to traverse over the vector and store the frequency of each element in a hash table, where the\nelement is the key and the frequency is the value. Then, we identify the element with the greatest frequency and subtract this frequency\nfrom the number of elements in the vector to get the minimum number of deletions required.\n1\nint32_t min_deletions_for_all_equal(const std::vector<int32_t>& vec) {\n2\nstd::unordered_map<int32_t, int32_t> frequency_map;\n3\nfor (int32_t val : vec) {\n4\n++frequency_map[val];\n5\n} // for\n6\n7\nint32_t max_freq_encountered = 0;\n8\nfor (auto [val, freq] : frequency_map) {\n9\nif (freq > max_freq_encountered) {\n10\nmax_freq_encountered = freq;\n11\n} // if\n12\n} // for\n13\n14\nreturn vec.size() - max_freq_encountered;\n15\n} // min_deletions_for_all_equal()\nOne potential solution to this problem is to iterate over the given elements and map each element to the first index at which it occurs in the56.\nvector. Thus, whenever you encounter a repeated element, you will be able to easily identify that you have seen its value before as well as\nthe first position it occurred at. This will allow you to keep track of the maximum distance you have seen so far.\n1\nint32_t max_repeated_distance(const std::vector<int32_t>& vec) {\n2\nstd::unordered_map<int32_t, int32_t> first_index_of_value;\n3\nint32_t result = 0;\n4\nfor (int32_t i = 0; i < vec.size(); ++i) {\n5\nauto first_index_it = first_index_of_value.find(vec[i]);\n6\nif (first_index_it == first_index_of_value.end()) {\n7\nfirst_index_of_value[vec[i]] = i;\n8\n} // if\n9\nelse {\n10\nresult = std::max(result, i - first_index_it->second);\n11\n} // else\n12\n} // for i\n13\nreturn result;\n14\n} // max_repeated_distance()\nΘ(𝑛2)57. If you were to try to brute force this problem and check every possible subarray, you would end up with an solution since there are\nΘ(𝑛2) possible subarrays that can be generated out of the input array of 𝑛. Other techniques such as two pointer and sliding window do not\nwork here, since the array gives you no information on how to move the two pointers or adjust your window (largely due to the fact that\nvalues can be negative). Instead, the optimal solution is keep track of an unordered map that maps each prefix sum to the number of times\nthat sum is encountered. The algorithm is as follows:\n• Traverse the array and use the unordered map to keep track of the number of times you have encountered each prefix sum.\n• If the current prefix sum ever equals 𝑘, add one to the solution.\n• Look in the unordered map to see how many prefixes have a sum equal to the difference between the current sum and 𝑘(e.g., sum -\n𝑘). We add this value to our solution (since we can remove these prefixes from the current sum to get a subarray that sums to 𝑘).\n• After traversing the entire array, return the calculated solution.\n[2, -3, 4, -2, 1, -1]For example, consider the example input and −1. We start traversing the array and visit the first value𝑘=\n[2].of 2, which corresponds to a prefix of Since this prefix is not equal to 𝑘, and the difference between it and 𝑘(2 - (-1) = 3) do not exist\nin the unordered map, we do not update our solution. This prefix is added to the map with an initial count of 1.\nKey\nValue\n2\n1\nNumber of Subarrays: 0\n[2, -3], -1.The next prefix we encounter is which has a sum of This prefix is equal to 𝑘, so we increment the number of subarrays\nencountered by 1. The difference between the prefix sum and 𝑘, or -1 - (-1) = 0, does not exist in the map, so we do not add anything\nadditional to the solution. Lastly, we add the prefix sum of -1 to the map, with an initial count of 1.\nKey\nValue\n2\n1\n-1\n1\nNumber of Subarrays: 1\n[2, -3, 4],The next prefix we encounter is which has a sum of 3. This prefix sum is not equal to 𝑘, and the difference between this\nsum and 𝑘, or 3 - (-1) = 4, does not exist in the map, so we do not increment the number of subarrays at this step. We add the prefix sum of\n3 to the map, with an initial count of 1.\nKey\nValue\n2\n1\n-1\n1\n3\n1\nNumber of Subarrays: 1", "word_count": 752, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b277bd25-f81f-5ba1-88ab-a43d622559a0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 595, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n583\n[2, -3, 4, -2],The next prefix we encounter is which has a sum of 1. This prefix sum is not equal to 𝑘, so we do not increment\nour solution from this check. However, the difference between this sum and 𝑘, or 1 - (-1) = 2, does exist in the map with a count of 1,\nso we add 1 to our solution. Why? Our unordered map indicates that there was 1 prefix that we previously encountered that summed to\n2, so we can remove that 1 prefix from our current prefix to obtain a contiguous subarray with a sum of -1. That is, our current prefix\n[2, -3, 4, -2] [2])does not sum to -1, but one earlier prefix (in this case, sums to the difference between the current prefix sum\n[2] [2, -3, 4, -2] [-3, 4, -2]and 𝑘, so we can simply remove from the beginning of to get a subarray that sums to −1.𝑘=\nNote that if the prefix count for 2 were 2 instead of 1, we would add 2 to our solution (since there would be 2 prefixes that we would be able\nto remove from our current prefix to obtain a sum of 𝑘). Lastly, we add the original prefix sum of 1 to the map, with an initial count of 1.\nKey\nValue\n2\n1\n-1\n1\n3\n1\n1\n1\nNumber of Subarrays: 2\n[2, -3, 4, -2, 1],The next prefix we encounter is which has a sum of 2. This prefix sum is not equal to 𝑘, but the difference\nbetween this sum and 𝑘, or 2 - (-1) = 3, does exist in the map with a count of 1, so we add 1 to our solution. Since we encountered another\nprefix sum of 2, we increment its count in the unordered map.\nKey\nValue\n2\n2\n-1\n1\n3\n1\n1\n1\nNumber of Subarrays: 3\n[2, -3, 4, -2, 1, -1],The last prefix we encounter is which has a sum of 1. This prefix sum is not equal to 𝑘, but the difference\nbetween this sum and 𝑘, or 1 - (-1) = 2, does exist in the map with a count of 2, so we add 2 to our solution. Since we encountered another\nprefix sum of 1, we increment its count in the unordered map.\nKey\nValue\n2\n2\n-1\n1\n3\n1\n1\n2\nNumber of Subarrays: 5\nWe have completed our traversal of the array, so our solution is 5. An implementation of this solution is shown below:\n1\nint32_t num_subarrays_with_sum_k(const std::vector<int32_t>& int32_tvec, k) {\n2\nstd::unordered_map<int32_t, int32_t> prefix_count;\n3\nint32_t num_subarrays_k = 0, prefix_sum = 0;\n4\nfor (int32_t i = 0; i < vec.size(); ++i) {\n5\nprefix_sum += vec[i];\n6\nif (prefix_sum == k) {\n7\n++num_subarrays_k;\n8\n} // if\n9\nauto diff_it = prefix_count.find(prefix_sum - k);\n10\nif (diff_it != prefix_count.end()) {\n11\nnum_subarrays_k += diff_it->second;\n12\n} // if\n13\n++prefix_count[prefix_sum];\n14\n} // for i\n15\nreturn num_subarrays_k;\n16\n} // num_subarrays_with_sum_k()", "word_count": 521, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "833f0ac7-1a29-5e62-9dd6-a5d5c9d5cf1b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 596, "real_page_number": null, "text": "584\nChapter 17. Hash Tables and Collision Resolution\n58. Performing a deep copy of the list without the random pointer is fairly straightforward (see chapter 8). The tricky part of this problem\ninvolves performing a deep copy of the list so that each random pointer points to correct node of the newly copied list. One solution is to\nuse a hash table to map each node in the original list to the corresponding copy in the new list. We first build our deep copy normally\nwithout considering the random pointer (i.e., only setting the next value properly), inserting each copy into the hash table. Then, we iterate\nover each node of the original list and use the hash table to find the deep copy of the node associated with its random pointer, and then set\nthe random pointer of the current node’s copy to this deep copy. An implementation is shown below:\n1\nNode* copy_random_list(Node* head) {\n2\nif (!head) {\n3\nreturn nullptr;\n4\n} // if\n5\nstd::unordered_map<Node*, Node*> orig_to_copy;\n6\n7\n// Deep copy of original list sans random pointer, adding the copy to the unordered map\n8\nnewNode* head_copy = Node(head->val);\n9\norig_to_copy.emplace(head, head_copy);\n10\nNode* orig = head->next;\n11\nNode* copy = head_copy;\n12\nwhile (orig) {\n13\nnewNode* orig_copy = Node(orig->val);\n14\norig_to_copy.emplace(orig, orig_copy);\n15\ncopy->next = orig_copy;\n16\ncopy = orig_copy;\n17\norig = orig->next;\n18\n} // while\n19\n20\n// Use unordered map to identify deep copy of random node, and set in new list\n21\norig = head;\n22\ncopy = head_copy;\n23\nwhile (orig) {\n24\nnullptr;copy->random = orig->random ? orig_to_copy[orig->random] :\n25\norig = orig->next;\n26\ncopy = copy->next;\n27\n} // while\n28\n29\nreturn head_copy;\n30\n} // copy_random_list()\nThis problem can be solved using hash tables, since we can take advantage of fast lookups to retrieve information on each rider or route.59.\nThere are several ways to tackle this problem, but one strategy is to keep track of unordered maps that:\n• map each rider ID to the number of rides they have taken\n• map each rider ID to information that determines if they are currently checked into a ride (i.e., start station and timestamp)\n• map each route to the total time taken and the number of rides (for calculating both the average time and the route ridership)\nEvery time a rider checks in, we add them to a map to keep track of their starting station and timestamp, and every time they check out, we\ntake this information to help compute the average time for each route. One implementation of this solution is provided below:\n1\nclass TransitManager {\n2\nprivate:\n3\n// Used to keep track of where a rider started if they are currently on a route\n4\nstruct CurrentRideInfo {\n5\nstd::string check_in_station;\n6\nint64_t timestamp{};\n7\n};\n8\n9\n// Used to keep track of information needed to calculate route info\n10\nstruct CurrentRouteInfo {\n11\nint64_t total_time{};\n12\nint64_t num_rides_on_route{};\n13\n};\n14\n15\nstd::unordered_map<int64_t, int64_t> rider_to_num_rides;\n16\nstd::unordered_map<int64_t, CurrentRideInfo> rider_to_current_ride;\n17\n// This stores each route as a string, but there are other ways to do this\n18\nstd::unordered_map<std::string, CurrentRouteInfo> route_to_info;\n19\n20\npublic:\n21\nvoid check_in(int64_t const int64_trider_id, std::string& station_name, timestamp) {\n22\n++rider_to_num_rides[rider_id];\n23\nrider_to_current_ride[rider_id] = CurrentRideInfo{.check_in_station = station_name,\n24\n.timestamp = timestamp};\n25\n} // check_in()\n26\n27\n// ... continued on next page ...", "word_count": 577, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e53ffe31-5c3c-51bd-8934-a2114420e5cd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 597, "real_page_number": null, "text": "17.9 Solving Problems Using Hash Tables\n585\n28\nvoid check_out(int64_t const int64_trider_id, std::string& station_name, timestamp) {\n29\nauto [start_station, start_timestamp] = rider_to_current_ride[rider_id];\n30\nstd::string route = start_station + '-' + station_name;\n31\n32\nCurrentRouteInfo& route_info = route_to_info[route];\n33\nroute_info.total_time += (timestamp - start_timestamp);\n34\n++route_info.num_rides_on_route;\n35\n36\nrider_to_current_ride.erase(rider_id);\n37\n} // check_out()\n38\n39\ndouble get_avg_time(const conststd::string& start_station, std::string& end_station) {\n40\nstd::string route = start_station + '-' + end_station;\n41\nconst CurrentRouteInfo& route_info = route_to_info[route];\n42\nreturn static_cast<double>(route_info.total_time) / route_info.num_rides_on_route;\n43\n} // get_avg_time()\n44\n45\nint64_t get_num_rides(int64_t rider_id) {\n46\nreturn rider_to_num_rides[rider_id];\n47\n} // get_num_rides()\n48\n49\nint64_t get_num_route(const conststd::string& start_station, std::string& end_station) {\n50\nstd::string route = start_station + '-' + end_station;\n51\nreturn route_to_info[route].num_rides_on_route;\n52\n} // get_num_route()\n53\n};\nThis problem can be solved using a hash table that maps each account name to their account balance, with each of the operations looking up60.\neach account name and modifying their balance. One possible implementation of the problem is shown below:\n1\nclass TransactionManager {\n2\nprivate:\n3\ndouble>std::unordered_map<std::string, account_to_balance;\n4\n5\npublic:\n6\nbool open(const std::string& account_name) {\n7\nauto account_it = account_to_balance.find(account_name);\n8\nif (account_it != account_to_balance.end()) {\n9\nreturn false;\n10\n} // if\n11\n// NOTE: emplace actually returns whether the key already exists, so you\n12\n// can actually just call emplace and return the bool in the return pair here\n13\naccount_to_balance.emplace(account_name, 0);\n14\nreturn true;\n15\n} // open()\n16\n17\nbool deposit(const doublestd::string& account_name, amount) {\n18\nauto account_it = account_to_balance.find(account_name);\n19\nif (account_it == account_to_balance.end()) {\n20\nreturn false;\n21\n} // if\n22\naccount_it->second += amount;\n23\nreturn true;\n24\n} // deposit()\n25\n26\nbool withdraw(const doublestd::string& account_name, amount) {\n27\nauto account_it = account_to_balance.find(account_name);\n28\nif (account_it == account_to_balance.end() || account_it->second < amount) {\n29\nreturn false;\n30\n} // if\n31\naccount_it->second -= amount;\n32\nreturn true;\n33\n} // withdraw()\n34\n35\nbool transfer(const const doublestd::string& src_account, std::string& dest_account, amount) {\n36\nauto src_account_it = account_to_balance.find(src_account);\n37\nauto dest_account_it = account_to_balance.find(dest_account);\n38\nif (src_account_it == account_to_balance.end() ||\n39\ndest_account_it == account_to_balance.end()) {\n40\nreturn false;\n41\n} // if\n42\nreturn withdraw(src_account, amount) && deposit(dest_account, amount);\n43\n} // transfer()\n44\n};", "word_count": 379, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab39bd90-256b-5f65-b63a-7b72a4813f69", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 598, "real_page_number": null, "text": "586\nChapter 17. Hash Tables and Collision Resolution\n61. This problem can be solved using a hash table that maps table names to table info, where each method looks up the table and applies any\nrequested changes. One possible implementation of the problem is shown below:\n1\nclass Database {\n2\nprivate:\n3\nstruct TableData {\n4\nstd::unordered_map<int64_t, std::vector<std::string>> row_id_to_data;\n5\nsize_t>std::unordered_map<std::string, column_name_to_index;\n6\nint64_t max_id = 0;\n7\n};\n8\n9\nstd::unordered_map<std::string, TableData> table_data;\n10\n11\npublic:\n12\nbool create(const TableInfo& table_info) {\n13\nauto table_it = table_data.find(table_info.table_name);\n14\nif (table_it != table_data.end()) {\n15\nreturn false;\n16\n} // if\n17\nauto& column_map = table_data[table_info.table_name].column_name_to_index;\n18\nfor (size_t i = 0; i < table_info.column_names.size(); ++i) {\n19\ncolumn_map[table_info.column_names[i]] = i;\n20\n} // for\n21\nreturn true;\n22\n} // create()\n23\n24\nint64_t insert(const conststd::string& table_name, std::vector<std::string>& row_data) {\n25\nauto& table = table_data[table_name];\n26\ntable.row_id_to_data.emplace(++table.max_id, row_data);\n27\nreturn table.max_id;\n28\n} // insert()\n29\n30\nbool remove(const int64_tstd::string& table_name, row_id) {\n31\nauto& table = table_data[table_name];\n32\nauto row_it = table.row_id_to_data.find(row_id);\n33\nif (row_it == table.row_id_to_data.end()) {\n34\nreturn false;\n35\n} // if\n36\ntable.row_id_to_data.erase(row_it);\n37\nreturn true;\n38\n} // remove()\n39\n40\nselect(const int64_tstd::optional<std::string> std::string& table_name, row_id,\n41\nconst std::string& column_name) {\n42\nconst auto& table = table_data[table_name];\n43\nauto row_it = table.row_id_to_data.find(row_id);\n44\nif (row_it == table.row_id_to_data.end()) {\n45\nreturn std::nullopt;\n46\n} // if\n47\nauto col_it = table.column_name_to_index.find(column_name);\n48\nif (col_it == table.column_name_to_index.end()) {\n49\nreturn std::nullopt;\n50\n} // if\n51\nreturn row_it->second[col_it->second];\n52\n} // select()\n53\n};", "word_count": 264, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "259160cf-f557-5f00-b104-99cad05b9aac", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 599, "real_page_number": null, "text": "Chapter 18\nTrees\n18.1\nIntroduction to Trees\nA tree is a mathematical abstraction that plays an important role in the design and analysis of many important algorithms. In a tree, information\nis organized hierarchically, and we can use this hierarchical structure to capture common properties of data.\nFor example, arithmetic expressions can be expressed using trees. Consider the expression (6+3)×(5−2). In the figure below, we build a\ntree where our operands are leaf nodes and our operators are internal (non-leaf) nodes.\n×\n−\n2\n5\n+\n3\n6\nWhy would we ever want to turn our math expressions into trees? By translating this mathematical expression into a tree, we are able to elegantly\nsolve the expression using a simple tree traversal algorithm. In fact, many different calculators use these types of trees — known as expression\n— to solve human-readable math expressions that adhere to the order of operations.trees\nThe expression tree is just one example of the versatility of trees. In this chapter, we will analyze the tree structure and explore several tree\nalgorithms that can be used to solve different types of problems.\nRemark: Before we can introduce the concept of a tree, we have to introduce the concept of a graph. A graph is simply a collection of nodes\nthat are connected by edges. A tree is simply a restricted form of a graph; that is, every tree is a graph, but not every graph is a tree! In this\nchapter, we will focus primarily on trees — we will explore the broader category of graphs in the next chapter.", "word_count": 264, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b6bb991b-37cb-5022-bb86-16557bbe2c2c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 600, "real_page_number": null, "text": "588\nChapter 18. Trees\nTo begin, we will first introduce different types of trees. A simple tree is an acyclic, connected graph. A graph is if there are no cyclesacyclic\n(i.e., there exist no two points in the graph that can be connected by more than one route). A graph is if there exists a path from aconnected\nnode to any other node in the graph (i.e., there are no disjoint sets). A few examples are shown below:\nA\nB\nC\nD\nE\nF\nacyclic, connected\n(this is a simple tree)\nA\nB\nC\nD\nE\nF\nacyclic, not connected\n(this is a simple tree)not\nA\nB\nC\nD\nE\nF\nnot acyclic, connected\n(this is a simple tree)not\nA\nB\nC\nD\nE\nF\nnot acyclic, not connected\n(this is a simple tree)not\nA rooted tree is a type of simple tree that contains a special node called the root. All edges in a rooted tree branch away from this root node,\nand any node can be selected as the root. An example of a rooted tree is shown below, where node A is the root:\nA\nD\nJ\nI\nH\nC\nG\nF\nB\nE\nBelow lists some general tree terminology (many of these terms are derived from terms used for family relationships):\n• Root: The root of a tree is the \"top-most\" vertex in the tree, from which all other nodes are directed away from. In the tree above, node A\nis the root of the tree.\n• Parent/Child: For every direct connection in the tree, we define the higher node as the parent, and the lower node as the child. In the\ntree above, node A is the parent of nodes B, C, and D. Node B is the parent of node E. Similarly, nodes B, C, and D are the children of\nnode A, and node E is the child of node B.\n• Sibling: Two nodes are siblings if they share the same parent. In the tree above, B, C, and D are siblings, since they share the parent A.\n• Descendant: The descendants of a given node consist of any node that exists along a path from the given node to a leaf node (including\nthe leaf node). In the tree above, nodes B and E are both descendants of node A.\n• Ancestor: The ancestors of a given node consist of any node that exists along the path from the given node to the root node (including\nthe root node itself). In the tree above, nodes B and A are both ancestors of node E.\nExternal (Leaf) Node: An external node (or leaf node) is a node that has no children. In the tree above, nodes E to J are examples of•\nexternal (leaf) nodes.\n• Internal Node: An internal node is a node with children. In the tree above, nodes A to D are examples of internal nodes.\n• Depth: The depth of a node represents how far it is from the top of the tree. The root has a depth of 1. If a node is one edge away from\nthe root, it has a depth of 2. If a node is two edges away from the root, it has a depth of 3. We can define the depth of a node recursively:\n– depth(empty) = 0\n– depth(node) = depth(parent) + 1\n• Height: The height of a node represents how far it is from the bottom of the tree. Leaf nodes have a height of 1. If a node has more than\none child, its height is equal to one plus the largest height of any of its children. We can define the height of a node recursively:\n– height(empty) = 0\n– height(node) = max(height(children)) + 1\n• In a tree, the largest height of any node and the largest depth of any node should be the same value.\nAn ordered tree is a tree that has a linear ordering for the children of each node. In other words, the children of an ordered tree are defined in\nsome predetermined order (e.g., left child and right child). Consider the following:\nA\nD\nC\nB\nA\nC\nD\nB\nA\nD\nB\nC\nA\nB\nD\nC\nA\nC\nB\nD\nA\nB\nC\nD\nAre these trees identical? If they are ordered, then yes, they are identical, because non-ordered trees do not enforce an ordering for siblings.not\nHowever, if these trees are ordered, then no, they are not identical, since the linear order of children does matter (e.g., a tree where B is to the\nleft of C is different from one where C is to the left of B).", "word_count": 790, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3aef1582-a4cd-5eae-8625-d70b038cd954", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 601, "real_page_number": null, "text": "18.2 Binary Trees\n589\nExample 18.1 Consider the following tree. Which node is the root? Which nodes are leaf nodes? Which nodes are internal nodes? What\nis the maximum depth of the tree? What is the height of node B?\nA\nD\nC\nH\nG\nB\nF\nK\nJ\nI\nE\nIn this tree, node A is the root. Nodes A, B, C, and F are internal nodes because they have children. Nodes D, E, G, H, I, J, and K are leaves\nbecause they have no children. The maximum depth of the tree is 4 (this is the number of levels in the tree). The height of B is equal to\nmax(height(E), height(F)) + 1. Since F is the child with the larger height (height(E) = 1, height(F) = 2), the height of B is 2 + 1 = 3.\n18.2\nBinary Trees\nA binary tree is a common type of ordered tree in which each node can have at most two children (a left child and a right child). For the\nmajority of this chapter, we will be dealing with binary trees. There are two primary ways to represent a binary tree in memory: (1) using an\narray-based approach, and (2) using a pointer-based approach.\n¸ 18.2.1\nArray-Based Tree Implementation\nIf we implement a binary tree using an array, we essentially flatten out the tree and store its nodes sequentially in memory, level by level. This is\nidentical to how we used an array to represent a binary heap. (For simplicity, we assume 1-indexing as with bef)\nA\nC\nG\nF\nB\nE\nD\nI\nH\nA\nB\nC\nD\nE\nF\nG\nH\nI\n[0] [1] [2] [3] [4] [5] [6] [7] [8] [9]\nNote: There is a reason why index 0 is empty... this will be explained soon.\nTo implement a tree using an array, we will follow these rules:binary\narray.1• The root of the binary tree is placed at index 1 of the\n• The left child of the node at index 𝑖should be placed at index 2𝑖. If node 𝑖has no left child, then index 2𝑖should be empty.\n• The right child of the node at index 𝑖should be placed at index 2𝑖+1. If node 𝑖has no right child, then index should be empty.2𝑖+1\nThe array-based approach worked well when we dealt with binary heaps. This is because binary heaps are complete binary trees. Recall that a\ncomplete binary tree is a binary tree with depth 𝑑such that:\n• All nodes at a depth of 1, 2, 3, …, 𝑑−1, have the maximum number of nodes possible (e.g., all levels but the bottom are filled).\n• All nodes at a depth of 𝑑are filled from left to right with no gaps.\nBecause binary are guaranteed to be complete, we can ensure that our underlying array will have no gaps (except perhaps index 0, if weheaps\nstart our heap at index 1). However, binary need not be complete, so we cannot avoid potential gaps in our underlying array. For example,trees\nif we implemented the following binary tree using an array, indices 5, 6, and 8 would be empty:\nA\nC\nE\nB\nD\nF\nA\nB\nC\nD\nE\nF\n[0] [1] [2] [3] [4] [5] [6] [7] [8] [9]\nNote: There is a reason why index 0 is empty... this will be explained soon.\n1Likewithheaps,youdon’thavetophysicallyputtherootatindex1. It’sperfectlyfinetoputtherootatindex0,aslongasyouareabletoindexchildrencorrectly.\nYou can do this by using the same formulas as above, but first converting index 𝑖into whenever you have to calculate the index of a child, or by using𝑖−1\nalternativeformulastocalculatetheindicesoftheleftandrightchildren.", "word_count": 641, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7e0097d7-0688-5267-8034-475062b848a7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 602, "real_page_number": null, "text": "590\nChapter 18. Trees\nBecause indices in the underlying array may be skipped, an array-based approach can be space prohibitive for sparse trees (as memory would be\nwasted by allocating space for nodes that do not exist). In the worst case, we could get a stick like this, which would require us to allocate a giant\narray that remains mostly empty:\nA\nB\nC\nD\nA\nB\nC\nD\n[0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10][11][12][13][14][15]\nNote: There is a reason why index 0 is empty... this will be explained soon.\nThe following table illustrates the time complexity of operations for an array-based binary tree with 𝑛nodes:\nOperation\nComplexity\nInsert Key (Best Case)\nΘ(1)\nInsert Key (Worst Case)\nΘ(𝑛)\nRemove Key (Worst Case)\nΘ(𝑛)\nFind Parent\nΘ(1)\nFind Child\nΘ(1)\nSpace Required (Best Case)\nΘ(𝑛)\nSpace Required (Worst Case)\nΘ(2𝑛)\nNote that we do not have rules for determining where an element goes when it is inserted (this will change when we talk about different types of\nbinary trees, such as the tree). For a pure binary tree, we can insert a node anywhere in the tree.binary search\nInsert Key\nIn the best case for insertion, the root node does not have two children, allowing us to identify an open index immediately (e.g., we can directly\ninsert the new node as the left or right child of the root without having to traverse the remaining elements). In the worst case for insertion, we\ncould have to traverse the entire array before we find an open position to insert the new element (and if the array is completely full, we may also\nhave to reallocate the underlying array).\nRemove Key\nSince there are no rules for how data is stored in a binary tree, removal takes time in the worst case, since we may have to visit every nodeΘ(𝑛)\nin the tree before we find the one we want to remove. Furthermore, if we are asked to remove a nonexistent value, we would have to traverse all\nthe nodes in the tree before concluding the element doesn’t exist, which takes time.Θ(𝑛)\nFind Parent and Child\nFinding the parent or child of a node takes constant time in an array implementation. Given a node at index 𝑖(assuming the root is at index 1),\nits parent must be at index 𝑖∕2, its left child must be at index 2𝑖, and its right child must be at index 2𝑖+1. This is just basic arithmetic, which\ncan be done in time.Θ(1)\nSpace Complexity\nRegarding the space complexity, the best case occurs when the tree is complete and there are no gaps in the underlying array. In this case,\nwe would only need memory to store 𝑛nodes. The worst case occurs when we have a stick, as shown in the example above. When thisΘ(𝑛)\nΘ(2𝑛)happens, only a single node can exist at each depth of the tree, which forces us to use memory. This is because the maximum number of\n2𝑑−1,nodes that can exist at any depth 𝑑(and thus the number of array positions needed to support that depth) is equal to assuming the root has\na depth of 1. If we have a stick, the depth of our tree would be 𝑛, and the array size we need to support a depth of 𝑑is\n+…+2𝑛−2+2𝑛−1 2𝑛−1 Θ(2𝑛)20+21+22 = =\nIn general, the worst-case space complexity of an array-based approach depends on the maximum number of children each node could have. If\neach node in the tree could have at most 3 children, then the maximum number of nodes that can exist at a depth 𝑑(and thus the amount of\n3𝑑−1.additional memory we have to allocate to support that depth) is equal to If we have a stick, the depth would still be 𝑛, but the array size we\nneed to support a depth of 𝑛now becomes\n1+…+3𝑛−2+3𝑛−130+31+32 =\n2(3𝑛−1) Θ(3𝑛)=\nTo solve the above equations, the formula for the sum of a geometric series was used (where 𝑎represents the starting term, 𝑟represents the\ncommon ratio, and 𝑛represents the number of terms in the sequence):\n𝑎\n(1−𝑟𝑛\n1−𝑟\n)\nIn general, given a tree with 𝑛nodes, where each node can have at most 𝑘children (where 𝑘is a constant), the worst-case space complexity of\nimplementing this tree using an array-based approach is\n1\n(1−𝑘𝑛\n1−𝑘\n)\n=\n1\n𝑘−1(𝑘𝑛−1) Θ(𝑘𝑛)=", "word_count": 746, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7fd4bb38-d5eb-5f3f-8825-b8ad7ce3933e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 603, "real_page_number": null, "text": "18.2 Binary Trees\n591\nExample 18.2 Suppose you are trying to implement an array-based tree whose internal nodes can have up to 10 children. What are the\nbest- and worst-case space complexities of implementing this tree, if it has 𝑛nodes?\nIn the best case, there are no gaps in the underlying array, which allows you to store the 𝑛elements sequentially, requiring space. In theΘ(𝑛)\nworst case, you have a stick where every depth only has a single element. When this happens, you can use the formula for a geometric series to\nΘ(10𝑛).show that the space complexity is\n¸ 18.2.2\nPointer-Based Tree Implementation\nAn alternative to the array-based approach is the pointer-based approach. In the pointer-based approach, each node of a binary tree stores\npointers to its left and right children:\n1\ntemplate <typename T>\n2\nstruct Node {\n3\nT val;\n// value of type T\n4\nNode* left;\n// pointer to left child, nullptr if no left child\n5\nNode* right;\n// pointer to right child, nullptr if no right child\n6\n};\nIf we implement a binary tree this way, we would get the following time complexities for a binary tree with 𝑛nodes:\nOperation\nComplexity\nInsert Key (Best Case)\nΘ(1)\nInsert Key (Worst Case)\nΘ(𝑛)\nRemove Key (Worst Case)\nΘ(𝑛)\nFind Parent\nΘ(𝑛)\nFind Child\nΘ(1)\nSpace Required (Best Case)\nΘ(𝑛)\nSpace Required (Worst Case)\nΘ(𝑛)\nInsert Key\nleft right nullptr)The best case for insert happens when the root either has no left child or no right child (e.g., if or is — when this\nhappens, we can just insert the new node as a child of the root in constant time. The worst case of insert is still if we get unlucky whenΘ(𝑛)\nsearching for a open spot and end up looking at every node before we find one with no left or right child.\nRemove Key\nRemoving a key is still in the worst case: this happens when we try to remove an element that does not exist (we would need to look atΘ(𝑛)\nevery element before we know that the element is not in the tree), or if we get unlucky and the element we want to remove is among the last ones\nwe visit.\nFind Parent and Child\nIf each node does not store a parent pointer, the worst-case time complexity of finding the parent of any given node is Θ(𝑛); this is because we\nwill have to visit every node and check if any of its children match the node we are given. However, finding the child of a node can be done in\nconstant time because each node stores pointers to its children.\nSpace Complexity\nUnlike the array-based approach, the space complexity required to build a pointer-based tree with 𝑛nodes is always Θ(𝑛). This is because\nmemory is only allocated for nodes when they are created — there is no need to store additional space for nodes that do not exist, which we\nneeded for an array-based tree.\nparent NodeAn alternative would be to store a pointer within each node, as shown in the definition below. If the node is the root, the parent\nnullptr;pointer is otherwise, it points to the node’s parent. Using this approach, the time complexity of finding a node’s parent would be\ninstead of Θ(𝑛), since we have direct access to the parent.Θ(1)\n1\ntemplate <typename T>\n2\nstruct Node {\n3\nT val;\n// value of type T\n4\nNode* parent;\n// pointer to parent, nullptr if root\n5\nNode* left;\n// pointer to left child, nullptr if no left child\n6\nNode* right;\n// pointer to right child, nullptr if no right child\n7\n};\nparentHowever, this implementation is rarely used because it is memory intensive, and the benefit of a isn’t often worth the extra memory.\nparent parentUsually, we do not need a pointer, since the behavior of a pointer can be emulated using recursion (i.e., when a recursive call\nunrolls, we automatically move from a child to its parent; there’s no need to store an explicit pointer).", "word_count": 683, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3e365053-8101-5680-ba17-31487342a9a4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 604, "real_page_number": null, "text": "592\nChapter 18. Trees\n¸ 18.2.3\nConverting a Generic Tree into a Binary Tree\n𝑇′If you are given a generic tree 𝑇, where each node can have more than two children, you can turn this generic tree into a tree using thebinary\nfollowing procedure (starting from the root):\n𝑇′.1. If a node 𝑣has 𝑘children 𝑣1, 𝑣2, 𝑣3, …, 𝑣𝑘, first set as the left child of 𝑣in𝑣1\n𝑇′.2. Then, set 𝑣2, 𝑣3, …, 𝑣𝑘so that they become a chain of right children of in𝑣1\n3. Repeat these two steps recursively for the remaining nodes in the tree.\nFor instance, suppose we are given the following generic tree, and we want to convert it into a binary tree:\nA\nD\nC\nH\nG\nB\nF\nK\nJ\nI\nE\nUsing the algorithm discussed previously, we first consider the root node A. Node A has three children: B, C, and D, so we set B as the left child\nof A, and then set C and D as a chain of right children of B:\nA\nB\nC\nD\nWe will repeat these steps recursively. Next, we look at B, which has two children: E and F. We set E as the left child of B, and set F as the right\nchild of E:\nA\nB\nC\nD\nE\nF\nNode E has no children, so we do not have to do any additional work for E. Node F, however, has three children: I, J, and K. We will set node I\nas the left child of node F, and set nodes J and K as a chain of right children of node I.\nA\nB\nC\nD\nE\nF\nI\nJ\nK\nNode C has two children: G and H. We will set G as the left child of C, and H as the right child of G:\nA\nB\nC\nD\nG\nH\nE\nF\nI\nJ\nK", "word_count": 325, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f36b1f0e-e702-5d98-9394-8a6ad7bbfd65", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 605, "real_page_number": null, "text": "18.2 Binary Trees\n593\nThe remaining nodes (D, G, H, I, J, K) do not have any children, so we are done. We have successfully converted our original generic tree into a\nbinary tree. If you paid close attention, you may have noticed that this is very similar to a pairing heap, where the left child of a node represents\nits child, and the right child of a node represents its sibling. The original tree is on the left, and the binary tree we created is on the right (where\nthe right children are displayed horizontally to highlight the similarities with a pairing heap). Even though a pairing heap can have more than\ntwo children, we still used the two-pointer structure of a binary tree to represent it!\nA\nD\nC\nH\nG\nB\nF\nK\nJ\nI\nE\nA\nD\nC\nH\nG\nB\nF\nK\nJ\nI\nE\nExample18.3 You are given a generic tree, where each node can have more than 2 children. Each node in the generic tree is represented as:\n1\ntemplate <typename T>\n2\nstruct GenericNode {\n3\nT val;\n4\nstd::vector<GenericNode*> children;\n// empty if no children\n5\nGenericNode(T x) : val{ x } {}\n6\n};\nWrite a function that takes in a generic tree and constructs a tree with the same values as the generic tree. Return the root of thebinary\nnewly constructed binary tree. Each node of a binary tree is represented as follows:\n1\ntemplate <typename T>\n2\nstruct BinaryNode {\n3\nT val;\n4\nBinaryNode* left;\n5\nBinaryNode* right;\n6\nnullptr nullptrBinaryNode(T x) : val{ x }, left{ }, right{ } {}\n7\n};\nTo solve this problem, we will use the algorithm described earlier. First, we will look at the children of the root. If the root has no children, we\ncan just create a copy of the node and attach it to the binary tree we are trying to construct. However, if the root does have children, we have to\n(1) set the first child as the root’s left child and (2) set the remaining children as a chain of right children of the first child.\n1\ntemplate <typename T>\n2\nBinaryNode<T>* convert_generic_to_binary(GenericNode<T> *generic_tree) {\n3\nnewBinaryNode<T>* binary_tree = BinaryNode<T>{ generic_tree->val };\n4\nreturn helper(generic_tree, binary_tree);\n5\n} // convert_generic_to_binary()\n6\n7\ntemplate <typename T>\n8\nBinaryNode<T>* helper(GenericNode<T>* generic_node, BinaryNode<T>* binary_node) {\n9\nif (generic_node->children.empty()) {\n10\nreturn binary_node;\n11\n} // if\n12\nGenericNode<T>* next_generic = generic_node->children[0];\n13\nnewBinaryNode<T>* next_binary = BinaryNode<T>{ next_generic->val };\n14\nbinary_node->left = helper(next_generic, next_binary);\n15\nBinaryNode<T>* current = next_binary;\n16\nfor (size_t i = 1; i < generic_node->children.size(); ++i) {\n17\ncurrent->right = helper(generic_node->children[i],\n18\nnew BinaryNode<T>{ generic_node->children[i]->val });\n19\ncurrent = current->right;\n20\n} // for\n21\nreturn binary_node;\n22\n} // helper()\nThe helper function defined on line 8 takes in a node from the generic tree and its counterpart value in a binary tree, and it recursively constructs\nthe children of the binary node based on the children of the generic node. If a generic node has no children (base case), we can attach its\ncorresponding binary node to the binary tree we are building (this is handled by the recursive call; when we finish processing a node, it gets\nreturned and is thus attached to its parent when the recursive call finishes on line 14). Otherwise, we create a binary node for the first child of\n(next_binarythe generic node on line 13), recursively attach the children of this generic node, and then attach this newly constructed binary\nnode as the left child of the binary node on line 14. Then, on lines 15-20, we set the remaining children of the generic node as a chain of right\nnext_binary.children of the new binary node The completed binary node is then returned, and it is attached as the left child of its parent as\nthe recursion unrolls.", "word_count": 656, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8b23c0ab-13f0-5a3c-a0eb-e85593401412", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 606, "real_page_number": null, "text": "594\nChapter 18. Trees\n18.3\nTree Traversals\nIf you want to process every node in a tree, you will have to complete a tree traversal: a systematic method for processing every node in a tree.\nIn this section, we will look at four different tree traversal methods: the preorder, inorder, postorder, and level-order traversals, each of which\nvisit the nodes of a tree in a different order. The first three of these traversals (preorder, inorder, postorder) are recursive, while the level-order\ntraversal is iterative. The names of these traversals actually provide insight into their behavior: the prefix of the traversal name (\"pre\", \"in\", and\n\"post\") tells us when to process the parent node (i.e., the node you are currently visiting) relative to its children. In a preorder traversal, the\nparent node is processed its left and right children; in a postorder traversal, the parent node is processed its left and right children;before after\nand in an inorder traversal, the parent node is processed its left and right children.in between\n¸ 18.3.1\nPreorder Traversal\nTo conduct a preorder traversal, you would complete the following steps, in this order:\n1. process the parent node (the node the recursion is currently on)\n2. recursively process the left subtree\n3. recursively process the right subtree\nThe code for a preorder traversal is shown below:\n1\nvoid preorder(Node* root) {\n2\nif return;(!root)\n3\nprocess(root->val);\n// process the current node\n4\npreorder(root->left);\n// recurse into left child\n5\npreorder(root->right);\n// recurse into right child\n6\n} // preorder()\nprocess()Note that the function in the code above is just a placeholder for the work that is done on each node. For example, to print out the\nprocess()nodes in a preorder traversal, you would replace with a print statement:\n1\nvoid preorder(Node *root) {\n2\nif return;(!root)\n3\nstd::cout << root->val << '\\n';\n4\npreorder(root->left);\n5\npreorder(root->right);\n6\n} // preorder()\nExample 18.4 What is the preorder traversal of the following tree? That is, if you were to conduct a preorder traversal of the tree, in what\norder would you process the nodes?\n7\n2\n6\n3\n5\n4\n1\n8\n9\nIn a preorder traversal, we will always process a node before we recursively process its left and right children. There are three steps we have to\ncomplete at each node: process its value, recursively visit its left child, and then recursively visit its right child. To illustrate this process, we\nwill label each node with three letters — 𝑃, 𝐿, and 𝑅— that represent each of these three steps. We will mark each letter as \"completed\" as\nsoon as we finish each step (i.e., we will mark the 𝑃step of a node as completed after we finish processing its value, we will mark the 𝐿step of\na node as completed as soon as we finish processing its left subtree, and we will mark the 𝑅step of a node as completed as soon as we finish\nprocessing its right subtree). In a preorder traversal, 𝑃must be completed before 𝐿, and 𝐿must be completed before 𝑅.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R", "word_count": 551, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "21c31db5-4e01-5de4-babb-b9b69843f4b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 607, "real_page_number": null, "text": "18.3 Tree Traversals\n595\nWe start at the root (7), which is our current stack frame. The first step is to process the data of the current node, so the first element in our\npreorder traversal is 7. In fact, the first element of a preorder traversal is always the root, since a node is always processed before its children.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7\nstack frames over time\nPreorder traversal: 7\nAfter processing the value of the root node, we mark its 𝑃step as complete. The next step is to recursively process the left child (𝐿), so we\nmake a recursive call on node 4, which becomes the value of our current stack frame.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\nstack frames over time\nPreorder traversal: 7\nFirst, we will process the value of our current node, so 4 is the next value in our preorder traversal. We then mark the 𝑃step of node 4 as\ncompleted and make a recursive call on the left child of 4 (node 9), which becomes the value of our current stack frame.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\nstack frames over time\nPreorder traversal: 7, 4\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\nstack frames over time\nPreorder traversal: 7, 4\nWe process the value of our current node, so 9 is next in our preorder traversal. Since 9 has no children, the recursive calls on its left and right\nchildren can be completed trivially, and we can mark the 𝐿and 𝑅steps of node 9 as complete. Since all the work for node 9 is finished, the\nrecursive call unrolls, and we return to the stack frame of node 4. The 𝐿step of node 4 is now complete.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\nstack frames over time\nPreorder traversal: 7, 4, 9\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\nstack frames over time\nPreorder traversal: 7, 4, 9", "word_count": 523, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "67ae00d7-b9a1-576d-a832-f71d37ff1a44", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 608, "real_page_number": null, "text": "596\nChapter 18. Trees\nSince the recursive call on the left child is done, the next step is to make a recursive call on the right child of node 4, which is node 1. Node 1\nthus becomes the value of our current stack frame.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\nstack frames over time\nPreorder traversal: 7, 4, 9\nWe first process the value of our current node, so 1 is next in our preorder traversal. We then make a recursive call on 1’s left child, or node 8.\nNode 8 then becomes the node on our current stack frame.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\nstack frames over time\nPreorder traversal: 7, 4, 9, 1\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\nstack frames over time\nPreorder traversal: 7, 4, 9, 1\nThe value of node 8 is processed first, so 8 is next in our preorder traversal. Since node 8 has no left or right children, we do not need to do any\nmore work for this node. The recursion unrolls, and we return to the stack frame of node 1.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8\nNow, we will make a recursive call on the right child of node 1. However, node 1 has no right child, so this step can be done trivially. All three\nsteps for node 1 are now complete, so the recursion unrolls, and we return to the stack frame of node 4.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8\nAll three steps for node 4 are done, so the recursion unrolls, and we return to the stack frame of node 7. Since the entire left subtree of node 7\nhas been completely processed, we can mark the 𝐿step of the root node as complete.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8", "word_count": 606, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9bb7a995-7ba3-5f70-b98d-f3824c9ce396", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 609, "real_page_number": null, "text": "18.3 Tree Traversals\n597\nOur next step is to make a recursive call on the right child of 7, or node 2.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8\nWe then process the value of node 2, so 2 is next in our preorder traversal. Since 2 has no left child, the recursive call on its left child can be\ncompleted trivially. We then make a recursive call on the right child of 2, or node 6. Node 6 now becomes the node on our current stack frame.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2\nWe now process the value of node 6 and add it to our preorder traversal. Then, we make a recursive call on the left child of node 6, or node 5.\nNode 5 now becomes the node on our current stack frame.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2, 6\nWe process the value of node 5 by adding it to our preorder traversal. Since 5 has no left or right children, no more work needs to be done on\nthis node. The recursion unrolls, and we return to the stack frame of node 6.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2, 6, 5\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2, 6, 5\nNow, we will make a recursive call on the right child of node 6, or node 3.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2, 6, 5", "word_count": 658, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "85ace4e0-3126-5459-98b0-8eda43c449c5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 610, "real_page_number": null, "text": "598\nChapter 18. Trees\nWe process the value of node 3 by adding it to our preorder traversal. Since 3 has no left or right children, no more work needs to be done on\nthis node. The recursion unrolls, and we return to the stack frame of node 6.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2, 6, 5, 3\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2, 6, 5, 3\nAll three steps for node 6 have been completed, so the recursion unrolls, and we return to the stack frame of node 2.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\n7\n2\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2, 6, 5, 3\nAll three steps for node 2 have also been completed, so the recursion unrolls, and we return to the stack frame of node 7.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\nP L R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\n7\n2\n7\nstack frames over time\nPreorder traversal: 7, 4, 9, 1, 8, 2, 6, 5, 3\nThe function returns, and the traversal is complete. The preorder traversal of this tree is: 7, 4, 9, 1, 8, 2, 6, 5, 3.\n¸ 18.3.2\nInorder Traversal\nTo conduct an inorder traversal, you would complete the following steps, in this order:\n1. recursively process the left subtree\n2. process the parent node (the node the recursion is currently on)\n3. recursively process the right subtree\nThe code for an inorder traversal is shown below:\n1\nvoid inorder(Node* root) {\n2\nif return;(!root)\n3\ninorder(root->left);\n// recurse into left child\n4\nprocess(root->val);\n// process the current node\n5\ninorder(root->right);\n// recurse into right child\n6\n} // inorder()", "word_count": 571, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a5e96418-e7e7-532f-b75a-f20e96232aa7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 611, "real_page_number": null, "text": "18.3 Tree Traversals\n599\nExample 18.5 What is the inorder traversal of the following tree? That is, if you were to conduct a inorder traversal of the tree, in what\norder would you process the nodes?\n7\n2\n6\n3\n5\n4\n1\n8\n9\nWe will use the same procedure as before, but this time we will recursively process a node’s left subtree before we process the node’s value\nitself. In other words, 𝐿must be completed before 𝑃, and 𝑃must be completed before 𝑅.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nOnce again, we will start at the root. The first step is to recursively process the left subtree of the root, so we make a recursive call on node 4\n(and 4 becomes the node of our current stack frame).\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7\nstack frames over time\nInorder traversal:\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\nstack frames over time\nInorder traversal:\nOur current node is node 4. However, before we can add 4 to the inorder traversal, we have to recursively process its left child. Thus, we make a\nrecursive call on 4’s left child, or 9. Node 9 now becomes the node on our current stack frame.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\nstack frames over time\nInorder traversal:", "word_count": 344, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7712a61d-5445-55b6-9f31-05233f21a8c9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 612, "real_page_number": null, "text": "600\nChapter 18. Trees\nWe are now at node 9. We must recursively process 9’s left child, then add 9 to our inorder traversal, then recursively process 9’s right child.\nSince 9 has no left or right children, the recursive calls can be completed trivially, and 9 is added to our inorder traversal. The recursion unrolls,\nand we return to the stack frame of node 4.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\nstack frames over time\nInorder traversal: 9\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\nstack frames over time\nInorder traversal: 9\nSince the left subtree of node 4 has been completely processed, we can add 4 to our inorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\nstack frames over time\nInorder traversal: 9, 4\nWe then make a recursive call to 4’s right child, or node 1.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\nstack frames over time\nInorder traversal: 9, 4\nOur current node is now node 1. Before we can add 1 to our traversal, we must make a recursive call on 1’s left child, or node 8. Since 8 has no\nleft child, the 𝐿step can be completed trivially, and 8 gets added to the inorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\nstack frames over time\nInorder traversal: 9, 4\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\nstack frames over time\nInorder traversal: 9, 4, 8", "word_count": 472, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3843f18c-a832-53e5-ba0c-daed9e715ed1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 613, "real_page_number": null, "text": "18.3 Tree Traversals\n601\nThe recursion unrolls, and we return to node 1. Since the left subtree of 1 has been fully processed, we can add 1 to the inorder traversal. Then,\nwe make a recursive call to 1’s right child (which can be done trivially since 1 has no right child).\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\nstack frames over time\nInorder traversal: 9, 4, 8, 1\nAll three steps for node 1 are complete, so the recursion unrolls to node 4.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\nstack frames over time\nInorder traversal: 9, 4, 8, 1\nAll three steps for node 4 are complete, so the recursion unrolls to node 7. Since the entire left subtree of 7 has been processed, we can now add\n7 to the inorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7\nNow, we will make a recursive call on 7’s right child, or node 2.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7\nWe first make a recursive call on 2’s left child (which can be done trivially since 2 has no left child). Then, we can add 2 to our inorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7, 2\nNext, we make a recursive call on 2’s right child, or node 6.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7, 2", "word_count": 571, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f185c471-1647-5ade-ad61-3f0f7b2edfb0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 614, "real_page_number": null, "text": "602\nChapter 18. Trees\nWe then make a recursive call on 6’s left child, or node 5. Since 5 has no children, we can add 5 to our inorder traversal, as the 𝐿and 𝑅steps\ncan be completed trivially.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7, 2, 5\nThe recursion unrolls, and we return to node 6. Since the left subtree of node 6 has been fully processed, we can add 6 to our inorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7, 2, 5, 6\nWe then make a recursive call on 6’s right child, or node 3. Since 3 has no children, we can add 3 to our inorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7, 2, 5, 6, 3\nThe recursion unrolls, and we return to node 6. All the steps for node 6 are complete, so the recursion unrolls again to node 2. The steps for\nnode 2 are also complete, so we return to the root node. Since all three steps for the root are complete, the function returns, and the traversal is\ncomplete. The inorder traversal of this tree is: 9, 4, 8, 1, 7, 2, 5, 6, 3.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7, 2, 5, 6, 3\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\n7\n2\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7, 2, 5, 6, 3\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\nL P R\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\n7\n2\n7\nstack frames over time\nInorder traversal: 9, 4, 8, 1, 7, 2, 5, 6, 3", "word_count": 693, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "736d61f8-03a5-5e49-82c4-2756b31fe9e3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 615, "real_page_number": null, "text": "18.3 Tree Traversals\n603\n¸ 18.3.3\nPostorder Traversal\nTo conduct a postorder traversal, you would complete the following steps, in this order:\n1. recursively process the left subtree\n2. recursively process the right subtree\n3. process the parent node (the node the recursion is currently on)\nThe code for a postorder traversal is shown below:\n1\nvoid postorder(Node* root) {\n2\nif return;(!root)\n3\npostorder(root->left);\n// recurse into left child\n4\npostorder(root->right);\n// recurse into right child\n5\nprocess(root->val);\n// process the current node\n6\n} // postorder()\nExample 18.6 What is the postorder traversal of the following tree? That is, if you were to conduct a postorder traversal of the tree, in\nwhat order would you process the nodes?\n7\n2\n6\n3\n5\n4\n1\n8\n9\nWe will use the same procedure as before, but this time we will process both the left and right subtrees of a node before we add its value to the\npostorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nWe start at the root node. Since this is a postorder traversal, we first make a recursive call on its left child, or node 4.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7\nstack frames over time\nPostorder traversal:\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\nstack frames over time\nPostorder traversal:\nWe then make a recursive call on 4’s left child, or node 9.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\nstack frames over time\nPostorder traversal:", "word_count": 372, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab977f08-d43f-504f-93ca-2a67dce76f6d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 616, "real_page_number": null, "text": "604\nChapter 18. Trees\nNext, we make recursive calls on 9’s left and right children. However, since 9 is a leaf node, these steps can be completed trivially. Since the 𝐿\nand 𝑅steps are done, we can then add 9 to our postorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\nstack frames over time\nPostorder traversal: 9\nThe recursion unrolls, and we return to node 4. Since we have finished processing 4’s left child, we will now make a recursive call to 4’s right\nchild, or node 1.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\nstack frames over time\nPostorder traversal: 9\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\nstack frames over time\nPostorder traversal: 9\nNow, we will make a recursive call to 1’s left child, or node 8. Node 8 has no children, so its 𝐿and 𝑅steps can be completed trivially. 8 is then\nadded to our postorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\nstack frames over time\nPostorder traversal: 9, 8\nThe recursion unrolls, and we return to the stack frame of node 1.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\nstack frames over time\nPostorder traversal: 9, 8\nNext, we make a recursive call on 1’s right child. However, 1 has no right child, so this step can be completed trivially. We can now add 1 to the\npostorder traversal, and the recursion unrolls back to node 4.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\nstack frames over time\nPostorder traversal: 9, 8, 1\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\nstack frames over time\nPostorder traversal: 9, 8, 1", "word_count": 566, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7a5a622f-7988-548b-9cb0-d1b6642e625c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 617, "real_page_number": null, "text": "18.3 Tree Traversals\n605\nWe can now add 4 to the postorder traversal. The recursion then unrolls back to the root.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\nstack frames over time\nPostorder traversal: 9, 8, 1, 4\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7\nstack frames over time\nPostorder traversal: 9, 8, 1, 4\nNow, we make a recursive call to the right child of 7, or node 2.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\nstack frames over time\nPostorder traversal: 9, 8, 1, 4\nWe then make a recursive call to 2’s left child. Since 2 has no left child, this step can be completed trivially. We follow with a recursive call to\n2’s right child, or node 6.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\nstack frames over time\nPostorder traversal: 9, 8, 1, 4\nNow, we make a recursive call on the left child of node 6, or node 5. Since 5 has no left or right children, we can immediately add it to our\npostorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\nstack frames over time\nPostorder traversal: 9, 8, 1, 4, 5\nThe recursion unrolls, and we return to node 6. Next, we will make a recursive call to 6’s right child, or 3. Since 3 has no left or right children,\nwe can immediately add it to our postorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\nstack frames over time\nPostorder traversal: 9, 8, 1, 4, 5\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\nstack frames over time\nPostorder traversal: 9, 8, 1, 4, 5, 3", "word_count": 659, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "18a379b3-67fb-52d2-ba4d-620de28da4ad", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 618, "real_page_number": null, "text": "606\nChapter 18. Trees\nThe recursion unrolls again, and we return to node 6. Since the left and right children of 6 have both been processed, we can add 6 to our\npostorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\nstack frames over time\nPostorder traversal: 9, 8, 1, 4, 5, 3, 6\nThe work for node 6 is done, so the recursion unrolls to node 2. Similarly, because the left and right children of 2 have both been processed, we\ncan add 2 to our postorder traversal.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\n7\n2\nstack frames over time\nPostorder traversal: 9, 8, 1, 4, 5, 3, 6, 2\nThe work for node 2 is done, so the recursion unrolls back to the root. We have finished processing both the left and right subtrees of the root, so\nwe can add 7 to the postorder traversal. The function returns, and the traversal is complete.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\nL R P\n7 7\n4\n7\n4\n9\n7\n4\n7\n4\n1\n7\n4\n1\n8\n7\n4\n1\n7\n4\n7 7\n2\n7\n2\n6\n7\n2\n6\n5\n7\n2\n6\n7\n2\n6\n3\n7\n2\n6\n7\n2\n7\nstack frames over time\nPostorder traversal: 9, 8, 1, 4, 5, 3, 6, 2, 7\nThe postorder traversal of the tree is 9, 8, 1, 4, 5, 3, 6, 2, 7.\nTo summarize, the preorder, inorder, and postorder traversals are three methods you can use to process the nodes in a tree. In a preorder traversal,\nyou first process the value of the current node, then you recursively process its left subtree, then you recursively process its right subtree. In an\ninorder traversal, you first recursively process the current node’s left subtree, then you process the value of the current node, then you recursively\nprocess the current node’s right subtree. In a postorder traversal, you first recursively process the current node’s left subtree, then you recursively\nprocess the current node’s right subtree, then you process the value of the current node itself.\n1\nvoid preorder(Node* p) {\n2\nif return;(!p)\n3\nprocess(p->val);\n4\npreorder(p->left);\n5\npreorder(p->right);\n6\n} // preorder()\n1\nvoid inorder(Node* p) {\n2\nif return;(!p)\n3\ninorder(p->left);\n4\nprocess(p->val);\n5\ninorder(p->right);\n6\n} // inorder()\n1\nvoid postorder(Node* p) {\n2\nif return;(!p)\n3\npostorder(p->left);\n4\npostorder(p->right);\n5\nprocess(p->val);\n6\n} // postorder()\n¸ 18.3.4\nLevel-Order Traversal\nThe level-order traversal is another traversal method that can be used to process the nodes of a tree. In a level-order traversal, nodes are\nprocessed in order of increasing depth, where nodes on the same level (i.e., nodes that have the same depth) are processed from left to right.\nExample 18.7 What is the level-order traversal of the following tree? That is, if you were to conduct a level-order traversal of the tree, in\nwhat order would you process the nodes?\n7\n2\n6\n3\n5\n4\n1\n8\n9\nThe level-order traversal processes nodes in order of increasing depth, from left to right: 7, 4, 2, 9, 1, 6, 8, 5, 3.", "word_count": 690, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c4306cdd-5c12-5729-a256-6c376f66774c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 619, "real_page_number": null, "text": "18.3 Tree Traversals\n607\nUnlike the other three traversal methods, the level-order traversal is iterative rather than recursive. To conduct a level-order traversal, a queue is\nused in place of recursive calls (using an algorithm known as a search, which will be covered in the next chapter). The code for abreadth-first\nlevel-order traversal is shown below:\n1\nvoid level_order(Node* root) {\n2\nif return;(!root)\n3\nstd::queue<Node*> bfs;\n4\nbfs.push(root);\n// push root into queue\n5\nwhile (!bfs.empty()) {\n// while queue not empty\n6\nNode* curr = bfs.front();\n7\nbfs.pop();\n8\nprocess(curr);\n9\n// push current node's left and right children into queue\n10\nif (curr->left) {\n11\nbfs.push(curr->left);\n12\n} // if\n13\nif (curr->right) {\n14\nbfs.push(curr->right);\n15\n} // if\n16\n} // while\n17\n} // level_order()\nIn the code, nodes are pushed into the queue in level order (i.e., nodes closer to the root are pushed in nodes closer to the leaves). Sincebefore\nqueuessupportfirst-in, first-out(FIFO)behavior, weareabletoprocessthenodesinthesameorderthattheyarepushedintothequeue. Consider\nthe tree in the previous example:\n7\n2\n6\n3\n5\n4\n1\n8\n9\nFirst, we push the root into the queue, as shown on line 4:\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n7\nLevel-order traversal:\nThen, as long as the queue is not empty, we would repeat the following:\n1. Take out the node at the front of the queue.\n2. Process the node (in this case, print it to the level-order traversal).\n3. Push the node’s left child into the queue, if one exists.\n4. Push the node’s right child into the queue, if one exists.\nwhileDuring the first iteration of the loop, we take out the node at the front of the queue (node 7), add it to our level-order traversal, and push\n7’s children (nodes 4 and 2) into the queue.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n4\n2\nLevel-order traversal:\n7\nDuring the second iteration, we take out the node at the front of the queue (node 4), add it to our level-order traversal, and push 4’s children\n(nodes 9 and 1) into the queue.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n2\n9\n1\nLevel-order traversal:\n7, 4", "word_count": 393, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "72ccc584-c45f-5a85-a1f4-d666a97426bf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 620, "real_page_number": null, "text": "608\nChapter 18. Trees\nDuring the third iteration, we take out node 2, add it to our level-order traversal, and push 2’s child (node 6) into the queue.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n9\n1\n6\nLevel-order traversal:\n7, 4, 2\nDuring the fourth iteration, we take out node 9 and add it to our level-order traversal. Since 9 has no children, nothing gets pushed into the queue.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n1\n6\nLevel-order traversal:\n7, 4, 2, 9\nDuring the fifth iteration, we take out node 1, add it to our level-order traversal, and push 1’s child (node 8) into the queue.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n6\n8\nLevel-order traversal:\n7, 4, 2, 9, 1\nDuring the sixth iteration, we take out node 6, add it to our level-order traversal, and push 6’s children (5 and 3) into the queue.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n8\n5\n3\nLevel-order traversal:\n7, 4, 2, 9, 1, 6\nDuring the seventh iteration, we take out node 8 and add it to our level-order traversal. 8 has no children, so nothing gets pushed into the queue.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n5\n3\nLevel-order traversal:\n7, 4, 2, 9, 1, 6, 8\nDuring the eighth iteration, we take out node 5 and add it to our level-order traversal. 5 has no children, so nothing gets pushed into the queue.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\n3\nLevel-order traversal:\n7, 4, 2, 9, 1, 6, 8, 5\nDuring the ninth iteration, we take out node 3 and add it to our level-order traversal. 3 has no children, so nothing gets pushed into the queue.\nAfter this iteration, the queue is empty, and our level-order traversal is complete.\n7\n2\n6\n3\n5\n4\n1\n8\n9\nQueue:\nLevel-order traversal:\n7, 4, 2, 9, 1, 6, 8, 5, 3", "word_count": 337, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "75436f59-af50-5d10-bf6a-afd9bfcb805d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 621, "real_page_number": null, "text": "18.3 Tree Traversals\n609\nLevel-order traversals are often useful for solving tree problems that require you to obtain information about each level of a tree. However, some\nof these problems may require you to keep track of which nodes belong to each level (e.g., how does your algorithm know that node 2 is on the\nsame level as node 4, but not node 9?).\nIt turns out that we can use the size of the queue to obtain this information. Immediately after we finish processing all the nodes in a level,\nthe size of the queue represents tree. For example, we start with the root, 7. We know that node 7 isthe number of nodes in the next level of the\nthe only node in its level, since it is the root. Thus, when we finish processing 7, the size of the queue represents the number of nodes on the\nsecond level (in this case, the size is 2, since nodes 4 and 2 are in the queue). Similarly, once we finish processing the second level (nodes 4 and\n2), the size of the queue is 3 (nodes 9, 1, and 6). Thus, there must exist 3 nodes in the third level of the tree. As a result, if you are asked to solve\na problem that requires you to distinguish between nodes at different levels of the tree, you can use the size of the queue to help you process the\ntree one level at a time. One example of such a problem is provided below:\nExample 18.8 You are given the root of a binary tree whose nodes have the following structure:\n1\nstruct Node {\n2\nint32_t val;\n3\nNode* left;\n4\nNode* right;\n5\n};\nWrite a function that finds the of the binary tree. In other words, find the level of the tree that has the largest cumulativemaximum level sum\nsum and return this value.\nExample: Suppose you are given the following binary tree.\n3\n6\n5\n0\n1\n-7\n-2\n-1\n9\nThe first level has a sum of 3, the second level has a sum of -2 + 6 = 4, the third level has a sum of -1 + 1 + 5 = 5, and the fourth level has a\nsum of 9 - 7 + 0 = 2. Since 5 is the largest level sum in this tree, you would return 5.\nTo solve this problem, we will need to sum up all the nodes at each level and keep track of the largest level sum we’ve encountered so far. A\nlevel-order traversal would be ideal for this problem, but our algorithm will need to process each level one at a time. As a result, we can use the\nsize of our traversal queue to determine how many nodes we need to sum at each level.\nwhileTo do this, we will add a slight adjustment to the original level order traversal so that each iteration of the loop processes an entire\nof the tree rather than a single node. This can be done by calculating the size of the queue at the beginning of the while loop (which welevel\nwill denote as 𝑛), and then running an inner loop that processes 𝑛nodes at a time. We add up all the nodes in this loop, and if the result is larger\nthan the largest sum we’ve encountered so far, we update this largest sum. After the entire tree is processed, we return this value.\nThe code is shown below:\n1\nint32_t max_level_sum(Node* root) {\n2\nint32_t std::numeric_limits<int32_t>::min();curr_max =\n// smallest int\n3\nstd::queue<Node*> bfs;\n4\nbfs.push(root);\n5\nwhile (!bfs.empty()) {\n6\nsize_t level_size = bfs.size();\n// number of nodes in next level\n7\nint32_t level_sum = 0;\n8\n// loop through all the nodes in the level\n9\nfor (size_t i = 0; i < level_size; ++i) {\n10\nNode* curr = bfs.front();\n11\nbfs.pop();\n12\nlevel_sum += curr->val;\n// add element to level sum\n13\nif (curr->left) {\n14\nbfs.push(curr->left);\n// push left child if exists\n15\n} // if\n16\nif (curr->right) {\n17\nbfs.push(curr->right);\n// push right child if exists\n18\n} // if\n19\n} // for i\n20\n// if level_sum is larger than largest level sum we've seen, update\n21\nif (level_sum > curr_max) {\n22\ncurr_max = level_sum;\n23\n} // if\n24\n} // while\n25\nreturn curr_max;\n26\n} // max_level_sum()", "word_count": 743, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c5a3ff8f-4765-556d-aed3-c6f6b0743c86", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 622, "real_page_number": null, "text": "610\nChapter 18. Trees\n18.4\nSolving Problems Using Trees\nBecause trees are recursive structures, we can use recursion to elegantly solve many types of tree problems. A common approach toward solving\ntree problems is to use a method to traverse the tree in a certain manner. In this section, we will focus on problems that can be solvedtraversal\nusing a preorder or postorder traversal. In general, many of these tree problems can be solved using the following four-step approach:\n• Identify the base case of the problem. The base case should run if the input allows you to solve the problem trivially.\n• Process the left subtree recursively (this solves the problem for the subtree rooted at the left child).\n• Process the right subtree recursively (this solves the problem for the subtree rooted at the right child).\n• Complete the work needed to solve the problem for the subtree rooted at the current node (e.g., combine the results from the left and\nright subtrees to obtain the solution for the entire tree).\nThe ordering of bullets 2, 3, and 4 above depends on the problem you are trying to solve. If you need to operate on a node before you can operate\non its children, bullet point 4 would go before bullet points 2 and 3, and the overall approach would be similar to a preorder traversal. On the\nother hand, if you need to operate on a node’s children before you can operate on the node itself, bullet point 4 would go after bullet points 2 and\n3, and the overall approach would be similar to a postorder traversal. We will look at a few examples below.\nExample 18.9 You are given a binary tree, where each node is represented as follows:\n1\nstruct Node {\n2\nint32_t val;\n3\nNode* left;\n4\nNode* right;\n5\n};\nWrite a function that returns the maximum height of the binary tree. In other words, return the number of nodes in the longest path from the\nroot node to the farthest leaf node. The function header is:\nint32_t find_max_height(Node* root);\nExample: Given the following binary tree, you would return 3, since the root node has a height of 3.\n15\n17\n19\n14\n12\nWhen you are presented with a tree problem, it is beneficial to think recursively. Given a node, your function should return the maximum height\nof the binary tree that is rooted at that node. Would knowing the maximum heights of the node’s left and right children help you solve this\nproblem? If so, we can use the steps above to solve the problem recursively.\nIn this case, knowing the maximum heights of a node’s children does help us find the maximum height of the node itself. If a node’s\nleft child has a height of 𝐿, and its right child has a height of 𝑅, the height of the node itself must be max(𝐿, 𝑅) + 1. Knowing this, we can\nimplement a solution to this problem using a postorder traversal.\nStep 1. Identify the base case of the problem.\n(root == nullptr).In this problem, the base case runs if we are given an empty tree This is because we can find the height of an empty tree\ntrivially — an empty tree must have a height of 0.\n1\nint32_t find_max_height(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, height must be 0\n5\n} // if\n6\n...\n7\n} // find_max_height()\nSteps 2 and 3. Process the left and right subtrees recursively.\nNow, we want to retrieve the maximum heights of the left and right subtrees. Since trees are recursive structures, we can accomplish this by\nfind_max_height()recursively calling on the left and right children (lines 7 and 8). This will allow us to obtain the maximum heights of\nthe left and right children:\n1\nint32_t find_max_height(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, height must be 0\n5\n} // if\n6\n// Steps 2 and 3: Recursively process the left and right children\n7\nint32_t height_left_child = find_max_height(root->left);\n8\nint32_t height_right_child = find_max_height(root->right);\n9\n...\n10\n} // find_max_height()", "word_count": 730, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bd4c392b-bcff-5672-adc7-a4c0f28ded12", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 623, "real_page_number": null, "text": "18.4 Solving Problems Using Trees\n611\nStep 4. Complete the work needed to solve the problem for the subtree rooted at the current node.\nNow that we’ve obtained the maximum height of the left and right subtrees, we have to combine these results to calculate the maximum height\nof the node we are currently processing. In this problem, we know that the height of a node is equal to one plus the largest height of any of its\nchildren. We can implement this as follows:\n1\nint32_t find_max_height(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, height must be 0\n5\n} // if\n6\n// Steps 2 and 3: Recursively process the left and right children\n7\nint32_t height_left_child = find_max_height(root->left);\n8\nint32_t height_right_child = find_max_height(root->right);\n9\n// Step 4: Combine the results from the left and right subtrees\n10\nreturn 1 + max(height_left_child, height_right_child);\n11\n} // find_max_height()\nOur function is now complete.\nExample 18.10 You are given a binary tree, where each node is represented as follows:\n1\nstruct Node {\n2\nint32_t val;\n3\nNode* left;\n4\nNode* right;\n5\n};\nWrite a function that returns the depth of any leaf node in the binary tree. In other words, return the depth of the shallowest leafminimum\nnode in the tree (where the root has a depth of 1). The function header is:\nint32_t find_min_depth(Node* root);\nExample: Given the following binary tree, you would return 2, since the minimum depth of any leaf node is 2 (for the node with value 12).\n15\n17\n19\n14\n12\nOnce again, this problem can be solved recursively. For any node in the tree, if we know the minimum depth of any node in its left and right\nsubtrees, we can use this information to calculate the minimum depth of the node itself. Thus, we will follow the steps of a postorder traversal to\nsolve this problem.\nStep 1. Identify the base case of the problem.\n(root == nullptr).In this problem, the base case runs if we are given an empty tree This is because we can find the minimum depth of an\nempty tree trivially — an empty tree must have a minimum depth of 0.\n1\nint32_t find_min_depth(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, min depth must be 0\n5\n} // if\n6\n...\n7\n} // find_min_depth()\nSteps 2 and 3. Process the left and right subtrees recursively.\nNow, we want to retrieve the minimum leaf depths of the left and right subtrees. Since trees are recursive structures, we can accomplish this by\nfind_min_depth()recursively calling on the left and right children (lines 7 and 8). This will allow us to obtain the minimum leaf depths\nof the subtrees rooted at each child:\n1\nint32_t find_min_depth(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, min depth must be 0\n5\n} // if\n6\n// Steps 2 and 3: Recursively process the left and right children\n7\nint32_t depth_left_child = find_min_depth(root->left);\n8\nint32_t depth_right_child = find_min_depth(root->right);\n9\n...\n10\n} // find_min_depth()", "word_count": 570, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "237f763b-0768-5f1e-b7b3-92ee7d855a1e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 624, "real_page_number": null, "text": "612\nChapter 18. Trees\nStep 4. Complete the work needed to solve the problem for the subtree rooted at the current node.\nNow that we’ve obtained the results for the left and right children, we have to combine them to find the minimum leaf depth of the subtree rooted\nat the node we are currently processing. Intuitively, if a node’s left child has a minimum leaf depth of 𝐿, and its right child has a minimum leaf\ndepth of 𝑅, the minimum leaf depth of the tree rooted at the node itself should be min(𝐿, 𝑅) + 1. However, consider the following tree:\n15\n17\n19\n14\nnullptr),Here, the left child of the root has a minimum depth of 0 (since it is a and the right child of the root has a minimum depth of 2.\nHowever, if we just took the minimum of 0 and 2 and added 1, we would end up concluding that the root node has a minimum depth of 0 + 1 =\n1. This would be incorrect! Instead, if either 𝐿or 𝑅is equal to zero, we want to add one to the of 𝐿and 𝑅so that we aren’t includingmaximum\nnullptr in our calculations. The code is shown below:\n1\nint32_t find_min_depth(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, min depth must be 0\n5\n} // if\n6\n// Steps 2 and 3: Recursively process the left and right children\n7\nint32_t depth_left_child = find_min_depth(root->left);\n8\nint32_t depth_right_child = find_min_depth(root->right);\n9\n// Step 4: Combine the results from the left and right subtrees\n10\nif (depth_left_child == 0 || depth_right_child == 0) {\n11\nreturn 1 + max(depth_left_child, depth_right_child);\n12\n} // if\n13\nreturn 1 + min(depth_left_child, depth_right_child);\n14\n} // find_min_depth()\nWe can also write lines 10-13 in one line using the ternary/conditional operator:\nreturn 1 + (min(depth_left_child, depth_right_child) ?\nmin(depth_left_child, depth_right_child) :\nmax(depth_left_child, depth_right_child));\nOur function is now complete.\nExample 18.11 You are given a binary tree, where each node is represented as follows:\n1\nstruct Node {\n2\nint32_t val;\n3\nNode* left;\n4\nNode* right;\n5\n};\nWrite a function that returns the sum of all the left leaves in a given binary tree. The function header is:\nint32_t left_leaf_sum(Node* root);\nExample: Given the following binary tree, you would return 26, since that is the sum of all left leaves (12 + 14).\n15\n17\n19\n14\n12\nLike before, this problem can also be solved recursively. For any node in the tree, if we know the total left leaf sum of the subtrees rooted at the\nleft and right children, we can use this information to calculate the total left leaf sum of the tree rooted at the node itself. Thus, we will follow\nthe steps of a postorder traversal to solve this problem.\nStep 1. Identify the base case of the problem.\n(root == nullptr).In this problem, the base case runs if we are given an empty tree This is because the left leaf sum of an empty tree is\ntrivially 0.\n1\nint32_t left_leaf_sum(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, left leaf sum must be 0\n5\n} // if\n6\n...\n7\n} // left_leaf_sum()", "word_count": 577, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8cc0287c-0893-5920-b47c-3e3474aab14e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 625, "real_page_number": null, "text": "18.4 Solving Problems Using Trees\n613\nSteps 2 and 3. Process the left and right subtrees recursively.\nNow, we want to retrieve the left leaf sums of the left and right subtrees. Since trees are recursive structures, we can accomplish this by\nleft_leaf_sum()recursively calling on the left and right children (lines 7 and 8). This will allow us to obtain the left leaf sums of the left\nand right children:\n1\nint32_t left_leaf_sum(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, left leaf sum must be 0\n5\n} // if\n6\n// Steps 2 and 3: Recursively process the left and right children\n7\nint32_t sum_left_child = left_leaf_sum(root->left);\n8\nint32_t sum_right_child = left_leaf_sum(root->right);\n9\n...\n10\n} // left_leaf_sum()\nStep 4. Complete the work needed to solve the problem for the subtree rooted at the current node.\nNow we have to combine these results to calculate the left leaf sum of the node we are currently processing. There are two cases that can happen.\nIf the node’s left child is a leaf node, we would combine the value of the left child with the left leaf sum of the right subtree. If the node’s left\nchild is not a leaf, we would combine the left leaf sum of the left subtree with the left leaf sum of the right subtree.\n15\n17\n19\n14\n12\nroot->left->val + left_leaf_sum(root->right)\nrecurse into right\nsubtree to get left\nleaf sum of 14\n15\n17\n19\n14\n16\n18\n12\nleft_leaf_sum(root->left) + left_leaf_sum(root->right)\nrecurse into left\nsubtree to get left\nleaf sum of 12\nrecurse into right\nsubtree to get left\nleaf sum of 14\nThe code from step 4 is shown below:\n1\nint32_t left_leaf_sum(Node* root) {\n2\n// Step 1: Identify the base case (case that can be solved trivially)\n3\nif (!root) {\n4\nreturn 0;\n// if root is nullptr, left leaf sum must be 0\n5\n} // if\n6\n// Steps 2 and 3: Recursively process the left and right children\n7\nint32_t sum_left_child = left_leaf_sum(root->left);\n8\nint32_t sum_right_child = left_leaf_sum(root->right);\n9\n// Step 4: Combine the results from the left and right subtrees\n10\nif (root->left && !root->left->left && !root->left->right) {\n11\nreturn root->left->val + sum_right_child;\n12\n} // if\n13\nreturn sum_left_child + sum_right_child;\n14\n} // left_leaf_sum()\nOur function is now complete. Note that we can further optimize the function above by making a recursive call to the left subtree only when the\nleft child is not a leaf.", "word_count": 436, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "922cf606-40eb-5dd1-9fa5-567b1876105b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 626, "real_page_number": null, "text": "614\nChapter 18. Trees\nExample 18.12 A binary tree is defined to be a (or proper) binary tree if every node either has zero or two children. In other words, forfull\na binary tree to be full, every internal node must have two children. Given a binary tree, write a function that turns this tree into a full binary\ntree by removing all nodes that have only one child.\nExample: Given the following tree:\nA\nC\nE\nB\nD\nG\nH\nF\nyou would remove nodes B, C, and G, since they only have one child. The resulting tree would look like this:\nA\nE\nD\nH\nF\nTo solve this problem, we will have to traverse the binary tree and identify all the nodes that only have one child. However, the method we use\nto traverse the tree is important, since we must recursively process the left and right children of a node before we can delete the node itself.\nThus, processing the tree in a bottom-up fashion should be the way to go, and a postorder traversal should be used.\nOur function can be written as follows. First, we recursively process the left and right children to remove nodes with only one child in these\nsubtrees. Then, we check if the current node has one child. If it does, we delete the current node and attach its child to its parent. Otherwise, we\nignore the node and continue traversing the rest of the tree. The code is shown below:\n1\nNode* remove_single_children(Node* root) {\n2\n// Base case: empty tree\n3\nif (!root) {\n4\nreturn nullptr;\n5\n} // if\n6\n// Recursively process the left and right children\n7\nroot->left = remove_single_children(root->left);\n8\nroot->right = remove_single_children(root->right);\n9\n// If node has 0 or 2 children, no need to do anything (just return node)\n10\nif ((!root->left && !root->right) || (root->left && root->right)) {\n11\nreturn root;\n12\n} // if\n13\n// If node has 1 child, delete node and replace with child\n14\nNode* child = root->left ? root->left : root->right;\n15\ndelete root;\n16\nreturn child;\n17\n} // remove_single_children()\nExample 18.13 Write a function that can be used to invert (mirror) a binary tree.\nExample: Given the following tree on the left, the function would turn it into the tree on the right:\n1\n3\n7\n6\n2\n5\n4\n1\n2\n4\n5\n3\n6\n7", "word_count": 405, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "911a673c-63b3-5645-b672-300755162fbc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 627, "real_page_number": null, "text": "18.5 Constructing Binary Trees from Preorder, Inorder, and Postorder Traversals\n615\nTo solve this problem, we will need to traverse the tree and flip each node’s left and right children. One possible solution can be written as\nfollows. As we traverse through the tree, we first swap the left subtree of the current node with the right subtree. Then, we recursively invert the\nleft and right subtrees of the current node via a preorder traversal. The code is shown below:\n1\nvoid invert_binary_tree(Node* root) {\n2\n// Base case: empty tree\n3\nif (!root) {\n4\nreturn;\n5\n} // if\n6\n// Swap left subtree and right subtree\n7\nstd::swap(root->left, root->right);\n8\n// Recursively invert left and right subtrees\n9\ninvert_binary_tree(root->left);\n10\ninvert_binary_tree(root->right);\n11\n} // invert_binary_tree()\n18.5\nConstructing Binary Trees from Preorder, Inorder, and Postorder Traversals\nIf you are given two traversals of a binary tree: (1) an inorder traversal and (2) either a preorder or postorder traversal, you can use these\ntraversals to construct a unique binary tree. This is because you can use the preorder or postorder traversal to determine the root of each subtree,\nand the inorder traversal to partition the remaining elements into a left and right subtree. We will look at a few examples in this section.\n¸ 18.5.1\nConstructing a Binary Tree from Inorder and Preorder Traversals\nExample 18.14 You are given a binary tree with the following inorder and preorder traversals. Use these traversals to draw out this tree.\nInorder: 5, 7, 1, 4, 3, 6, 2\nPreorder: 3, 4, 7, 5, 1, 2, 6\nFirst, we will use the preorder traversal to determine the root. In a preorder traversal, the root is always visited first, so we know that 3 must be\nthe root of the entire tree.\nInorder: 5, 7, 1, 4, 3, 6, 2\nPreorder: 3, 4, 7, 5, 1, 2, 6\n3\nOnce we have identified the root, we will use the inorder traversal to partition the remaining elements into left and right subtrees. Since an\ninorder traversal traverses the tree \"in order,\" all elements in the traversal that come before 3 must be to the left of 3, and all elements in the\ntraversal that come after 3 must be to the right of 3.\nInorder: 5, 7, 1, 4, 3 , 6, 2\nPreorder: 3, 4, 7, 5, 1, 2, 6\n3\n2,6\n4,7,5,1\nWe now repeat the same procedure for each of the subtrees. We set the element that comes first in the preorder traversal as the root of the subtree,\nand then we use the inorder traversal to partition the elements into left and right subtrees.\nIn the example, the elements 4, 7, 5, and 1 make up the left subtree. Of these elements, 4 is first in the preorder traversal, so 4 must be the\nroot of the left subtree. Knowing this, we will now look for the remaining elements in the left subtree (7, 5, and 1) in our inorder traversal. All\nthree of these elements precede 4 in the inorder traversal, so they must all be to the left of 4.\nInorder: 5, 7, 1, 4 , 3, 6, 2\nPreorder: 3, 4, 7, 5, 1, 2, 6\n3\n2,6\n4\n7,5,1", "word_count": 549, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a31e622d-687b-5027-a21a-988cf3bc6104", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 628, "real_page_number": null, "text": "616\nChapter 18. Trees\nThe elements 7, 5, and 1 make up the left subtree of 4. Since 7 appears first in the preorder traversal, it must be the root of this subtree. If we\nlook at the inorder traversal, 5 comes before 7, and 1 comes after 7. Thus, 5 must be to the left of 7, and 1 must be to the right of 7. The entire\nleft subtree is now complete.\nInorder: 5, 7 , 1, 4, 3, 6, 2\nPreorder: 3, 4, 7, 5, 1, 2, 6\n3\n2,6\n4\n7\n1\n5\nWe can repeat this process for the right subtree of the root. Since 2 appears before 6 in the preorder traversal, 2 must be the root of the right\nsubtree. Since 6 appears before 2 in the inorder traversal, 6 must be to the left of 2, and thus is the left child of 2. The entire tree is now complete.\nInorder: 5, 7, 1, 4, 3, 6, 2\nPreorder: 3, 4, 7, 5, 1, 2, 6\n3\n2\n6\n4\n7\n1\n5\n¸ 18.5.2\nConstructing a Binary Tree from Inorder and Postorder Traversals\nExample 18.15 You are given a binary tree with the following inorder and postorder traversals. Use these traversals to draw out this tree.\nInorder: 3, 6, 5, 4, 2, 1, 7\nPostorder: 3, 6, 4, 2, 7, 1, 5\nSolution: First, we will use the postorder traversal to determine the root. In a postorder traversal, the root is always visited last, so we know that\n5 must be the root of the entire tree.\nInorder: 3, 6, 5, 4, 2, 1, 7\nPostorder: 3, 6, 4, 2, 7, 1, 5\n5\nNow that we have identified the root, we will use the inorder traversal to partition the remaining elements into left and right subtrees. All\nelements in the inorder traversal that come before 5 must be to the left of 5, and all elements that come after 5 must be to the right of 5.\nInorder: 3, 6, 5 , 4, 2, 1, 7\nPostorder: 3, 6, 4, 2, 7, 1, 5\n5\n4,2,7,1\n3,6", "word_count": 362, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "684472c1-25ce-5cd8-a500-40f829bc4a0b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 629, "real_page_number": null, "text": "18.5 Constructing Binary Trees from Preorder, Inorder, and Postorder Traversals\n617\nThe left subtree of 5 includes 3 and 6. Since 6 comes last in the postorder traversal for this subtree, 6 must be the root of the left subtree. Now,\nwe will determine where 3 goes using the inorder traversal. In the inorder traversal, 3 comes before 6, so 3 must be the left child of 6.\nInorder: 3, 6 , 5, 4, 2, 1, 7\nPostorder: 3, 6, 4, 2, 7, 1, 5\n5\n4,2,7,1\n6\n3\nWe can repeat this process for the right subtree of 5. Of the four elements in the right subtree, 1 appears last in the postorder traversal, so it must\nbe the root of the right subtree. Now, we can look at the inorder traversal to determine the positions of the other elements. Since 4 and 2 come\nbefore 1 in the inorder traversal, they must be in the left subtree of 1. Since 7 comes after 1 in the traversal, 7 must be the right subtree of 1.\nInorder: 3, 6, 5, 4, 2, 1 , 7\nPostorder: 3, 6, 4, 2, 7, 1, 5\n5\n1\n7\n4,2\n6\n3\nSince 2 comes after 4 in the postorder traversal, 2 must be the root of 1’s left subtree. Using the inorder traversal, 4 comes before 2, so 4 must be\nthe left child of 2. Our tree is now complete.\n5\n1\n7\n2\n4\n6\n3\n¸ 18.5.3\nConstructing a Binary Tree from Preorder and Postorder Traversals\nIf you are given either a preorder or postorder traversal, you can build a unique binary tree as long as you are also given the inorder traversal.\nHowever, if you are only given a preorder and a postorder traversal, you may not be able to construct a unique binary tree. Without the inorder\ntraversal, there is no way to determine if a node should go to the left or right of the root, which could cause issues if a node only has one child.\nExample 18.16 Draw two binary trees that have different structures but share the same preorder and postorder traversals. If this is not\npossible, explain why.\nThe easiest way to construct two different trees with the same preorder and postorder traversals is to draw a tree where one of the nodes only has\na left child, and then draw the same tree with this left child as the right child. For example, the following two trees share the exact same preorder\nand postorder traversals (preorder: 1, 2, 4, 3; postorder: 4, 2, 3, 1). This is because, without the inorder traversal, we cannot determine if a child\ngoes to the left or right of its parent.\n1\n3\n2\n4\n1\n3\n2\n4", "word_count": 469, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8a70c8db-a2af-5d18-b3c9-a2708d2dff6e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 630, "real_page_number": null, "text": "618\nChapter 18. Trees\n18.6\nBinary Search Trees\n¸ 18.6.1\nBinary Search Tree Structure\nOne pitfall of the standard binary tree is that its elements are not ordered in any manner. This can make searching difficult: if you wanted to find\na given element, that element could be anywhere in the tree! To address this issue, we will introduce the binary search tree (BST), which is a\nbinary tree whose elements are ordered based on a sorting invariant. A standard non-empty binary search tree exhibits the following properties:\n• The key of any node is always greater than all of the keys in its left subtree.\n• The key of any node is always less than or equal to all of the keys in its right subtree.\n• Both the left and right subtrees of any node are also binary search trees themselves.\nFor example, consider the following three binary trees:\n11\n17\n19\n14\n7\n8\n2\n11\n17\n19\n14\n7\n6\n2\n11\n17\n19\n14\n7\n13\n2\nOnly the first tree is a binary search tree. The second tree is not a binary search tree because the right child of 7, or 6, is smaller than 7. The\nthird tree is not a binary search tree because 13 is in the left subtree of 11, even though 13 is larger than 11.\nRemark: In the definition above, we specify that equal keys in a binary search tree always go into the right subtree. However, does it matter\nwhich side equal values are sent to? As long as you are consistent with where you put duplicates, it shouldn’t matter. In our definition, we\noperator<,send duplicates to the right subtree because this allows our tree to be implemented using just the default operator often used to\noperator<,order STL containers (i.e., if send to left subtree, else send to right subtree). However, it is possible for a tree to be defined\ndifferently and still be a valid binary search tree.\nAnother convention for managing duplicates would be to store a counter for each element in the tree representing the number of copies that\nelement has in the tree. For example, we can use the following to represent a binary tree that has a duplicate 5 and a duplicate 7 (the number\nin parentheses is the counter associated with the key):\n5(2)\n6(1)\n7(2)\n3(1)\n4(1)\nThis approach has its advantages: it makes the height of the BST independent of the number of duplicates, and it may make the balancing\nprocess easier for certain types of self-balancing BSTs (the concept of balancing will be covered in the next section). However, we will not\nbe using this convention in this class, unless otherwise stated.\nFor a tree to be a valid binary search tree, only the rules specified above need to be met. As a result, it is possible for two binary search trees to\nhave different structures, even if they have the same keys. Two examples are shown below:\n6\n7\n8\n5\n5\n2\n5\n7\n8\n6\n5\n2\nSince keys in a binary search tree are ordered in a way such that all keys to the left are smaller and all keys to the right are larger, the inorder\ntraversal of a binary search tree will always visit its keys in sorted order.\nExample 18.17 Consider the following binary search tree. What is the inorder traversal of this BST?\n5\n7\n8\n6\n5\n2\nThe inorder traversal of a binary search tree always visits the keys in sorted order. Therefore, the inorder traversal of the tree is: 2, 5, 5, 6, 7, 8.", "word_count": 610, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0d9c92a1-314c-57a4-94c9-def2e1d00509", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 631, "real_page_number": null, "text": "18.6 Binary Search Trees\n619\nExample 18.18 You are given a binary search tree with 𝑛nodes. What is the worst-case time complexity of printing all the keys in this\nbinary search tree in sorted order?\nSince the tree is a binary search tree, we know that its inorder traversal would visit the keys in sorted order. Thus, all we need to do is to conduct\nan inorder traversal of the tree and print each key out along the way. Since an inorder traversal visits each node at most once, the worst-case time\ncomplexity of this task is Θ(𝑛).\n¸ 18.6.2\nSearching in a Binary Search Tree\nBinary search trees support efficient searching, since you will only need to look down one side of the tree at each level. If you want to search for\na target key 𝑘, and 𝑘is smaller than the current root element you are looking at, then 𝑘must be in the left subtree if it exists. Otherwise, if 𝑘\nis larger than the current root element, then 𝑘must be in the right subtree if it exists. You can think of this as a \"binary search\" of the tree’s\nelements, since you are removing half of the search space every time; hence why this data structure is called a tree!binary search\nFor example, if we wanted to search for 70 in the following binary search tree, we would first compare 70 with the root, 53. Since 70 is\nlarger than 53, we would look down the right subtree of 53, rooted at 85. We would then compare 70 with 85; since 70 is smaller, we would\nlook down the left subtree of 85, rooted at 70. We’ve successfully found 70, so our search is complete.\n53\n85\n88\n70\n26\n34\n19\n70 > 53, so look down right subtree\n70 < 85, so look down left subtree\nwe found 70, so search is complete\nnullptrIf we ever reach a without finding the key we want to find, then the key must not exist in the binary search tree. For example, if we\nwanted to search for 71 in the tree, we would attempt to look down the right subtree of 70. However, 70 has no right child, so we can conclude\nthat 71 doesn’t exist in the tree.\n53\n85\n88\n70\n26\n34\n19\n71 > 53, so look down right subtree\n71 < 85, so look down left subtree\n71 > 70, so look down right subtree\nnullptr,we hit a so 71 must not exist\nThe searching algorithm can be implemented both iteratively and recursively. The following iterative implementation returns a pointer to the\nnullptr.node with key 𝑘if it exists; otherwise, it returns\n1\ntemplate <typename T>\n2\nstruct Node {\n3\nT val;\n// assume T supports operator<\n4\nNode* left;\n5\nNode* right;\n6\n};\n7\n8\ntemplate <typename T>\n9\nNode<T>* tree_search(Node<T>* root, T k) {\n10\nwhile nullptr(root != && k != root->val) {\n11\nif (k < root->val) {\n12\nroot = root->left;\n13\n} // if\n14\nelse {\n15\nroot = root->right;\n16\n} // else\n17\n} // while\n18\nreturn root;\n19\n} // tree_search()", "word_count": 535, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "93fcb93c-f53c-539a-bcdd-a3db976b2afe", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 632, "real_page_number": null, "text": "620\nChapter 18. Trees\nThe following implementation does the same thing, but with recursion instead of iteration. Note that this implementation is tail recursive, since\nthe recursive call is always done last.\n1\ntemplate <typename T>\n2\nstruct Node {\n3\nT val;\n// assume T supports operator<\n4\nNode* left;\n5\nNode* right;\n6\n};\n7\n8\ntemplate <typename T>\n9\nNode<T>* tree_search(Node<T>* root, T k) {\n10\nif nullptr(root == || root->val == k) {\n11\nreturn root;\n12\n} // if\n13\nif (k < root->val) {\n14\nreturn tree_search(root->left, k);\n15\n} // if\n16\nreturn tree_search(root->right, k);\n17\n} // tree_search()\nWhat is the time complexity of searching for a key in a binary search tree? The time complexity of search depends on the height of the tree. In\nthe best case, the key you want to find is in the root, allowing you to find it in time. However, in the worst-case, the binary search tree couldΘ(1)\nbe in the form of a stick, and you may have to search the entire tree to find a given key. This results in runtime, where 𝑛is the number ofΘ(𝑛)\nnodes in the tree. For instance, if you wanted to search for 5 in this tree, you would have to traverse through every node:\n1\n2\n3\n4\n5\n5 > 1, so look down right subtree\n5 > 2, so look down right subtree\n5 > 3, so look down right subtree\n5 > 4, so look down right subtree\nwe found 5, so search is complete\nWhat about the average case? On average, a binary search tree is relatively balanced (i.e., there are about as many items to the left as there are to\nthe right), allowing you to ignore half of the remaining elements at every level. Since we are halving the search space at every iteration, the\naverage-case time complexity of search on a binary search tree is Θ(log(𝑛)). Note that is also the height of an average-case binaryΘ(log(𝑛))\nsearch tree; as mentioned, the complexity of search depends on a tree’s height.\n¸ 18.6.3\nInserting into a Binary Search Tree\nTo insert a key into a binary search tree, we follow a procedure that is similar to search: we start at the root and trace a path downwards, but this\nnullptrtime we want to look for a position to append the node. For example, suppose we wanted to insert 71 into the binary search tree that\nwas introduced earlier:\n53\n85\n88\n70\n26\n34\n19\nnullptrWe look down the tree in a similar manner. However, once we find a position that 71 can go in, we insert the new node there.\n53\n85\n88\n70\n71\n26\n34\n19\n71 > 53, so look down right subtree\n71 < 85, so look down left subtree\n71 > 70, so look down right subtree\nnullptr,we hit a so insert 71 here", "word_count": 495, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1c58ede2-3f9c-57c7-9a41-1f39f04401c4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 633, "real_page_number": null, "text": "18.6 Binary Search Trees\n621\n(*&root),The implementation for BST insertion is shown below. Note that the function is passed in a which allows uspointer by reference\ntree_insert()to directly assign the new node to its desired position. By passing the pointer by reference, any changes that the function\ntree_insert()makes to the pointer will also be reflected in the calling function. In other words, if changes what the pointer is pointing at,\ntree_insert().the pointer is also changed in the function that called\n1\ntemplate <typename T>\n2\nstruct Node {\n3\nT val;\n// assume T supports operator<\n4\nNode* left;\n5\nNode* right;\n6\nNode(T k) : val{ k } {}\n7\n};\n8\n9\ntemplate <typename T>\n10\nvoid tree_insert(Node<T>*& root, T k) {\n11\nif nullptr)(root == {\n12\nnewroot = Node{k};\n13\n} // if\n14\nelse if (k < root->val) {\n15\ntree_insert(root->left, k);\n16\n} // else if\n17\nelse {\n18\ntree_insert(root->right, k);\n19\n} // else\n20\n} // tree_insert()\nSimilar to search, the time complexity of insert depends on the height of the tree. In the best case, the new node can be directly inserted as either\nthe left or right child of the initial root, which would take time. In the worst case, the binary search tree could be in the form of a stick,Θ(1)\nrequiring the algorithm to traverse the entire tree before finding the position to insert the new node — this would take time. On average,Θ(𝑛)\nhowever, the binary search tree should be relatively balanced, allowing insertions to be done in time.Θ(log(𝑛))\nExample 18.19 You are given a binary search tree with 𝑛nodes. Write a function to find the node with the smallest key. What are the\naverage-case and worst-case time complexities of this function?\nIf you are looking at any node in a binary search tree that has a left child, that left child must have a smaller value than the node you are looking\nat. Thus, to find the minimum value in a binary search tree, keep on recursing on the left child until you reach a node that has no left child at all.\nThe code is shown below:\n1\nNode* bst_tree_min(Node* root) {\n2\nif nullptr)(root == {\n3\nreturn nullptr;\n4\n} // if\n5\nwhile (root->left) {\n6\nroot = root->left;\n7\n} // while\n8\nreturn root;\n9\n} // bst_tree_min()\nIn the worst case, you could end up getting a left-facing stick, which could cause this function to visit every node before finding the node with\nthe smallest key, taking time. However, given an average-case tree that is relatively balanced, the time complexity of finding the smallestΘ(𝑛)\nkey would be Θ(log(𝑛)).\nExample 18.20 We define a as a tree in which none of its nodes have any siblings. When dealing with binary search trees, we oftenstick\nconsider sticks to be \"worst-case\" trees, since they exhibit worst-case behavior for search, insertion, and removal. For instance, theΘ(𝑛)\nfollowing binary search tree is a stick:\n1\n2\n3\n4\nSuppose you wanted to insert the following four keys into a binary search tree: 1, 2, 3, and 4. How many insertion orders would end up\ncreating a stick? What if you wanted to insert 𝑛unique keys into a binary search tree instead of 4 — how many insertion orders, in terms of\n𝑛, would end up creating a stick?", "word_count": 579, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9d0e86c1-cf44-5b80-b437-2de337c590fb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 634, "real_page_number": null, "text": "622\nChapter 18. Trees\nThere are two intuitive cases: if you insert the values in ascending or descending order, you will create a rightward or leftward facing stick:\nInsertion order: 1, 2, 3, 4\n1\n2\n3\n4\nInsertion order: 4, 3, 2, 1\n4\n3\n2\n1\nHowever, these are not the only two situations where you could end up with a stick! A stick does not have to be a straight line — for instance,\nthe following tree would also be considered a stick:\nInsertion order: 1, 4, 2, 3\n1\n4\n2\n3\nIt turns out you can create a stick tree if, at every step, you always insert the smallest or largest of the remaining elements into the tree. For\ninstance, you can either insert 1 or 4 into the tree first if you want to create a stick. If you insert 1 first, the next insertion can either be 2 or 4\n(since 2 is the smallest of the remaining elements, and 4 is the largest). Similarly, if you insert 4 first, the next insertion can either be 1 or 3. We\ncan express these options using a decision tree, as shown below:\nempty\n1\n4\n1\n2\n1\n4\n4\n1\n4\n3\n1\n2\n3\n1\n2\n4\n1\n4\n2\n1\n4\n3\n4\n1\n2\n4\n1\n3\n4\n3\n1\n4\n3\n2\n1\n2\n3\n4\n1\n2\n4\n3\n1\n4\n2\n3\n1\n4\n3\n2\n4\n1\n2\n3\n4\n1\n3\n2\n4\n3\n1\n2\n4\n3\n2\n1\n1insert\ninsert4\n2insert\ninsert4\n1insert\ninsert3\n3insert\ninsert4\n2insert\ninsert3\n2insert\ninsert3\n1insert\ninsert2\n4insert\n3insert\n3insert\n2insert\ninsert3\ninsert2\ninsert2\ninsert1\nAs shown, there are eight possible stick orientations that can be constructed using the four unique keys. Since you can insert either the smallest\nor largest of the remaining elements at every step of the process, there are two values you can choose for the first insertion, two values you can\nchoose for the second insertion, and two values you can choose for the third insertion. Once you insert the first three elements, you only have\none choice left for the fourth insertion, since there is only one element left. Putting this together, the total number of sticks we can construct\nusing four unique keys is equal to 2 2 2 1 = 8, as illustrated above.× × ×\nWe can generalize this process for 𝑛insertions. If you want to insert 𝑛unique keys to create a stick, there are two values you can choose\nfor the first insertion (i.e., smallest or largest), two values you can choose for the second insertion, …, and two values you can choose for the\n(𝑛−1)th (𝑛−1)thinsertion. Once you insert the element, you only have one choice left for the last insertion, as there is only one element left.\n𝑛th 2𝑛−1 2𝑛−1Since you can make two choices for the first insertions, and one choice for the insertion, there are a total of insertion(𝑛−1) ×1=\norders that would yield a stick.", "word_count": 539, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d9749a40-cb9b-5439-927a-5f14ad507cd9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 635, "real_page_number": null, "text": "18.6 Binary Search Trees\n623\nRemark: If you are given 𝑛unique keys to insert into a binary search tree, there are a total of insertion orders that are possible for these𝑛!\nelements (since you have 𝑛choices for the first insertion, choices for the second insertion, and so on). This leads to an interesting𝑛−1\nquestion: how many different binary search trees can be constructed using 𝑛unique elements? The answer here is not 𝑛!, since multiple\ninsertion orders could produce the same binary search tree (e.g., the insertion orders 2, 1, 3 and 2, 3, 1 produce the same tree).\nTo answer this question, you would have to enumerate through all the keys and count the number of unique binary search trees that can be\nconstructed with each key as the root. If we denote the smallest key as key 0, the second smallest key as key 1, ..., and the largest key as key\n𝑛−1, we can recursively express the total number of binary search trees that can be built with 𝑛unique keys as 𝐶𝑛below:\n𝐶𝑛=\n⎧\n⎪\n⎨\n⎪⎩\n1,\nif 𝑛=0\n1,\nif 𝑛=1\n∑𝑛−1\n(𝐶𝑖×𝐶𝑛−𝑖−1),𝑖=0\nif 𝑛>1\nHere, represents the total number of binary search trees that can be constructed if key 𝑖were the root (where 𝑖starts at 0). This is𝐶𝑖×𝐶𝑛−𝑖−1\nbecause there are 𝑖keys to the left of key 𝑖, and keys to the right of key 𝑖. As a result, there are 𝐶𝑖ways to construct the left subtree𝑛−𝑖−1\nof key 𝑖and ways to construct the right subtree of key 𝑖. Thus, by the fundamental counting principal, we can conclude that there are𝐶𝑛−𝑖−1\n∑𝑛−1binary search trees that can be constructed with key 𝑖as the root. The summation𝐶𝑖×𝐶𝑛−𝑖−1\nis then applied to enumerate through all𝑖=0\nkeys as potential roots of the tree (i.e., the total number of BSTs for 𝑛unique keys = number of BSTs if key 0 were the root + number of\nBSTs if key 1 were the root + ... + number of BSTs if key were the root).𝑛−1\nIt turns out that this recurrence relation can be rewritten using the following closed-form formula. The proof has been omitted because the\nmathematics involved is too complex for this class, but there are many resources online that explain the process, if you are curious.\n𝐶𝑛=\n1\n𝑛+1\n(\n2𝑛\n𝑛\n)\n=\n(2𝑛)!\n(𝑛+1)!𝑛!\nIn mathematics, the sequence of natural numbers 𝐶𝑛for are known as the Catalan numbers, and they show up in many𝑛= 0,1,2,3,…\ndifferent counting problems.\nExample 18.21 You are given the following seven integers, which you want to insert into an empty BST:\n101, 183, 203, 280, 281, 370, 376\nHow many distinct BSTs can you construct using these seven elements? What percentage of these trees are sticks? What percentage of\nthese trees are complete trees?\nUsing the equation above, we can calculate the number of unique binary search trees with seven elements as:\n14!𝐶7 =\n=4298!7!\nThus, there are a total of 429 unique binary search trees that can be built using seven elements. In the previous example, we showed that there\n2𝑛−1 27−1are a total of stick formations that can be constructed with 𝑛elements. Since 7, the total number of sticks we can generate is 64.𝑛= =\nThe percentage of stick trees is therefore equal to\nnumber of sticks with 7 elements\n64=number of BSTs with 7 elements\n14.92%=429\nAs for completeness, there is only one arrangement that allows the binary search tree to be complete (recall that completeness means that all\nlevels but the bottom level must be completely filled, and the bottom level must be filled from left to right with no gaps). In fact, for any 𝑛, there\nis only one distinct structure to organize the nodes in for the entire tree to be complete. For this example, the configuration is shown below:\n280\n370\n376\n281\n183\n203\n101\nThus, only 1 of the 429 possible BST configurations is complete, and the percentage of seven-node binary search trees that are complete is\n1/429 = 0.23%.", "word_count": 691, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "adc240a8-8789-52db-a8b9-eec18ecd7779", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 636, "real_page_number": null, "text": "624\nChapter 18. Trees\nExample 18.22 array_to_BST()You are given a sorted array of 𝑛unique integers. Implement the function, which takes in the sorted\nleft right, [left, right]array and two indices and and constructs a binary search tree containing values in the range with the\npossible height. Your function should run in worst-case time.Θ(𝑛)minimum\n1\nstruct Node {\n2\nint32_t val;\n3\nNode* left;\n4\nNode* right;\n5\nNode(int32_t nullptr nullptrx) : val{ x }, left{ }, right{ } {}\n6\n};\n7\n8\narray_to_BST(int32_t size_t size_tNode* arr[], left, right);\nSince the array is sorted, the element in the middle of the sorted array must be the root if we want our final BST to be as balanced as possible.\nFurthermore, the element in the middle of the left half of the sorted array should be the left child of the root, and the element in the middle of\nthe right half of the sorted array should be the right child of the root. We can therefore use recursion to write an elegant solution to this problem\nthat runs in linear time.\n1\narray_to_BST(int32_t size_t size_tNode* arr[], left, right) {\n2\nif (left > right) {\n3\nreturn nullptr;\n4\n} // if\n5\nsize_t mid = left + (right - left) / 2;\n6\nnewNode* root = Node(arr[mid]);\n7\nroot->left = array_to_BST(arr, left, mid - 1);\n8\nroot->right = array_to_BST(arr, mid + 1, right);\n9\nreturn root;\n10\n} // array_to_BST()\n¸ 18.6.4\nRemoving from a Binary Search Tree\nSo far, we have covered search and insertion. However, what if we wanted to remove a key from a binary search tree? Removal is slightly more\ncomplicated, since we will have to detach a node while still maintaining the sorted property of a binary search tree. There are four different\nconditions we have to consider when removing a node from a binary search tree:\n1. The node we want to remove has no children.\n2. The node we want to remove has no left child.\n3. The node we want to remove has no right child.\n4. The node we want to remove has two children.\n1. The node we want to remove has no children.\nThis condition is trivial; if we want to remove a leaf node, we can just snip off the leaf.\n53\n85\n88\n70\n71\n26\n34\n47\n19\n12\n53\n85\n70\n71\n26\n34\n47\n19\n12\nremove 88\n2. The node we want to remove has no left child.\nIf the node we want to remove has no left child, we can just replace it with its right child. In the example below, 34 has no left child, so we just\nreplace it with its right child, 47.\n53\n85\n70\n71\n26\n34\n47\n19\n12\n53\n85\n70\n71\n26\n47\n19\n12\nremove 34", "word_count": 482, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6d5eaf48-3da4-5e13-8fbf-940767289939", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 637, "real_page_number": null, "text": "18.6 Binary Search Trees\n625\n3. The node we want to remove has no right child.\nIf the node we want to remove has no right child, we can just replace it with its left child. In the example below, 19 has no right child, so we just\nreplace it with its left child, 12.\n53\n85\n70\n71\n26\n47\n19\n12\n53\n85\n70\n71\n26\n47\n12\nremove 19\n4. The node we want to remove has two children.\nThis is a trickier case, since the node to remove points to two things instead of one. As a result, we will need a procedure for replacing the\ndeleted node in a way that preserves the binary search tree property.\nA key observation to make here is that any node in the left subtree of the deleted node must be smaller than any node in the right subtree of\nthe deleted node. Thus, we can preserve the binary search tree property by either (1) replacing the deleted node with the smallest node in its\nright subtree, or (2) replacing the deleted node with the largest node in its left subtree.\nFor example, suppose we wanted to remove 53 from the following binary search tree. We can either replace 53 with the smallest element in\nits right subtree (70), or with the largest element in its left subtree (47). Both approaches would maintain the binary search tree property:\n53\n85\n70\n71\n26\n47\n12\nReplace 53 with 70,\nthen delete original 70\n70\n85\n71\n26\n47\n12\nOR\nReplace 53 with 47,\nthen delete original 47\n47\n85\n70\n71\n26\n12\nremove 53\nThe smallest node in the right subtree of a given node 𝑘is known as the inorder successor of node 𝑘. Similarly, the largest node in the left\nsubtree of a given node 𝑘is known as the inorder predecessor of node 𝑘. This is because the inorder successor of 𝑘comes directly after (i.e.,\nsucceeds) 𝑘in an inorder traversal, and the inorder predecessor of 𝑘comes directly before (i.e., precedes) 𝑘in an inorder traversal.\nInorder traversal: 12, 26, 47, 53, 70, 71, 85\nInorder predecessor of 53\nInorder successor of 53\nBecause the inorder successor and predecessor are directly adjacent to the node we want to delete, it is safe to replace the deleted node with\neither of these values. For instance, if we replaced 53 with 70 (and then deleted the original 70 in 53’s right subtree), the elements of the tree\nwould still be in sorted order. This would also be the case if we replaced 53 with 47 (and then deleted the original 47 in 53’s left subtree).\nReplacing a deleted node with either its inorder successor or predecessor is the simplest way to approach deletion, as it maintains the BST\nproperty without requiring you to reconfigure the other elements in the tree.\nThe process for finding the inorder successor or predecessor of any node is also relatively straightforward. To identify the inorder successor\nof a node, go right once, then go as far left as possible until you reach a node without a left child. To identify the inorder predecessor of a node,\ngo left once, then go as far right as possible until you reach a node without a right child.\nExample 18.23 Consider the following tree. What is the inorder successor and inorder predecessor of node A?\nA\nC\nF\nK\nQ\nP\nE\nJ\nO\nI\nN\nB\nD\nH\nM\nG\nL", "word_count": 584, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "edb7905d-034e-58a0-944e-f9032181c646", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 638, "real_page_number": null, "text": "626\nChapter 18. Trees\nTo identify the inorder successor, we go right once, and then go as far left as possible (until we hit a node with no left child). In this case, node I\nis the the inorder successor of node A:\nA\nC\nF\nK\nQ\nP\nE\nJ\nO\nI\nN\nB\nD\nH\nM\nG\nL\nTo identify the inorder predecessor, we go left once, and then go as far right as possible (until we hit a node with no right child). In this case, we\ngo left to B, but B has no right children. Thus, node B is the inorder predecessor of node A:\nA\nC\nF\nK\nQ\nP\nE\nJ\nO\nI\nN\nB\nD\nH\nM\nG\nL\nThe code for remove is shown below. In this version, the deleted node is replaced with the inorder successor (the inorder predecessor version is\nnot shown, but it follows the same logic).\n1\ntemplate <typename T>\n2\nstruct Node {\n3\nT val;\n// assume T supports operator<\n4\nNode* left;\n5\nNode* right;\n6\n};\n7\n8\n// Removes the node from the tree with value \"val\"\n9\ntemplate <typename T>\n10\nvoid consttree_remove(Node<T>*& root, T& val) {\n11\nNode<T>* node_to_delete = root;\n12\nNode<T>* inorder_successor;\n13\n// Recursively finds the node containing the value (\"val\") to remove\n14\nif nullptr)(root == {\n15\nreturn;\n16\n} // if\n17\nelse if (val < root->val) {\n18\ntree_remove(root->left, val);\n19\n} // else if\n20\nelse if (root->val < val) {\n21\ntree_remove(root->right, val);\n22\n} // else if\n23\nelse {\n24\n// Check for simple cases where at least one subtree is empty\n25\nif nullptr)(root->left == {\n26\nroot = root->right;\n27\ndelete node_to_delete;\n28\n} // if\n29\nelse if nullptr)(root->right == {\n30\nroot = root->left;\n31\ndelete node_to_delete;\n32\n} // else if\n33\nelse {\n34\n// Node to delete has both left and right subtrees\n35\ninorder_successor = root->right;\n36\nwhile (inorder_successor->left) {\n37\ninorder_successor = inorder_successor->left;\n38\n} // while\n39\n// Replace value with inorder successor's value\n40\nnode_to_delete->val = inorder_successor->val;\n41\n// Remove the inorder successor from right subtree\n42\ntree_remove(root->right, inorder_successor->val);\n43\n} // else\n44\n} // else\n45\n} // tree_remove()", "word_count": 387, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f1ab0867-050a-5591-aee1-93fee1be0c5b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 639, "real_page_number": null, "text": "18.7 AVL Trees\n627\nLike search and insert, the time complexity of remove depends on the height of the tree. In the average case, the tree is relatively balanced,\nallowing removal to be done in time, where 𝑛is the number of nodes in the tree. This is because finding a node and its inorderΘ(log(𝑛))\nsuccessor or predecessor would take time if the tree were balanced. In the worst case, you could end up with a stick, which couldΘ(log(𝑛))\ncause removal to run in up to time (since finding the inorder successor or predecessor of a stick may take time).Θ(𝑛) Θ(𝑛)\nSummary of Binary Search Tree Time Complexities\nOperation\nBest-Case Time\nAverage-Case Time\nWorst-Case Time\nFinding a value\nΘ(1)\nΘ(log(𝑛))\nΘ(𝑛)\nInserting a value\nΘ(1)\nΘ(log(𝑛))\nΘ(𝑛)\nDeleting a value\nΘ(1)\nΘ(log(𝑛))\nΘ(𝑛)\n18.7\nAVL Trees\n¸ 18.7.1\nProperties of AVL Trees\nBinary search trees provide us with average-case time complexities for search, insertion, and removal. However, they do provideΘ(log(𝑛)) not\nus with this guarantee in the worst case. The worst-case time complexity of search, insertion, and removal in a standard binary search tree with\n𝑛nodes is Θ(𝑛), since we may end up with a stick tree that forces us to traverse every node. For example, searching for 5 in the following binary\nsearch tree would require us to visit every node, resulting in a linear traversal.\n1\n2\n3\n4\n5\n5 > 1, so look down right subtree\n5 > 2, so look down right subtree\n5 > 3, so look down right subtree\n5 > 4, so look down right subtree\nwe found 5, so search is complete\nTo obtain a better time complexity bound in the worst case, we need to ensure that this worst-case stick tree never happens. One method is to\nthe tree with every insertion and deletion. By doing so, we can fix the tree whenever it becomes too lopsided, essentially preventing abalance\nstick tree from ever happening. If we can guarantee that the size of the left and right subtrees are relatively equal at all times, then the time\ncomplexities of search, insertion, and removal will be in the worst case.Θ(log(𝑛))\nThis brings us to the AVL tree, a self-balancing binary search tree named after its inventors, Georgy Adelson-Velsky and Evgenii Landis.\nAn AVL tree is a binary search tree that enforces the property, which states that the heights of the left and right subtrees of anyheight balance\nnode in the tree can only differ by at most one. If this property is ever broken after an operation, the AVL tree will automatically use torotations\ncorrect its imbalance. This allows an AVL tree to remain balanced at all times, guaranteeing a time complexity bound on search,Θ(log(𝑛))\ninsertion, and removal.\nTo review, the of a node represents how far it is from the bottom of the tree, measured upward from the leaf nodes. Leaf nodes have aheight\nheight of 1 (by definition), and the height of all other nodes in the tree can be defined recursively:\n• height(empty) = 0\n• height(node) = max(height(children)) + 1\nExample 18.24 Consider the following four trees, which we will denote as trees A, B, C, and D (from left to right). Which of these trees\nare valid AVL trees? Recall that an AVL tree is a binary search tree that satisfies the height balance property, where the heights of the\nchildren of any node differ by at most one.\n5\n7\n2\n3\n5\n7\n6\n2\n3\n5\n7\n9\n8\n2\n3\n1\n5\n6\n4\n3\nOnly trees A and B are valid AVL trees. Tree C is not a valid AVL tree because node 7 is imbalanced (the height of 7’s left child is 0, and the\nheight of 7’s right child is 2, resulting in a difference greater than 1). Tree D is not a valid AVL tree because it is not a binary search tree, since 4\nshould not be in the right subtree of 5.", "word_count": 675, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f68a6475-c438-5769-99eb-dbc3ec97b3e6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 640, "real_page_number": null, "text": "628\nChapter 18. Trees\nExample 18.25 Prove that the height of an AVL tree with 𝑛nodes is 𝑂(log(𝑛)).\nTo show that the height of an AVL tree cannot exceed 𝑂(log(𝑛)), we have to show that, even if try to create the \"worst\" AVL tree possible (a tree\nwhere the height is as large as possible, using the fewest number of nodes), the maximum height is still bounded by 𝑂(log(𝑛)). To start, we will\nfirst try to determine the minimum number of nodes needed to build an AVL tree of height ℎ(which we will denote as 𝑛ℎ).\nWe know that the minimum number of nodes needed to construct an AVL tree of height 0 is 0, so 0. Similarly, we know that the𝑛0 =\nminimum number of nodes needed to construct an AVL tree of height 1 is 1, so 1. With these base cases, we can recursively define the𝑛1 =\nminimum number of nodes to construct an AVL tree of height ℎas\n𝑛ℎ=1+𝑛ℎ−1+𝑛ℎ−2\nfor 1. This is because an AVL of height ℎwith the minimum number of nodes consists of a root node (1 node), an AVL subtree of heightℎ>\n(which has minimum nodes), and an AVL subtree of height (which has minimum nodes). Putting this together, an AVLℎ−1 𝑛ℎ−1 ℎ−2 𝑛ℎ−2\ntree of height ℎmust have minimum nodes. Any more, and the tree would no longer be using the minimum number of nodes;1+𝑛ℎ−1+𝑛ℎ−2\nany fewer, and the tree would no longer be balanced.\nWe know that and 𝑛ℎ−2; as +𝑛ℎ−2, we can then conclude that 2𝑛ℎ−2. Since the equation is defined𝑛ℎ>𝑛ℎ−1 𝑛ℎ−1 > 𝑛ℎ=1+𝑛ℎ−1 𝑛ℎ>\nrecursively, also implies that 2𝑛ℎ−4, and 2𝑛ℎ−6, and so on. This allows us to perform the following substitution:𝑛ℎ>2𝑛ℎ−2 𝑛ℎ−2 > 𝑛ℎ−4 >\n2(2𝑛ℎ−4𝑛ℎ>2𝑛ℎ−2 >\n) 2(2(2𝑛ℎ−6>\n))>…\nℎIf we generalize the above substitution, we can conclude that, for any <𝑖<0\n,2\n2𝑖𝑛(ℎ−2𝑖)𝑛ℎ>\nℎWe know that the base case 1, so we can substitute𝑛1 =\nfor 𝑖to get−12\n𝑛ℎ>2\n(\nℎ\n−12\n)\n𝑛(\nℎ−2\n(\nℎ\n−12\n))\n𝑛ℎ>2\n(\nℎ\n−12\n)\n𝑛1\n𝑛ℎ>2\n(\nℎ\n−12\n)\nSolving for ℎ, we get\nℎ<2log(𝑛ℎ)+2 𝑂(log(𝑛))=\nThus, the height of an AVL tree is 𝑂(log(𝑛)).\nExample 18.26 Suppose you have an AVL tree of height 9. What is the minimum number of nodes that can be in this AVL tree? What is\nthe maximum number of nodes that can be in this AVL tree?\nTo calculate the minimum number of nodes in an AVL tree of height 9, we can use the equation in the previous example:\n𝑛ℎ=1+𝑛ℎ−1+𝑛ℎ−2\nWe know that an AVL tree of height 0 has 0 nodes, and an AVL tree of height 1 has 1 node, so and 1. Our goal is to solve for 𝑛9, the𝑛0 𝑛1=0 =\nminimum number of nodes in an AVL tree of height 9:\n• 𝑛2 1+𝑛1+𝑛0= =1+1+0=2\n• 𝑛3 1+𝑛2+𝑛1= =1+2+1=4\n• 𝑛4 1+𝑛3+𝑛2= =1+4+2=7\n• 𝑛5 1+𝑛4+𝑛3= =1+7+4=12\n• 𝑛6 1+𝑛5+𝑛4= =1+12+7=20\n• 𝑛7 1+𝑛6+𝑛5= =1+20+12=33\n• 𝑛8 1+𝑛7+𝑛6= =1+33+20=54\n• 𝑛9 1+𝑛8+𝑛7= =1+54+33=88\nSince 88, an AVL tree of height 9 must have at least 88 nodes.𝑛9 =\nFor an AVL tree of height 9 to have the maximum number of nodes possible, every level must be completely filled. In section 18.2, we showed\n2ℎ−1that a binary tree of height ℎcan have at most nodes. Thus, the maximum number of nodes possible in an AVL tree of height 9 is\n29 511.−1=", "word_count": 656, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a2d79afe-60d4-5510-b35f-47be0f6b067b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 641, "real_page_number": null, "text": "18.7 AVL Trees\n629\n¸ 18.7.2\nBalance Factor\nTo balance an AVL tree, we will consider its balance factor. The balance factor of any node is equal to the difference between the heights of\nthat node’s left and right children. In an AVL tree implementation, we typically store the height of a node in its definition, as shown:\n1\nstruct Node {\n2\nint val;\n3\nint height;\n4\nNode* left;\n5\nNode* right;\n6\n// returns height of left child\n7\nint left_height() {\n8\nreturn left ? left->height : 0;\n9\n} // left_height()\n10\n// returns height of right child\n11\nint right_height() {\n12\nreturn right ? right->height : 0;\n13\n} // right_height()\n14\n};\nThis allows us to calculate a node’s balance factor using the following equation:\nbalance_factor = left_height() - right_height()\nFor an AVL tree to be balanced, all of its nodes must have a balance factor of -1, 0, or +1.\nbalance_factor(n) == 0, n• If the left and right subtrees of node have the same height.\nbalance_factor(n) == +1, n• If the left subtree of node is taller than the right subtree by one.\nbalance_factor(n) == -1, n• If the right subtree of node is taller than the left subtree by one.\nbalance_factor(n)If the absolute value of exceeds 1, that means the node is out of balance, since the difference between the heights of\nits left and right children is greater than 1. When this happens, the AVL tree will have to self-balance using rotations.\nExample 18.27 What is the balance factor of each node in this tree?\n2\n5\n3\n4\n1\nTo solve this problem, we will start from the bottom of the tree and move upwards, calculating the balance factor along the way. First, we will\nlook at node 4. Node 4 has no children, so its balance factor is 0. Similarly, node 1 also has no children, so its balance factor is 0 as well.\n2\n5\n3\n4\n1\n0\n0\nNext, we look at node 3. The height of 3’s left child is 0, and the height of 3’s right child is 1. Thus, the balance factor of node 3 is 0 - 1 = -1.\n2\n5\n3\n4\n1\n0\n-1\n0\nNext, we look at node 5. The height of 5’s left child is 2, and the height of 5’s right child is 0. Thus, the balance factor of node 5 is 2 - 0 = +2.\n2\n5\n3\n4\n1\n0\n0\n-1\n+2", "word_count": 428, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b49c01b2-6a3f-5d0d-bd22-03853fe36c9b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 642, "real_page_number": null, "text": "630\nChapter 18. Trees\nLastly, we will look at the root node, 2. The height of 2’s left child is 1, and the height of 2’s right child is 3. Thus, the balance factor of node 2\nis 1 - 3 = -2.\n2\n5\n3\n4\n1\n0\n0\n-1\n+2\n-2\n¸ 18.7.3\nAVL Tree Rotations\nThe tree in the above example is not balanced, since some of its nodes have a balance factor whose absolute value exceeds 1. To balance a tree,\nwe start from the nodes at the bottom of the tree and move upwards toward the root, calculate the balance factor of each node along the way, and\nperform whenever we encounter a node whose balance factor is not -1, 0, or +1. Using this procedure, there are four unique situationsrotations\nthat may arise, each of which result in different rotations:\n1. An imbalanced node has a balance factor than +1, and its left child has a balance factor that is either 0 or +1.greater\n• When this happens, fix the imbalance by conducting a on the subtree rooted at the imbalanced node.right rotation\n2. An imbalanced node has a balance factor than -1, and its right child has a balance factor that is either 0 or -1.less\n• When this happens, fix the imbalance by conducting a on the subtree rooted at the imbalanced node.left rotation\n3. An imbalanced node has a balance factor than +1, and its left child has a balance factor of -1.greater\n• When this happens, fix the imbalance by first conducting a on the subtree rooted at the imbalanced node’s left child,left rotation\nand then conducting a on the subtree rooted at the imbalanced node.right rotation\n4. An imbalanced node has a balance factor than -1, and its right child has a balance factor of +1.less\n• When this happens, fix the imbalance by first conducting a on the subtree rooted at the imbalanced node’s right child,right rotation\nand then conducting a on the subtree rooted at the imbalanced node.left rotation\n1. An imbalanced node has a balance factor greater than +1, and its left child has a balance factor that is either 0 or +1.\nThis scenario happens when you encounter an imbalanced node whose left subtree causes the imbalance, and the left side of that subtree is\nequal in height or taller than the right side of that subtree. Examples of this scenario are shown in the following trees, where the darker shaded\nnode is the first node we encounter with a balance factor greater than +1 (starting from the bottom and moving up), and the lighter shaded node\nis the left child of the imbalanced node:\n3\n2\n1\n0\n+1\n+2\n5\n3\n4\n2\n1\n0\n+1\n+1\n0\n+3\n4\n5\n3\n2\n1\n0\n+1\n+2\n+2\n0\nTo fix the tree in this situation, we will perform a right rotation on the subtree rooted at the node with the balance factor greater than +1 (i.e.,\nthe darker shaded node). To conduct a right rotation, complete the following steps:\n1. Take the root of the subtree that you want to rotate (the darker shaded node) and set its left child to the right child of its original left child.\n2. Take the original left child (the lighter shaded node) and set its right child to its original parent (the darker shaded node).\n3. Return the new root of the rotated subtree to the caller that invoked the rotation (which should be the parent of the initial imbalanced\nnode, if there is one). If the node you want to rotate (the darker shaded node) has a parent, reset its parent’s child so that it points to the\nnew root of the rotated subtree (which ends up being the lighter shaded node).\nFor instance, let’s fix the rightmost tree in the previous illustration. If we move from the bottom of the tree upwards, the first imbalanced node\nwe encounter is the node with a value of 3. This node has a balance factor of +2, and its left child has a balance factor of +1, which fits this\ncondition. Thus, to fix this tree, we will conduct a right rotation on the subtree rooted at node 3.\n4\n5\n3\n2\n1\n0\n+1\n+2\n+2\n0", "word_count": 734, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ca225941-d9a4-5dc2-9d0c-0cc97a28fd93", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 643, "real_page_number": null, "text": "18.7 AVL Trees\n631\nnullptr.To perform the rotation, we first set 3’s left child to the right child of 2. Since 2 has no right child, 3’s left child becomes\n4\n5\n3\n2\nnull\n1\nThen, we set 2’s right child to 3.\n4\n5\n2\n1\n3\nnull\nLastly, we update 4’s left child so it points to the new root of the left subtree, node 2.\n4\n5\n2\n1\n3\nnull\nThis is now our new tree after the right rotation. Since all of the nodes have a balance factor of -1, 0, or +1, the tree is balanced, and no other\nrotations need to be completed.\n4\n5\n2\n3\n1\n0\n0\n0\n+1\n0\n2. An imbalanced node has a balance factor less than -1, and its right child has a balance factor that is either 0 or -1.\nThis scenario happens when you encounter an imbalanced node whose right subtree causes the imbalance, and the right side of that subtree is\nequal in height or taller than the left side of that subtree. Examples of this scenario are shown in the following trees, where the darker shaded\nnode is the first node we encounter with a balance factor less than -1 (starting from the bottom and moving up), and the lighter shaded node is\nthe right child of the imbalanced node:\n1\n2\n3\n-2\n-1\n0\n1\n3\n4\n5\n2\n-3\n0\n-1\n-1\n0\n2\n3\n4\n5\n1\n0\n-2\n-2\n-1\n0\nTo fix the tree in this situation, we will perform a left rotation on the node with the balance factor less than -1 (i.e., the darker shaded node). To\nconduct a left rotation on a node, complete the following steps:\n1. Take the root of the subtree you want to rotate (the darker shaded node) and set its right child to the left child of its original right child.\n2. Take the original right child (the lighter shaded node) and set its left child to its original parent (the darker shaded node).\n3. Return the new root of the rotated subtree to the caller that invoked the rotation (which should be the parent of the initial imbalanced\nnode, if there is one). If the node you want to rotate (the darker shaded node) has a parent, reset its parent’s child so that it points to the\nnew root of the rotated subtree (which ends up being the lighter shaded node).", "word_count": 416, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e215ba5b-0324-5f8a-bdb1-71ba68872851", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 644, "real_page_number": null, "text": "632\nChapter 18. Trees\nAs an example, we will fix the rightmost tree in the previous illustration. If we move from the bottom of the tree upwards, the first imbalanced\nnode we encounter is the node with the value of 3. This node has a balance factor of -2, and its right child has a balance factor of -1, which fits\nthis condition. Thus, to fix this tree, we will conduct a left rotation on the subtree rooted at node 3.\n2\n3\n4\n5\n1\n0\n-2\n-2\n-1\n0\nnullptr.To perform the rotation, we first set 3’s right child as the left child of 4. Since 4 has no left child, 3’s right child becomes\n2\n3\n4\n5\nnull\n1\nThen, we set 4’s left child to 3.\n2\n1\n4\n5\n3\nnull\nLastly, we update 2’s right child so it points to the new root of the right subtree, node 4.\n2\n1\n4\n5\n3\nnull\nThis is now our new tree after the left rotation on node 3. Since all of the nodes have a balance factor of -1, 0, or +1, the tree is balanced, and no\nother rotations need to be completed.\n2\n4\n5\n3\n1\n0\n-1\n0\n0\n0", "word_count": 212, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "79a91bfb-5147-5748-872a-d86a21a13267", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 645, "real_page_number": null, "text": "18.7 AVL Trees\n633\n3. An imbalanced node has a balance factor greater than +1, and its left child has a balance factor of -1.\nThis scenario happens when you encounter an imbalanced node whose left subtree causes the imbalance, and the right side of that subtree is\ntaller than the left side of that subtree. Examples of this scenario are shown in the following trees, where the darker shaded node is the first node\nwe encounter with a balance factor greater than +1 (starting from the bottom and moving up), and the lighter shaded node is the left child of the\nimbalanced node:\n3\n1\n2\n-1\n0\n+2\n5\n2\n4\n3\n1\n0\n-1\n0\n+1\n+3\n4\n5\n3\n1\n2\n-1\n0\n+2\n+2\n0\nTo fix the tree in this situation, we must first perform a left rotation on the subtree rooted at the imbalanced node’s (i.e., the lighterleft child\nshaded node), and then perform a right rotation on the subtree rooted at the imbalanced node itself (i.e., the darker shaded node).\nAsanexample,wewillfixtherightmosttreeinthepreviousillustration. Ifwemovefromthebottomofthetreeupwards,thefirstimbalanced\nnode we encounter is the node with the value of 3. This node has a balance factor of +2, but its left child has a balance factor of -1 (a sign\nchange). Thus, to fix this tree, we will need to conduct rotations: a left rotation on node 1, followed by a right rotation on node 3.two\n4\n5\n3\n1\n2\n-1\n0\n+2\n+2\n0\n(nullptr),We start off by conducting a left rotation on node 1. To do so, 1’s right child is set to 2’s left child 2’s left child is set to its original\nparent (node 1), and 3’s left child is set to the new root of the subtree (node 2).\n4\n5\n3\n2\n1\n0\n+1\n+2\n+2\n0\nThe above tree is the result of performing a left rotation on node 1. After the left rotation is done, we then conduct a right rotation on the original\n(nullptr),imbalanced node, or node 3. To perform this right rotation, 3’s left child is set to 2’s right child 2’s right child is set to its original\nparent (node 3), and 4’s left child is set to the new root of the subtree (node 2).\n4\n5\n2\n3\n1\n0\n0\n0\n+1\n0\nNone of the remaining nodes in the tree are imbalanced, so no other rotations need to be completed.\n4. An imbalanced node has a balance factor less than -1, and its right child has a balance factor of +1.\nThis scenario happens when you encounter an imbalanced node whose right subtree causes the imbalance, and the left side of that subtree is\ntaller than the right side of that subtree. Examples of this scenario are shown in the following trees, where the darker shaded node is the first\nnode we encounter with a balance factor less than -1 (starting from the bottom and moving up), and the lighter shaded node is the right child of\nthe imbalanced node:\n1\n3\n2\n-2\n0\n+1\n1\n4\n5\n2\n3\n-3\n-1\n0\n+1\n0\n2\n3\n5\n4\n1\n0\n-2\n-2\n0\n+1", "word_count": 562, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7d786110-3bcd-5a5e-8109-cbb02c84745a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 646, "real_page_number": null, "text": "634\nChapter 18. Trees\nTo fix the tree in this situation, we must first perform a right rotation on the subtree rooted at the imbalanced node’s (i.e., the lighterright child\nshaded node), and then perform a left rotation on the subtree rooted at the imbalanced node itself (i.e., the darker shaded node).\nAsanexample,wewillfixtherightmosttreeinthepreviousillustration. Ifwemovefromthebottomofthetreeupwards,thefirstimbalanced\nnode we encounter is the node with the value of 3. This node has a balance factor of -2, but its right child has a balance factor of +1 (a sign\nchange). Thus, to fix this tree, we will need to conduct rotations: a right rotation on node 5, followed by a left rotation on node 3.two\n2\n3\n5\n4\n1\n0\n-2\n-2\n0\n+1\n(nullptr),We start off by conducting a right rotation on node 5. To do so, 5’s left child is set to 4’s right child 4’s right child is set to its\noriginal parent (node 5), and 3’s right child is set to the new root of the subtree (node 4).\n2\n3\n4\n5\n1\n0\n-2\n-2\n-1\n0\nThen, we will conduct a left rotation on the original imbalanced node, or node 3. To perform this left rotation, we set 3’s right child to 4’s left\n(nullptr),child 4’s left child to its parent (node 3), and 2’s right child to the new root of the subtree (node 4).\n2\n4\n5\n3\n1\n0\n-1\n0\n0\n0\nNone of the remaining nodes in the tree are imbalanced, so no other rotations need to be completed.\nRemark: Right rotations are done when a node has a balance factor > +1, and left rotations are done when a node has apositive negative\nbalance factor < -1. Luckily, this is intuitive, since left is associated with negative, and right is associated with positive. However, you must\npay attention to the third and fourth conditions above, where there exists a sign change between an imbalanced node and its corresponding\nchild (left child for balance factors > +1, right child for balance factors < -1). In these cases, you will need to rotate the subtree rooted at the\nimbalanced node’s child before you can rotate the subtree rooted at the imbalanced node itself!\nThe rotation process on an AVL tree can therefore be summarized as follows:\n• If the balance factor of a node is greater than +1, check the balance factor of its left child.\n– If the left child has a negative balance factor, perform a left rotation on the subtree rooted at that left child.\n– Perform a right rotation on the subtree rooted at the current parent node (always done regardless of balance factor of left child).\n• If the balance factor of a node is less than -1, check the balance factor of its right child.\n– If the right child has a positive balance factor, perform a right rotation on the subtree rooted at the right child.\n– Perform a left rotation on the subtree rooted at the current parent node (always done regardless of balance factor of right child).\nThe pseudocode is shown below:\n1\nAlgorithm check_and_balance(Node* n):\n2\nif balance_factor(n) > +1:\n3\nif balance_factor(n->left) < 0:\n4\nn->left = rotate_left(n->left)\n5\nn = rotate_right(n)\n6\nelse if balance_factor(n) < -1:\n7\nif balance_factor(n->right) > 0:\n8\nn->right = rotate_right(n->right)\n9\nn = rotate_left(n)\nrotate_left() rotate_right()In the pseudocode above, and return the new root after the rotation, which allows the parent of the\nrotated node to be easily assigned.", "word_count": 615, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1b88ad49-45db-55d8-bc7e-dd517b82b0d7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 647, "real_page_number": null, "text": "18.7 AVL Trees\n635\nSummaryofAVLTreeRotations\nStraightLeft-LeftImbalance\nStraightRight-RightImbalance\nZigzagLeft-RightImbalance\nZigzagRight-LeftImbalance\n➀\n➁\n➀\n➁\n➀\n➁\n➂\n➀\n➁\n➂\nZ\n𝑑\nY\n𝑐\nX\n𝑏\n𝑎\nrotate right\nY\nZ\n𝑑\n𝑐\nX\n𝑏\n𝑎\nX\nY\nZ\n𝑑\n𝑐\n𝑏\n𝑎\nrotate left\nY\nZ\n𝑑\n𝑐\nX\n𝑏\n𝑎\nZ\n𝑑\nX\nY\n𝑐\n𝑏\n𝑎\nrotate left on left child\nZ\n𝑑\nY\n𝑐\nX\n𝑏\n𝑎\nrotate right\nY\nZ\n𝑑\n𝑐\nX\n𝑏\n𝑎\nX\nZ\n𝑑\nY\n𝑐\n𝑏\n𝑎\nrotate right on right child\nX\nY\nZ\n𝑑\n𝑐\n𝑏\n𝑎\nrotate left\nY\nZ\n𝑑\n𝑐\nX\n𝑏\n𝑎\n1Rotation\n1Rotation\n2Rotations\n2Rotations\nLeftRotation\nX\nY\nZ\n𝑒\n𝑑\n𝑐\nW\n𝑏\n𝑎\nrotate left on subtree\nrooted at node\nX\nY\nZ\n𝑒\n𝑑\n𝑐\nW\n𝑏\n𝑎\nset right child of node to\nleft child of right child\nX\nW\n𝑏\n𝑎\nset left child of original\nright child to rotated node\nZ\n𝑒\nY\n𝑑\n𝑐\nX\nW\n𝑏\n𝑎\nset right child of original\nparent to new root of subtree\nZ\n𝑒\nY\n𝑑\n𝑐\nRightRotation\nY\nZ\n𝑒\n𝑑\nX\n𝑐\nW\n𝑏\n𝑎\nrotate right on subtree\nrooted at node\nY\nZ\n𝑒\n𝑑\nX\n𝑐\nW\n𝑏\n𝑎\nset left child of node to\nright child of left child\nY\nZ\n𝑒\n𝑑\nset right child of original\nleft child to rotated node\nW\n𝑎\nX\n𝑐\n𝑏\nY\nZ\n𝑒\n𝑑\nset left child of original\nparent to new root of subtree\nW\n𝑎\nX\n𝑐\n𝑏", "word_count": 281, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b50d12c7-ae47-5c53-a494-c4836b5749c6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 648, "real_page_number": null, "text": "636\nChapter 18. Trees\nRotations are very useful in practice because a single rotation always takes constant time (since you are just swapping pointers around), and they\nensure a logarithmic tree height. This allows search, insertion, and removal to all take worst-case time, an improvement over theΘ(log(𝑛)) Θ(𝑛)\nworst-case time of a standard non-balancing binary search tree.\nExample 18.28 You are given the following stick tree. Using the rules of rotation above, balance this stick. To do this, start from the\nbottom node and move upwards toward the root, calculate the balance factor, and rotate whenever necessary.\nA\nB\nC\nD\nTo solve this problem, we will start by calculating the balance factor of all the nodes in this tree, starting from the bottom. Node D has no\nchildren, so it has a balance factor of 0. Node C has a left child of height 1 and a right child of height 0, so its balance factor is 1 - 0 = +1. Node\nB has a left child of height 0 and a right child of height 2, so its balance factor is 0 - 2 = -2. Thus, node B is an imbalanced node.\nA\nB\nC\nD\n-2\n+1\n0\nSince node B has a negative imbalance,However, before we can rotate node B, we must first check to see if we need to conduct a double rotation!\nwe look at the balance factor of its right child to see if there is a sign change. In this case, there is: node B has a balance factor of -2, but node C\nhas a balance factor of +1. Thus, we will need to rotate right on node C before we can rotate left on node B! The process of rotating right on\nnode C is shown below, where C’s left child is set to D’s right child, D’s right child is set to C, and B’s right child is set to D.\nA\nB\nC\nD\nnull\nA\nB\nD\nC\nnull\nA\nB\nD\nC\nnull\nA\nB\nD\nC\n-2\n0\n-1\nNow, we can rotate left on node B. B’s right child is set to D’s left child, D’s left child is set to B, and A’s left child is set to D.\nA\nB\nD\nC\nnull\nA\nD\nC\nB\nnull\nA\nD\nC\nB\nnull\nA\nD\nC\nB\n0\n0\n0\nThe rotation on B is complete, and none of the nodes in the subtree rooted at B remain imbalanced. We continue up the tree and calculate the\nbalance factor of node A. Since A’s left child has a height of 2, and A’s right child has a height of 0, the balance factor of A is 2 - 0 = +2.\nA\nD\nC\nB\n+2\n0\n0\n0", "word_count": 471, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "12172de2-5361-5bac-8d4c-be1ae8d4a53f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 649, "real_page_number": null, "text": "18.7 AVL Trees\n637\nThe left child of node A has a balance factor of 0, so there is no need for a double rotation. Since A’s balance factor is positive, we will rotate\nright on A. This is done by setting A’s left child to D’s right child (or node C) and D’s right child to its original parent (node A). After the\nrotation, node D becomes the new root of the tree.\nA\nD\nC\nB\nA\nC\nD\nB\nD\nA\nC\nB\n+1\n0\n0\n-1\nThe right rotation on node A is now complete, and the tree is balanced.\n¸ 18.7.4\nInserting and Removing Elements\nThe behaviors of inserting and removing elements from an AVL tree are nearly identical to their corresponding behaviors in a standard binary\nsearch tree. However, there is a difference: the insertion or removal of an element in an AVL tree may cause an imbalance that will need to be\nresolved. As a result, AVL trees require additional checks after each insertion and removal to ensure that the tree remains balanced, and that any\nimbalanced nodes are corrected using rotations. The rules for insertion and deletion are shown below.\nInsertion into an AVL Tree\n1. Insert the node to its correct position normally, as if the tree were a standard non-self-balancing binary search tree (without considering\nimbalances).\n2. After the node is inserted, check the balance factors of the ancestors of the new node, starting from the bottom of the tree and moving\n(|balance factor|toward the top. If any of these nodes are imbalanced > 1), perform the appropriate rotations to balance that node. Once\nyou fix the first imbalanced node you encounter (via either a single or double rotation), you are done, and the AVL tree is guaranteed to\nbe balanced.\nDeletion from an AVL Tree\n1. Delete the target node from the tree normally, as if the tree were a standard non-self-balancing binary search tree (using either the inorder\nsuccessor or inorder predecessor).\nAfter the node is removed, travel up the tree from the parent of the removed node. At every imbalanced node encountered, perform2.\nthe appropriate rotations. Unlike the insertion process, this restructuring step may unbalance multiple ancestors, so you must continue\nchecking and rebalancing up to the root.\nAn important distinction between insertion and deletion is that insertion only needs to fix the imbalanced node encountered along the pathfirst\nfrom the newly inserted node to the root (using either a single or double rotation). Once this node is rotated, the AVL tree is guaranteed to be\nbalanced. For example, consider the following AVL tree:\n6\n7\n8\n2\n3\n5\n1\nSuppose we want to insert 4 into this AVL tree. We would first insert 4 into the tree as if it were a standard binary search tree, without worrying\nabout imbalances and rotations.\n6\n7\n8\n2\n3\n5\n4\n1\n4 < 6, so look down left subtree\n4 > 2, so look down right subtree\n4 > 3, so look down right subtree\n4 < 5, so look down left subtree\nnullptr,we hit a so so insert 4 here", "word_count": 529, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1634cd78-69de-5dcc-841a-96bda939e165", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 650, "real_page_number": null, "text": "638\nChapter 18. Trees\nThen, look for the first imbalanced node along the path from the newly inserted node to the root, starting from the newly inserted node and\nmoving upwards. This ends up being node 3, which has a balance factor of -2. However, since 5 has a balance factor of +1, we have a sign\nchange (i.e., the right-left zigzag case), so we must rotate right on 5 before we can rotate left on 3.\n6\n7\n8\n2\n3\n5\n4\n1\n6\n7\n8\n2\n3\n4\n5\n1\n6\n7\n8\n2\n4\n5\n3\n1\nHowever, once we finish rotating node 3, we do need to check nodes 2 or 6 for a potential imbalance. This is because an AVL tree cannot\nalways be fixed after an insertion using only a single or a double rotation! Why is this the case? Notice that, after an insertion, the only way an\nimbalance can happen is if the new node ends up extending a branch way too long (e.g., the insertion of 4 above made the right subtree of 3 too\nlong). However, when you fix the first imbalanced node you encounter using either a single or double rotation, you end up \"pulling\" the newly\ninserted node upward, which shortens the height of its branch. Since the new node is pulled upward, any imbalance caused by the insertion is\nremoved, which repairs the entire tree. For example, by fixing node 3 above, we were able to turn a subtree of height 2 into a subtree of height 1,\nwhich balances the entire tree.\nDeletion is a different story. Fixing an AVL tree after an insertion requires at most two rotations, but fixing an AVL tree after a deletion may\nrequire up to rotations (where the term comes from the height of the tree). This is because a deleted node can only create anΘ(log(𝑛)) log(𝑛)\nimbalance by making its branch shorter. However, each fix ends up making a subtree of the tree shorter. If a shortened subtree was alreadyalso\nshorter than its sibling, a fix will be required at a higher level of the tree, which could result in multiple rotations before the tree returns to a\nbalanced state. For example, consider the following AVL tree:\n13\n21\n26\n29\n31\n32\n33\n30\n28\n27\n23\n24\n25\n22\n18\n20\n19\n15\n16\n17\n14\n5\n8\n10\n11\n12\n9\n7\n6\n3\n4\n2\n1\nSuppose we remove 4 from this tree. This ends up creating an imbalance at node 3. To fix this, we will rotate right on 3.\n13\n21\n26\n29\n31\n32\n33\n30\n28\n27\n23\n24\n25\n22\n18\n20\n19\n15\n16\n17\n14\n5\n8\n10\n11\n12\n9\n7\n6\n3\n2\n1\n-1\n+2\n+1\n-1", "word_count": 476, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7303a2af-05fd-5e3b-9611-2621fdba29dd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 651, "real_page_number": null, "text": "18.7 AVL Trees\n639\nAfter the right rotation, the tree looks like this:\n13\n21\n26\n29\n31\n32\n33\n30\n28\n27\n23\n24\n25\n22\n18\n20\n19\n15\n16\n17\n14\n5\n8\n10\n11\n12\n9\n7\n6\n2\n3\n1\n-2\n-1\nNotice that the rotation ended up shortening the height of 5’s left subtree. This causes an imbalance on node 5, since 5’s shorter child was\nshortened even more! To fix this imbalance, we have to conduct a left rotation on 5.\n13\n21\n26\n29\n31\n32\n33\n30\n28\n27\n23\n24\n25\n22\n18\n20\n19\n15\n16\n17\n14\n5\n8\n10\n11\n12\n9\n7\n6\n2\n3\n1\n-2\n-1\n13\n21\n26\n29\n31\n32\n33\n30\n28\n27\n23\n24\n25\n22\n18\n20\n19\n15\n16\n17\n14\n8\n10\n11\n12\n9\n5\n7\n6\n2\n3\n1\n-2", "word_count": 155, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "35d98495-1efb-576b-bb7a-076fd7b99515", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 652, "real_page_number": null, "text": "640\nChapter 18. Trees\nThis rotation ended up shortening the height of 13’s left subtree. This ended up creating an imbalance on node 13, since the left subtree of 13\nwas already shorter than its right subtree. We have to fix this imbalance by conducting a left rotation on 13.\n13\n21\n26\n29\n31\n32\n33\n30\n28\n27\n23\n24\n25\n22\n18\n20\n19\n15\n16\n17\n14\n8\n10\n11\n12\n9\n5\n7\n6\n2\n3\n1\n-2\n21\n26\n29\n31\n32\n33\n30\n28\n27\n23\n24\n25\n22\n13\n18\n20\n19\n15\n16\n17\n14\n8\n10\n11\n12\n9\n5\n7\n6\n2\n3\n1\n0\nThe tree is now fully balanced. The deletion of a single node ended up triggering three rotations!\nRemark: In the tree above, every imbalance satisfied either the left-left or right-right case, so we were able to correct each imbalance using a\nsingle rotation. However, we could have arranged the nodes of the tree in such a way that a rotation is needed to fix every imbalancedouble\nup the tree. In this case, a total of up to six rotations could have been triggered!\nJust how many rotations could result from a single deletion? This depends on the size of the tree. The smallest tree that can trigger up to two\nrotations (either a single or double rotation) after a single deletion has size 4 — this can be easily proven by drawing some trees out. The\nsmallest tree that can trigger up to four rotations after a single deletion has size 12. The smallest tree that can trigger up to six rotations after\na single deletion has size 33 (like the tree above). The smallest tree that can trigger up to eight rotations after a single deletion has size 88.\n4, 12, 33, 88, if you paid close attention to a previous example, this sequence should be somewhat familiar to you. Recall the example…\nwhere we calculated the minimum number of nodes in an AVL tree of height 9:\n• 𝑛2 1+𝑛1+𝑛0= =1+1+0=2\n• 4𝑛3 1+𝑛2+𝑛1= =1+2+1=\n• 𝑛4 1+𝑛3+𝑛2= =1+4+2=7\n• 12𝑛5 1+𝑛4+𝑛3= =1+7+4=\n• 𝑛6 1+𝑛5+𝑛4= =1+12+7=20\n• 33𝑛7 1+𝑛6+𝑛5= =1+20+12=\n• 𝑛8 1+𝑛7+𝑛6= =1+33+20=54\n• 88𝑛9 1+𝑛8+𝑛7= =1+54+33=\nThesequence4, 12, 33, 88, representthe minimumnumberofnodes required forAVL treesofoddheights. Isthis somecrazycoincidence,…\nor is this all related? Unsurprisingly, it turns out that these two relationships are in fact related. If you look at the previous 33-node tree\nexample, the children of the root are themselves minimum sized trees of heights 5 and 6:\n13\n21\n26\n29\n31\n32\n33\n30\n28\n27\n23\n24\n25\n22\n18\n20\n19\n15\n16\n17\n14\n5\n8\n10\n11\n12\n9\n7\n6\n3\n4\n2\n1\nminimum sized AVL tree of height 5\nminimum sized AVL tree of height 6", "word_count": 532, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5d797b0e-d33a-5a17-914c-38032f89a5cd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 653, "real_page_number": null, "text": "18.7 AVL Trees\n641\nA tree of size 12 is the smallest that can trigger up to four rotations, which occurs if the children of the root are minimum sized AVL trees of\nheights 3 and 4. For instance, deleting node 1 from the following tree of size 12 would trigger four rotations (RR-4, RL-2, RR-10, RL-5):\n5\n10\n12\n11\n7\n9\n8\n6\n2\n4\n3\n1\nminimum sized AVL\ntree of height 3\nminimum sized AVL\ntree of height 4\nSimilarly, an AVL tree of size 88 (which is too big to draw on the page) can trigger up to eight rotations if the children of the root are\nminimum sized AVL trees of heights 7 and 8.\nWhy is this true? To trigger more than one rotation upon a deletion, we have to delete a node from the of a tree’s two children.shorter\nConsider an arbitrary AVL tree of height 3, where one child has height and the other has height 𝑛−2:𝑛> 𝑛−1\nℎ𝑛−2\nℎ𝑛−1\nIf we delete a node from the smaller subtree and create an imbalance, rotations will be needed to fix that subtree. Since a rotation can only\nshorten the height of a subtree, a rebalance of the left subtree above would shorten its height to 𝑛−3:\nℎ𝑛−3\nℎ𝑛−1\nThe difference in heights between the two subtrees is now greater than 1, so a rotation on the root is needed to balance the entire tree. This\ncan take up to two rotations (in this example, a double rotation is needed if the left child of the right subtree has a greater height than the\nright child of the right subtree, which results in the zigzag right-left case).\nℎ𝑛−2\nℎ𝑛−2\nThe maximum number of rotations needed to fix a tree after a deletion is thus the maximum number of rotations needed to fix the smaller\nsubtree, plus two rotations for the root. Since the smallest possible height a child in an AVL tree of height 𝑛can have is 𝑛−2, we can define\nthe maximum number of rotations needed after deleting a node from an AVL tree of height 𝑛as:\nmax_rotation(ℎ𝑛) max_rotation(ℎ𝑛−2)+2=\nWe know that an AVL of height 3 can only support either a single or double rotation after a deletion, so max_rotation(ℎ3) = 2. We also know\nthat an AVL tree of height less than 3 cannot produce any rotations after a deletion, since there is no way for a node to end up with a balance\nfactor greater than 1 or less than -1. Thus, max_rotation(ℎ2) = 0. We can use this information to recursively calculate the maximum number\nof rotations that are possible after a deletion for an AVL tree of any height:\n• max_rotation(ℎ4) = max_rotation(ℎ2) + 2 = 0 + 2 = 2\n• max_rotation(ℎ5) = max_rotation(ℎ3) + 2 = 2 + 2 = 4\n• max_rotation(ℎ6) = max_rotation(ℎ4) + 2 = 2 + 2 = 4\n• max_rotation(ℎ7) = max_rotation(ℎ5) + 2 = 4 + 2 = 6\n• max_rotation(ℎ8) = max_rotation(ℎ6) + 2 = 4 + 2 = 6\n• max_rotation(ℎ9) = max_rotation(ℎ7) + 2 = 6 + 2 = 8\n• …\nThis is precisely why the maximum number of rotations possible after a single deletion mirrors the sequence of odd AVL heights. Every time\nyou reach a new odd height, you can potentially trigger another single or double rotation after a deletion!", "word_count": 570, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b678fc89-6584-5819-804a-d482852b1ae7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 654, "real_page_number": null, "text": "642\nChapter 18. Trees\nExample 18.29 How many rotations are needed to perform the following operations on an initially empty AVL tree? Note that a double\nrotation operation counts as two rotations.\nInsert 17, Insert 13, Insert 14, Insert 15, Insert 16, Delete 13\nFirst, we will insert 17 into the AVL tree. This can be done trivially:\n17\n0\nNext, we insert 13. Since 13 is smaller than 17, we add it as the left child of 17.\n17\n13\n0\n+1\nNext, we insert 14. As 14 is smaller than 17, we look at the left child of 17, or 13. Since 14 is larger than 13, we add 14 as the right child of 13.\n17\n13\n14\n0\n-1\n+2\nThe tree is now imbalanced, so we will need to perform a rotation. Since 17 has a balance factor of +2 and 13 has a balance factor of -1, we\nmust first conduct a left rotation on 13, followed by a right rotation on 17. This results in rotations, as shown.two\n17\n13\n14\n0\n-1\n+2\n17\n14\n13\n+1\n0\n+2\n14\n17\n13\n0\n0\n0\nNext, we insert 15. Walking down the tree, 15 ends up getting placed as the left child of 17. This doesn’t unbalance the tree in any way, so no\nrotations are needed.\n14\n17\n15\n13\n-1\n0\n+1\n0\nNext, we insert 16, which ends up getting placed as the right child of 15.\n14\n17\n15\n16\n13\n-2\n0\n+2\n-1\n0\nThe tree is now imbalanced. To fix the tree, we will have to conduct a left rotation on 15, followed by a right rotation on 17. This requires two\nrotations. In total, we have completed four rotations so far.\n14\n17\n15\n16\n13\n-2\n0\n+2\n-1\n0\n14\n17\n16\n15\n13\n-2\n0\n+2\n0\n+1\n14\n16\n17\n15\n13\n-1\n0\n0\n0\n0", "word_count": 328, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ecaff014-ffd2-5f9c-a175-4dde4055e50f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 655, "real_page_number": null, "text": "18.8 The STL Map Container\n643\nNext, we will delete 13. Since 13 has no children, we can just remove the node without having to deal with the inorder predecessor or successor.\nHowever, the tree ends up being imbalanced after the removal of 13.\n14\n16\n17\n15\n-2\n0\n0\n0\nTo fix this, we will perform a single left rotation on 14.\n14\n16\n17\n15\n-2\n0\n0\n0\n16\n17\n14\n15\n-1\n0\n0\n+1\nThe tree is now balanced, and all operations are complete. A total of 5 rotations were needed to complete the given insertions.\nSummary of AVL Tree Time Complexities\nOperation\nBest-Case Time\nAverage-Case Time\nWorst-Case Time\nFinding a value\nΘ(1)\nΘ(log(𝑛))\nΘ(log(𝑛))\nInserting a value\nΘ(log(𝑛))\nΘ(log(𝑛))\nΘ(log(𝑛))\nDeleting a value\nΘ(log(𝑛))\nΘ(log(𝑛))\nΘ(log(𝑛))\n18.8\nThe STL Map Container\nstd::map<> <map> std::map<>The intheC++ libraryisanassociativecontainerthatstoreskey-valuepairs. Eachitemina isrepresented\nstd::pair<>, .first .second std::map<>as a where the key is the value and the value is the value. Keys in a must be unique.\nstd::unordered_map<>, std::map<>Much like an a supports fast lookup of values associated with each key. However, unlike an\nstd::unordered_map<>, std::map<>the keys in a are based on a sorting criterion. Because maps need to support efficientordered\nsearch, insertion, and removal while also keeping track of the ordering of its keys, they are implemented using hashing (which does notnot\ntrees.2preserve order). Instead, C++ maps are internally implemented using self-balancing binary search\nFor example, let’s consider the table of food prices that we introduced in the previous chapter (reproduced below). If we were to insert these\nstd::map<> std::unordered_map<>,food items into a instead of a we could potentially end up with something like this under the\nhood (note that this is not exact, as it is just one example of what the map could look like):\nFood Item\nPrice\nApple\n$3.99\nAvocado\n$4.99\nBanana\n$2.49\nFlour\n$2.29\nGinger\n$2.69\nMilk\n$2.09\nTofu\n$3.79\nstd::map<> under the hood\nFlour\n2.29\nMilk\n2.09\nTofu\n3.79\nGinger\n2.69\nAvocado\n4.99\nBanana\n2.49\nApple\n3.99\nSearch, insertion, and deletion behave as expected for a self-balancing binary search tree. For instance, if we wanted to add the key-value pair\n{Carrot, $2.39} \"Carrot\"into this map, we would find and insert into the correct sorted position, as shown below:\nstd::map<> under the hood\nFlour\n2.29\nMilk\n2.09\nTofu\n3.79\nGinger\n2.69\nAvocado\n4.99\nBanana\n2.49\nCarrot\n2.39\nApple\n3.99\n2Mapsinthestandardlibraryaretypicallyimplementedusing insteadofAVLtrees. Bothimplementationsaccomplishthesamegoalofbalancingred-blacktrees\natreeafteramodification,andbothhavethesameworst-case timecomplexitiesforsearch,insertion,anddeletion.Θ(log(𝑛))", "word_count": 460, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "16e0a8a2-ed5b-55ca-8622-f36856dcebd6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 656, "real_page_number": null, "text": "644\nChapter 18. Trees\nSince maps use a self-balancing binary search tree structure to keep track of the ordering of its keys, the complexities of search, insertion, and\ndeletion are instead of Θ(1). This is the tradeoff between a map and an unordered map; if you want to keep track of the ordering ofΘ(log(𝑛))\nkeys, the average-case complexities of these three operations are logarithmic rather than constant.\nOperation\nstd::unordered_map<>\nstd::map<>\nSearch for Key\naverage-case timeΘ(1)\naverage-case timeΘ(log(𝑛))\nworst-case timeΘ(𝑛)\nworst-case timeΘ(log(𝑛))\nInsert Key\naverage-case timeΘ(1)\naverage-case timeΘ(log(𝑛))\nworst-case timeΘ(𝑛)\nworst-case timeΘ(log(𝑛))\nDelete Key\naverage-case timeΘ(1)\naverage-case timeΘ(log(𝑛))\nworst-case timeΘ(𝑛)\nworst-case timeΘ(log(𝑛))\nstd::map<>,To instantiate a pass in the types of the key and the mapped value (similar to the syntax of an unordered map):\nstd::map<KEY_TYPE, VALUE_TYPE> map_name;\nFor example, to create a map that maps a food name (string) into a price (double), we could declare a map as follows:\ndouble>std::map<std::string, food_prices;\nWe can then go through and insert our data into the map:\nfood_prices.insert({\"Apple\", 3.99});\nfood_prices.insert({\"Avocado\", 4.99});\nfood_prices.insert({\"Banana\", 2.49});\nfood_prices.insert({\"Flour\", 2.29});\nfood_prices.insert({\"Ginger\", 2.69});\nfood_prices.insert({\"Milk\", 2.09});\nfood_prices.insert({\"Tofu\", 3.79});\noperator[]Similar to the unordered map, can be used to retrieve the value associated with a key. For instance, if you wanted to retrieve the\n\"Tofu\", food_prices[\"Tofu\"]price of you can just call to get its price, 3.79.\ndouble price_tofu = food_prices[\"Tofu\"];\nstd::cout << price_tofu << '\\n';\n// prints 3.79\noperator[]Behind the scenes, calling on a key results in a lookup in the map’s underlying binary search tree.\nFlour\n2.29\nMilk\n2.09\nTofu\n3.79\nGinger\n2.69\nAvocado\n4.99\nBanana\n2.49\nApple\n3.99\noperator[]However, if is called on a key that does not exist, it will be automatically inserted into the container (similar to an unordered\noperator[]map). Thus, you have to be careful not to use on a key before you know that it actually exists. To check if a key actually exists in\n.find()a map, you can use the member function:\ntemplate <typename typenameK, V>\nV>::find(constiterator std::map<K, K& key);\nkeyChecks if exists in the container. If it exists, the function returns an iterator to the element with that key. If it does not exist, the function\nendreturns an iterator that points one past the end (i.e., the iterator).\nstd::map<> std::unordered_map<>.Much of the functionality supported by is also supported by A list of these operations is shown\nbelow (these behave similarly to their corresponding operations in an unordered map):\ntemplate <typename typenameK, V>\nbool> V>::insert(conststd::pair<iterator, std::map<K, std::pair<K, V>& p);\nstd::map<>Attempts to insert a key-value pair into the container. Since keys in an must be unique, an insertion is done only if the\nkey does not already exist in the container. Returns a pair consisting of an iterator to the inserted element (or the element that prevented\nboolinsertion) and a for whether the insertion took place.\ntemplate <typename typename typename...K, V, Args>\nbool>std::pair<iterator, std::map<K, V>::emplace(Args&&... args);\nargs. std::map<>Attempts to insert a key-value pair into the container, constructed in-place using Since keys in an must be unique, an\ninsertion is done only if the key does not already exist in the container. Returns a pair consisting of an iterator to the inserted element (or the\nboolelement that prevented insertion) and a for whether the insertion took place.", "word_count": 563, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e32f4c44-6ac8-50fd-8833-48aa50efca67", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 657, "real_page_number": null, "text": "18.8 The STL Map Container\n645\ntemplate <typename typename typename...K, V, Args>\nbool>std::pair<iterator, std::map<K, V>::try_emplace(K&& k, Args&&... args);\nbool> V>::try_emplace(conststd::pair<iterator, std::map<K, K& k, Args&&... args);\nk args, kInserts a new element into the container with key and value constructed using but only if does not exist as a key in the table\ninsert() emplace(), try_emplace()already. Unlike and does not move from rvalue arguments if an insertion does not happen,\nstd::unique_ptr<>which makes them useful when working with maps whose values are move-only types such as (covered in chapter\nemplace(), try_emplace()27). Also, unlike treats the arguments of the key and mapped value separately, so you do not need\nstd::pair<> try_emplace(key, value_arg1, value_arg2, ...)the constructor arguments to directly construct a (i.e.,\nemplace(key, value) value_arg...works instead of which passes in an instance of the value directly, while can be used to\nvalue emplace().construct in-place). The return value is the same as that of\ntemplate <typename typenameK, V>\nbool std::map<K, V>::empty();\nboolReturns a indicating whether the map is empty.\ntemplate <typename typenameK, V>\nsize_t std::map<K, V>::size();\nReturns the number of elements in the container.\ntemplate <typename typenameK, V>\nvoid std::map<K, V>::clear();\nErases all elements in the container, dropping size to 0.\ntemplate <typename typenameK, V>\nV>::erase(constiterator std::map<K, iterator pos);\nposErases the element pointed to by and returns an iterator to the element following the one that was removed.\ntemplate <typename typenameK, V>\nV>::erase(const constiterator std::map<K, iterator first, iterator last);\n[first, last)Erases all elements in the iterator range and returns an iterator to the element following the last element removed.\ntemplate <typename typenameK, V>\nsize_t V>::erase(conststd::map<K, K& key);\nkeyErases the element with the key equivalent to and returns the number of elements removed (which is always 1 since keys are unique).\ntemplate <typename typenameK, V>\nsize_t V>::count(conststd::map<K, K& key);\nkey. std::map<>,Returns the number of elements whose key compares equal to For an this function returns 0 if the key does not exist,\nand 1 if it does.\nBecause maps store their elements in sorted order, they also support additional functionalities that rely on this invariant. For instance, iterators in\nstd::unordered_map<>an are typically only useful for iteration purposes, since there is no guarantee on the order of keys in the container.\nstd::map<>, std::map<>However, in a we can easily access its keys in sorted order. As a result, iterators in a can be used to identify and\nvisit keys based on their sorted position.\nBecause of this, one big difference is that maps support reverse iterators, while unordered maps do not. Thus, maps (and other ordered\nassociative containers) support bidirectional iterators.\ntemplate <typename typenameK, V>\niterator std::map<K, V>::begin();\nstd::map<> (.cbegin() constReturns an iterator pointing to the first element in the container returns a version of this iterator). By\ndefault, this is the position of the smallest key in the map.\ntemplate <typename typenameK, V>\niterator std::map<K, V>::end();\nstd::map<> (.cend() constReturns an iterator pointing to one past the last element in the container returns a version of this iterator).\nBy default, this is the position one past the largest key in the map.\ntemplate <typename typenameK, V>\niterator std::map<K, V>::rbegin();\nstd::map<> (.crbegin() constReturns an iterator pointing to the last element in the container returns a version of this iterator).\nBy default, this is the position of the largest key in the map.\ntemplate <typename typenameK, V>\niterator std::map<K, V>::rend();\nstd::map<> (.crend() constReturns an iterator pointing to one before the first element in the container returns a version of this\niterator). By default, this is the position one before the smallest key in the map.", "word_count": 621, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2b62010d-d623-5d43-ad39-2251d10f9cbb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 658, "real_page_number": null, "text": "646\nChapter 18. Trees\nFlour\n2.29\nMilk\n2.09\nTofu\n3.79\nGinger\n2.69\nAvocado\n4.99\nBanana\n2.49\nApple\n3.99\nfood_prices.begin()\nfood_prices.rend()\n(one before the beginning)\nfood_prices.end()\n(one past the end)\nfood_prices.rbegin()\n{\"Banana,\"Incrementinganiteratorinamapmovesittothenextsortedposition. Forexample,ifaniteratorwerepointingtothekey-valuepair\n2.49}, {\"Flour\", 2.29}.incrementing it would move it to the next key-value pair in alphabetical order, or Thus, iterating through a map\nfrom the begin iterator to the end iterator is akin to performing an inorder traversal of the tree.\n.lower_bound(), .upper_bound(), .equal_range(),Maps also support and which behave similarly to their counterparts in\nthe algorithm library. These operations can be used to obtain the relative position of a key in the map.\ntemplate <typename typenameK, V>\nV>::lower_bound(constiterator std::map<K, K& key);\nkey.Returns an iterator pointing to the first key-value pair that is less than If no such element is found, the end iterator is returned.not\ntemplate <typename typenameK, V>\nV>::upper_bound(constiterator std::map<K, K& key);\nkey.Returns an iterator pointing to the first key-value pair that is greater than If no such element is found, the end iterator is returned.\ntemplate <typename typenameK, V>\nV>::equal_range(conststd::pair<iterator, iterator> std::map<K, K& key);\nkey,Returns a pair of iterators, where the first iterator points to the first element that is not less than and the second iterator points to the\nkey.first element greater than An end iterator is returned if no such elements are found.\nFor instance, consider the following code, using the food prices above:\n1\nauto p = food_prices.lower_bound(\"Mango\");\n// returns iter to key-value pair\n2\nstd::cout << p->first << '\\n';\n// prints \"Milk\"\n3\nstd::cout << p->second << '\\n';\n// prints 2.09\n.lower_bound() \"Mango\".Here, returns an iterator to the first key-value pair in the map whose key is not less than This ends up being\n\"Milk\", \"Milk\" \"2.09\"which is why and are printed in the example code above.\nExample 18.30 Design a time-based key value data structure that can be used to store multiple values for the same key based on timestamp.\nThis data structure should be able to retrieve the value of any key at any given timestamp. An outline of this structure is provided below:\n1\nclass TimedMap {\n2\nprivate:\n3\n// TODO: Add any data structures here!\n4\npublic:\n5\nTimedMap() {}\n6\n7\n// sets the key to a value at a given timestamp\n8\nvoid set(const const int32_tstd::string& key, std::string& value, timestamp) {\n9\n// TODO: Implement code here\n10\n} // set()\n11\n12\n// gets the value of the key at the specified timestamp\n13\nget(const int32_tstd::string std::string& key, timestamp) {\n14\n// TODO: Implement code here\n15\n} // get()\n16\n};\nThis class supports two members:\nset():• Sets the value of a key at a given timestamp.\nget(): \"\".• Gets the value associated with the key at the given timestamp. If there are no values, returns the empty string\nExample: Given the following sequence of key-value sets:\nset(\"fruit\", \"apple\", 1)•\nset(\"fruit\", \"banana\", 5)•\nset(\"fruit\", \"cherry\", 7)•\nset(\"vegetable\", \"carrot\", 4)•\nThen the following is true:\nget(\"fruit\", 1) \"apple\". \"fruit\" \"apple\"• returns This is because the value of was set to at timestamp 1.\nget(\"fruit\", 6) \"banana\". \"fruit\" \"banana\"• returns This is because the value of was set to at timestamp 5, but was\n\"cherry\" \"fruit\" \"banana\".not changed to until timestamp 7, so the value of at timestamp 6 remains\nget(\"vegetable\", 3) \"\", \"vegetable\"• returns since did not exist as a key in the map yet at timestamp 3.", "word_count": 612, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c835232a-0a62-549f-89ee-2e64f4cb832d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 659, "real_page_number": null, "text": "18.8 The STL Map Container\n647\nSince we may have to retrieve the value of a key at any given timestamp, we will have to store all key-value updates in a separate data structure\nso that they can be referenced later. This data structure must support efficient search based on a provided timestamp value.\nAs covered in previous chapters, search is more efficient if the key to search on is stored in order, as that allows you to perform asorted\nbinary search instead of a linear search to find the value you want. Thus, it would be prudent to consider a container sorted byΘ(log(𝑛)) Θ(𝑛)\ntimestamp for storing updates. Furthermore, since each timestamp is associated with a value, we can map each timestamp to its corresponding\nstd::map<>.value in a sorted lookup container like a For example, we could use the following map to represent the change history of\n\"fruit\" (where key is the timestamp, and value is the fruit value):\n5\n\"banana\"\n7\n\"cherry\"\n1\n\"apple\"\n\"fruit\"Here, if we wanted to query the value of at timestamp 6, we could look for the largest key in the map that is less than or equal to 6.\nstd::map<>’s .upper_bound()To do so, we can take advantage of the member function, which returns an iterator pointing to the first\n.upper_bound() .begin()key-value pair that is greater than the given timestamp (in this case, 6). If returns the iterator, we know that\nall the timestamps in the map must be larger than the key, so no value must exist for the key at that timestamp (and we return an empty string).\n.upper_bound()Otherwise, since returns an iterator to the first timestamp than the given timestamp, we would return the key-valuegreater\npair that directly precedes it.\nTimedMap \"fruit\", \"vegetable\").Additionally, our will need to store the history of multiple keys (e.g., Since each key will need\nto store its own history, we will need a separate lookup container to map each key to its corresponding history map. Since there is no need to\nstd::unordered_map<>.access the keys in sorted order, this can be done using an An implementation of this solution is shown below:\n1\nclass TimedMap {\n2\nprivate:\n3\nstd::map<int32_t,std::unordered_map<std::string, std::string>> time_map;\n4\npublic:\n5\nTimedMap() {}\n6\n7\n// sets the key to a value at a given timestamp\n8\nvoid set(const const int32_tstd::string& key, std::string& value, timestamp) {\n9\ntime_map[key].emplace(timestamp, value);\n10\n} // set()\n11\n12\n// gets the value of the key at the specified timestamp\n13\nget(const int32_tstd::string std::string& key, timestamp) {\n14\nconst auto& map_for_key = time_map[key];\n15\nauto it = map_for_key.upper_bound(timestamp);\n16\nreturn it == map_for_key.begin() ? \"\" : std::prev(it)->second;\n17\n} // get()\n18\n};\nset() get()? std::unordered_map<>,What are the time complexities of and To set a value, we first look up a key in an which takes\nstd::map<>constant time. Then, we insert the timestamp into a associated with the key; insertion into a map takes time forΘ(log(𝑛)) 𝑛\nset()timestamps. Thus, the total time complexity of a single call to is Θ(log(𝑛)).\nTo get a value, we have to find the closest timestamp that is smaller; this requires a binary search, which takes time. Since this isΘ(log(𝑛))\nget()the most expensive operation, the overall time complexity of a single call to is also Θ(log(𝑛)).\nRemark: There is an additional optimization we can make if we can guarantee that the timestamps are set in order. If this is theincreasing\nstd::map<>case, then we no longer need to use a to maintain the timestamps in sorted order; we can just append them to the back of a\nstd::vector<> and they will be naturally sorted because of the order in which timestamps are set. With this implementation, the time\nget() set()complexity of would still be because we cannot avoid the binary search, but the time complexity of would becomeΘ(log(𝑛))\nsince appending to a vector takes constant time.Θ(1)", "word_count": 667, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c41e2853-1e2d-5d33-9901-6d053b74acc6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 660, "real_page_number": null, "text": "648\nChapter 18. Trees\n18.9\nThe STL Set Container\nIn chapter 13, we introduced the implementation of an ordered set using a sorted vector, which allows elements to be searched in time.Θ(log(𝑛))\nHowever, the sorted vector implementation is not as ideal for insertions and removals, since its contents may need to be shifted to ensure the\ncontiguity of elements. If we want to remove this inefficiency while still maintaining time complexities for search, insertion, andΘ(log(𝑛))\ndeletion, we can use a self-balancing binary search tree instead.\nstd::map<>, std::set<> <set>Much like the the STL container, found in the library, is implemented using a self-balancing binary\nstd::set<> std::unordered_set<>, std::set<>search tree. The is similar to the except that the elements in a are ordered based\non a sorting criterion. Thus, sets are a good container type to consider if you want to keep track of a collection of keys, where the ordering of\nkeys is important to remember.\n1\nstd::set<std::string> eecs281staff;\n2\neecs281staff.insert(\"ajzhou\");\n3\neecs281staff.insert(\"akiek\");\n4\neecs281staff.insert(\"bingscha\");\n5\neecs281staff.insert(\"felichri\");\n6\neecs281staff.insert(\"hartsoe\");\n7\neecs281staff.insert(\"jakehage\");\n8\neecs281staff.insert(\"oliverhi\");\n9\neecs281staff.insert(\"pgossman\");\n10\neecs281staff.insert(\"sukang\");\n11\n...\nfelichri\njakehage\npgossman\nsukang\noliverhi\nhartsoe\nakiek\nbingscha\najzhou\nBecause sets store their elements in sorted order, the time complexities of search, insertion, and deletion are Θ(log(𝑛)). Note that the values in a\nstd::set<>set are distinct. A few common operations are summarized below:\ntemplate <typename K>\nbool> std::set<K>::insert(conststd::pair<iterator, K& key);\nstd::set<>Attempts to insert a key into the container. Since keys in an must be unique, an insertion is done only if the key does not\nalready exist in the container. Returns a pair consisting of an iterator to the inserted element (or the element that prevented insertion) and a\nbool for whether the insertion took place.\ntemplate <typename typenameK, InputIterator>\nvoid std::set<K>::insert(InputIterator first, InputIterator last);\n[first, last) std::set<>.Attempts to insert all elements from the range into the\ntemplate <typename K>\nvoid std::set<K>::insert(std::initializer_list<K> ilist);\nAttempts\nto\ninsert\nall\nthe\nelements\nfrom\nthe\ninitializer\nlist\nilist\ninto\nthe\nstd::set<>\n(for\nexample,\nthe\nline\nmy_set.insert({1, 3, 5, 7}) inserts the elements 1, 3, 5, and 7).\ntemplate <typename K>\nbool std::set<K>::empty();\nboolReturns a indicating whether the set is empty.\ntemplate <typename K>\nsize_t std::set<K>::size();\nReturns the number of elements in the set.\ntemplate <typename K>\niterator std::set<K>::begin();\nstd::set<> (.cbegin() constReturns an iterator pointing to the first element in the container returns a version of this iterator\niterator). By default, this is the position of the smallest value in the set.\ntemplate <typename K>\niterator std::set<K>::end();\nstd::set<> (.cend() constReturns an iterator pointing to one past the last element in the container returns a version of this iterator).\nBy default, this is the position one past the largest value in the set.\ntemplate <typename K>\niterator std::set<K>::rbegin();\nstd::set<> (.crbegin() constReturns an iterator pointing to the last element in the container returns a version of this iterator).\nBy default, this is the position of the largest value in the set.\ntemplate <typename K>\niterator std::set<K>::rend();\nstd::set<> (.crend() constReturns an iterator pointing to one before the first element in the container returns a version of this\niterator). By default, this is the position one before the smallest value in the set.\ntemplate <typename K>\nvoid std::set<K>::clear();\nErases all elements in the container, dropping size to 0.", "word_count": 550, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "aea559f5-53c8-5ba5-98bf-b93d1def7fa4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 661, "real_page_number": null, "text": "18.9 The STL Set Container\n649\ntemplate <typename K>\nstd::set<K>::erase(constiterator iterator pos);\nposErases the element pointed to by and returns an iterator to the element following the one that was removed.\ntemplate <typename K>\nstd::set<K>::erase(const constiterator iterator first, iterator last);\n[first, last)Erases all elements in the iterator range and returns an iterator to the element following the last element removed.\ntemplate <typename K>\nsize_t std::set<K>::erase(const K& key);\nkeyErases the element with the key equivalent to and returns the number of elements removed (which is always 1 since keys are unique).\ntemplate <typename K>\nstd::set<K>::find(constiterator K& key);\nkeyChecks if exists in the container. If it exists, the function returns an iterator to the element with that key. If it does not exist, the function\nendreturns an iterator that points one past the end (i.e., the iterator).\ntemplate <typename K>\nsize_t std::set<K>::count(const K& key);\nkey. std::set<>,Returns the number of elements whose key compares equal to For a this function returns 0 if the key does not exist,\nand 1 if it does.\ntemplate <typename K>\nstd::set<K>::lower_bound(constiterator K& key);\nkey.Returns an iterator pointing to the first key in the set that is less than If no such element is found, the end iterator is returned.not\ntemplate <typename K>\nstd::set<K>::upper_bound(constiterator K& key);\nkey.Returns an iterator pointing to the first key in the set that is greater than If no such element is found, the end iterator is returned.\ntemplate <typename K>\nstd::set<K>::equal_range(conststd::pair<iterator, iterator> K& key);\nkey,Returns a pair of iterators, where the first iterator points to the first element that is not less than and the second iterator points to the\nkey.first element greater than An end iterator is returned if no such elements are found.\nExample 18.31 EECS 281 has a Discord server that is actively used by both staff and students as an additional platform for providing help.\nLike with any platform, there are some people who use it more than others. Users who contribute more are awarded a higher server rank:\nranks that are closer to 1 indicate a higher level of engagement in the server. Those with upper echelon ranks are often respected as valuable\nmembers of the EECS 281 Discord community, as shown:", "word_count": 381, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0f1b0622-867e-5b8e-a4d6-c216545224b7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 662, "real_page_number": null, "text": "650\nChapter 18. Trees\nYou are implementing a class that can be used to identify who the highest ranked contributors in the Discord server are. For this example,\nyou are more concerned with the quality of messages that are sent (rather than quantity), so the rank of a user will be determined by the\nuser’s count. Words in a message are separated by spaces (e.g., the message \"worst-case time complexity\" has a word count of 3).total word\nAssume that all usernames are unique. An outline of this class is shown below:\n1\nclass DiscordRankManager {\n2\nprivate:\n3\n// TODO: Add any data structures here!\n4\npublic:\n5\nDiscordRankManager() {}\n6\n7\n// Invoked when a user posts a message in the server\n8\nvoid post_message(const conststd::string& user, std::string& message) {\n9\n// TODO: Implement code here\n10\n} // post_message()\n11\n12\n// Gets the word count of a specific user\n13\nint32_t get_word_count(const std::string& user) {\n14\n// TODO: Implement code here\n15\n} // get_word_count()\n16\n17\n// Gets all users with word count in range [min_word_count, max_word_count]\n18\nget_user_range(int32_t int32_tstd::vector<std::string> min_word_count, max_word_count) {\n19\n// TODO: Implement code here\n20\n} // get_word_count_range()\n21\n22\n// Gets the user with the highest word count\n23\nstd::string get_highest_ranked_user() {\n24\n// TODO: Implement code here\n25\n} // get_highest_ranked_user()\n26\n};\nThe methods of this class are summarized below:\npost_message(): Takesinausernameandamessage,indicatesthatamessagehasbeensent. Assumetheusernameandmessage•\nstring will both never be empty.\npost_message(\"slime\", \"@everyone\") \"slime\"– For example, a call to means that user sent the message\n\"@everyone\" in the server.\nget_word_count():• Gets the word count of the specified user. If the user has not posted anything, return 0.\npost_message(\"slime\", \"@everyone\") \"slime\"– For example, if is called (and posts no other messages),\nget_word_count(\"slime\") should return 1.\nget_user_range(): Returns all users with a word count within the specified range. If there are no users, return an empty vector.•\nget_highest_ranked_user():• Gets the user with the highest word count. If two users have the same word count, the one\nwith the lexicographically smaller username has the higher rank. If there are no users, return an empty string.\n\"slime\" get_highest_ranked_user() \"slime\".– For example, if has the highest word count, then should return\n\"slime\" \"doubledelete\" \"doubledelete\"– If both and have the same word count, then would have the higher rank\n(\"d\" < \"s\").since their username is lexicographically smaller\nExample: Consider the following conversation in the EECS 281 Discord (assume these are the only messages that have been sent):\n7/21/20229:29:47PM\nKhuldraeseth: Which is heavier, a pound of feathers or a pound of gold?\n7/21/20229:30:18PM\nslime: pound of feathers\n7/21/20229:30:25PM\nKhuldraeseth: yes\n7/21/20229:30:32PM\ndoubledelete: depends. pound as in lb or currency\n7/21/20229:31:06PM\nKhuldraeseth: weight\n7/21/20229:31:52PM\nKhuldraeseth: Which is heavier, an ounce of feathers or an ounce of gold?\n7/21/20229:33:43PM\nslime: going to use my phone a friend\n7/21/20229:34:07PM\nKhuldraeseth: what is \"friend\"? this is CS\n7/21/20229:34:27PM\nslime: i don’t know it’s the last lifeline i have left\n7/21/20229:36:41PM\ndoubledelete: friend class is the only friend i have\n7/21/20229:37:32PM\niamr: An ounce of gold weighs more on the scales of society\n7/21/20229:38:02PM\nKhuldraeseth: also more on the scale in my bathroom\n7/21/20229:39:39PM\nKhuldraeseth: The key is that gold is measured in pounds Troy and ounces Troy, which are respectively less\nand greater than their Avoirdupois counterparts\nIn this example:\nget_highest_ranked_user() \"Khuldraeseth\",• would return since this is the user with the highest word count among\nthese messages.\nget_word_count(\"doubledelete\") \"doubledelete\"• wouldreturn15,sincethatisthetotalnumberofwordssentbyuser\n(7 words in first message at 9:30:32 PM, 8 words in second message at 9:36:41 PM).\nget_user_range(1, 30) [\"slime\", \"doubledelete\", \"iamr\"]• would return (in any order), since this is the list of\nusers whose total word count falls in the range [1, 30].", "word_count": 683, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0a8e8073-2e28-5d56-a8c9-2da6dd19b214", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 663, "real_page_number": null, "text": "18.9 The STL Set Container\n651\nThere is a lot to digest with this problem, so let us break it down into smaller components. First, we will need a way to keep track of the word\ncount of each user on the server. This can be done using a lookup container that maps each person’s username to their total word count. Since\nstd::unordered_map<>the order of usernames does not matter in determining word count, an would suffice in storing this data. Every\ntime a new message is posted in the server, we count the number of words in the string and add it to the total word count of its user. This allows\nget_word_count()us to implement the function by simply querying the map, as shown:\n1\nclass DiscordRankManager {\n2\nprivate:\n3\nint32_t>std::unordered_map<std::string, user_to_word_count_map;\n4\npublic:\n5\nDiscordRankManager() {}\n6\n7\nvoid post_message(const conststd::string& user, std::string& message) {\n8\n// get word count of message\n9\nstd::stringstream msg_stream(message);\n10\nstd::string word;\n11\nint32_t msg_word_count = 0;\n12\nwhile (msg_stream >> word) {\n13\n++msg_word_count;\n14\n} // while\n15\n// add word count to map\n16\nuser_to_word_count_map[user] += msg_word_count;\n17\n} // post_message()\n18\n19\nint32_t get_word_count(const std::string& user) {\n20\nauto it = user_to_word_count_map.find(user);\n21\nreturn it == user_to_word_count_map.end() ? 0 : it->second;\n22\n} // get_word_count()\n23\n};\nget_user_range()However, the method makes things a bit trickier. Notice that this method requires us to identify users based on word\nstd::unordered_map<>count values. Since our previous maps from username to word count, it will not help us here; instead, we need a\nway to map from word count back to username. Therefore, we will use another lookup container to support this mapping. In this case, the word\ncount values should be stored in sorted order, as that allows us to more efficiently identify all word counts that fall into a given range. As a\nstd::map<> get_user_range()result, we will use a to map from word count to user. A updated solution with implemented is shown\nstd::map<>below; note that we will need to update the whenever a new message is posted, as the word count for a given user would change.\n1\nclass DiscordRankManager {\n2\nprivate:\n3\nint32_t>std::unordered_map<std::string, user_to_word_count_map;\n4\nstd::map<int32_t, std::string> word_count_to_user_map;\n5\npublic:\n6\nDiscordRankManager() {}\n7\n8\nvoid post_message(const conststd::string& user, std::string& message) {\n9\n// get word count of message\n10\nstd::stringstream msg_stream(message);\n11\nstd::string word;\n12\nint32_t msg_word_count = 0;\n13\nwhile (msg_stream >> word) {\n14\n++msg_word_count;\n15\n} // while\n16\n// get old word count of user, and remove old value from corresponding map\n17\nint32_t& total_word_count = user_to_word_count_map[user];\n18\nword_count_to_user_map.erase(total_word_count);\n19\n// get new word count by adding in the new message, and add it into map\n20\ntotal_word_count += msg_word_count;\n21\nword_count_to_user_map[total_word_count] = user;\n22\n} // post_message()\n23\n24\nint32_t get_word_count(const std::string& user) {\n25\nauto it = user_to_word_count_map.find(user);\n26\nreturn it == user_to_word_count_map.end() ? 0 : it->second;\n27\n} // get_word_count()\n28\n29\nget_user_range(int32_t int32_tstd::vector<std::string> min_word_count, max_word_count) {\n30\nauto lower_bound = word_count_to_user_map.lower_bound(min_word_count);\n31\nauto upper_bound = word_count_to_user_map.upper_bound(max_word_count);\n32\n// return vector with all contents between lower_bound and upper_bound iterators\n33\nstd::vector<std::string> users;\n34\nfor (auto it = lower_bound; it != upper_bound; ++it) {\n35\nusers.push_back(it->second);\n36\n} // for it\n37\nreturn users;\n38\n} // get_user_range()\n39\n};", "word_count": 560, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fd732621-b21e-5986-8178-8c0fc774e9d5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 664, "real_page_number": null, "text": "652\nChapter 18. Trees\nThis implementation is not fully correct, however, as it incorrectly assumes that the word counts of all the users are distinct. However, multiple\nusers may have the same word count! Thus, instead of mapping each word count to a single user, we have to map each word count to a container\nof users. What type of container should be use? In this problem, the ordering of users with the same word count does matter (as we will see\nget_highest_ranked_user()when we implement the method), so our container should ideally be sorted as well. Since we can safely\nstd::set<>assume that all usernames are distinct, we can use a to store all users that share the same word count. This change is reflected\nbelow (differences on line 4, 18-23, 26, and 39-42):\n1\nclass DiscordRankManager {\n2\nprivate:\n3\nint32_t>std::unordered_map<std::string, user_to_word_count_map;\n4\nstd::map<int32_t, std::set<std::string>> word_count_to_user_map;\n5\npublic:\n6\nDiscordRankManager() {}\n7\n8\nvoid post_message(const conststd::string& user, std::string& message) {\n9\n// get word count of message\n10\nstd::stringstream msg_stream(message);\n11\nstd::string word;\n12\nint32_t msg_word_count = 0;\n13\nwhile (msg_stream >> word) {\n14\n++msg_word_count;\n15\n} // while\n16\n// get old word count of user, and remove user's old entry from map\n17\nint32_t& total_word_count = user_to_word_count_map[user];\n18\nstd::set<std::string>& users_with_word_count =\nword_count_to_user_map[total_word_count];\n19\nusers_with_word_count.erase(user);\n20\n// if no user has this word count anymore, remove the word count from the map\n21\nif (users_with_word_count.empty()) {\n22\nword_count_to_user_map.erase(total_word_count);\n23\n} // if\n24\n// get new word count by adding in the new message, and add it into map\n25\ntotal_word_count += msg_word_count;\n26\nword_count_to_user_map[total_word_count].insert(user);\n27\n} // post_message()\n28\n29\nint32_t get_word_count(const std::string& user) {\n30\nauto it = user_to_word_count_map.find(user);\n31\nreturn it == user_to_word_count_map.end() ? 0 : it->second;\n32\n} // get_word_count()\n33\n34\nget_user_range(int32_t int32_tstd::vector<std::string> min_word_count, max_word_count) {\n35\nauto lower_bound = word_count_to_user_map.lower_bound(min_word_count);\n36\nauto upper_bound = word_count_to_user_map.upper_bound(max_word_count);\n37\n// return vector with all contents between lower_bound and upper_bound iterators\n38\nstd::vector<std::string> users;\n39\nfor (auto it = lower_bound; it != upper_bound; ++it) {\n40\nconst std::set<std::string>& user_set = it->second;\n41\nusers.insert(users.end(), user_set.begin(), user_set.end());\n42\n} // for it\n43\nreturn users;\n44\n} // get_user_range()\n45\n};\nSince the map is sorted by word count, we can identify the highest ranked user by finding the user associated with the largest key in the map. If\nthere are multiple users with this largest word count, we would return the first user in the corresponding set (since it is sorted lexicographically).\nget_highest_ranked_user()An implementation of the method is shown below:\n1\nclass DiscordRankManager {\n2\nprivate:\n3\nint32_t>std::unordered_map<std::string, user_to_word_count_map;\n4\nstd::map<int32_t, std::set<std::string>> word_count_to_user_map;\n5\npublic:\n6\nDiscordRankManager() {}\n7\n8\n/* ALL OTHER METHODS ARE THE SAME AS ABOVE */\n9\n10\nstd::string get_highest_ranked_user() {\n11\nif (user_to_word_count_map.empty()) {\n12\nreturn \"\";\n13\n} // if\n14\nauto word_count_it = word_count_to_user_map.rbegin();\n15\nconst std::set<std::string>& user_set = word_count_it->second;\n16\nreturn *user_set.begin();\n17\n} // get_highest_ranked_user()\n18\n};", "word_count": 496, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "652543f6-59c6-5b64-8f63-bceb96c0a568", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 665, "real_page_number": null, "text": "18.10 The STL Multimap and Multiset Containers\n653\n18.10\nThe STL Multimap and Multiset Containers (✽)\nstd::multimap<> std::multiset<> <map> <set>The STL also provides the and containers (located in the and libraries, respec-\ntively), which are maps and sets that support duplicate keys. Both of these containers are implemented internally as self-balancing binary search\ntrees that support duplicate elements. To iterate through all elements in a multimap or multiset that match a given key, you can use the iterator\n.equal_range()range returned by the member function.\ntemplate <typename typenameK, V>\nV>::equal_range(conststd::pair<iterator, iterator> std::multimap<K, K& key);\nstd::multiset<K>::equal_range(conststd::pair<iterator, iterator> K& key);\nReturns an iterator range containing all elements in the container with the given key. Like with other STL algorithms, the first iterator\nreturned is inclusive, and the second iterator returned is exclusive. Since multimaps and multisets are sorted, the first iterator points to the\nkey, key.first element not less than and the second iterator points to the first element greater than\nYou will not need to know how to use these two containers in this class, but it is good to recognize that they exist. Because multimaps and\nmultisets store their data in sorted order, they support one key feature that is supported by unordered multimaps and unordered multisets:not\nthe order of elements with identical keys is determined by the insertion.order of\nFor example, consider the following code:\n1\nint32_t>std::multimap<std::string, classes;\n2\nclasses.insert({\"EECS\", 280});\n3\nclasses.insert({\"EECS\", 183});\n4\nclasses.insert({\"EECS\", 376});\n5\nclasses.insert({\"EECS\", 281});\n6\nclasses.insert({\"EECS\", 370});\n7\nclasses.insert({\"EECS\", 203});\n8\n9\nauto iter_range = classes.equal_range(\"EECS\");\n10\nfor (auto it = iter_range.first; it != iter_range.second; ++it) {\n11\nstd::cout << it->first << \" \" << it->second << '\\n';\n12\n} // for it\nThe output of this code is:\nEECS 280\nEECS 183\nEECS 376\nEECS 281\nEECS 370\nEECS 203\nBecause we inserted the key \"EECS\" into the multimap with the value 280 first, we also ended up visiting this key-value pair first. The same\napplies for the remaining elements; the order of elements with identical keys in the multimap matches the insertion order of these elements.\nThis is because duplicate elements are always inserted to the of any existing node with the same key in the underlying binary search tree,right\nand thus will always be visited later than keys that have already been inserted so far.\nAlthough you do not need to know them, multimaps and multisets can still be used to elegantly solve different types of problems. For\ninstance, consider the streaming median problem that we covered at the end of chapter 10. In that problem, we were given a stream of data,\nand we wanted to calculate the median of all values we have seen so far at any point in the stream. We previously solved this problem using\na two-heap approach, where one heap stored all values in the smaller half of values seen, and the other heap stored all values in the larger\nhalf of values seen. Although this solution works well, it is actually possible to write a cleaner solution using multisets that exhibits the same\nasymptotic performance as the two-heap solution.\nstd::multiset<>Behind the scenes, a is simply a self-balancing binary search tree that supports duplicate keys. We can use this\ncontainer to keep track of the streaming median with just one iterator that either points to the median element (if the tree size is odd) or the first\nof the middle two values (if the tree size is even). The algorithm is summarized as follows:\nstd::multiset<> (std::multiset<>::iterator)• First, instantiate a (self-balancing binary search tree) and an iterator that\nkeeps track of the middle element.\nval• Every time you encounter a new data value in the stream, there are three scenarios that can happen:\nval1. The multiset is currently empty. In this case, insert and set the iterator to this new element.\n2. The size of the multiset is currently odd, which means the iterator is currently pointing to the median value.\nval val– If is smaller than the current value of the iterator, the insertion of would cause the current value of the iterator to\nbecome the second of the median values (since the size of the set would become even). We want the iterator to point to the\nfirst of the middle two values, so we must decrement the iterator after the insertion.\nval val– If is larger than (or equal to) the current value of the iterator, the insertion of would cause the iterator to point to the\nfirst of the middle two values. We want the iterator to point to the first of the middle two values, so the iterator does not need\nto be modified after the insertion.\n3. The size of the multiset is currently even, which means the iterator is currently pointing to the first of the middle two values.\nval val– If is smaller than the current value of the iterator, the insertion of would cause the current value of the iterator to\nbecome the new median. Thus, the iterator does not need to be modified after the insertion.\nval val– If is larger than (or equal to) the current value of the iterator, the insertion of would cause the iterator to point to the\nvalue directly before the new median. We want the iterator to point to the median, so we must increment the iterator after the\ninsertion.", "word_count": 908, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8d51f693-e387-5008-a0ed-8a3bad3a3d44", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 666, "real_page_number": null, "text": "654\nChapter 18. Trees\nBy following this algorithm, we can calculate the median of a stream of numbers by either dereferencing the iterator if size is odd, or taking the\naverage of the iterator and the value of its inorder successor if size is even. For example, consider the following stream of values:\n19 12 17 16 14 17 ...\nUsing a self-balancing binary search tree and a single iterator, we can calculate the median of the numbers we have encountered at any point in\nthe stream. We start by inserting the first element into the binary search tree, 19, and setting the iterator to this value.\n19\nmid\nmidThe size of the tree is odd, so points to the current streaming median.\nmid. midThe next value in the stream is 12, which is smaller than the current value of Since 12 < and the size of the tree is odd before the\nmid midinsertion, we decrement (since multisets are sorted, now points to the inorder predecessor of 19).\n19\n12\nmid\nmidThe size of the tree is even, so points to the first of the middle two values. Thus, the streaming median is now (12 + 19) / 2 = 15.5.\n≥midmid.The next value in the stream is 17, which is larger than the current value of Since 17 and the size of the tree is even before insertion,\nmidwe increment from 12 to 17.\n17\n19\n12\nmid\nmidThe size of the tree is odd, so points to the median value. Thus, the streaming median is now 17.\nmid. midThe next value in the stream is 16, which is smaller than the current value of Since 16 < and the size of the tree is odd before\ninsertion, we decrement the iterator from 17 to 16.\n17\n19\n12\n16\nmid\nmidThe size of the tree is even, so points to the first of the middle two values. Thus, the streaming median is now (16 + 17) / 2 = 16.5.\nmid. midThe next value in the stream is 14, which is smaller than the current value of Since 14 < and the size of the tree is even before\nmid. midinsertion, we do not need to move This is because the value of (which used to point to the first of the middle two values) has now\nbecome the median value after the insertion of a value smaller than it.\n17\n19\n14\n16\n12\nmid\nmidThe size of the tree is odd, so points to the median value. Thus, the streaming median is now 16.\n≥midmid.The next value in the stream is 17, which is larger than the current value of Since 17 and the size of the tree is odd before insertion,\nmid. midwe do not need to move This is because the value of (which used to point to the median) has now become the first of the middle two\nvalues after the insertion of a value larger than it.\n17\n19\n17\n14\n16\n12\nmid\nmidThe size of the tree is even, so points to the first of the middle two values. Thus, the streaming median is now (16 + 17) / 2 = 16.5.", "word_count": 550, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e75ed67e-ddd4-52eb-8f9a-a6ed52c134fa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 667, "real_page_number": null, "text": "18.11 Tries\n655\nstd::multiset<>This procedure can be repeated for the remaining elements in the stream. Since a is always balanced, the time complexity\nmidof inserting an element or moving the iterator is bounded by in the worst case. This matches the time complexity of the two-heapΘ(log(𝑛))\nsolution discussed at the end of chapter 10. An implementation of this algorithm using a multiset is shown below, within a median finder class:\n1\nclass MedianFinder {\n2\nprivate:\n3\nstd::multiset<int32_t> data;\n// self-balancing BST\n4\nstd::multiset<int32_t>::iterator mid;\n5\npublic:\n6\nMedianFinder() : mid{data.end()} {}\n7\n8\n// insert a new value \"val\" into the collection of values seen so far\n9\nvoid add_value(int32_t val) {\n10\nsize_t prev_size = data.size();\n11\ndata.insert(val);\n12\n// if container was empty before insertion, set mid to this element\n13\nif (prev_size == 0) {\n14\nmid = data.begin();\n15\n} // if\n16\n// if container was odd before insertion and val < *mid, decrement mid\n17\nelse if (val < *mid && prev_size % 2 == 1) {\n18\n--mid;\n19\n} // else if\n20\n// if container was even before insertion and val >= *mid, increment mid\n21\nelse if (val >= *mid && prev_size % 2 == 0) {\n22\n++mid;\n23\n} // else if\n24\n} // add_value()\n25\n26\n// returns the median of all values seen so far\n27\n// assumes this does not get called if nothing has been added so far\n28\ndouble find_median() {\n29\n// if container is odd, mid points to the median\n30\n// if container is even, mid points to the first of the middle two values\n31\n// std::next() returns an iterator pointing to ++mid (see chapter 11)\n32\nreturn static_cast<double>(*middata.size() % 2 == 1 ? *mid : + *std::next(mid)) / 2;\n33\n} // find_median()\n34\n};\n18.11\nTries (✽)\n¸ 18.11.1\n(✽)Trie Structure\nIn this section, we will discuss the trie data structure (short for tree, but commonly pronounced as \"try\" to distinguish it from the wordretrieval\n\"tree\"). Tries are also known as trees. Tries aren’t required knowledge for this class, but they are included here because they occasionallyprefix\nshow up during coding interviews.\nA trie is a tree-like data structure that stores characters at each node. Each branch down the tree can be used to represent a word, where\nwords that share a common prefix share the same ancestor node in the tree. Tries can be used to organize a set of strings and perform fast\nlookups to determine if a string exists in the set. Tries are also helpful for conducting quick prefix lookups, and they play an important role in\nalgorithms ranging across many different fields, from search engine development to genome analysis. An example of a trie is shown below:\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*", "word_count": 571, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "54766dec-b296-5b3f-b6a0-d552fbdf7bcb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 668, "real_page_number": null, "text": "656\nChapter 18. Trees\nThe * nodes are used to indicate a complete word. For example, the branch\nA\nP\nP\n*\nrepresents the word \"app\". On the other hand, the branch\nA\nP\nP\nL\nE\n*\nrepresents the word \"apple\". Because \"app\" and \"apple\" share a similar prefix, they also share ancestor nodes in the trie.\nHow can we implement a trie? One implementation is shown below. First, we will define an object representing each node of the trie. If the trie\ncan only store the 26 letters of the alphabet, we can store the children of each node in an array of size 26.\n1\nconstexpr size_t ALPHABET_SIZE = 26;\n2\n3\nstruct TrieNode {\n4\n{nullptr};TrieNode* children[ALPHABET_SIZE] =\n5\nbool false;is_end_of_word =\n6\n~TrieNode();\n7\n};\ncharHowever, we will implement a trie that can work beyond the 26 letters of the English alphabet. Since a can take on 128 different ASCII\nvalues, we will instead store each node’s children in an array of size 128. (Note: You can also store the children in an associative container,\nstd::unordered_map<> char TrieNode*such as an that maps a to a associated with that character. However, that approach is more\nmemory intensive since hash tables require more memory than arrays.)\n1\nconstexpr size_t CHAR_SIZE = 128;\n2\n3\nstruct TrieNode {\n4\n{nullptr};TrieNode* children[CHAR_SIZE] =\n5\nbool false;is_end_of_word =\n6\n~TrieNode();\n7\n};\nTrieNode trueAs shown previously, each also stores an \"end of word\" Boolean that is set to if the node represents the end of a word (i.e.,\nthey perform the functionality of the * nodes in the above illustration). Since our trie will manage dynamic memory, we also define a destructor\nTrieNode,for each which iterates through the map and frees the memory associated with each of the children. The implementation of the\ndestructor is shown below.\n1\nTrieNode::~TrieNode() {\n2\nfor (TrieNode* child : children) {\n3\ndelete child;\n4\nnullptr;child =\n5\n} // for child\n6\n} // ~TrieNode()\n¸ 18.11.2\n(✽)Inserting into a Trie\nOur trie should be able to support word insertion, search, and removal. To insert a string into the trie, we traverse through each character of the\nstring and walk down the trie until we encounter a letter that is present in the string but not in the corresponding position in the trie. When this\nhappens, we allocate a new trie node at that position and build out the branch for the new string (appending a terminating * node at the end). For\nexample, suppose we wanted to insert the string \"bean\" into the following trie:\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*", "word_count": 540, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c5e114b8-7b3e-529d-9fef-5a3a435476af", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 669, "real_page_number": null, "text": "18.11 Tries\n657\nWe start off by traversing through the characters of \"bean\" and walking down the trie. The first character is \"B\", so we check to see if \"B\" is a\nchild of the root. It is, so we walk down to this node.\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*\nThe next character of \"bean\" is \"E\", so we check to see if \"E\" is a child of \"B\". It is, so we walk down to this node.\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*\nThe next character of \"bean\" is \"A\", so we check to see if \"A\" is a child of \"E\". It is, so we walk down to this node.\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*", "word_count": 392, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d6627f94-2fed-510c-a33e-e59507aaf296", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 670, "real_page_number": null, "text": "658\nChapter 18. Trees\nTrieNodeThe next character of \"bean\" is \"N\", so we check to see if \"N\" is a child of \"A\". It is not, so we will have to construct a new with the\nTrieNode std::unordered_map<>value \"N\" at this position. To do so, we will allocate a new for \"N\" and add it to the of the current\nnode \"A\". We would continue adding new nodes for each additional character in the string. However, \"N\" is the last letter of the string \"bean\",\nso we mark it as the end of a word.\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\nN\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*\nThe string has been successfully inserted into the trie. The code for the insert operation is shown below:\n1\nclass Trie {\n2\npublic:\n3\nvoid insert(const std::string& word);\n4\nbool contains(const const;std::string& word)\n5\nbool remove(const std::string& word);\n6\nvoid clear();\n7\nsize_t const;size()\n8\n~Trie();\n9\nprivate:\n10\nnullptr;TrieNode* root =\n11\nsize_t sz = 0;\n12\nbool size_thas_valid_children(TrieNode* nodes[], size);\n13\n};\n14\n15\nvoid Trie::insert(const std::string& word) {\n16\nif nullptr)(root == {\n17\nnewroot = TrieNode;\n18\n} // if\n19\nTrieNode* curr = root;\n20\nfor (size_t i = 0; i < word.size(); ++i) {\n21\nif nullptr)(curr->children[word[i]] == {\n22\nnewcurr->children[word[i]] = TrieNode;\n23\n} // if\n24\ncurr = curr->children[word[i]];\n25\n} // for i\n26\nif (!curr->is_end_of_word) {\n27\ntrue;curr->is_end_of_word =\n28\n++sz;\n29\n} // if\n30\n} // insert()\nOn line 16, we first check to see if the trie is currently empty. If it is, we create the root node on line 17. Then, we iterate through the characters\n(word) forof the string we want to insert in the loop on line 20. For each character, we check to see if it already exists as a child of the\ncurrent node. If the character does not exist as a child, we add it on line 22. Then, we walk down the trie by visiting the node corresponding\nto the next character on line 24. Lastly, after we visit the last character of the string, we mark its node as the final character by setting its\nis_end_of_word trueBoolean to on line 27. The overall time complexity of this operation is Θ(𝑘), where 𝑘is the length of the string that\nis inserted. However, since the length of a string is typically a small constant, we often describe trie insertion as a constant time operation.", "word_count": 511, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "53984b4d-725e-5575-93bc-019556278b29", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 671, "real_page_number": null, "text": "18.11 Tries\n659\n¸ 18.11.3\n(✽)Searching in a Trie\nTo check if a string exists in a trie, we can follow a similar process. Similar to insert, we would traverse through the given string and walk down\nthe trie. However, if at any point we encounter a character in the string that does not exist at the current position of the trie, we can immediately\nconclude that the string does not exist in the trie. If we are able to walk down all the characters of the string without encountering any missing\nis_end_of_wordletters, we will use the value of the Boolean of the final letter to determine if the string itself actually exists in the trie. The\ncode for determining if a string exists in the trie is shown below:\n1\nbool Trie::contains(const conststd::string& word) {\n2\nTrieNode* curr = root;\n3\nif nullptr)(curr == {\n4\nreturn false;\n5\n} // if\n6\nfor (size_t i = 0; i < word.size(); ++i) {\n7\nif nullptr)(curr->children[word[i]] == {\n8\nreturn false;\n9\n} // if\n10\ncurr = curr->children[word[i]];\n11\n} // for i\n12\nreturn curr->is_end_of_word;\n13\n} // contains()\nThe time complexity of checking whether a string exists in the trie is worst-case Θ(𝑘), where 𝑘is length of the string to check. Once again,\nassuming that strings have constant size, we can treat the search process as something that takes constant time.\n¸ 18.11.4\n(✽)Removing from a Trie\nRemoving a string from the trie is slightly trickier. For example, suppose we wanted to remove \"bank\" from the following trie:\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\nN\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*\nWe cannot just remove the characters associated with \"bank\", since other strings in the trie may rely on it (e.g., \"banker\" and \"bankrupt\"). In this\ncase, the correct behavior would be to remove the * node associated with the \"K\" in \"bank\" (shaded above):\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\nN\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*", "word_count": 509, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ce7ce5c1-441c-5e42-9365-bc3d22d1dad3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 672, "real_page_number": null, "text": "660\nChapter 18. Trees\nHowever, if we wanted to remove the word \"appearance\" from the following trie, we would be able to delete the last four letters from the trie\n(shaded below), since no other words depend on them.\nB\nE\nT\nT\nE\nR\n*\n*\nA\nR\nD\n*\n*\nN\n*\n*\nA\nT\nT\nL\nE\n*\nE\nR\nY\n*\n*\nH\n*\n*\nN\nK\nR\nU\nP\nT\n*\nE\nR\n*\nD\n*\nA\nN\nA\n*\n*\nA\nP\nP\nL\nY\n*\nE\n*\nE\nA\nR\nA\nN\nC\nE\n*\n*\nA\nR\nE\nN\nT\nL\nY\n*\n*\nL\n*\n*\nE\nX\n*\nR\nT\nU\nR\nE\n*\n*\nB\nO\nV\nE\n*\nU\nT\n*\nL\nE\n*\n*\nThe basic idea here is that nodes in the trie can only be deallocated if no other words depend on them (i.e., they have no children). Thus, in our\ndeletion algorithm, we would walk down the branch of the word we want to delete (and verify that the word actually exists in the trie), set the\nis_end_of_word false,value of the final character to and then delete any nodes along the branch that have no children (starting from the\nbottom of the trie and moving upward). The code is shown below (the Boolean returned indicates if the removal was successful):\n1\nbool Trie::remove(const std::string& word) {\n2\nTrieNode* curr = root;\n3\nif nullptr)(curr == {\n4\nreturn false;\n5\n} // if\n6\nnullptr);std::vector<TrieNode*> nodes_of_word(word.size(),\n7\nfor (size_t i = 0; i < word.size(); ++i) {\n8\nTrieNode* child = curr->children[word[i]];\n9\nif nullptr)(child == {\n10\nreturn false;\n11\n} // if\n12\nnodes_of_word[i] = child;\n13\ncurr = child;\n14\n} // for i\n15\nif (!curr->is_end_of_word) {\n16\nreturn false;\n17\n} // if\n18\nfalse;curr->is_end_of_word =\n19\n--sz;\n20\nnullptr;TrieNode* node_ptr =\n21\nfor (size_t i = word.size() - 1; i > 0; --i) {\n22\nnode_ptr = nodes_of_word[i];\n23\nif (!node_ptr->is_end_of_word && !has_valid_children(node_ptr->children, CHAR_SIZE)) {\n24\ndelete node_ptr;\n25\nnullptr;node_ptr =\n26\n} // if\n27\nelse {\n28\nreturn false;\n29\n} // else\n30\nnullptr;nodes_of_word[i - 1]->children[word[i]] =\n31\n} // for i\n32\nnode_ptr = nodes_of_word[0];\n33\nif (!node_ptr->is_end_of_word && !has_valid_children(node_ptr->children, CHAR_SIZE)) {\n34\ndelete node_ptr;\n35\nnullptr;node_ptr =\n36\nnullptr;root->children[word[0]] =\n37\n} // if\n38\nreturn true;\n39\n} // remove()\n40\n41\nbool size_tTrie::has_valid_children(TrieNode* nodes[], size) {\n42\nfor (size_t i = 0; i < size; ++i) {\n43\nif nullptr)(nodes[i] != {\n44\nreturn true;\n45\n} // if\n46\n} // for i\n47\nreturn false;\n48\n} // has_valid_children()", "word_count": 462, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "040d6c59-7294-5115-92f3-4d7ff4820465", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 673, "real_page_number": null, "text": "18.11 Tries\n661\nIn the code for deletion, we store a vector of parent pointers so that we can walk back up the trie and deallocate memory for nodes that are no\nlonger needed. On lines 10 and 16, we exit early if the string we want to delete does not exist in the trie. If the string does exist, we set its\nis_end_of_word falseBoolean to (line 18) and decrement the size counter (line 19). Then, we walk back up the tree and deallocate\nnodes that have no children and are not the end of an existing word (lines 20 to 39). Similar to search, the time complexity of removing a string\nfrom a trie is worst-case Θ(𝑘), where 𝑘is length of the string to check. If we assume that strings have constant size, we can treat the removal\nprocess as a constant time operation as well.\n¸ 18.11.5\n(✽)Implementing the Trie Destructor\n.clear()The following code implements the remaining functionality of the trie. The function recursively clears the trie in postorder fashion,\nTrieNode*deallocating all of its memory and bringing its size to zero (notice that the entire trie is cleaned up on line 2, since the destructor\n.clear() .size()recursivelydeletesallofitschildren). Thetriedestructorutilizesthe methodtocleanupthetrieafteritisdestructed. The\nfunction, as its name suggests, returns the size of the trie.\n1\nvoid Trie::clear() {\n2\ndelete root;\n3\nnullptr;root =\n4\nsz = 0;\n5\n} // clear()\n6\n7\nsize_t constTrie::size() {\n8\nreturn sz;\n9\n} // size()\n10\n11\nTrie::~Trie() {\n12\nclear();\n13\n} // ~Trie()\nOutside of programming interviews, why should we care about the trie data structure? Tries do have extensive practical applications. For\ninstance, they can be used to develop spell checkers or autocomplete functionality (e.g., when you type something into a search engine, you\nwill likely get a bunch of search suggestions that start with the letters you typed in). This data structure also plays an important role in routing\nsystems. Most routers store IP addresses in an optimized version of a trie — this allows routers to conduct fast IP lookups to determine where\ndata packets should be forwarded. As an example, the router in the following illustration uses a trie to look up the destination IP prefix of an\nincoming data packet, and it uses the result to determine which port to send the data packet out from.\nPort\nDestination IP Prefix Range\n1\n11000000 to 11000011\n2\n11000100 to 11000111\n3\n11001000 to 11001111\n4\n11010000 to 11011111\n110\n1\n1\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n➊➋\n➌\n➍\ndata packet\n192.*\n-\n195.*\n196.*\n-\n199.*\n200.*\n-\n207.*\n208.*\n-\n223.*\n➊\n➍\n➋\n➌", "word_count": 473, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f7e5fbd8-a285-525a-9db5-660634d598d4", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 674, "real_page_number": null, "text": "662\nChapter 18. Trees\nChapter 18 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. What is the worst-case time complexity of searching for an element in an hash table containing 𝑛elements, where separate chaining is used,\nand elements are chained together using an AVL tree? An example of this is depicted in the following figure:\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n2. Consider the following tree:\nF\nG\nI\nH\nB\nD\nE\nC\nA\nWhich of the following nodes is NOT an internal node?\nA) Node B\nB) Node D\nC) Node F\nD) Node I\nE) All of the above are internal nodes\n3. What is the worst-case auxiliary space complexity of implementing a binary tree using an underlying array of size 𝑛?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛2)D)\nΘ(2𝑛)E)\n4. Consider two binary trees, one implemented using an array-based approach and one implemented using a pointer-based approach. Which of\nthe following correctly depicts the worst-case time complexities of inserting an arbitrary value into each binary tree?\nA) Array-based: Θ(1)\nPointer-based: Θ(1)\nB) Array-based: Θ(1)\nPointer-based: Θ(𝑛)\nC) Array-based: Θ(log(𝑛))\nPointer-based: Θ(log(𝑛))\nD) Array-based: Θ(log(𝑛))\nPointer-based: Θ(𝑛)\nE) Array-based: Θ(𝑛)\nPointer-based: Θ(𝑛)\n5. If you used an array to represent a binary tree, where the root is at index 1 and the left and right children of node 𝑛are 2𝑛and 2𝑛+1\nrespectively, which of the following indices would be filled in a rightward-facing stick (e.g., a tree where there are no left children)?\nA) Index 2\nB) Index 17\nC) Index 31\nD) Index 49\nE) Index 64\n6. Suppose that you are using an array to represent a tree that can have three children: a left child, a middle child, and a right child. The root\nnode has index 1, its children have indices 2, 3, 4, and so on (e.g., node 2’s children would have indices 5, 6, and 7). Which of the following\nindices would be filled in a leftward-facing stick (e.g., every node with children can only have a left child)?\nA) Index 13\nB) Index 26\nC) Index 41\nD) Index 81\nE) Index 82", "word_count": 401, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "464646bf-2520-526f-9963-9ccbfb5a0747", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 675, "real_page_number": null, "text": "18.11 Tries\n663\n7. Suppose that you are using an array to represent a tree that can have three children: a left child, a middle child, and a right child. The root\n3𝑛,node has index 1. You know that all indices of your array are empty with the exception of indices that follow the form where 𝑛is an\ninteger (e.g., 1, 3, 9, 27, 81, etc.). What can you infer about this tree?\nA) This tree is a stick, and every node only has a left child\nB) This tree is a stick, and every node only has a middle child\nC) This tree is a stick, and every node only has a right child\nD) The best-case time complexity of searching this tree for an element is Θ(log(𝑛))\nE) The worst-case time complexity of searching for an element in this tree is Θ(log(𝑛))\n8. Suppose that you want to insert 10 distinct elements into a binary search tree. How many worst-case trees are possible for these 10 elements?\n21 210),Note: the following are the first ten powers of two (from to in case you find them useful: 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024.\nA) 2\nB) 511\nC) 512\nD) 1023\nE) 1024\n9. For which of the following element insertion orders would the resulting binary search tree exhibit a worst-case complexity whenΘ(𝑛)\nsearching for a value?\nA) 51, 13, 38, 41, 49, 46, 47, 22, 53, 8\nB) 53, 8, 51, 13, 22, 49, 47, 38, 41, 46\nC) 51, 53, 8, 46, 49, 38, 41, 13, 47, 22\nD) 8, 13, 46, 41, 47, 53, 51, 38, 22, 49\nE) None of the above\nFor questions 10-13, consider the tree below:\nG\nH\nI\nJ\nL\nK\nE\nF\nB\nC\nD\nA\n10. Node ___ is the inorder predecessor of node G, and node ___ is the inorder successor of node G.\nA) A, L\nB) D, K\nC) E, H\nD) F, H\nE) F, K\n11. Which of the following statements about the tree is FALSE?\nA) Node B is a sibling of node I\nB) Node E is an ancestor of node C\nC) Node K is a descendant of node H\nD) Node L is a child of node J\nE) None of the above\n12. Which of the following statements about the tree is TRUE?\nA) The height of node B is one greater than the height of node A\nB) The height of node F is one greater than the height of node C\nC) The height of node F is equal to the height of node B\nD) The height of node I is equal to the height of node B\nE) More than one of the above\n13. Which of the following is the correct postorder traversal of this tree?\nA) G, E, B, A, C, D, F, H, I, J, K, L\nB) D, A, C, B, F, E, K, L, J, I, H, G\nC) A, D, C, B, F, E, G, K, L, J, I, H\nD) A, D, C, B, F, E, K, L, J, I, H, G\nE) None of the above", "word_count": 536, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "20694202-ef90-56dd-9a53-3f4632a1fbac", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 676, "real_page_number": null, "text": "664\nChapter 18. Trees\n14. Which of the following statements is TRUE?\nA) The inorder successor of the root node can have two children\nB) The array-based binary tree implementation is most efficient for trees where most nodes only have at most one child\nC) The worst-case time complexity of inserting an element into a pointer-based binary tree is better than the worst-case time complexity\nof inserting an element into an array-based one\nD) For a pointer-based implementation of a binary tree with 𝑛nodes, the best-case auxiliary space complexity is equal to the worst-case\nauxiliary space complexity\nE) More than one of the above\n15. A binary tree is defined to be if every node either has zero or two children. A binary tree is defined to be if every depth ofproper complete\nthe tree has the maximum number of nodes possible (with the potential exception of the lowest depth, which must be filled from left to\nright). Suppose you insert the following ten elements into a binary search tree, in the following order:\n16, 44, 37, 51, 41, 30, 66, 69, 64, 13\nWhich of the following statements is TRUE?\nA) The resulting tree is proper and complete\nB) The resulting tree is proper but not complete\nC) The resulting tree is complete but not proper\nD) The resulting tree is neither proper nor complete\nE) Not enough information is given to answer the question\n16. Suppose that you know that a tree has fewer than five nodes. Which of the following CANNOT be true?\nA) The tree is proper and complete\nB) The tree is proper but not complete\nC) The tree is complete but not proper\nD) The tree is neither proper nor complete\nE) None of the above (i.e., all of the above could potentially be true)\n17. You are given the following function that takes in the root of a binary tree:\n1\nvoid foo(Node* root) {\n2\nif (!root) {\n3\nreturn;\n4\n} // if\n5\nvisit(root->value);\n6\nfoo(root->left);\n7\nfoo(root->right);\n8\n} // foo()\nWhat does this function do?\nA) It conducts a preorder traversal\nB) It conducts a postorder traversal\nC) It conducts an inorder traversal\nD) It conducts a level order traversal\nE) None of the above\n18. You are given the following function that takes in the root of a binary tree:\n1\nvoid bar(Node* root) {\n2\nif (!root) {\n3\nreturn;\n4\n} // if\n5\nbar(root->left);\n6\nvisit(root->value);\n7\nbar(root->right);\n8\n} // bar()\nWhat does this function do?\nA) It conducts a preorder traversal\nB) It conducts a postorder traversal\nC) It conducts an inorder traversal\nD) It conducts a level order traversal\nE) None of the above", "word_count": 454, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c21c1fa3-dd07-5076-9d43-de9cb52046ac", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 677, "real_page_number": null, "text": "18.11 Tries\n665\n19. You are given the following function that takes in the root of a binary tree:\n1\nint32_t baz(Node* root) {\n2\nif (!root) {\n3\nreturn 0;\n4\n} // if\n5\nreturn 1 + std::max(baz(root->left), baz(root->right));\n6\n} // baz()\nWhat does this function do?\nA) It returns the number of internal nodes in the tree\nB) It returns the number of leaf nodes in the tree\nC) It returns the number of leaf nodes in the tree, plus one for the root\nD) It returns the height of the tree\nE) None of the above\n20. You are given the following function that takes in the root of a binary tree:\n1\nint32_t qux(Node* root) {\n2\nif (!root) {\n3\nreturn 0;\n4\n} // if\n5\nif (!root->left && !root->right) {\n6\nreturn 0;\n7\n} // if\n8\nreturn 1 + qux(root->left) + qux(root->right);\n9\n} // qux()\nWhat does this function do?\nA) It returns the number of internal nodes in the tree\nB) It returns the number of leaf nodes in the tree\nC) It returns the number of leaf nodes in the tree, plus one for the root\nD) It returns the height of the tree\nE) None of the above\n21. Which of the following statements is/are TRUE?\nI. Only knowing the preorder and postorder traversals of a binary tree is not enough to uniquely identify that binary tree.\nII. Given a binary search tree of integers, the first number in its postorder traversal must be the smallest element in the tree.\nIII. In a binary search tree, the inorder predecessor of the root node cannot have a right child.\nA) I only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n22. Suppose you are given the following preorder and inorder traversals of a binary tree:\nPreorder: 12, 6, 9, 18, 15, 13, 11\nInorder: 6, 18, 9, 12, 13, 15, 11\nWhat is the postorder traversal of this tree?\nA) 9, 18, 6, 11, 13, 15, 12\nB) 9, 18, 6, 13, 11, 15, 12\nC) 18, 9, 6, 12, 13, 11, 15\nD) 18, 9, 6, 13, 11, 15, 12\nE) None of the above\n23. Suppose you are given the following preorder and inorder traversals of a binary tree:\nPreorder: 14, 6, 23, 18, 37, 41, 2, 29\nInorder: 23, 18, 6, 37, 14, 2, 29, 41\nWhat is the postorder traversal of this tree?\nA) 18, 23, 6, 37, 2, 29, 41, 14\nB) 18, 23, 37, 6, 29, 2, 41, 14\nC) 23, 18, 37, 6, 2, 29, 41, 14\nD) 37, 18, 23, 6, 29, 2, 41, 14\nE) None of the above", "word_count": 464, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a8e5951d-5b3b-52a1-aba3-3aab40c902f9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 678, "real_page_number": null, "text": "666\nChapter 18. Trees\n24. Suppose you are given the following postorder and inorder traversals of a binary tree:\nPostorder: 14, 22, 43, 18, 23, 35, 12, 27, 16\nInorder: 22, 14, 23, 43, 18, 16, 12, 35, 27\nWhat is the preorder traversal of this tree?\nA) 14, 43, 22, 18, 23, 16, 27, 35, 12\nB) 14, 43, 22, 18, 35, 23, 12, 27, 16\nC) 16, 23, 22, 14, 18, 43, 27, 12, 35\nD) 16, 23, 22, 18, 14, 43, 27, 12, 35\nE) None of the above\n25. Suppose you are given the following postorder and inorder traversals of a binary tree:\nPostorder: 32, 5, 2, 7, 11, 19, 28, 20\nInorder: 5, 32, 20, 28, 11, 7, 2, 19\nWhat is the preorder traversal of this tree?\nA) 20, 5, 32, 28, 11, 19, 7, 2\nB) 20, 5, 32, 28, 19, 11, 7, 2\nC) 20, 28, 11, 7, 2, 19, 5, 32\nD) 20, 32, 5, 2, 7, 11, 19, 28\nE) None of the above\n26. For a certain binary tree, its inorder traversal is the exact reverse of its postorder traversal. What can you infer about this binary tree?\nA) No node in the tree has a left child (i.e., a rightward-facing stick)\nB) No node in the tree has a right child (i.e., a leftward-facing stick)\nC) Its preorder traversal is the exact reverse of its inorder traversal\nD) Its preorder traversal is the same as its postorder traversal\nE) More than one of the above\n27. Which of the following data structures is most suitable for conducting a level order traversal of a tree?\nA) Hash table\nB) Array\nC) Linked list\nD) Stack\nE) Queue\n28. Suppose you are given the following preorder and inorder traversals of a binary tree:\nPreorder: 15, 24, 10, 33, 9, 16, 21\nInorder: 10, 24, 15, 33, 16, 9, 21\nWhat is the level order traversal of this tree?\nA) 10, 24, 15, 33, 9, 16, 2\nB) 10, 24, 16, 2, 9, 33, 15\nC) 15, 24, 10, 16, 33, 9, 2\nD) 15, 24, 33, 10, 9, 16, 2\nE) None of the above\n29. Suppose you are given the following postorder and inorder traversals of a binary tree:\nPostorder: 62, 47, 11, 25, 42, 34, 50\nInorder: 50, 11, 62, 47, 34, 25, 42\nWhat is the level order traversal of this tree?\nA) 11, 25, 34, 42, 47, 50, 62\nB) 50, 11, 62, 34, 47, 42, 25\nC) 50, 34, 11, 42, 47, 25, 62\nD) 50, 34, 11, 47, 62, 42, 25\nE) None of the above", "word_count": 442, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d613161a-d82c-5c0c-9ec4-bf8ffb22d097", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 679, "real_page_number": null, "text": "18.11 Tries\n667\nFor questions 30-35, consider the tree below:\n63\n11\n72\n35\n14\n73\n31\n38\n46\n24\n97\n45\n80\n42\n27\n20\n85\n19\n77\n30. In this tree, the inorder predecessor of the root node has a value of 𝑎, and the inorder successor of the root has a value of 𝑏. What is the\nvalue of 𝑎+𝑏?\nA) 35\nB) 62\nC) 70\nD) 80\nE) None of the above\n31. In this tree, the first element of a preorder traversal has a value of 𝑐, and the last element of a postorder traversal has a value of 𝑑. What is\nthe value of 𝑐+𝑑?\nA) 94\nB) 108\nC) 126\nD) 140\nE) None of the above\n32. What is the value of the first element in an inorder traversal of this tree?\nA) 63\nB) 77\nC) 85\nD) 97\nE) None of the above\n33. What is the value of the third to last element in an inorder traversal of this tree (i.e., [ , __, X, __, __ ])?…\nA) 14\nB) 31\nC) 35\nD) 73\nE) None of the above\n34. The root node of 63 is the _____ value in an inorder traversal of this tree.\n1stA)\n10thB)\n11thC)\n19thD) (last)\nE) None of the above\n35. In which of the following traversals does the value of 24 (left child of root) occur at the latest position?\nA) Preorder\nB) Postorder\nC) Inorder\nD) Level order\nE) More than one of the above (i.e., there is a tie for the latest occurrence of 24)", "word_count": 272, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2457e52e-1f48-5aa2-b6c3-d6b37d5020bf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 680, "real_page_number": null, "text": "668\nChapter 18. Trees\n36. Consider a tree that satisfies the following conditions:\n1. The tree is a binary search tree of integers.\n2. The number of elements in the root node’s left and right subtrees are the same.\n3. There are no duplicate values in the tree.\n4. The first element of an inorder traversal is 1.\n5. The last element of an inorder traversal is 13.\n6. The last element of a postorder traversal is 5.\nWhat is the largest possible integer you can attain by summing up all the values in a tree that satisfies the above constraints?\nA) 61\nB) 70\nC) 82\nD) 91\nE) None of the above\n37. In a binary tree with 12 nodes, what is the smallest possible number of leaf nodes that can exist?\nA) 1\nB) 2\nC) 3\nD) 6\nE) None of the above\n38. In a binary tree with 281 nodes, how many of the nodes are leaves?complete\nA) 35\nB) 70\nC) 140\nD) 141\nE) None of the above\n39. Suppose you want to insert the value 19 into a binary search tree. Which of the following is an sequence of nodes that could beinvalid\nvisited during the insertion?\nA) 23, 14, 16, 18, 21, 20\nB) 15, 16, 20, 17, 21, 18\nC) 33, 30, 27, 24, 21, 18\nD) 1, 4, 36, 5, 34, 17, 18\nE) None of the above\n40. Given a binary search tree of size 𝑛, what is the worst-case time complexity of finding the inorder successor of the root node, if you use the\nmost efficient algorithm?\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n41. Which of the following correctly summarizes the worst-case time complexities of searching for a value in a binary tree versus a binary\nsearch tree, each of size 𝑛?\nA) Binary tree: Θ(log(𝑛))\nBinary search tree: Θ(log(𝑛))\nB) Binary tree: Θ(log(𝑛))\nBinary search tree: Θ(𝑛)\nC) Binary tree: Θ(𝑛)\nBinary search tree: Θ(log(𝑛))\nD) Binary tree: Θ(𝑛)\nBinary search tree: Θ(𝑛)\nE) None of the above\n42. Given a tree with 𝑛nodes, where each node can have at most 𝑘children, what is the worst-case auxiliary space that could be required to\nimplement this tree using an array-based approach?\nA) Θ(𝑛)\nB) Θ(𝑛𝑘)\nC) Θ(𝑘log(𝑛))\nΘ(𝑛𝑘)D)\nΘ(𝑘𝑛)E)\n43. Given a binary search tree with 𝑛nodes and height ℎ, what is the worst-case time complexity of searching for a value, in terms of 𝑛and ℎ?\nA) Θ(log(ℎ))\nB) Θ(log(𝑛))\nC) Θ(𝑛log(ℎ))\nD) Θ(ℎ)\nE) None of the above", "word_count": 430, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "194e4f09-8cbe-5362-9dd1-3f289f991e2b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 681, "real_page_number": null, "text": "18.11 Tries\n669\n44. In a complete binary tree of size 𝑛, the maximum number of nodes in the tree that could have exactly one child is _____ if 𝑛is odd, and\n_____ if 𝑛is even.\nA) 0, 0\nB) 0, 1\nC) 1, 0\n⌊𝑛∕2⌋,D) 𝑛∕2\n⌈𝑛∕2⌉,E) 𝑛∕2\n45. You are given a with the following preorder traversal:binary search tree\nPreorder: 5, 3, 1, 2, 4, 7, 6\nWhat is the postorder traversal of this tree?\nA) 2, 1, 4, 3, 6, 7, 5\nB) 2, 1, 4, 3, 7, 6, 5\nC) 1, 2, 4, 3, 6, 7, 5\nD) 1, 2, 4, 3, 7, 6, 5\nE) Not enough information is provided to answer this question\n46. True or false? The following tree is balanced.\n4\n5\n6\n7\n3\n2\n1\nA) True\nB) False\n47. Which of the following statements about AVL trees is TRUE?\nA) To balance any tree with a node that has a negative balance factor, one should conduct a single rightward rotation about this node\nB) To balance any tree with a node that has a negative balance factor, one should conduct a single leftward rotation about this node\nC) A tree with more elements on its right side than its left side will have a positive balance factor\nD) Both A and B\nE) None of the above\n48. You are given an array of unsorted elements, and you want to sort these elements and print them out in sorted order. What is the worst-case\ntime complexity of doing this if you push all the elements into an AVL tree and then perform an inorder traversal?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛2)D)\nΘ(𝑛2E) log(𝑛))\n𝑛43𝑛elements,49. What is the worst-case time complexity of searching for an element in an AVL tree with in terms of 𝑛?\nA) Θ(log(𝑛))\nB) Θ(𝑛)\nC) Θ(𝑛log(𝑛))\nΘ(𝑛4)D)\nΘ(3𝑛)E)\n50. For which of the following operations is a triple rotation possible?\nA) Inserting an element into an AVL tree containing 31 nodes\nB) Inserting an element into an AVL tree containing 32 nodes\nC) Inserting an element into an AVL tree containing 33 nodes\nD) Both A and B\nE) None of the above", "word_count": 377, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "81c75092-b7e6-5efd-be8a-e155bc5b6189", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 682, "real_page_number": null, "text": "670\nChapter 18. Trees\n51. Consider the following AVL tree:\n29\n34\n40\n45\n31\n32\n30\n24\n28\n22\nWhat does the AVL tree look like after 33 is inserted and all rotations are completed?\nA)\n31\n34\n40\n45\n32\n33\n29\n30\n24\n28\n22\nB)\n32\n34\n40\n45\n33\n29\n31\n30\n24\n28\n22\nC)\n31\n33\n40\n45\n34\n32\n29\n30\n24\n28\n22\nD)\n34\n40\n45\n29\n31\n32\n33\n30\n24\n28\n22\nE)\n33\n40\n45\n34\n29\n31\n32\n30\n24\n28\n22\n52. Insert the following elements into an empty AVL tree, rebalancing when necessary.\n23, 26, 24, 25, 29, 11, 13, 9, 8, 16, 17\nAfter all the elements are inserted, what is the level-order traversal of this tree?\nA) 23, 11, 24, 8, 16, 26, 9, 13, 17, 25, 29\nB) 23, 11, 25, 8, 16, 24, 29, 9, 13, 17, 26\nC) 24, 13, 26, 9, 17, 25, 29, 8, 11, 16, 23\nD) 24, 13, 26, 17, 9, 25, 29, 16, 23, 8, 11\nE) None of the above\n53. Consider the following AVL tree:\n57\n83\n61\n32\n47\n54\n45\n19\n10\nSuppose you deleted 57 from the AVL tree above and rebalanced the tree by replacing the root with the inorder successor. What is the\ntraversal of the resulting tree?level-order\nA) 32, 19, 61, 10, 47, 83, 45, 54\nB) 47, 32, 61, 19, 45, 54, 83, 10\nC) 54, 32, 83, 19, 47, 61, 10, 45\nD) 61, 32, 83, 19, 47, 10, 45, 54\nE) None of the above", "word_count": 273, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "99bb4b3d-6f5f-5ae3-97d0-aa3bdc5eb92b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 683, "real_page_number": null, "text": "18.11 Tries\n671\n54. Consider the following AVL tree:\n57\n83\n61\n32\n47\n54\n45\n19\n10\nSuppose you deleted 57 from the AVL tree above and rebalanced the tree by replacing the root with the inorder predecessor. What is the\ntraversal of the resulting tree?postorder\nA) 10, 19, 32, 45, 47, 54, 61, 83\nB) 10, 19, 45, 47, 32, 61, 83, 54\nC) 10, 45, 19, 47, 61, 32, 83, 54\nD) 45, 54, 10, 47, 83, 19, 61, 32\nE) None of the above\n55. Suppose you have a tree with a height of 5, where the leaf nodes have height 1. Let 𝑎represent the maximum number of nodes in a balanced\nAVL tree of height 5, and let 𝑏represent the minimum number of nodes in a balanced AVL tree of height 5. What is the value of 𝑎−𝑏?\nA) 12\nB) 19\nC) 20\nD) 31\nE) 43\nFor questions 56-57, consider the following stick tree:\nA\nB\nC\nD\nE\n56. You are told to balance this stick. To do this, start from the bottom node and move upwards toward the root, calculate the balance factor,\nand rotate whenever necessary. After the stick is balanced, which node becomes the root node?\nA) A\nB) B\nC) C\nD) D\nE) E\n57. After the stick is balanced, what is the balance factor of the root node?\nA) -2\nB) -1\nC) 0\nD) 1\nE) 2\n58. You are told that a certain AVL tree has the following postorder traversal:\n8, 11, 21, 14, 30, 28, 35, 37, 34, 26\nSuppose you add the values of 9 and 29 to this AVL tree, in this order, and balance the tree accordingly. What is the traversal ofpreorder\nthe resulting tree?\nA) 26, 14, 8, 9, 11, 21, 34, 28, 30, 29, 37, 35\nB) 26, 14, 9, 8, 11, 21, 34, 29, 28, 30, 37, 35\nC) 26, 14, 34, 8, 21, 28, 37, 11, 30, 35, 9, 29\nD) 26, 14, 34, 9, 21, 29, 34, 8, 11, 28, 30, 35\nE) Not enough information is provided to answer this question", "word_count": 359, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4a2ad2f7-3112-582e-ac3f-9f25f5354aa1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 684, "real_page_number": null, "text": "672\nChapter 18. Trees\n59. Given an empty AVL tree, what is the minimum number of rotations required to insert 𝑛values into the AVL tree, provided that you know\nwhat these 𝑛values are beforehand and can choose the insertion order?\nA) 0\nB) 1\nC) log(𝑛)\nD) 𝑛\nE) 𝑛log(𝑛)\n60. Which of the following is a valid AVL tree?\nA)\n30\n33\n40\n28\n32\n23\n27\n15\nB)\n25\n34\n38\n37\n14\n16\n12\n10\nC)\n31\n39\n35\n36\n24\n28\n25\n20\n21\nD)\n29\n34\n38\n41\n37\n33\n26\n19\nE) More than one of the above\n61. You are given the following AVL tree. How many more insertions (at a minimum) will you need if you want the root to have a height of 6,\nassuming that rebalancing is completed after each insertion? The height of a tree consisting of a single node is 1 (so the root of the tree\ncurrently has a height of 4).\n24\n36\n42\n45\n39\n30\n33\n27\n12\n18\n21\n15\n6\n9\n3\nA) 5\nB) 6\nC) 7\nD) 8\nE) 9\n62. How many rotations are needed to perform the following operations on an initially empty AVL tree? Note that a double rotation operation\ncounts as two rotations. For this problem, deletions replace the node to be removed with the inorder successor.\nInsert 17, Insert 11, Insert 15, Insert 14, Insert 12, Insert 16, Insert 13, Delete 15\nA) 3\nB) 4\nC) 5\nD) 6\nE) 7", "word_count": 257, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1f2cd0d9-cde6-5665-a63a-ed1d09dcfc09", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 685, "real_page_number": null, "text": "18.11 Tries\n673\n63. You are given the following AVL tree. What is the maximum number of rotations that can be induced from a single deletion from this tree?\nFor this problem, deletions replace the node to be removed with the inorder successor. A double rotation operation counts as two rotations.\n8\n11\n12\n10\n9\n3\n6\n7\n4\n5\n1\n2\nA) 1\nB) 2\nC) 3\nD) 4\nE) 5\n64. What is the smallest possible size for an AVL tree where the deletion of a single node could trigger seven rotations? A double rotation\noperation counts as two rotations.\nA) 33\nB) 88\nC) 128\nD) 129\nE) It is impossible for a single deletion to ever invoke seven rotations\n65. What is the maximum number of comparisons that could be required when inserting an element into an AVL tree of size 33? Only consider\ncomparisons with other values in the tree to determine the position of insertion (i.e., before any rebalancing takes place).\nA) 4\nB) 5\nC) 6\nD) 7\nE) 8\n66. You want to design an AVL tree that supports duplicate elements and satisfies the following binary search tree invariants at all times:\n• All values to the left of a given node are strictly smaller\n• All values to the right of a given node are greater than or equal\nWhich of the following implementation strategies can guarantee that these invariants will always hold true for all possible inputs?\nI. Upon insertion into the AVL tree, always insert equal values as a new node into the left subtree.\nII. Upon insertion into the AVL tree, always insert equal values as a new node into the right subtree.\nIII. Store an extra counter with each element that represents the number of times it appears in the AVL tree, and have duplicates\nincrement that counter upon insertion.\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) II and III only\n67. Which of the following could be the postorder traversal of a balanced binary search tree?\nI. 11, 12, 14, 13, 17, 16, 15\nII. 11, 13, 12, 16, 15, 17, 14\nIII. 12, 11, 14, 16, 17, 15, 13\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) I, II, and III", "word_count": 392, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b99aef86-5c50-53df-b460-d83359869e5b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 686, "real_page_number": null, "text": "674\nChapter 18. Trees\nstd::map<>68. WhichofthefollowingdatastructuresismostcommonlyusedtoimplementorderedlookupcontainersintheSTL,suchasthe\nstd::set<>?and\nA) Linked list\nB) Hash table\nC) Binary heap\nD) Self-balancing binary search tree\nE) More than one of the above\nT, T operator< operator==69. Suppose you are given an object of custom type where supports and but does not implement a hash\nT,function. Assume no hash function is written for which of the following statements is true?\nT std::map<>, std::unordered_map<>A) can be used as the key type of an but not an\nT std::unordered_map<>, std::map<>B) can be used as the key type of an but not an\nT std::unordered_map<> std::map<>C) can be used as the key type for both and\nT std::unordered_map<> std::map<>D) can be used as the key type for neither nor\nE) None of the above\nstd::map<> K.70. Suppose you wanted toiterate overa and print allkeys ina that arestrictlyless than orequal tosome value Toaccomplish\nthis, you should use _______ to find the point at which to start the traversal and _______ to find the point at which to end the traversal.\nstd::map::begin()A)\nstd::map::upper_bound(K)\nstd::map::upper_bound(K)B)\nstd::map::end()\nstd::map::find(K)C)\nstd::map::begin()\nstd::map::begin()D)\nstd::map::lower_bound(K)\nstd::map::begin()E)\nstd::map::end()\nstd::map<> K.71. Suppose you wanted to iterate over a and print all keys in a that are strictly greater than some value To accomplish this,\nyou should use _______ to find the point at which to start the traversal and _______ to find the point at which to end the traversal.\nstd::map::begin()A)\nstd::map::lower_bound(K)\nstd::map::upper_bound(K)B)\nstd::map::end()\nstd::map::find(K)C)\nstd::map::end()\nstd::map::begin()D)\nstd::map::lower_bound(K)\nstd::map::begin()E)\nstd::map::end()\nstd::map<> std::map<>:72. You are given a of values. The following statements are true about this\nint32_t.• The types of the key and value in this map are both\n{15, 16}• The key-value pair exists in this map.\n.upper_bound(15) {17, 18}.• Calling on this map returns an iterator that points to the key-value pair\nstd::map<>?Which of the following key-value pairs cannot possibly be in this\n{13, 14}A)\n{14, 15}B)\n{16, 15}C)\n{18, 17}D)\nE) More than one of the above\n73. The Project 4 autograder just got released! On the first day, eight students submitted. The scores that each of these students received on\ntheir first submission are shown in the table below.\nStudent ID\n522\n883\n768\n417\n130\n614\n349\n265\nScore\n34.9\n56.7\n21.4\n75.4\n61.1\n35.7\n23.1\n45.7\nstd::map<int32_t, double> P4Scores,Suppose these students are inserted into a named where represents the keyStudent ID\nit = P4Scores.end(), (--it)->first?and represents the value. If what is the value ofScore\nA) 130\nB) 265\nC) 417\nD) 768\nE) 883\n74. Suppose you wanted to print out the student ID with the highest score, 417. Which of the following would successfully accomplish this?\nstd::unordered_map<double, int32_t> P4ScoresA) Inserting student 417 first into a named with as the key andScore\nit P4Scores.begin(), it->secondas the value, setting an iterator to and printingStudent ID\nstd::unordered_map<double, int32_t> P4ScoresB) Inserting student 417 last into a named with as the key andScore\nit P4Scores.end(), (--it)->secondas the value, setting an iterator to and printingStudent ID\nstd::map<double, int32_t> P4ScoresC) Inserting student 417 last into a named with as the key and asScore Student ID\nit P4Scores.begin(), it->secondthe value, setting an iterator to and printing\nstd::map<double, int32_t> P4ScoresD) Inserting student 417 first into a named with as the key and asScore Student ID\nit P4Scores.end(), (--it)->secondthe value, setting an iterator to and printing\nstd::map<double, int32_t> P4ScoresE) Inserting student 417 last into a named with as the key and asScore Student ID\nit P4Scores.end(), it->secondthe value, setting an iterator to and printing", "word_count": 651, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "232af521-95be-52d5-83f0-b5e6f17ccf1f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 687, "real_page_number": null, "text": "18.11 Tries\n675\nstd::unordered_map<>75. Whichofthefollowingcorrectlysummarizestheaverage-casetimecomplexitiesofinserting𝑛elementsintoa\nstd::map<>?versus a\nstd::unordered_map<>:A) Θ(𝑛)\nstd::map<>: Θ(𝑛)\nstd::unordered_map<>:B) Θ(𝑛)\nstd::map<>: Θ(𝑛log(𝑛))\nΘ(𝑛2)std::unordered_map<>:C)\nstd::map<>: Θ(𝑛)\nΘ(𝑛2)std::unordered_map<>:D)\nstd::map<>: Θ(𝑛log(𝑛))\nE) None of the above\n76. You are given a list of distinct integers that are not guaranteed to be in any order. If you want to print these integers out in order, which of\nthe following implementations could potentially give you the worst asymptotic runtime for completing this task?\nA) Inserting each integer into a minimum priority queue one-by-one, and then popping out and printing each value\nstd::sort(), .begin() .end()B) Inserting each integer into a vector one-by-one and calling and then iterating from to\nC) Inserting each integer into a binary search tree one-by-one, and then performing an inorder traversal\nstd::set<> .begin() .end()D) Inserting each integer into a one-by-one, and then iterating from to\nE) All of the methods above share the same worst-case time complexity\nmin_val max_val.77. You are given a binary search tree, as well as two integers, and Implement a function that removes all nodes in the\nmin_val max_valBST whose values are less than or greater than while maintaining the binary search tree property. You may assume\n≤max_val.min_valthat Each node of the tree is defined as follows:\n1\nstruct Node {\n2\nint32_t val;\n3\nNode* left;\n4\nNode* right;\n5\nNode(int32_t left{nullptr}, right{nullptr}val_in) : val{val_in}, {}\n6\n};\nmin_val max_valExample: Given the tree on the left and the values = and = 15, your function should trim the tree so that all−12\nelements are in the range (as shown by the tree on the right):[−12,15]\n8\n17\n18\n15\n9\n-14\n-5\n8\n15\n9\n-5\nint32_t int32_tNode* trim_bst(Node* root, min_val, max_val);\nYour solution should run in time and use auxiliary space, where 𝑛is the number of nodes in the tree.𝑂(𝑛) 𝑂(𝑛)\n78. Given a binary tree, return the values of the rightmost nodes at each level, ordered from top to bottom. Each node is defined as follows:\n1\nstruct Node {\n2\nint32_t val;\n3\nNode* left;\n4\nNode* right;\n5\nNode(int32_t left{nullptr}, right{nullptr}val_in) : val{val_in}, {}\n6\n};\n[1, 3, 4].Example: Given the following tree, you would return\n1\n3\n4\n2\n5\nstd::vector<int32_t> get_rightmost_values(Node* root);\nYour solution should run in time and use auxiliary space, where 𝑛is the number of nodes in the tree.𝑂(𝑛) 𝑂(𝑛)", "word_count": 421, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "356249dc-3384-57d6-b494-10c7f853d7a8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 688, "real_page_number": null, "text": "676\nChapter 18. Trees\n79. You are given a binary tree with 𝑛nodes. Write a function that checks if it is possible to partition the tree into two subtrees that have equal\ntrue,sums after removing exactly edge of the original tree. For example, the following tree would return as removing the edgeone\nmarked with a double slash would split the tree into two smaller subtrees that both have a sum of 16:\nval, *left, *rightEach node of the tree shares the same structure as a node in the previous two problems (with and member variables).\nbool equal_sum_subtree(Node* root);\nYour solution should run in time and use auxiliary space.𝑂(𝑛) 𝑂(𝑛) Hint: trees are recursive structures, so think recursion! It may be\nhelpful to write a helper function that can find the sum of a subtree when given its root.\n80. You are given the root of a binary search tree of integers. Implement a function that converts this binary search tree into a binarycomplete\nval, *left, *rightmin-heap. Each node of the tree shares the same structure as a node in the previous problems (with and members).\nNode* convert_bst_to_min_heap(Node* bst_root);\nRecall that a min-heap is a complete tree (all levels are filled except possibly the last, which must be filled from left to right) such that the\nvalue of each node is less than or equal to the values of its children. For example, given the complete binary search tree on the left, one\nsolution would be to convert it into the binary min-heap on the right:\nBinary Search Tree\n4\n6\n5\n2\n3\n1\nMin-Heap\n1\n5\n6\n2\n4\n3\nYour solution should run in time and use auxiliary space, where 𝑛is the number of nodes in the tree.𝑂(𝑛) 𝑂(𝑛)\n81. You are given two binary trees. Implement a function that merges these two trees together by summing up the nodes that overlap. Return\nthe root node of the combined tree. For example, given the left two trees are input, you would return the tree on the right.\nInput Tree 1\n1\n2\n3\n5\nInput Tree 2\n2\n3\n7\n1\n4\nOutput Tree\n3\n5\n7\n4\n4\n5\nNode* merge_trees(Node* t1, Node* t2);\nYour solution should run in time and use auxiliary space, where 𝑛is the number of nodes in the tree.𝑂(𝑛) 𝑂(𝑛)", "word_count": 395, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6b55dc01-3473-5f76-b30f-65458be7c6f6", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 689, "real_page_number": null, "text": "18.11 Tries\n677\n82. You are given a sorted array. Implement a function that builds a balanced binary search tree from the contents of this array and returns the\nroot node of the newly built tree.\narray_to_bst(const std::vector<int32_t>&Node* nums);\nYour solution should run in time and use auxiliary space, where 𝑛is the size of the input array.𝑂(𝑛) 𝑂(𝑛)\nStockPriceTrackerImplement the following class, which can be used to return the price of a known stock at any point in time. This83.\nclass supports two operations that you will need to implement:\nvoid update(const double int32_tstd::string& symbol, price, timestamp);•\nsymbol price timestamp.– This indicates that the price of the stock with symbol changed to a value of at timestamp\ndouble get_price(const int32_tstd::string& symbol, timestamp);•\nsymbol timestamp. symbol– This returns the price of the stock with symbol at timestamp You may assume that the exists\ntimestamp.and has been updated before at a timestamp less than or equal to\nHere are some example operations that demonstrate this behavior:\n1\nStockPriceTracker spt;\n2\nspt.update(\"NVDA\", 739.00, 1);\n3\nspt.update(\"MSFT\", 409.49, 3);\n4\nspt.update(\"TSLA\", 188.71, 4);\n5\nspt.update(\"NVDA\", 739.07, 3);\n6\nspt.update(\"TSLA\", 188.65, 6);\n7\n8\n// The following prints out the prices of the three stocks at timestamp 5\n9\nstd::cout << spt.get_price(\"NVDA\", 5) << std::endl; // prints 739.07\n10\nstd::cout << spt.get_price(\"MSFT\", 5) << std::endl; // prints 409.49\n11\nstd::cout << spt.get_price(\"TSLA\", 5) << std::endl; // prints 188.71\nAn outline of this class is provided below:\n1\nclass StockPriceTracker {\n2\nprivate:\n3\n// TODO: Add any data structures here!\n4\npublic:\n5\nvoid update(const double int32_tstd::string& symbol, price, timestamp) {\n6\n// TODO: Implement this\n7\n} // update()\n8\n9\ndouble get_price(const int32_tstd::string& symbol, timestamp) {\n10\n// TODO: Implement this\n11\n} // get_price()\n12\n};\nChapter 18 Exercise Solutions\n1. The correct answer is (B). The worst-case time complexity of finding the index an element should be at is for separate chaining.Θ(1)\nAfter finding this index, the worst-case time complexity of finding an element in the attached AVL tree is Θ(log(𝑛)). This is the dominant\ncomplexity, so the time complexity of the overall procedure is also Θ(log(𝑛)).\n2. The correct answer is (E). An internal node has children; all four nodes have children, so they are all internal nodes.\n2𝑛−13. The correct answer is (E). In the case of a rightward-facing stick tree, the last element would be found at index for an array-based\nΘ(2𝑛).binary tree. Thus, the worst-case auxiliary space complexity would be\n4. The correct answer is (E). The worst-case of insert is for both cases. For an array-based tree, we could have to traverse the entireΘ(𝑛)\narray before we find an open position to insert the new element (and if the array was completely full, we would also need to reallocate). The\nsame applies for a pointer based tree; regardless of what algorithm you use to insert a new node into the tree, you could end up visiting\nnodes if you get unlucky and never find an open spot.Θ(𝑛)\n2𝑛nodes,5. The correct answer is (C). The right child of the root is located at index 3. Since each additional row of the binary tree adds we\nwould keep on adding powers of two until we get an answer choice, which happens to be 31. In addition, the rightmost node of any row has\n2𝑛−1,an index of the form which can also be used to get 31.", "word_count": 588, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "24b854fd-c2f9-5b36-9119-b1a4f47be57e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 690, "real_page_number": null, "text": "678\nChapter 18. Trees\n3𝑛nodes6. The correct answer is (C). The left child of the root is located at index 2. Since each additional row of this tree has (as the tree\n3𝑛untilcan have three children), we can continue adding we get an answer of 41. We start with +1 instead of +3 in this case because we\nare dealing with the left child.\n7. The correct answer is (B). This tree only has middle children, as shown below:\n8. The correct answer is (C). Given any set of elements, you can either add the smallest or the largest element to build a worst-case tree.\nFor the first 9 elements, you can select either the smallest or largest value to add, which is 2 choices for 9 decisions. Once you reach the\nfinal value, you only have one remaining element to insert. Thus, the number of ways you can build a worst-case tree using 10 elements is\n29 512.2×2×2×2×2×2×2×2×2×1= =\n9. The correct answer is (B). Every value added to the tree in this sequence is either the smallest or largest of the elements that are remaining:\n• 53 is the largest element in the set of elements 53, 8, 51, 13, 22, 49, 47, 38, 41, 46\n• 8 is the smallest element in the set of elements 8, 51, 13, 22, 49, 47, 38, 41, 46\n• 51 is the largest element in the set of elements 51, 13, 22, 49, 47, 38, 41, 46\n• 13 is the smallest element in the set 13, 22, 49, 47, 38, 41, 46\n• 22 is the smallest element in the set 22, 49, 47, 38, 41, 46\n• 49 is the largest element in the set 49, 47, 38, 41, 46\n• 47 is the largest element in the set 47, 38, 41, 46\n• 38 is the smallest element in the set 38, 41, 46\n• 41 is the smallest element in the set 41, 46\nIf you draw this tree out, you will see that it is a stick, which would result in worst-case search for 46 (or any number bigger than 41 and\nless than 47).\n10. The correct answer is (D). To identify the inorder predecessor, we go left once, and then as far right as possible (i.e., keep on going right\nuntil you reach a node with no right child): this gives us node F. To identify the inorder successor, we go right once and then as far left as\npossible: this gives us node H.\n11. The correct answer is (A). Nodes B and I do not share the same parent, so node B is not a sibling of node I.\n12. The correct answer is (D). If we consider leaf nodes as nodes with a height of 1, choice (A) is false because node F has height 1 (left) and\nnode C has height 2 (since D is its child). Choice (B) is false because node B has height 3 (counting from the farthest leaf that is a child of\nB, B is the 3rd node from the bottom) and node A has height 1 (leaf). Choice (C) is false because node F has height 1 and node B has\nheight 3. Choice (D) is true because nodes B and I both have height 3 (counting from the farthest leaf that is a child of I, I is the 3rd node\nfrom the bottom).\n13. The correct answer is (D). For a postorder traversal, we print out the left child, then the right child, then the parent. Here, we go all the\nway to the very left and print out node A (as it has no children). Then, we go up a level to B. We check if node B has a right child: it does,\nso we visit it, node C. We then check if node C has a left child (it doesn’t) or a right child (it does, so we head down to D). D is a leaf, so we\nprint it out and move back to C, print out C, and then move back to B. Now that node B is all covered, we move up a level to node E and\ncheck if it has a right child. It does, so we visit, node F. Node F is a leaf, so we print it out before moving back to E. We’ve covered all of\nE’s children, so then we print out E. Since the left side of the tree is done, we complete the same process for the right side (check of left\nchildren, then check for right children, then visit the parent). This produces the sequence in choice (D).\n14. The correct answer is (D). Choice (A) is false because the inorder successor cannot have two children; otherwise, it wouldn’t be the inorder\nsuccessor since there is a left child. Choice (B) is false because using an array for a sparse tree is very wasteful, as many of the indices are\nleft empty. Choice (C) is false since the worst-case time complexity of inserting an element is for both array-based and pointer-basedΘ(𝑛)\nbinary trees. Choice (D) is true, however, since the best-case and worst-case auxiliary space complexities for a pointer-based binary tree\nare both Θ(𝑛), as it only needs as much space as the number of elements there are (no need for wasted space, as with the array-based\nimplementation).", "word_count": 907, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f5166b89-8e5a-5ff8-8b25-6619ec29bf44", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 691, "real_page_number": null, "text": "18.11 Tries\n679\n15. The correct answer is (D). The following tree is created:\n16\n44\n51\n66\n69\n64\n37\n41\n30\n13\nThis tree is not complete since 44 has children but 13 (which is to the left of 44 at the same depth) does not. This tree is not proper since 51\nonly has a right child.\n16. The correct answer is (B). This is the smallest tree you can build that is proper but not complete:\n17. The correct answer is (A). A preorder traversal visits the parent, then recursively the left child, then recursively the right child, which\nmatches this code.\n18. The correct answer is (C). An inorder traversal visits the left child, then the parent, then the right child. This matches the pattern for an\ninorder traversal.\n19. The correct answer is (D). This matches the equation for the height of a tree, which is 1 + the maximum height of the left and right child.\n20. The correct answer is (A). This returns the number of internal nodes in the tree, since it is counting the number of nodes where the left and\nright child are not nullptr.\n21. The correct answer is (C). Statement I is true because a preorder and postorder alone cannot differentiate between a single left child or\nright child of a node (see example 18.5.3 for an explanation why). Statement II is false if the smallest element in the tree has a right child,\nsince the postorder traversal would visit that right child before its parent. Statement III is true because the root’s inorder predecessor by\ndefinition cannot have a right child, since the right child would then the largest value smaller than the root.\n22. The correct answer is (D). When given a preorder and inorder traversal, the first thing we do is to identify the root node of this tree. This is\n12, as 12 is the first element of the preorder traversal. Now that we know this, let’s split the inorder traversal into the left and right subtrees:\n6, 18, 9, 12, 13, 15, 11\nWe look back at the preorder traversal to determine what the roots of the left and right subtrees are. The root of the left subtree must be 6,\nas it appears before 18 and 9. Similarly, the root of the right subtree must be 15, as it appears before 13 and 11:\n12, 6, 9, 18,\n15, 13, 11\nNow let’s go back to the inorder and split the left and right subtree into even smaller subtrees:\n6, 18, 9, 12, 13, 15, 11\nFrom this, we can tell that 18 and 9 form the right subtree of 6, and that 13 and 11 are the left and right children of 15. Returning to the\npreorder, we can conclude that 9 is the child of 6 (since 9 comes before 18). Then, going back to the inorder, we can conclude that 18 is the\nleft child of 9 (since 18 precedes 9). The final tree is as follows:\n12\n15\n11\n13\n6\n9\n18", "word_count": 516, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "83ad8604-f183-5fa2-a0b7-4ff047f01a00", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 692, "real_page_number": null, "text": "680\nChapter 18. Trees\n23. The correct answer is (B). Using the same process as the previous problem, we would get the following tree:\n14\n41\n2\n29\n6\n37\n23\n18\n24. The correct answer is (C). The process for identifying this tree is identical to the process described in the previous question. However,\nsince we have a postorder rather than a preorder traversal, we have to consider the last element in the postorder as the root of its respective\nsubtree (rather than the first element as with the preorder). The final tree is as follows:\n16\n27\n12\n35\n23\n18\n43\n22\n14\n25. The correct answer is (B). Using the same process as the previous problem, we would get the following tree:\n20\n28\n19\n11\n7\n2\n5\n32\n26. The correct answer is (A). In a postorder traversal, the last element is the root. Thus, if the inorder is the exact reverse of the postorder, the\nroot node must be the first element in order, or the leftmost node. We can continue this process for the subtrees that remain: the right child\nof the root node must be the left node of its subtree, and its right child must be the leftmost node of its own subtree, etc. This means that\neach node can only have a right child, else the last element of the postorder would not be the first of the inorder. Choice (C) is false because\nthe preorder would be the same as its inorder, and choice (D) is false because the preorder would be the reverse of the postorder.\n27. The correct answer is (E). Queues work well for level order traversals, since elements that are discovered first are also explored first,\nallowing the search to travel downwards level by level since elements at one level must all be considered before moving to the next. As you\nwill see in the next chapter, this is known as a breadth-first search.\n28. The correct answer is (D). This is the tree that matches the given preorder and inorder traversals:\n15\n33\n9\n21\n16\n24\n10", "word_count": 355, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "336d1676-b19d-55f2-9762-0d06dc0cfacf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 693, "real_page_number": null, "text": "18.11 Tries\n681\n29. The correct answer is (C). This is the tree that matches the given preorder and inorder traversals:\n50\n34\n42\n25\n11\n47\n63\n30. The correct answer is (B). To find the inorder predecessor, we go left once and then as far right as possible. Since the left child of the root,\n24, does not have any right children, it itself is the inorder predecessor. To find the inorder successor, we go right once and then as far left\nas possible. In this case, we end up at 38 (heading down the leftmost children of 11 until an element is found without a left child). The sum\nof 24 and 38 is 62.\n31. The correct answer is (C). The first element of a preorder traversal and the last element of a postorder traversal are both the root, which\nhas a value of 63. Thus, the value of 𝑐+𝑑is 63 + 63 = 126.\n32. The correct answer is (D). To determine the first element in an inorder traversal, we go left as far as possible until we reach a node with no\nleft child. In this case, this is node 97.\n33. The correct answer is (B). The third to last element in an inorder traversal is 31. The only two elements in a later position are 73 and 35,\nwhich are to the right of 31.\n34. The correct answer is (C). There are ten nodes in the left subtree of 63. Therefore, 63 must be the 11th node in an inorder traversal.\nThe correct answer is (E). There is a tie between the inorder and postorder traversal, where 24 is in the 10th position. 24 is 2nd in the35.\npreorder and level order traversals.\n36. The correct answer is (A). The last element of a postorder traversal is the root of the tree, so 5 must be the root. The first element of an\ninorder traversal is the smallest element in the tree, which must be 1. As a result, if you want to maximum the sum of the tree’s values, there\ncan only be 4 values in the left subtree of 5 to ensure no duplicates: 1, 2, 3, and 4. The last element of an inorder traversal is the largest\nvalue in the tree, and since the number of elements in the left and right subtrees are equal, the values in the right subtree must be 10, 11, 12,\nand 13 to satisfies all the conditions. The total sum of this tree is therefore 1 + 2 + 3 + 4 + 5 + 10 + 11 + 12 + 13 = 61.\n37. The correct answer is (A). A stick tree will always have only one leaf node, so one is the answer for a binary tree of any size.\n⌈𝑛∕2⌉are38. The correct answer is (D). In a complete tree with 𝑛nodes, leaf nodes.\n39. Thecorrectansweris(B).Whensearchinginabinarysearchtree, queriesforvalueslessthanthetargetvalueof19shouldbeanincreasing\nsubsequence that gets closer to 19, while queries for values larger than 19 should be a decreasing subsequence that gets closer to 19. In\nthis case, the subsequence 20, 21 in choice (B) cannot be possible, since there would be no way we would encounter 21 if we already\nencountered 20 earlier (since our search path would look in the left subtree of 20, in which 21 cannot exist).\n40. The correct answer is (C). To identify the inorder successor, we traversal one node to the right, and then go as far left as possible from this\nnode’s left child. As a result, it is possible for a tree to be configured in a way such that finding the inorder successor could require you to\nvisit every node, resulting in a time complexity of Θ(𝑛).\n41. The correct answer is (D). Since the binary search tree is not guaranteed to be balanced, there is no invariant that is preventing it from\nbecoming a stick like tree. Thus, the worst-case time complexity of search could potentially end up as for both binary trees and binaryΘ(𝑛)\nsearch trees.\nΘ(𝑘𝑛)42. The correct answer is (E). In general, the space complexity of a tree with at most 𝑘children is (see the end of 18.2.1 for an\nexplanation).\n43. The correct answer is (D). When searching in a binary search tree, every comparison brings you down a level of the tree (i.e., you can\neither visit the left child or the right child). Therefore, the number of comparisons you need is limited by the height of the tree, or ℎ,\nresulting in a worst-case time complexity of Θ(ℎ).\n44. The correct answer is (B). Since complete binary trees must be filled from left to right, if we number each node in a tree in level order\n(where root is one), then an even numbered node must be a left child, and an odd numbered node must be a right child. Because of this, if\nthere are an even number of nodes, only one node can have one child (the parent of the final element in a level order traversal), and if there\nare an odd number of nodes, no nodes can have only one child (due to the invariant of adding children from left to right on any given level).", "word_count": 901, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2c2d69f0-6f94-535d-b20d-a3a678d484a4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 694, "real_page_number": null, "text": "682\nChapter 18. Trees\n45. The correct answer is (A). Although it may initially seem that this problem is not solvable, you are actually given the inorder traversal as\nwell due to the fact that the tree is a binary search tree. In a BST, the inorder traversal is simply the elements in ascending order: 1, 2, 3, 4,\n5, 6, 7. Using this information, we can construct the following tree that matches these traversals:\n5\n7\n6\n3\n4\n1\n2\n46. The correct answer is (B). Node 3 has a balance factor of +2, and node 5 has a balance factor of -2. Thus, the tree is not balanced.\n47. The correct answer is (E). None of the options are true. Choice (A) and (B) are false because it is not enough to conduct a single rotation\nto balance a tree with a negative balance factor. Depending on the size of the negative balance factor and the structure of the tree, multiple\nrotations may be needed. Option (C) is false because a tree with more elements on its right side will have a negative balance factor.\nThe correct answer is (C). The time complexity of inserting 𝑛elements into an AVL tree with an insert complexity of is48. Θ(log(𝑛))\nΘ(𝑛log(𝑛)). The time complexity of printing these elements out is Θ(𝑛). The insertion process dominates, so the overall time complexity is\nΘ(𝑛log(𝑛)).\n49. The correct answer is (B). The worst-case time complexity of searching for an element in an AVL tree with 𝑛elements is Θ(log(𝑛)). Since\n𝑛43𝑛elementsthere are in the tree, the worst-case time complexity is:\nΘ(log(𝑛43𝑛)) Θ(log(𝑛4)+log(3𝑛)) Θ(4log(𝑛)+𝑛log(3)) Θ(log(𝑛)+𝑛) Θ(𝑛)= = = =\n50. The correct answer is (E). Insertions will always require either 1 or 2 rotations (after the single or double rotation, the rest of the tree is\nguaranteed to be balanced). Deletions, on the other hand, may require up to rotations.Θ(log(𝑛))\n51. The correct answer is (A). After inserting 33 as the right child of 32, there is a zigzag imbalance between 29 (-2) and 34 (+1). To fix\nthis, a right rotation is performed on 34 (34 becomes the right child of 31, and 32 becomes the left child of 34), and then a left rotation is\nperformed on 29 (29 becomes 31’s left child, and 30 becomes 29’s right child). This gives us the tree in option (A).\n52. The correct answer is (C). The process is shown below:", "word_count": 409, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3f47f538-dd4b-50e9-9d5c-611229db6a78", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 695, "real_page_number": null, "text": "18.11 Tries\n683\n53. The correct answer is (A). The process is shown below:\n54. The correct answer is (D). The process is shown below:\n2ℎ−1.55. The correct answer is (B). The maximum number of nodes an AVL tree of height 𝐻can have is This is the case where all ℎlevels\nof the tree are entirely filled, if leaf nodes have height 1. Here, since we have an AVL tree of height 5, the maximum number of nodes\n25this tree can have is −1, or 31. The minimum number of nodes an AVL tree can have is 1 + + 𝑁(ℎ−2), where is the𝑁(ℎ−1) 𝑁(ℎ)\nminimum number of nodes an AVL tree of height ℎcan have. Why is this the case? A tree with height ℎmust have at least one child with\nheight ℎ−1, so we’ll give the other child a height of to minimize the number of nodes we have. Thus, is theℎ−2 1+𝑁(ℎ−1)+𝑁(ℎ−2)\nroot node + minimum number of nodes in bigger child + minimum number of nodes in smaller child. We know that is 0 (since empty𝑁(0)\ntrees have zero nodes) and is 1 (since this is a root-only tree, which only has one node). We can solve for recursively knowing𝑁(1) 𝑁(5)\nthis information:\n• 𝑁(0)=0\n• 𝑁(1)=1\n• 𝑁(2) 1+𝑁(1)+𝑁(0)= =1+1+0=2\n• 𝑁(3) 1+𝑁(2)+𝑁(1)= =1+2+1=4\n• 𝑁(4) 1+𝑁(3)+𝑁(2)= =1+4+2=7\n• 𝑁(5) 1+𝑁(4)+𝑁(3)= =1+7+4=12\nSince and 12, the value of 19.𝑎= 𝑏= 𝑎−𝑏=31 31−12=\n56. The correct answer is (E). Starting from the bottom and moving up, the first imbalance that requires a rotation is the -2 balance factor on C\nwith +1 balance factor on D. We rotate right on D and then rotate left on C. After these rotations, E becomes B’s left child, and C and D\nbecome E’s left and right children, respectively. The next imbalance occurs with node B, which has a +2 balance factor. Its child, E, has a\nbalance factor of 0, so we can just perform a right rotation on B. This moves B to the right child of E, D to the left child of B, and E to the\nright child of A. The last imbalance happens at node A with a balance factor of -2. After rotating left on A, we get the following tree:\nE\nB\nD\nA\nC\n57. The correct answer is (C). The balance factor of the root node is 0, as shown above (heights of left and right subtrees are the same).", "word_count": 440, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e050a913-aec6-5571-ad53-078edb379e6f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 696, "real_page_number": null, "text": "684\nChapter 18. Trees\n58. The correct answer is (C). While you are only given the postorder traversal, the inorder traversal can also be determined from the\ninformation given. Since this is an AVL tree, the inorder is simply the elements in sorted order: 8, 11, 14, 21, 26, 28, 30, 34, 35, 37 (recall\nthat AVL is just a balanced binary search tree). Adding 9 and 29 produces the following:\n59. The correct answer is (A). Since you know the values you want to insert, you can avoid any rotations at all by recursively inserting the\nmedian values into the tree. This ensures that you will never insert a value that could potentially cause an imbalance.\nThe correct answer is (D). Tree (A) is not a valid AVL tree because 28 is in the right subtree of 30. Tree (B) is not a valid AVL tree60.\nbecause the balance factor of 34 is -2. Tree (C) is not a valid AVL tree because the balance factor of 39 is +2. Only tree (D) is a valid AVL\ntree with no balance factor whose absolute value is greater than 1.\n61. The correct answer is (B). This can be best solved by trying out some values, but a total of six insertions are needed at a minimum. One\nsuch insertion order that works is 2, 4, 10, 23, 46, and 1. The idea is that you will have to insert one node to be the one at a depth of 6, and\none additional node to balance each of the five levels above it.\n62. The correct answer is (D). Six rotations are needed: twice after inserting 15 (RL11, RR17), twice while inserting 12 (RR14, RL11), and\ntwice after deleting 15 (RL12, RR16).\n63. The correct answer is (C). A maximum of 3 rotations can be induced after deleting 12: a right rotation on 11, a left rotation on 3, and a\nright rotation on 8.\n64. The correct answer is (B). See the remark in section 18.7.4, which goes over the formula for calculating the maximum rotations possible\nfrom a single deletion.\n65. The correct answer is (D). See the remark in section 18.7.4. The possible smallest tree of height 7 has a size of 33, so you could have to\npotentially complete 7 values before you can complete an insertion in this tree.\n66. The correct answer is (C). I and II are actually not guaranteed to meet the invariants — even if an equal element is always inserted on one\nside of the tree, a future rotation could potentially bring it into the other side of the tree. Thus, only choice III can be successfully used to\nsatisfy the given invariants (since we do not have to worry about equal elements ending up on the wrong side).\n67. The correct answer is (D). Only I and III are balanced binary search trees (note that the inorder is just the values in ascending order). The\nthree trees are shown below:\nTree I\n15\n16\n17\n13\n14\n12\n11\nTree II\n14\n17\n15\n16\n12\n13\n11\nTree III\n13\n15\n17\n16\n14\n11\n12\n68. The correct answer is (D). Ordered lookup containers in the STL require time lookup, which can only be supported with aΘ(log(𝑛))\nself-balancing binary search tree of the provided choices. This can be done with an AVL tree, or any other self-balancing tree that supports\nthis logarithmic complexity (e.g., red-black tree).\noperator< operator==, T69. The correct answer is (A). Maps are ordered using and so the custom object can be inserted into a\nstd::map<> Twithout any issue. However, hash tables require a key to be hashed before insertion, so the absence of a hash function for\nstd::unordered_map<>.means that it cannot be used as the key of an", "word_count": 641, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "943bad44-08b7-5e19-8ecc-53ed649ea091", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 697, "real_page_number": null, "text": "18.11 Tries\n685\nK map::upper_bound(),70. The correct answer is (A). The first value that is greater than is referenced by so you should iterate from\nmap::begin() map::upper_bound(K) K.to to print all keys that are less than or equal to\nK map::upper_bound(),71. Thecorrectansweris(B).Similartothepreviousproblem, thefirstvaluethatisgreaterthan isreferencedby\nmap::upper_bound(K) map::end() K.so you should iterate from to to print all keys that are greater than\n.upper_bound(15)72. The correct answer is (C). Since calling returns an element with a key of 17, that means that 17 is the first key\nthat is strictly greater than 15. As a result, it would be impossible for an element with a key of 16 to exist in the map.\n--it firstThe correct answer is (E). The iterator points to the value before the end iterator, or the largest key in the map. Since73.\n(--it)->firstrepresents the key, the value of is 883.\n74. The correct answer is (D). To be able to easily access the highest score value, we would want to store the score as the key of an ordered\nmap, which removes options (A) and (B). The value of the largest score would then be accessible as the iterator directly before the end\niterator, or option (D).\nstd::unordered_map<>The correct answer is (B). The average-case time complexity of inserting an element into an is Θ(1), and75.\nstd::map<>the average-case time complexity of inserting an element into a is Θ(log(𝑛)). Thus, the overall time complexity of inserting\nstd::unordered_map<>, std::map<>.𝑛elements are for an and for aΘ(𝑛) Θ(𝑛log(𝑛))\n76. The correct answer is (C). All of the options are in the worst-case, with the exception of the binary search tree. Note that aΘ(𝑛log(𝑛))\nbinary search tree can only guarantee if it is balanced, but that is not the case here. As a result, you could end up with an overallΘ(𝑛log(𝑛))\nΘ(𝑛2)time complexity of if the tree is oriented as a stick like tree.\n77. One possible solution is as follows. Here, we have to make sure that a node’s left and right children are fixed before we remove the node.\nThis is best done by traversing the tree in a postorder fashion. If a node is smaller than the minimum allowable value, we remove the node\nand set its right child as the new root. On the other hand, if a node is larger than the maximum allowable value, we remove the node and set\nits left child as the new root.\n1\nint32_t int32_tNode* trim_bst(Node* root, min_val, max_val) {\n2\nif (!root) {\n3\nreturn root;\n4\n} // if\n5\nroot->left = trim_bst(root->left, min_val, max_val);\n6\nroot->right = trim_bst(root->right, min_val, max_val);\n7\nif (root->val < min_val) {\n8\nNode* right_child = root->right;\n9\ndelete root;\n10\nreturn right_child;\n11\n} // if\n12\nif (root->val > max_val) {\n13\nNode* left_child = root->left;\n14\ndelete root;\n15\nreturn left_child;\n16\n} // if\n17\nreturn root;\n18\n} // trim_bst()\n78. The solution for this problem can be achieved by going down the tree level by level and returning the nodes on each level that is farthest to\nthe right. One method is to use a queue and iterate over the levels of the tree, similar to performing a level order traversal. Once the end of a\nlevel is reached, the remaining node is pushed into the result. An implementation of this solution is shown below:\n1\nstd::vector<int32_t> get_rightmost_values(Node* root) {\n2\nif (!root) {\n3\nreturn {};\n4\n} // if\n5\nstd::queue<Node*> bfs;\n6\nstd::vector<int32_t> result;\n7\nbfs.push(root);\n8\nwhile (!bfs.empty()) {\n9\nsize_t level_size = bfs.size();\n10\nnullptr;Node* current =\n11\nfor (size_t i = 0; i < level_size; ++i) {\n12\ncurrent = bfs.front();\n13\nbfs.pop();\n14\nif (current->left) {\n15\nbfs.push(current->left);\n16\n} // if\n17\nif (current->right) {\n18\nbfs.push(current->right);\n19\n} // if\n20\n} // for i\n21\nresult.push_back(current->val);\n22\n} // while\n23\nreturn result;\n24\n} // get_rightmost_values()", "word_count": 682, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0132b31c-f31b-5a31-9008-d1d34b0bcc8d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 698, "real_page_number": null, "text": "686\nChapter 18. Trees\nThis problem can be solved recursively as well by making a recursive call into the right child of each level. An implementation of this is\nshown below: note that an additional call to the left child afterward is needed in the case that the right subtree does not cover the full depth\nof the entire tree.\n1\nstd::vector<int32_t> get_rightmost_values(Node* root) {\n2\nstd::vector<int32_t> rhs_view;\n3\ntraverse(root, rhs_view, 0);\n4\nreturn rhs_view;\n5\n} // get_rightmost_values()\n6\n7\nvoid std::vector<int32_t>& int32_ttraverse(Node* root, rhs_view, level) {\n8\nif (!root) {\n9\nreturn;\n10\n} // if\n11\nif (rhs_view.size() == level) {\n12\nrhs_view.push_back(root->val);\n13\n} // if\n14\ntraverse(root->right, rhs_view, level + 1);\n15\ntraverse(root->left, rhs_view, level + 1);\n16\n} // traverse()\nOne possible solution is as follows. After removing an edge from a parent to a child, the subtree with the child as the root must be half79.\nthe sum of the original tree. We can store the sums of every subtree using recursion and a helper function that calculates the sum of each\nsubtree. After doing so, we can check if half the sum was recorded; if it is, we return true.\n1\nbool equal_sum_subtree(Node* root) {\n2\nstd::vector<int32_t> subtree_sums;\n3\nint32_t total_sum = sum(root, subtree_sums);\n4\n// remove the sum of the original tree since we can only split from non-root nodes\n5\nsubtree_sums.pop_back();\n6\nif (total_sum % 2) {\n7\nreturn false;\n8\n} // if\n9\nfor (int32_t i : subtree_sums) {\n10\nif (i == total_sum / 2) {\n11\nreturn true;\n12\n} // if\n13\n} // for i\n14\nreturn false;\n15\n} // equal_sum_subtree()\n16\n17\nint32_t std::vector<int32_t>&sum(Node* root, subtree_sums) {\n18\nif (!root) {\n19\nreturn 0;\n20\n} // if\n21\nint32_t left_sum = sum(root->left, visited);\n22\nint32_t right_sum = sum(root->right, visited);\n23\nint32_t subtree_sum = left_sum + right_sum + root->val;\n24\nsubtree_sums.push_back(subtree_sum);\n25\nreturn subtree_sum;\n26\n} // sum()\n80. Since the given binary search tree is already complete, we can reuse it to build our min-heap. To do so, we can store the inorder traversal of\nthe binary search tree in separate array. Then, we can perform a preorder traversal of the binary search tree and copy over the nodes from\nthe array to the nodes of the tree one by one, from left to right. This works because the elements in the array must be sorted from smallest\nto largest, and a preorder traversal ensures that parent values are always filled before their children, maintaining the heap invariant. One\nimplementation of this solution is shown below:\n1\nNode* convert_bst_to_min_heap(Node* bst_root) {\n2\nstd::vector<int32_t> vec;\n3\nsize_t idx = 0;\n4\ninorder(bst_root, vec);\n5\nwrite_min_heap(bst_root, vec, &idx);\n6\nreturn bst_root;\n7\n} // convert_bst_to_min_heap()\n8\n9\nvoid std::vector<int32_t>&inorder(Node* root, vec) {\n10\nif (!root) {\n11\nreturn;\n12\n} // if\n13\ninorder(root->left, vec);\n14\nvec.push_back(root->val);\n15\ninorder(root->right, vec);\n16\n} // inorder", "word_count": 495, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "549008e2-4dc7-540b-9103-abb265139e76", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 699, "real_page_number": null, "text": "18.11 Tries\n687\n18\nvoid std::vector<int32_t>& size_t*write_min_heap(Node* root, vec, idx) {\n19\nif (!root) {\n20\nreturn;\n21\n} // if\n22\nroot->val = vec[*idx++];\n23\nwrite_min_heap(root->left, vec, idx);\n24\nwrite_min_heap(root->right, vec, idx);\n25\n} // write_min_heap()\n81. The simplest solution is to use recursion to add the values of nodes together. This is shown below:\n1\nNode* merge_trees(Node* t1, Node* t2) {\n2\n// If either node is missing, return the value of the other node\n3\nif (!t1) {\n4\nreturn t2;\n5\n} // if\n6\nif (!t2) {\n7\nreturn t1;\n8\n} // if\n9\nt1->val += t2->val;\n10\nt1->left = merge_trees(t1->left, t2->left);\n11\nt1->right = merge_trees(t1->right, t2->right);\n12\nreturn t1;\n13\n} // merge_trees()\nAn alternative solution would be to use two stacks to keep track of the nodes, popping and adding values along the way. We start off by\npushing the root nodes into the stack. Then, while the stack is not empty, we remove a pair of nodes and add the values to the first tree. If a\nnode does not exist for the first tree, we simply append the node for the second tree (as the absence of a node is equivalent to a value of 0).\n1\nNode* merge_trees(Node* t1, Node* t2) {\n2\n// If either node is missing, return the value of the other node\n3\nif (!t1) {\n4\nreturn t2;\n5\n} // if\n6\nif (!t2) {\n7\nreturn t1;\n8\n} // if\n9\nstd::stack<Node*> s1, s2;\n10\ns1.push(t1);\n11\ns2.push(t2);\n12\nwhile (!s1.empty()) {\n13\nNode* curr1 = s1.top();\n14\nNode* curr2 = s2.top();\n15\ns1.pop();\n16\ns2.pop();\n17\ncurr1->val += curr2->val;\n18\nif (curr1->left && curr2->left) {\n19\ns1.push(curr1->left);\n20\ns2.push(curr2->left);\n21\n} // if\n22\nelse if (!curr1->left) {\n23\ncurr1->left = curr2->left;\n24\n} // else if\n25\nif (curr1->right && curr2->right) {\n26\ns1.push(curr1->right);\n27\ns2.push(curr2->right);\n28\n} // if\n29\nelse if (!curr1->right) {\n30\ncurr1->right = curr2->right;\n31\n} // else if\n32\n} // while\n33\n} // merge_trees()", "word_count": 344, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "737afb5e-0cd0-5648-b87c-5ae334f0ba5b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 700, "real_page_number": null, "text": "688\nChapter 18. Trees\n82. A realization to be made here is that, since the array is sorted, the element in the middle must be the root of the balanced tree. Furthermore,\nwe can conclude that the middle element of the elements to the left of the root and the middle element of the elements to the right of the root\nare the left and right children of the root node, respectively. This allows us to use recursion as an elegant solution to this problem, as shown:\n1\narray_to_bst(const std::vector<int32_t>&Node* nums) {\n2\nreturn array_to_bst_helper(nums, 0, nums.size());\n3\n} // array_to_bst()\n4\n5\narray_to_bst_helper(const std::vector<int32_t>& size_t size_tNode* nums, start_idx, end_idx) {\n6\nif (start_idx > end_idx) {\n7\nreturn nullptr;\n8\n} // if\n9\nsize_t middle_idx = (start_idx + end_idx) / 2;\n10\nnewNode* root = Node(arr[middle_idx]);\n11\nroot->left = array_to_bst_helper(nums, start_idx, middle_idx - 1);\n12\nroot->right = array_to_bst_helper(nums, middle_idx + 1, end_idx);\n13\nreturn root;\n14\n} // array_to_bst_helper()\nTo solve this problem, we will need to take advantage of two data structures: one to map each stock symbol to price/timestamp information,83.\nstd::unordered_map<>and another to map timestamp to price. We do not need to know the order of our stock symbols, so an is\nget_price()sufficient to store this information. However, our function will need to know about the order of timestamps, so we would\nstd::map<>.need to use an ordered container, such as a As a result, one potential solution is to store our data in the form of an unordered\nmap that maps each stock symbol to an ordered map from timestamp to price. Updating would therefore a price to this map, while getting\nthe price would find the largest update timestamp that precedes the given timestamp to query. An implementation of this is shown below:\n1\nclass StockPriceTracker {\n2\nprivate:\n3\nstd::map<int32_t, double>>std::unordered_map<std::string, stock_price_map;\n4\npublic:\n5\nvoid update(const double int32_tstd::string& symbol, price, timestamp) {\n6\nstock_price_map[symbol][timestamp] = price;\n7\n} // update()\n8\n9\ndouble get_price(const int32_tstd::string& symbol, timestamp) {\n10\n// based on assumption that symbol exists and a previous timestamp has been updated before\n11\nauto& timestamp_map = stock_price_map[symbol];\n12\nauto first_larger = timestamp_map.upper_bound(timestamp);\n13\nreturn (--first_larger)->second;\n14\n} // get_price()\n15\n};", "word_count": 380, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a5aac795-c3a4-5ef5-9bc5-1aa337577177", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 701, "real_page_number": null, "text": "Chapter 19\nGraphs and Elementary Graph Algorithms\n19.1\nIntroduction to Graphs\nIn the previous chapter, we discussed the concept of a tree. However, trees actually fall into a broader category of structures known as graphs. A\ngraph is defined as a set of vertices and a set of edges that connect pairs of vertices. We often𝐺= (𝑉,𝐸) 𝑉= {𝑣1,𝑣2,…} 𝐸= {𝑒1,𝑒2,…}\ndescribe edges in a graph using pairs of vertices, where the notation indicates that edge 𝑚connects vertices 𝑠and 𝑡.𝑒𝑚=(𝑣𝑠,𝑣𝑡)\nCompared to a tree, the definition of a graph is not as strict, as any structure constructed using a collection of vertices and edges can be\nconsidered a graph. Because of this, graphs can be categorized into many different groups based on their individual characteristics. One such\ncategory, as we have seen, is the tree: an acyclic, connected graph comprised of a collection of nodes (vertices) that are connected by edges.\nHowever, graphs can take on many more forms, beyond the stringent rules that define a tree. For example, it is entirely possible for a graph to\nhave (two edges connecting the same pair of vertices) or (an edge that connects a vertex with itself). The followingparallel edges self loops\ngraph is an example that incorporates these two features:\nA\nB\nC\nD\nE\nF\nG\nH", "word_count": 221, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dd5b44fe-291c-57f1-a4bd-6d864bca94a5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 702, "real_page_number": null, "text": "690\nChapter 19. Graphs and Elementary Graph Algorithms\nThe simplest type of graph, as its name implies, is the simple graph. A simple graph is a graph without any parallel edges or self loops. The\ngraph above is not a simple graph, because there is a parallel edge connecting vertices 𝐴and 𝐵, as well as a self loop on vertex 𝐹. However, if\nwe remove these features, then the graph would become a simple graph.\nA\nB\nC\nD\nE\nF\nG\nH\nNotice that a graph need not be fully connected for it to be considered a simple graph. Vertices 𝐺and 𝐻in the previous graph are disconnected\nfrom the other vertices, but the graph is still a simple graph since it has no parallel loops or self loops. For the rest of this chapter (and for this\nclass in general), you may assume that a graph is simple unless stated otherwise.\nA simple path in a graph is a sequence of edges leading from one vertex to another with no vertex appearing twice. For example, the\nfollowing is a simple path from vertex 𝐴to vertex 𝐹:\nA\nB\nC\nD\nE\nF\nG\nH\nWe consider a graph to be a connected graph if a simple path exists between any pair of vertices in the graph. A connected graph does not\nimply that every vertex is connected to every other vertex (such a graph would be a graph, which we will cover later); it only impliescomplete\nthat there exists at least one path that can get you from any vertex to any other vertex in the graph. For instance, the graph above is anot\nconnected graph because there is no way to reach vertices 𝐺or 𝐻from any of the other vertices. In order for the graph to be connected, we\nwould have to connect either vertex 𝐺or vertex 𝐻with any of the other six vertices, as shown below:\nA\nB\nC\nD\nE\nF\nG\nH\nA cycle is a path with distinct edges in the graph where the starting and ending vertices are the same. For example, the path 𝐴→𝐵→𝐶→𝐴is\na cycle in the following graph, since the path starts and ends on vertex 𝐴.\nA\nB\nC\nD\nE\nF\nG\nH\nIf there exist no cycles in a graph, then that graph is considered acyclic.\nExample 19.1 Consider the following three graphs. Which ones are simple graphs? Which ones are connected graphs? Which ones are\nacyclic graphs?\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nThe first graph is simple and acyclic, but not connected (since there is no path from vertices 𝐷or 𝐸to vertices 𝐴, 𝐵, and 𝐶). The second graph\nis connected, but not simple nor acyclic (since there are parallel edges, and 𝐹→𝐺→𝐻→𝐹is a cycle in the graph). The third graph is also\nconnected, but not simple nor acyclic (since there is a self loop, which trivially implies that there exists a cycle).", "word_count": 497, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3bddcba7-c48c-5e74-a7cd-610547eb82b3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 703, "real_page_number": null, "text": "19.1 Introduction to Graphs\n691\nWe can also categorize graphs based on the nature of their edges. A directed graph (or digraph) is a graph whose edges have direction. Edges\nin a directed graph are often denoted using an ordered pair of vertices, where the notation indicates that there is an edge𝑒𝑚=(𝑣𝑠,𝑣𝑡) 𝑣𝑠from\n𝑣𝑡, but not necessarily the other way around. On the other hand, an undirected graph is a graph whose edges do not have direction. Into\nan undirected graph, the order of vertices in an edge does not matter, and a connection from vertex 𝑣𝑠to vertex 𝑣𝑡implies that there is also a\nconnection from vertex 𝑣𝑡to vertex 𝑣𝑠.\nUndirected Graph\nA\nB\nC\nD\nE\nF\nG\nH\nDirected Graph\nA\nB\nC\nD\nE\nF\nG\nH\nThe concept of edge direction throws a wrench into our definition of a connected graph. Even though there are no disjoint vertices in the directed\ngraph on the previous page, the direction of the edges makes it impossible to travel between all pairs of vertices. For instance, even though there\nexists an edge between vertex 𝐴and vertex 𝐶, there is no way to travel from vertex 𝐶to vertex 𝐴since the edge only supports one direction. In\nsuch a case, would we still consider the graph as connected?\nTo handle this issue, \"connected\" directed graphs are split into two categories. A strongly connected graph is a directed graph where there\nexists a valid path between every pair of vertices, even after edge direction is taken into account. For example, the following graph is strongly\nconnected, as there exists a valid, directional path between all pairs of vertices.\nA\nB\nC\nD\nOn the other hand, a weakly connected graph is a directed graph that does not guarantee a valid path between every pair of vertices (due to\nedge direction), but would be connected if all of its directed edges were replaced with undirected edges. For example, the following graph is\nweakly connected, because there is no path from vertices 𝐵or 𝐷to any of the other vertices, even though the graph would be connected without\nconsidering edge direction.\nA\nB\nC\nD\nAnotherwaywecategorizegraphsisbylookingatwhethertheiredgesareweightedorunweighted. Inaweightedgraph, edgesmaybeassigned\na \"weight\" value that represents the distance or cost associated with traversing that edge. An example of a weighted graph is shown below:\nA\nB\nC\nD\nE\nF\n7\n2\n4\n3\n4\n5\n1\n5\n9\n5\nOn the other hand, the edges in an unweighted graph do not have any weight value associated with them. As a result, you can think of an\nunweighted graph as a graph where all the edges have the same distance or cost. The distinction between weighted and unweighted edges is\nimportant, as it could determine the graph algorithm that is best suited to solve a problem. Some algorithms, such as the standard breadth-first\nsearch and depth-first search algorithms, are designed for unweighted graphs, as they simply search for whether a path exists between two\nvertices. Other algorithms, such as Dijkstra’s algorithm and the Floyd-Warshall algorithm, are designed for weighted graphs, as they optimize\nfor the lowest path between two vertices (these \"shortest path\" graph algorithms will be covered in chapter 25).weighted", "word_count": 559, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dadc1243-1331-5b35-b06a-290f2d5b228c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 704, "real_page_number": null, "text": "692\nChapter 19. Graphs and Elementary Graph Algorithms\nAnother common term you may see with graphs is the term \"degree\". The degree of a vertex in a graph is the number of edges that are incident\nto that vertex, where self loops are counted twice. For example, the degree of vertex 𝐶in the following graph is three, since there are three\nedges that touch the vertex. The degree of vertex 𝐹is also three, since the self loop is counted twice, once for each end.\nA\nB\nC\nD\nE\nF\nG\nH\nIf a graph is directed, we also consider the direction of each edge when calculating degree. To do so, we split the concept of degree into\nin-degree and out-degree. The in-degree of a vertex is the total number of incoming edges that are directed toward it, while the out-degree of a\nvertex is the total number of outgoing edges that are directed away from it. For example, the in-degree of vertex 𝐶in the following directed\ngraph is two, since there are two edges that are directed toward it (from vertices 𝐴and 𝐵). The out-degree of vertex 𝐶is one, since only one\nedge is directed away from it (toward vertex 𝐷).\nA\nB\nC\nD\nE\nF\nG\nH\nAnother useful characteristic of a graph is the of its edges. For example, the following graph is rather sparse, since its vertices are notdensity\nconnected using many edges.\nA\nB\nE\nC\nD\nHowever, we can add edges to this graph to increase its density:\nA\nB\nE\nC\nD\nA\nB\nE\nC\nD\nA\nB\nE\nC\nD\nA\nB\nE\nC\nD\nA\nB\nE\nC\nD\ngraphs.1We can use the density of a graph to categorize graphs into two groups: sparse graphs and dense The formal definitions of these two\ntypes of graphs are described below:\n|𝐸| |𝑉| |𝐸| 𝑂(|𝑉|)).• A sparse graph is a graph where the number of edges is on the order of the number of vertices (i.e., is\nΘ(|𝑉|2)).|𝐸| |𝐸|• A dense graph is a graph where the number of edges is on the order of the number of vertices squared (i.e., is\nThe distinction between sparse and dense plays a important role in determining how a graph should be represented in memory. We will discuss\nthis in the next section.\nA complete graph is a type of dense graph where every pair of distinct vertices is connected by a unique edge. You can think of a complete\ngraph as the \"densest\" graph possible, where all pairs of vertices have a direct connection. To calculate the number of edges required to construct\na complete graph with 𝑛vertices, we can use the equation\n(𝑛|𝐸connected|=\n2\n) 𝑛(𝑛−1)=\n2\nΘ(𝑛2)=\n(𝑛This equation works because a complete graph has an edge between every pair of vertices, and there are\n2\n)\ndistinct pairs of vertices that can be\nconnected using an edge.\n1Theideaofsparsevs. denseisusefulforselectinggraphdatastructuresoralgorithms,butthedistinctioncanbevagueattimes,especiallyifthegraphathandis\nsubstantiallysmall. Becauseofthis,wewilltypicallyrefertographsthatare triviallysmallwhendiscussingtheconceptofgraphdensity,astoreducethenot\nambiguitybetweenthetwocategorizations.", "word_count": 559, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ad9ed7ba-d139-543d-b4aa-a674613cdb2a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 705, "real_page_number": null, "text": "19.1 Introduction to Graphs\n693\nA good strategy for determining whether a graph is sparse or dense is to look at its degree as its size increases. If the average degree of the\ngraph stays relatively constant as the number of vertices increases, then the graph is sparse. However, if the average degree of the graph grows\nwith the number of vertices, then the graph is dense. A few examples are covered below:\nExample 19.2 Consider a graph 𝐺that fits the definition of a tree. Is 𝐺a sparse or dense graph?\n|𝑉| |𝑉|−1By definition, a tree is an acyclic, connected graph. Because of this, we can conclude that a tree with nodes must have edges. Thus,\n|𝐸| 𝑂(|𝑉|),is so 𝐺must be sparse.\nExample 19.3 Consider a graph 𝐺that represents the internet, where each vertex represents a web page, and each edge represents a\nhyperlink that connects two web pages. Is 𝐺a sparse or dense graph?\n|𝐸|The number of links on each web page in the graph is relatively constant and does not depend on the total number of pages on the internet, so\n𝑂(|𝑉|).is Thus, 𝐺must be sparse. (For 𝐺to be dense, each web page would have to link to pretty much every other web page on the internet!)\nExample 19.4 Consider a graph 𝐺that represents a dictionary, where each vertex represents a word in the dictionary, and each edge\nconnects two words that share the same letter (e.g., the word \"apple\" in this graph would have a direct connection with every other word that\nstarts with \"a\"). Is 𝐺a sparse or dense graph?\n|𝑉|∕26,In this case, there are 26 disjoint sets within the graph (one for each letter). On average, each of these disjoint sets must have size where\n|𝑉| represents the total number of words in the dictionary. Since each of these disjoint sets is also complete, the average number of edges in each\n(|𝑉|∕26disjoint set is\n2\n) Θ(|𝑉|2). Θ(|𝑉|2),|𝐸|Thus, is and 𝐺must be dense. You can also obtain this result if you use the degree approach:=\n|𝑉|∕26,the average degree of any word in the graph is which grows with the size of the graph. This indicates that 𝐺is dense, which matches\nour conclusion from before.\nThere are two other categories of graphs that you may encounter when working with graph problems. One such graph is the directed acyclic\ngraph (DAG), which is a directed graph with no cycles. This type of graph is fairly common in many computer science problems, as directed\nacyclic graphs are able to elegantly characterize problems that involve dependencies (for example, Git version control can be represented using\na DAG). An example of a directed acyclic graph is shown below. One important algorithm for solving problems involving these graphs is\nsort, which will be covered in a later section.topological\nA\nB\nC\nD\nE\nF\nG\nH\nAnother special type of graph you may encounter is the bipartite graph. A bipartite graph is a graph whose vertices can be split into two\nindependent groups 𝑈and 𝑉, such that every edge connects a vertex in 𝑈to a vertex in 𝑉. For a graph to be bipartite, you must be able to color\nit using two colors such that every edge connects two nodes of colors. This invariant also implies that bipartite graphs cannot have anydifferent\ncycles of odd length. An example of a bipartite graph is shown below:\n𝑢1\n𝑢2\n𝑢3\n𝑢4\n𝑣1\n𝑣2\n𝑣3\n𝑣4\nBipartite graphs serve as a foundation for many important graph algorithms, such as network flow and graph matching.", "word_count": 603, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a12e63c9-d449-586c-9f1d-9182b697735c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 706, "real_page_number": null, "text": "694\nChapter 19. Graphs and Elementary Graph Algorithms\n19.2\nGraph Representations\nTwo important graph ADTs that can be used to represent a graph in memory are the and list. The benefits ofadjacency matrix adjacency\nchoosing one implementation over the other depend on the type of graph problem you are working on.\n¸ 19.2.1\nAdjacency Matrix\nAn adjacency matrix is a matrix 𝑚where the cell represents whether an edge exists from vertex 𝑖to vertex 𝑗. In an unweighted graph,𝑚[𝑖][𝑗]\nthe element at row 𝑖, column 𝑗of the adjacency matrix is 1 if and only if the edge exists in the set of edges 𝐸, and 0 otherwise. For(𝑣𝑖,𝑣𝑗)\ngraphs, an adjacency matrix is always symmetric across the diagonal (since a connection between 𝑖and 𝑗also implies a connectionundirected\nbetween 𝑗and 𝑖). An example of an adjacency matrix for an undirected graph is shown below:\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\n0\n1\n1\n0\n1\nB\n1\n0\n1\n0\n0\nC\n1\n1\n0\n1\n0\nD\n0\n0\n1\n0\n1\nE\n1\n0\n0\n1\n0\nBecause the adjacency matrix of an undirected graph is always symmetric, we can save time and space by only filling out half of the matrix. If\nwe do this, we must make sure to access the rows and columns in the correct order (such as making sure that the row index is never larger than\nthe column index).\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\n0\n1\n1\n0\n1\nB\n-\n0\n1\n0\n0\nC\n-\n-\n0\n1\n0\nD\n-\n-\n-\n0\n1\nE\n-\n-\n-\n-\n0\nHowever, in a graph, we will have to fill out the entire matrix. This is because the existence of a path from vertex A to B does not meandirected\nthat there is one the other way around. An example of an adjacency matrix for a directed graph is shown below:\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\n0\n1\n1\n0\n1\nB\n1\n0\n1\n0\n0\nC\n0\n0\n0\n1\n0\nD\n0\n0\n0\n0\n1\nE\n0\n0\n0\n0\n0\nAn adjacency matrix can also be used to represent graphs. If a graph is weighted, the cell would store the weight of the edge𝑚[𝑖][𝑗]weighted\ninfinity.2from vertex 𝑖to vertex 𝑗. If no edge exists between vertex 𝑖and vertex 𝑗, then the cell is assigned an edge weight of An𝑚[𝑖][𝑗]\nexample is shown below:\nA\nB\nC\nD\nE\n3\n5\n4\n6\n7\n1\n2\nA\nB\nC\nD\nE\nA\n∞\n2\n3\n∞\n7\nB\n1\n∞\n5\n∞\n∞\nC\n∞\n∞\n∞\n4\n∞\nD\n∞\n∞\n∞\n∞\n6\nE\n∞\n∞\n∞\n∞\n∞\nstd::numeric_limits<double>::infinity() <limits>Remark: To initialize a value to ∞, you can use from the library.\nΘ(|𝑉|2),|𝑉| |𝑉|×|𝑉|Given a graph with vertices, the time complexity to build an adjacency matrix is since you have to construct a matrix.\nBecause adjacency matrices store a value for every possible edge in the graph, the time complexity of looking up an edge is Θ(1), since you can\ndirectly index its cell in the matrix. However, adjacency matrices also have their disadvantages. The time required to iterate over all adjacent\nΘ(|𝑉|2),Θ(|𝑉|),vertices ofa vertex inan adjacencymatrixis and thetime required toiterate over edgesis regardlessof how densethe graphall\nis (this is because, if you wanted to find which vertices are adjacent to any vertex, you would have to iterate over the entire row of that vertex to\nsee where there exists an edge). This is fine for dense graphs, since each vertex may have many edges, but not for sparse graphs, where most\nvertices do not have direct connections with each other. Thus, an adjacency matrix is the best way to represent a sparse graph in memory.not\n2Dependingontheproblemyouaretryingtosolve,youcouldassignnon-existentedgeswithdifferentvalues. Inourexamples,wewillassignnon-existentedges\nwithaweightof∞becausewearetreatingedgeweightasacostthatyouwanttominimize. Thisisstandardbehaviorformostproblemswewillseeinthisclass,\nsinceweightedgraphsareoftenusedtosolveminimizationproblems.", "word_count": 738, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "18fd01de-064a-5e19-917a-3e3cb93583c8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 707, "real_page_number": null, "text": "19.2 Graph Representations\n695\n¸ 19.2.2\nAdjacency List\nInstead, if a graph is sparse, a better alternative is to use an adjacency list as the underlying representation. In an adjacency list, each vertex 𝑣is\nmapped to a list of edges that originate from 𝑣. For example, the following depicts an adjacency list for an undirected graph, where the presence\nof indicates that vertex A has a connection to vertices B, C, and E.𝐴→[𝐵,𝐶,𝐸]\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nCB E\nCA\nA B D\nC E\nA D\nAdjacency lists also work with directed graphs and weighted graphs. The following depicts an adjacency list for a directed graph (which is\nsimilar to the undirected representation, with the exception that 𝑋→𝑌does not necessarily guarantee 𝑌→𝑋).\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nC EB\nCA\nD\nE\nThe following depicts an adjacency list for a weighted graph. When dealing with weighted graphs, each edge in the adjacency list is paired with\nits weight (e.g., indicates that there is an edge from 𝐴to 𝐵with weight 2).𝐴→(𝐵,2)\nA\nB\nC\nD\nE\n3\n5\n4\n6\n7\n1\n2\nA\nB\nC\nD\nE\n(B,2) (C,3) (E,7)\n(A,1) (C,5)\n(D,4)\n(E,6)\n|𝑉| Θ(|𝐸|), Θ(|𝑉|+|𝐸|).Given a graph with vertices, the time complexity to build an adjacency list is and the space complexity is The time\nΘ(|𝐸|)complexity is because you have to iterate through each edge and insert it into the adjacency list, where each insertion takes timeΘ(1)\nΘ(|𝑉|+|𝐸|)(assuming the adjacency list is stored in a 2-D vector or a hash table). The space complexity is because storing all the vertices\nΘ(|𝑉|) Θ(|𝐸|)requires space, and storing all the edges in the lists requires space.\nWhat is the time complexity of processing all the edges of a in an adjacency list? If edges are randomly distributed, eachsingle vertex\n|𝐸| |𝑉|Θ(|𝐸|∕|𝑉|)vertex list should have a length of on average (since edges are distributed across vertices). Thus, in the average case, the time\n|𝐸|∕|𝑉|),it takes to process all edges of a given vertex is where the component comes from accessing the list of the vertex, and theΘ(1+ Θ(1)\nΘ(|𝐸|∕|𝑉|) component comes from iterating through the entire vertex list.\n𝑣1\n𝑣2\n𝑣3\n𝑣4\n𝑣5\n⋮\n|𝐸|Θ(\nelements|𝑉|)\n|𝐸|Θ(\nelements|𝑉|)\n|𝐸|Θ(\nelements|𝑉|)\n|𝐸|Θ(\nelements|𝑉|)\n|𝐸|Θ(\nelements|𝑉|)\ntime to accessΘ(1)\nlist of given vertex\n|𝐸|Θ(\ntime to process edges of vertex list|𝑉|)\n|𝐸|∕|𝑉|),Becausetheaveragecostofprocessingtheedgesofasinglevertexis theaveragecostofprocessing verticesintheadjacencylistΘ(1+ all\nΘ(|𝑉|×(1+ Θ(|𝑉|+|𝐸|), |𝑉| |𝐸| 𝑂(|𝑉|),|𝐸|∕|𝑉|)) |𝐸|∕|𝑉|)is sincea operationisperformed times. Thisisgreatforsparsegraphs,since= Θ(1+ =\nΘ(|𝑉|+|𝐸|) Θ(|𝑉|+𝑂(|𝑉|)) Θ(|𝑉|).which implies that the time complexity of iterating through all edges in an adjacency list is However,= =\nΘ(|𝑉|2), Θ(|𝑉|+|𝑉|2) Θ(|𝑉|2).|𝐸| Θ(|𝑉|+|𝐸|)for dense graphs, and the time complexity of iterating through all edges becomes This= = =\nis the same time complexity as that of an adjacency matrix! Thus, adjacency lists do not provide a performance improvement if the underlying\ngraph is dense (and could be arguably worse since adjacency lists do not support time edge lookup).Θ(1)", "word_count": 570, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f4987d8d-e151-5c6e-bfd6-a4376cabc60f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 708, "real_page_number": null, "text": "696\nChapter 19. Graphs and Elementary Graph Algorithms\n¸ 19.2.3\nSummary of Graph Representations\nA summary of the two graph representations is shown in the following table:\nAdjacency Matrix\nAdjacency List\nΘ(|𝑉|2)|𝑉|×|𝑉|• Implemented using a matrix. Uses space.\n• Typicallyimplemented usingeithera vector/list ofedges ora hash\ntable that maps each vertex to a vector/list of adjacent edges. Uses\nΘ(|𝑉|+|𝐸|) (|𝑉| Θ(|𝐸|)space vertices and elements across all\nthe vertex lists).\n• Should be used to represent dense graphs.\n• Should be used to represent sparse graphs.\n• Given two vertices 𝑣𝑖and 𝑣𝑗, you can check if an edge exists\nbetween these two vertices in time (by accessing the valueΘ(1)\nat row 𝑖, column 𝑗of the matrix).\n• Given two vertices 𝑣𝑖and 𝑣𝑗, you can check if an edge exists\nΘ(|𝑉|)between these two vertices in worst-case time (if 𝑣𝑖is con-\nnectedtoeveryothervertex, and𝑣𝑗isatthebackofitsvertexlist),\nΘ(|𝐸|∕|𝑉|)and average-case time (assuming edges are randomly\ndistributed).\n• The time complexity of iterating over all edges in the graph is\nΘ(|𝑉|2).worst-case\n• The time complexity of iterating over all edges in the graph is\nΘ(|𝑉|+|𝐸|).worst-case\n• Adding an edge to the graph takes time, since you can justΘ(1)\nupdate the value at the correct index of the matrix.\n• Adding an edge to the graph takes time, since you can justΘ(1)\nappend the new edge to the back of the correct vertex list.\n• Removing an edge from the graph takes time, since you canΘ(1)\njust update the value at the correct index of the matrix.\nΘ(|𝐸|)• Removing an edge from the graph takes worst-case time\n(if you end up searching all edges in the list to find the one you\nΘ(|𝐸|∕|𝑉|)want to remove), and average-case time (assuming edges\nare randomly distributed).\n• Supports both directed and undirected graphs.\n– If directed, the value at row 𝑖, column 𝑗indicates if an edge\nexists from vertex 𝑣𝑖to vertex 𝑣𝑗.\n– If undirected, the adjacency matrix is symmetrical across the\ndiagonal, and thus only cells of the matrix need to be|𝑉|2∕2\nfilled out (e.g., if you know that 1, you also know that(𝑣𝑖,𝑣𝑗)=\n1).(𝑣𝑗,𝑣𝑖)=\n• Supports both directed and undirected graphs.\n– If directed, the adjacency list contains each edge once in the\nedge set, where indicates the existence of an𝑣𝑖→[…, 𝑣𝑗, …]\nedge from 𝑣𝑖to 𝑣𝑗.\nIf undirected, the adjacency list contains each edge twice–\nin the edge set, where implies that𝑣𝑖→[…, 𝑣𝑗, 𝑣𝑗→…]\n…].[…, 𝑣𝑖,\n• Supports both weighted and unweighted graphs.\n– If weighted, row 𝑖, column 𝑗of the matrix stores the weight of\nthe edge from vertex 𝑣𝑖to 𝑣𝑗if an edge exists, or ∞if no edge\nexists.\nIf unweighted, row 𝑖, column 𝑗of the matrix stores 1 if an edge–\nexists from vertex 𝑣𝑖to 𝑣𝑗, or 0 if no edge exists.\n• Supports both weighted and unweighted graphs.\n– Ifweighted,eachedgeinthevertexlistispairedwithitsweight,\nwhere indicates that the edge from 𝑣𝑖to𝑣𝑖→[…, (𝑣𝑗, 𝑘), …]\n𝑣𝑗has weight 𝑘.\nIf unweighted, each edge in the vertex list appears on its own,–\nwith no associated weight value.\nExample 19.5 You are given a file containing airport data. Each line of this file includes information on the distance between two\nconnecting airports in the following format:\n<origin airport ID> <destination airport ID> <distance in miles>\nFor example, the following data indicates that there is a connection between SFO and LAX with distance 337 miles, there is a connection\nbetween JFK and ORD with distance 740 miles, and so on.\nSFO LAX 337\nJFK ORD 740\nMIA DFW 1121\nSFO BOS 2704\nORD DFW 802\n...\nAdjMatrix AdjListImplement the and classes, which read in this data and store it in the form of an adjacency matrix or adjacency list,\ninit()respectively. Each of these classes should support an function, which reads in the contents of the data file using input redirection\nadd_edge() remove_edge()(you may assume that the data file is well-formatted); an and function, which adds or removes the\nedge_weight()specified edge from the graph; and an function, which returns the weight of an edge between two given airports. You\n\"SFO LAX 337\"may assume the graph is directed, where implies an edge from SFO to LAX, but not necessarily the other way around.", "word_count": 744, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a44f176b-39ef-573f-a56c-6bd939c9849d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 709, "real_page_number": null, "text": "19.2 Graph Representations\n697\nAdjacency matrix class:\n1\nclass AdjMatrix {\n2\n// use this \"airports\" hash table to index into the adjacency matrix\n3\n// e.g., if \"SFO\" maps to 3, then \"SFO\" is assigned index 3 of matrix\n4\nint32_t>std::unordered_map<std::string, airports;\n5\n// underlying adjacency matrix\n6\nstd::vector<std::vector<double>> adj_matrix;\n7\npublic:\n8\n// this function initializes the \"airports\" hash table,\n9\n// assume this has already been implemented for you\n10\nvoid init_airports();\n11\n// TODO: reads in data and initializes adjacency matrix\n12\nvoid init();\n13\n// TODO: adds edge connecting airport a1 with airport a2\n14\nvoid add_edge(const const doublestd::string& a1, std::string& a2, weight);\n15\n// TODO: removes edge connecting airport a1 with airport a2\n16\nvoid remove_edge(const conststd::string& a1, std::string& a2);\n17\n// TODO: returns weight of edge between airports a1 and a2,\n18\n// and infinity if the edge does not exist\n19\ndouble edge_weight(const conststd::string& a1, std::string& a2);\n20\n};\nAdjacency list class:\n1\nstruct Edge {\n2\nstd::string dest;\n3\ndouble weight;\n4\n};\n5\n6\nclass AdjList {\n7\n// underlying adjacency list\n8\nstd::unordered_map<std::string, std::vector<Edge>> adj_list;\n9\npublic:\n10\n// TODO: reads in data and initializes adjacency list\n11\nvoid init();\n12\n// TODO: adds edge connecting airport a1 with airport a2\n13\nvoid add_edge(const const doublestd::string& a1, std::string& a2, weight);\n14\n// TODO: removes edge connecting airport a1 with airport a2\n15\nvoid remove_edge(const conststd::string& a1, std::string& a2);\n16\n// TODO: returns weight of edge between airports a1 and a2,\n17\n// and infinity if the edge does not exist\n18\ndouble edge_weight(const conststd::string& a1, std::string& a2);\n19\n};\ninit()First, let’s start with the adjacency matrix implementation. The function reads in the data from the file using input redirection and uses\nairportsit to initialize the contents of the matrix. Since we have an hash table that stores a mapping from all the airports to an index, we\ncan use it to initialize all the cells of the matrix with a starting value of ∞(we are initializing to infinity instead of zero because the graph is\nadd_edge()weighted). Then, we read in the contents of the file line by line and add edges using the function. The code is shown below:\n1\nvoid AdjMatrix::init() {\n2\nstd::vector<double>(airports.size(),adj_matrix.resize(airports.size(),\n3\nstd::numeric_limits<double>::infinity()));\n4\nstd::string orig, dest;\n5\ndouble weight;\n6\nwhile (std::cin >> orig >> dest >> weight) {\n7\nadd_edge(orig, dest, weight);\n8\n} // while\n9\n} // init()\nadd_edge()To implement the function, we simply look at the corresponding index of the matrix based on the origin and destination airports\nand update its value. The code is shown below:\n1\nvoid AdjMatrix::add_edge(const const doublestd::string& a1, std::string& a2, weight) {\n2\nadj_matrix[airports[a1]][airports[a2]] = weight;\n3\n} // add_edge()\nRemoving an edge follows a similar process: we check the correct index of the matrix based on the given input strings and update its value to\ninfinity. The code is shown below:\n1\nvoid AdjMatrix::remove_edge(const conststd::string& a1, std::string& a2) {\n2\nstd::numeric_limits<double>::infinity();adj_matrix[airports[a1]][airports[a2]] =\n3\n} // remove_edge()\nTo return the edge weight, we just need to return its weight value in the matrix.\n1\ndouble AdjMatrix::edge_weight(const conststd::string& a1, std::string& a2) {\n2\nreturn adj_matrix[airports[a1]][airports[a2]];\n3\n} // edge_weight()", "word_count": 549, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a6cd071f-d663-595e-b98d-69a9484916cb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 710, "real_page_number": null, "text": "698\nChapter 19. Graphs and Elementary Graph Algorithms\nOur implementation of the adjacency matrix is now complete. Next, we will look at the adjacency list implementation. To initialize the list with\nthe contents of the file, we would go through each line and push the destination airport into the vector associated with the origin airport, along\nwith its weight. The code is shown below:\n1\nvoid AdjList::init() {\n2\nstd::string orig, dest;\n3\ndouble weight;\n4\nwhile (std::cin >> orig >> dest >> weight) {\n5\nadd_edge(orig, dest, weight);\n6\n} // while\n7\n} // init()\n8\n9\nvoid AdjList::add_edge(const const doublestd::string& a1, std::string& a2, weight) {\n10\nadj_list[a1].push_back(Edge{a2, weight});\n11\n} // add_edge()\nTo remove an edge from the adjacency list, we go through the vertex list of the origin airport and remove the edge associated with the destination\nstd::remove_if()airport. This is shown below, using the STL function:\n1\nvoid AdjList::remove_edge(const conststd::string& a1, std::string& a2) {\n2\nstd::vector<Edge>& dest = adj_list[a1];\n3\nauto [&a2](constit = std::remove_if(dest.begin(), dest.end(), Edge& e) {\n4\nreturn e.dest == a2;\n5\n});\n6\ndest.erase(it, dest.end());\n7\n} // remove_edge()\nTo return the weight of a given edge, we would look through the vertex list of the origin airport and return the weight value associated with the\nstd::find_if()destination airport. The code is shown below, using the STL function:\n1\ndouble AdjList::edge_weight(const conststd::string& a1, std::string& a2) {\n2\nstd::vector<Edge>& dest = adj_list[a1];\n3\nauto [&a2](constit = std::find_if(dest.begin(), dest.end(), Edge& e) {\n4\nreturn e.dest == a2;\n5\n});\n6\nreturn std::numeric_limits<double>::infinity()(it == dest.end()) ? : it->weight;\n7\n} // edge_weight()\nOur implementations of both classes are now complete.\nExample 19.6 You are given an adjacency that represents a graph of airports. What are the best, worst, and average-case timematrix\ncomplexities of determining if a direct flight exists between two given airports 𝑋and 𝑌?\nFor an adjacency matrix, you can determine if an edge exists by just indexing into the correct position of the matrix, 𝑚[𝑋][𝑌]. This takes Θ(1)\ntime in all cases.\nExample 19.7 You are given an adjacency that represents a graph of airports. What are the best, worst, and average-case timelist\ncomplexities of determining if a direct flight exists between two given airports 𝑋and 𝑌?\nIn the best case, airport 𝑌could be at the front of airport 𝑋’s vertex list, or the list of airport 𝑋could be empty; this allows you to determine\nΘ(|𝑉|)whether an edge exists in time. In the worst case, the list of airport 𝑋has length (if it is connected to almost all other airports), andΘ(1)\nΘ(|𝑉|)you end up having to traverse the entire list before you discover if airport 𝑌exists as a connection; this would take time. In the average\nΘ(|𝐸|∕|𝑉|), Θ(|𝐸|∕|𝑉|)case, the list of airport 𝑋has length and you would need to traverse elements in an adjacency list before you can determine\nΘ(|𝐸|∕|𝑉|)if a direct flight exists between two airports; this takes time.\nExample 19.8 You are given an adjacency that represents a graph of airports. What are the best, worst, and average-case timematrix\ncomplexities of determining the closest other airport to a given airport 𝑋?\nΘ(|𝑉|).For an adjacency matrix, you have look at all the values in the row of airport 𝑋, which has length There is no way to do better, since you\nhave to look at all of airport 𝑋’s connections to determine which connection has the lowest weight, and the only way to look at all connections\nΘ(|𝑉|)in an adjacency matrix is to iterate over all the vertices. Thus, the time complexity of this task is in all cases.\nExample 19.9 You are given an adjacency that represents a graph of airports. What are the best, worst, and average-case timelist\ncomplexities of determining the closest other airport to a given airport 𝑋? Assume the lists are not sorted by weight.\nIn the best case, the vertex list of airport 𝑋has one element, allowing you to discover the closest connection in time. In the worst case,Θ(1)\nΘ(|𝑉|)airport 𝑋is connected to all other vertices, and you will end up searching all airports to find out which one is the closest. In the average\nΘ(|𝐸|∕|𝑉|), Θ(|𝐸|∕|𝑉|)case, the list of airport 𝑋has length allowing you to find the closest airport in average-case time.", "word_count": 727, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7b61daa3-e189-5c10-9127-65ec93d71917", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 711, "real_page_number": null, "text": "19.3 Depth-First Search\n699\nExample 19.10 You are given an adjacency that represents a graph of airports. What are the best and worst-case time complexitiesmatrix\nof determining if flights depart from airport 𝑋?any\nTo determine if any flights depart from airport 𝑋in an adjacency matrix, we will have to traverse the row of airport 𝑋to see where there exists\na connection. As soon as we encounter the first edge that does not have a value of infinity, we can conclude that airport 𝑋has an outgoing flight.\nOtherwise, if all the values in the row of airport 𝑋have a value of infinity, no flights depart from airport 𝑋. In the best case, the first edge in the\nrow is not infinity, allowing us to conclude that a departing flight exists in time. In the worst case, only the last edge in the row has aΘ(1)\nvalue that is not infinity (or all the edges have a value of infinity, when happens if airport 𝑋has no outgoing connections at all). When this\nΘ(|𝑉|).happens, the entire row needs to be traversed before we can determine if a departing flight exists, which results in a runtime of (We will\nΘ(|𝑉|)not discuss the average-case analysis here, since it depends on how dense or sparse the underlying graph is, but we consider it as an\n|𝑉|operation since, on average, we have to visit a fraction of all vertices before we find the first vertex with a connection.)\nExample 19.11 You are given an adjacency that represents a graph of airports. What are the best and worst-case time complexities oflist\ndetermining if flights depart from airport 𝑋?any\nTo determine if any flights depart from airport 𝑋in an adjacency list, we just need to check if its vertex list is empty. If the vertex list of a given\nairport is empty, then that airport does not have departing connections; otherwise, it does have departing connections. Checking the length of a\nvertex list always takes time.Θ(1)\nThe adjacency matrix and adjacency list are two ways we can represent the internal contents of a graph in memory. Once we represent a graph\nin memory, we can apply different graph algorithms on it to solve many types of important graph problems. For example, given a graph of\nairports and their connections, we can use graph algorithms to find the lowest-cost path between any two airports, or the path with the fewest\nintermediary layovers, just to name a few.\nGraphs can be argued as one of the most important structures in the field of computer science, as they have enormous practical applications\nin many of the things we use on a daily basis. Map applications such as Google Maps use graph algorithms to calculate optimal routes between\ntwo locations. Social media sites use graphs to represent friendships and connections, and they utilize graph algorithms to help come up with\nfriend suggestions or suggested posts. E-commerce sites like Amazon use graph algorithms to provide users with recommendations based on\nwhat they have viewed or bought before. Search engines like Google use a graph of the web to rank search results. In your own computer’s\noperating system, graphs are used to manage the resources they are being used by running processes to ensure that deadlocks do not occur. The\ninternet itself is also built on the foundation of graphs, and graph algorithms play a crucial role in allowing us to communicate with each other,\nall across the world. In fact, graph algorithms played a pivotal role in allowing professors to seamlessly livestream lectures to students all across\nthe world, back when classes were held online amidst the global pandemic in 2020 and 2021.\nBecause the material on graphs is so vast, its contents will be split across several chapters (and even then, we will not be able to cover\neverything). In the previous chapter, we covered a restricted form of a graph known as a tree, as well as algorithms that can be used to solve tree\nproblems. In this chapter, we covered the general implementation of a graph, and we will cover important elementary graph algorithms such as\ndepth-first and breadth-first search. In the next chapter, we will cover a special type of graph known as a spanning tree, as well as algorithms\nthat can be used to build spanning trees that minimize total edge weight. Lastly, in chapter 25, we will cover shortest path algorithms, which can\nbe used to identify a path between two vertices that minimizes total distance.\n19.3\nDepth-First Search\nThe depth-first search (DFS) algorithm is a core graph traversal algorithm that can be used to systematically explore the vertices and edges of a\ngraph that are reachable from a given source vertex. In a depth-first search, the algorithm starts at a source vertex and explores each branch\nof the graph until there are no more undiscovered vertices along its path, at which point it backtracks and continues alongas far as possible\nstd::stack<>another branch. The implementation of a depth-first search generally relies on recursion or the data structure. An outline of\nthe DFS algorithm is as follows:\n1. Mark the source vertex as visited, and then push its neighbors (i.e., adjacent vertices) into a stack. This can be done by pushing all\nstd::stack<>,neighbors into an actual or by performing a recursive call on every neighbor (which pushes neighbors onto the\nprogram call stack).\n2. Pop the vertex at the top of the stack and set it as the current vertex (this behavior is automatically done by a recursive call, if a recursive\napproach is used).\n3. Push the unvisited neighbors of the current vertex into the stack (or perform a recursive call on each neighbor, if a recursive approach is\nused) and mark them as visited.\n4. Repeat steps 2 and 3 until the stack is empty.", "word_count": 980, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f770f33d-9328-5270-a983-436c249da794", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 712, "real_page_number": null, "text": "700\nChapter 19. Graphs and Elementary Graph Algorithms\n¸ 19.3.1\nIterative Depth-First Search\nstd::stack<>)A depth-first search can be done both iteratively (using a or recursively (using the program stack). For example, consider the\nfollowing graph, which we will traverse using an depth-first search starting at vertex A. Here, we will classify visited nodes into twoiterative\ngroups: and processed. A vertex is a vertex that has been encountered and pushed into the stack; discovered vertices willdiscovered discovered\nbe represented with a light gray shading. A vertex is a vertex that has already been taken out of the stack and had its adjacency listprocessed\nstd::stack<>fully examined; processed vertices will be represented with a dark gray shading. We start by initializing a to track vertices\nyet to be explored.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nStack\nCurrent Vertex\nFirst, we will mark the starting vertex as visited. In this case, our starting vertex is vertex 𝐴, so we will mark 𝐴as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nStack\nCurrent Vertex\nA\nNext, we look at the vertices that vertex 𝐴is connected to, mark them as visited, and add them to the stack. The order we add these vertices\ndoesn’t matter, but for our example, we will add them in alphabetical order (i.e., 𝐵is inserted first, then 𝐷, then 𝐹).\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nF\nStack\nCurrent Vertex\nA\nNext, we take out the vertex at the top of the stack and set it as our current vertex. Here, vertex 𝐹is at the top of the stack, so we pop it and set it\nas our current vertex. We then push all of 𝐹’s unvisited neighbors into the stack. Vertex 𝐹is connected to vertices 𝐴, 𝐺, 𝐻, and 𝐿, but only 𝐺,\n𝐻, and 𝐿are unvisited. Thus, vertices 𝐺, 𝐻, and 𝐿are pushed into the stack and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nG\nH\nL\nStack\nCurrent Vertex\nF\nVertex 𝐿is now at the top of the stack, so we pop it and set it as our current vertex. We then push all of vertex 𝐿’s unvisited neighbors into the\nstack. Vertex 𝐿is connected to vertices 𝐹and 𝐾, but only 𝐾is unvisited. Thus, only vertex 𝐾is pushed into the stack and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nG\nH\nK\nStack\nCurrent Vertex\nL\nVertex 𝐾is now at the top of the stack, so we pop it off and set it as our current vertex. We then push all of vertex 𝐾’s unvisited neighbors into\nthe stack. Vertex 𝐾is connected to vertices 𝐿and 𝐽, but only 𝐽is unvisited. Thus, only vertex 𝐽is pushed into the stack and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nG\nH\nJ\nStack\nCurrent Vertex\nK", "word_count": 508, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "94714864-ee92-534c-a229-7888b8dce150", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 713, "real_page_number": null, "text": "19.3 Depth-First Search\n701\nVertex 𝐽is now at the top of the stack, so we pop it off and set it as our current vertex. We then push all of vertex 𝐽’s unvisited neighbors into\nthe stack. Vertex 𝐽is connected to vertices 𝐻and 𝐾— however, both 𝐻and 𝐾have already been marked as visited. Thus, nothing is pushed\ninto the stack at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nG\nH\nStack\nCurrent Vertex\nJ\nVertex 𝐻is now at the top of the stack, so we pop it off and set it as our current vertex. We then push all of vertex 𝐻’s unvisited neighbors into\nthe stack. Vertex 𝐻is connected to vertices 𝐹, 𝐺, and 𝐽, but similar to before, all three of these vertices have been marked as visited. Thus,\nnothing is pushed into the stack at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nG\nStack\nCurrent Vertex\nH\nVertex 𝐺is at the top of the stack, so we pop it and set it as our current vertex. We then push all of vertex 𝐺’s unvisited neighbors into the stack.\nVertex 𝐺is connected to vertices 𝐹, 𝐻, and 𝐼, but only vertex 𝐼is unvisited. Thus, vertex 𝐼is pushed into the stack and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nI\nStack\nCurrent Vertex\nG\nVertex 𝐼is now at the top of the stack, so we pop it off and set it as our current vertex. We then push all of vertex 𝐼’s unvisited neighbors into\nthe stack. Vertex 𝐼is connected to vertex 𝐺, but vertex 𝐺has already been marked as visited. Thus, nothing is pushed into the stack at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nStack\nCurrent Vertex\nI\nVertex 𝐷is now at the top of the stack, so we pop it off and set it as our current vertex. We then push all of vertex 𝐷’s unvisited neighbors into\nthe stack. Vertex 𝐷is connected to vertices 𝐴and 𝐶, but vertex 𝐴has already been marked as visited. Thus, only vertex 𝐶is pushed into the\nstack and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nC\nStack\nCurrent Vertex\nD\nVertex 𝐶is now at the top of the stack, so we pop it off and set it as our current vertex. We then push all of vertex 𝐶’s unvisited neighbors into\nthe stack. Vertex 𝐶is connected to vertices 𝐵, 𝐷, and 𝐸, but only vertex 𝐸has not yet been visited. Thus, vertex 𝐸is pushed into the stack and\nmarked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nE\nStack\nCurrent Vertex\nC", "word_count": 475, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6c3d4e65-987d-5a9b-a06f-5d48549a04e5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 714, "real_page_number": null, "text": "702\nChapter 19. Graphs and Elementary Graph Algorithms\nVertex 𝐸is now at the top of the stack, so we pop it and set it as our current vertex. We then push all of vertex 𝐸’s unvisited neighbors into the\nstack. Vertex 𝐸is connected to vertex 𝐶, but vertex 𝐶has already been marked as visited. Thus, nothing gets pushed into the stack at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nStack\nCurrent Vertex\nE\nVertex 𝐵is now at the top of the stack, so we pop it off and set it as our current vertex. We then push all of vertex 𝐵’s unvisited neighbors into\nthe stack. Vertex 𝐵is connected to vertices 𝐴and 𝐶, but both of these vertices have already been marked as visited. Thus, nothing gets pushed\ninto the stack at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nStack\nCurrent Vertex\nB\nThe stack is now empty, so our depth-first search is complete, and we have processed every vertex reachable from vertex 𝐴. Notice that, because\na depth-first search relies on a stack to process the nodes in a graph, the algorithm explores and processes vertices in the graph one branch at a\ntime in LIFO order. Here, the depth-first search first processed the branch 𝐴→𝐹→𝐿→𝐾→𝐽:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nG\nH\nStack\nCurrent Vertex\nJ\nAfter processing these five vertices, the search then backtracks to vertex 𝐹and explores the new branch 𝐴→𝐹→𝐻, which adds vertex 𝐻to\nthe list of processed nodes.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nG\nStack\nCurrent Vertex\nH\nThe search then backtracks to vertex 𝐹and explores the branch 𝐴→𝐹→𝐺→𝐼, which adds vertices 𝐺and 𝐼to the list of processed nodes.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nD\nStack\nCurrent Vertex\nI\nThe search then backtracks to vertex 𝐴and explores the branch 𝐴→𝐷→𝐶→𝐸, adding vertices 𝐷, 𝐶, and 𝐸to the list of processed nodes.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB\nStack\nCurrent Vertex\nE\nThe search then backtracks to vertex 𝐴and explores the final branch 𝐴→𝐵, which adds vertex 𝐵to the list of processed nodes.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nStack\nCurrent Vertex\nB", "word_count": 402, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5ffb0531-582e-55ed-8d3c-f1ea4df294a0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 715, "real_page_number": null, "text": "19.3 Depth-First Search\n703\nWhy do depth-first searches exhibit this \"branch by branch\" exploration behavior? This is a consequence of how stacks work. Because elements\nin a stack can only be cleared from top to bottom, as long as a branch of the graph still has unvisited vertices remaining in its path, these vertices\nwill get pushed to the top of the stack and will thus be processed before other vertices lower in the stack. As a result, if a depth-first search starts\nexploring a branch of the graph, it will continue exploring that branch until it runs out of unvisited elements to push into the stack.\nThe order in which branches are visited and the specific vertices that end up in each branch depend on the order in which vertices are\npushed into the stack. In our iterative DFS example, because vertex 𝐹was pushed into the stack vertices 𝐵and 𝐶, branches starting withafter\nwere explored first because vertex 𝐹ended up at the top of the stack. Furthermore, because vertex 𝐷being pushed into the stack𝐴→𝐹→…\nvertex 𝐵, we ended up exploring the branch 𝐴→𝐷→𝐶→𝐸instead of the branch 𝐴→𝐵→𝐶→𝐸(note: if a recursive depth-firstafter\nsearch were performed instead, the behavior would be slightly different, since a recursive call on one neighbor runs to completion before a\nrecursive call is made on another neighbor; we will cover the recursive implementation on the next page).\nBecause a depth-first search visits all vertices that are accessible from the starting vertex, it can be used to determine if a simple path\nexists between any two vertices in a graph. If it is possible to reach a vertex 𝑌from a vertex 𝑋, then vertex 𝑌must be encountered at some\npoint during the depth-first search starting at vertex 𝑋. On the other hand, if vertex 𝑌is not accessible from vertex 𝑋(e.g., it is in a separate\ndisconnected component), then the depth-first search starting at 𝑋will never visit 𝑌before completion. For example, if we disconnected the\nedge between vertices 𝐴and 𝐹, then a depth-first search starting at vertex 𝐴will only process vertices 𝐴, 𝐵, 𝐶, 𝐷, and 𝐸, as the remaining\nelements will never be pushed into the stack.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nStack\nCurrent Vertex\nB\nThe pseudocode for determining if a path exists between two vertices using a depth-first search is shown below:\n1\nAlgorithm DFS(source, destination):\n2\nmark source as visited\n3\npush source into stack\n4\nwhile stack is not empty:\n5\nget/pop candidate from top of stack\n6\nfor each neighbor of candidate:\n7\nif neighbor is unvisited:\n8\nmark neighbor as visited\n9\npush neighbor to top of stack\n10\nif neighbor is destination:\n11\nreturn true (success)\n12\nreturn false (failure)\n¸ 19.3.2\nRecursive Depth-First Search\nA depth-first search can also be performed recursively rather than iteratively. In the recursive approach, we will push vertices onto the program\nstack via recursive calls. By using recursive stack frames to keep track of vertices yet to be explored, we bypass the need to explicitly declare a\nseparate container, which we needed with the iterative approach. Consider the same graph as before, but this time, we will perform a recursive\ndepth-first search starting at vertex 𝐴.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nStack Frames\nCurrent Vertex\nA\nPreviously, in an iterative depth-first search, we would push all of vertex 𝐴’s neighbors into a stack. However, in the recursive approach, we can\naccomplish this behavior by on each of vertex 𝐴’s unvisited neighbors. Similar to before, we will conduct recursiveperforming a recursive call\ncalls in alphabetical order, so we will recursively search vertex 𝐵first. The recursive call pushes vertex 𝐵to the top of the call stack and sets it\nas our current vertex.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nB\nStack Frames\nCurrent Vertex\nB", "word_count": 655, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "585c074b-5171-56fa-82e3-c97080597cc2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 716, "real_page_number": null, "text": "704\nChapter 19. Graphs and Elementary Graph Algorithms\nWe then make recursive calls on all of vertex 𝐵’s unvisited neighbors. Vertex 𝐵has two neighbors, 𝐴and 𝐶, but 𝐴has already been visited.\nThus, we will make a recursive call on vertex 𝐶, which becomes our current vertex.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nB\nC\nStack Frames\nCurrent Vertex\nC\nNext, we will make recursive calls on all of 𝐶’s unvisited neighbors. Vertex 𝐶has three neighbors: 𝐵, 𝐷, and 𝐸. Vertex 𝐵has already been\nvisited, so we will make a recursive call on vertex 𝐷, which becomes our current vertex.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nB\nC\nD\nStack Frames\nCurrent Vertex\nD\nWe then make recursive calls on all of vertex 𝐷’s unvisited neighbors. Since all of 𝐷’s neighbors have been visited, no recursive calls are made,\nand the recursion unrolls back to vertex 𝐶.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nB\nC\nStack Frames\nCurrent Vertex\nC\nNext, we will make a recursive call on vertex 𝐶’s remaining unvisited neighbor, vertex 𝐸.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nB\nC\nE\nStack Frames\nCurrent Vertex\nE\nVertex 𝐸has no unvisited neighbors, so the recursion unrolls back to vertex 𝐶.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nB\nC\nStack Frames\nCurrent Vertex\nC\nAll of vertex 𝐶’s neighbors have been visited, so the recursion unrolls back to vertex 𝐵.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nB\nStack Frames\nCurrent Vertex\nB\nAll of vertex 𝐵’s neighbors have been visited, so the recursion unrolls back to vertex 𝐴.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nStack Frames\nCurrent Vertex\nA", "word_count": 318, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "347c0ded-38df-56d3-a6ef-740956196c92", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 717, "real_page_number": null, "text": "19.3 Depth-First Search\n705\nNow, we will make a recursive call on vertex 𝐹, which is vertex 𝐴’s only remaining unvisited vertex. Note that vertex 𝐷was unvisited before\nthe recursive call to 𝐵, but the recursive call to 𝐵ended up visiting 𝐷, so we don’t need to visit it again.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nStack Frames\nCurrent Vertex\nF\nNext, we will make a recursive call on each of vertex 𝐹’s unvisited neighbors. Vertex 𝐺is the first neighbor alphabetically, so we will make a\nrecursive call on 𝐺, which becomes our current vertex.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nStack Frames\nCurrent Vertex\nG\nWe then make recursive calls on all of 𝐺’s unvisited neighbors. Vertex 𝐺has three neighbors, vertices 𝐹, 𝐻, and 𝐼. Vertex 𝐹has already been\nvisited, so we will make a recursive call on vertex 𝐻.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nH\nStack Frames\nCurrent Vertex\nH\nWe then make a recursive call to all of 𝐻’s unvisited neighbors. Vertex 𝐽is the only unvisited neighbor of 𝐻, so we will make a recursive call\non vertex 𝐽, which becomes our current vertex.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nH\nJ\nStack Frames\nCurrent Vertex\nJ\nNext, we make a recursive call on 𝐽’s only unvisited neighbor, vertex 𝐾.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nH\nJ\nK\nStack Frames\nCurrent Vertex\nK\nWe then make a recursive call on 𝐾’s only unvisited neighbor, vertex 𝐿.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nH\nJ\nK\nL\nStack Frames\nCurrent Vertex\nL\nVertex 𝐿has no unvisited neighbors, so the recursion unrolls back to vertex 𝐾.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nH\nJ\nK\nStack Frames\nCurrent Vertex\nK", "word_count": 345, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "be55fa3e-6418-5fe2-b8bd-3fa7851e8ec8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 718, "real_page_number": null, "text": "706\nChapter 19. Graphs and Elementary Graph Algorithms\nVertex 𝐾has no unvisited neighbors, so the recursion unrolls back to vertex 𝐽.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nH\nJ\nStack Frames\nCurrent Vertex\nJ\nVertex 𝐽has no unvisited neighbors, so the recursion unrolls back to vertex 𝐻.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nH\nStack Frames\nCurrent Vertex\nH\nVertex 𝐻has no unvisited neighbors, so the recursion unrolls back to vertex 𝐺.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nStack Frames\nCurrent Vertex\nG\nNow, we will make a recursive call on vertex 𝐼, which is vertex 𝐺’s only remaining unvisited neighbor.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nI\nStack Frames\nCurrent Vertex\nI\nVertex 𝐼has no unvisited neighbors, so no recursive call is made. The recursion unrolls back to vertex 𝐺.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nG\nStack Frames\nCurrent Vertex\nG\nAll of vertex 𝐺’s neighbors have now been visited, so the recursion unrolls back to vertex 𝐹.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nF\nStack Frames\nCurrent Vertex\nF\nVertex 𝐹’s neighbors have also all been visited, so the recursion unrolls back to vertex 𝐴.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nA\nStack Frames\nCurrent Vertex\nA", "word_count": 257, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7e160d0f-571d-5868-8cef-afe745cead13", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 719, "real_page_number": null, "text": "19.3 Depth-First Search\n707\nAll of vertex 𝐴’s neighbors have now been visited. Since 𝐴was the source node, the recursive depth-first search starting at vertex 𝐴is\nnow complete, and we have successfully processed all vertices in the graph that are reachable from vertex 𝐴. The order of branches we\nexplored during the recursive search was different from the branches we explored during the iterative search: we first explored the branch\n𝐴→𝐵→𝐶→𝐷, then we backtracked to vertex 𝐶and explored the branch 𝐴→𝐵→𝐶→𝐸, then we backtracked to vertex 𝐴and explored\nthe branch 𝐴→𝐹→𝐺→𝐻→𝐽→𝐾→𝐿, then we backtracked to vertex G and explored the branch 𝐴→𝐹→𝐺→𝐼. This is because a\nrecursive call on a node automatically sets it as our current vertex.\n¸ 19.3.3\nDepth-First Search Time Complexity\nWhat is the time complexity of a depth-first search? This depends on whether the underlying graph is represented as an adjacency list or an\nadjacency matrix. In an adjacency list, each vertex is only visited once, and we only have to traverse the neighbors of each vertex whendirect\nwe iterate through its vertex list. As a result, each edge in an adjacency list is visited at most once in a directed graph, and at most twice in an\nundirected graph (an undirected edge connecting two vertices 𝐴and 𝐵is checked twice during the depth-first search: once when checking the\nneighbors of 𝐴, and once when checking the neighbors of 𝐵). Because each vertex and each edge is visited a constant number of times during a\nΘ(|𝑉|+|𝐸|).depth-first search, the overall time complexity of performing DFS on an adjacency list is worst-case On the other hand, traversing\n|𝑉| Θ(|𝑉|)through the edges of a vertex in an adjacency requires us to iterate through vertices in its row. Since this process is donematrix all\nΘ(|𝑉|2).|𝑉| times, once for each vertex, the overall time complexity of performing DFS on an adjacency matrix is worst-case\nIn summary, depth-first searches provide a method for traversing over all of the vertices in a graph reachable from a given source vertex. If\nyou recall from the previous chapter, trees themselves are a special type of graph. Thus, the preorder, inorder, and postorder traversals are\nactually variants of a depth-first search on a tree, since they rely on a recursive stack to explore the nodes of a tree. The idea of depth-first\nsearch actually appears in many different types of problems, and it will show up again in the future when we discuss algorithm families such as\nbacktracking and branch and bound.\nGraph Representation\nAdjacency List\nAdjacency Matrix\nDFS Time Complexity\nΘ(|𝑉|+|𝐸|)\nΘ(|𝑉|2)\n¸ 19.3.4\nSolving Problems Using Depth-First Search\nExample 19.12 You are given an unweighted, undirected graph in the form of an adjacency list (where 𝑛vertices are labeled from 0 to\n𝑛−1) and two vertices: a source and a destination. Write a function that returns all possible paths from the source vertex to the destination\nthat visits a vertex at most once (i.e., no cycles), in any order. The function header is shown below:\nstd::vector<std::vector<int32_t>> find_all_paths(const std::vector<std::vector<int32_t>>& graph,\nint32_t int32_tsource, dest);\nExample: Given the following graph:\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n1\n2\n3\n0\n4\n0\n3\n4\n0\n2\n4\n1\n2\n3\nyou should return the following paths, in any order:\n[[0, 1, 4, 2, 3], [0, 1, 4, 3], [0, 2, 3], [0, 2, 4, 3], [0, 3]]\nThis is a graph search problem, so we can solve it by performing a depth-first search. We will start with the source node and begin our search,\nkeeping track of our current path in a vector. If the destination is ever reached, we push the contents of this path into our solution vector. Since\nthe graph is undirected and we want to avoid cycles, we must mark vertices in our current path as so that we do not end up in a cycle. Anvisited\nillustration of this process is shown below. We start at vertex 0, our source vertex, and make a recursive call on each of its adjacent vertices (1, 2,\nand 3). We will start by recursing on vertex 1, which pushes it into the call stack. We then mark vertex 1 as visited and add it to our path.\n0\n1\n2\n3\n4\n1\nStack\n0\n1\nPath\nSolutions:", "word_count": 722, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3202519f-c107-548b-b8dc-34bfc1f176ef", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 720, "real_page_number": null, "text": "708\nChapter 19. Graphs and Elementary Graph Algorithms\nWe then make a recursive call on each of 1’s unvisited adjacent vertices. In this case, the only unvisited neighbor is 4, so we make a recursive\ncall on 4, which pushes it into the call stack. We then mark vertex 4 as visited and add it to our path.\n0\n1\n2\n3\n4\n1\n4\nStack\n0\n1\n4\nPath\nSolutions:\nWe then make a recursive call on each of 4’s unvisited vertices. In this case, the unvisited neighbors are 2 and 3. We will first make a recursive\ncall on 2, which pushes it into the call stack. We then mark vertex 2 as visited and add it to our path.\n0\n1\n2\n3\n4\n1\n4\n2\nStack\n0\n1\n4\n2\nPath\nSolutions:\nWe then make a recursive call on each of 2’s unvisited vertices. In this case, the only unvisited neighbor is 3, which is our destination vertex. As\na result, we can add 3 to our path and append it to our list of solutions.\n0\n1\n2\n3\n4\n1\n4\n2\n3\nStack\n0\n1\n4\n2\n3\nPath\nSolutions:\n[0, 1, 4, 2, 3]\nWe have finished processing all of 2’s unvisited neighbors, so the recursion unrolls back to vertex 4. We then consider 4’s other unvisited\nneighbor, which happens to be 3, our destination vertex. As a result, we can also this path to our list of solutions.\n0\n1\n2\n3\n4\n1\n4\n3\nStack\n0\n1\n4\n3\nPath\nSolutions:\n[0, 1, 4, 2, 3]\n[0, 1, 4, 3]\nWe have finished processing all of 4’s unvisited neighbors, so the recursion unrolls back to vertex 1. Additionally, since 4 was 1’s only unvisited\nneighbor, the recursion unrolls back to vertex 0. We now make a recursive call on 0’s next unvisited neighbor of 2, which pushes it into the call\nstack. We then mark vertex 2 as visited and add it to our path.\n0\n1\n2\n3\n4\n2\nStack\n0\n2\nPath\nSolutions:\n[0, 1, 4, 2, 3]\n[0, 1, 4, 3]\nWe then make a recursive call on each of 2’s unvisited vertices, which are 3 and 4. Since 3 is our destination, we can add this path to our solution.\n0\n1\n2\n3\n4\n2\n3\nStack\n0\n2\n3\nPath\nSolutions:\n[0, 1, 4, 2, 3]\n[0, 1, 4, 3]\n[0, 2, 3]\nWe then process 2’s unvisited vertex of 4. Recursing into vertex 4 gives us the following path, which we add to our solution (note that we also\nrecurse into vertex 1 after recursing into vertex 4, but that search does not lead to our destination vertex, so nothing gets added to the solution\nalong that search path).\n0\n1\n2\n3\n4\n2\n4\n3\nStack\n0\n2\n4\n3\nPath\nSolutions:\n[0, 1, 4, 2, 3]\n[0, 1, 4, 3]\n[0, 2, 3]\n[0, 2, 4, 3]", "word_count": 500, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e7b7b8cc-f883-5432-b3f6-3336ed377a58", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 721, "real_page_number": null, "text": "19.3 Depth-First Search\n709\nThe recursion then unrolls back to vertex 0. The last neighbor of 0 we have yet to consider is vertex 3, which is our destination vertex. We\ntherefore add this path to our solution. Since we have searched every neighbor of the source vertex, our algorithm is now complete, and our\nsolutions vector holds all the valid paths from the source to the destination.\n0\n1\n2\n3\n4\n3\nStack\n0\n3\nPath\nSolutions:\n[0, 1, 4, 2, 3]\n[0, 1, 4, 3]\n[0, 2, 3]\n[0, 2, 4, 3]\n[0, 3]\nAn implementation of this solution is shown below:\n1\nvoid find_all_paths_helper(const std::vector<std::vector<int32_t>>& graph,\n2\nint32_t int32_t std::vector<bool>&curr, dest, visited,\n3\nstd::vector<int32_t>& current_path,\n4\nstd::vector<std::vector<int32_t>>& solution) {\n5\ntrue;visited[curr] =\n6\ncurrent_path.push_back(curr);\n7\n8\nif (curr == dest) {\n9\n// we have reached the destination vertex, so append the current path to the solution\n10\nsolution.push_back(current_path);\n11\n} // if\n12\nelse {\n13\n// recurse into all adjacent vertices that are not visited\n14\nconst std::vector<int32_t>& neighbors = graph[curr];\n15\nfor (int32_t neighbor : neighbors) {\n16\nif (!visited[neighbor]) {\n17\nfind_all_paths_helper(graph, neighbor, dest, visited, current_path, solution);\n18\n} // if\n19\n} // for i\n20\n} // else\n21\n22\n// once you are done processing current vertex, remove it from path and mark it as unvisited\n23\ncurrent_path.pop_back();\n24\nfalse;visited[curr] =\n25\n} // find_all_paths_helper()\n26\n27\nstd::vector<std::vector<int32_t>> find_all_paths(const std::vector<std::vector<int32_t>>& graph,\n28\nint32_t int32_tsource, dest) {\n29\nstd::vector<bool> false);visited(graph.size(),\n30\nstd::vector<int32_t> current_path;\n31\nstd::vector<std::vector<int32_t>> solution;\n32\nfind_all_paths_helper(graph, source, dest, visited, current_path, solution);\n33\nreturn solution;\n34\n} // find_all_paths()\nExample 19.13 You are given 𝑛devices. Some of these devices are connected, while some are not. If device 𝑋is directly connected to\ndevice 𝑌, and device 𝑌is directly connected to device 𝑍, then device 𝑋is connected to device 𝑍.indirectly\nA network is a group of directly or indirectly connected devices. Two devices are a part of different networks if they are not connected in\nany manner, neither directly nor indirectly.\n𝑖th 𝑗thdevices, devices[i][j] = trueYou are given an 𝑛×𝑛adjacency matrix where if the and devices are directly connected,\ndevices[i][j] = false devices[i][i] = falseand otherwise. You may assume that without self loops. Implement the\nnum_devices() function, which returns the total number of networks that exist among the devices.\nint32_t num_devices(std::vector<std::vector<bool>>& devices);\nnum_devices()Example: Given the following devices, the function would return 3, since there exist three networks among these\ndevices (𝐴-𝐶, 𝐷-𝐸, and 𝐹-𝐼).\nA\nB\nC\nD\nE\nF\nG\nH\nI", "word_count": 432, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3a9ba3ff-e516-5d2c-b822-164bdfa08ae8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 722, "real_page_number": null, "text": "710\nChapter 19. Graphs and Elementary Graph Algorithms\nOne way to solve this problem is to use a depth-first search. A depth-first search can be used to identify all devices that are reachable from a\ngiven device, which allows you to determine the groups of devices that form each network. For example, if you start a depth-first search on\ndevice 𝐴, you will learn that devices 𝐵and 𝐶are part of the same network.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nTherefore, to solve this problem, we can iterate over all devices and conduct a depth-first search on a device as long as it has not been visited.\nThe number of searches we need is equal to the number of networks among the devices, since each search visits all devices within each network.\nIn our example, a depth-first search on device A ends up visiting devices 𝐵and 𝐶, a depth-first search on device 𝐷ends up visiting device 𝐸,\nand a depth-first search on device 𝐹ends up visiting devices 𝐺, 𝐻, and 𝐼. A total of three searches are needed to visit all the devices, so the total\nnumber of networks must be three. The problem can be solved recursively or iteratively. A recursive depth-first search solution is shown below:\n1\nvoid dfs_helper(std::vector<std::vector<bool>>& adj_matrix,\n2\nstd::vector<bool>& size_tvisited, i) {\n3\ntrue;visited[i] =\n4\nfor (size_t j = 0; j < visited.size(); ++j) {\n5\nif (adj_matrix[i][j] && !visited[j]) {\n6\ndfs_helper(adj_matrix, visited, j);\n7\n} // if\n8\n} // for j\n9\n} // dfs_helper()\n10\n11\nint32_t num_devices(std::vector<std::vector<bool>>& adj_matrix) {\n12\nstd::vector<bool> false);visited(adj_matrix.size(),\n13\nint32_t network_count = 0;\n14\nfor (size_t i = 0; i < visited.size(); ++i) {\n15\nif (!visited[i]) {\n16\n++network_count;\n17\ndfs_helper(adj_matrix, visited, i);\n18\n} // if\n19\n} // for i\n20\nreturn network_count;\n21\n} // num_devices()\nstd::stack<>Alternatively, an iterative depth-first search using a can also be used:\n1\nint32_t num_devices(std::vector<std::vector<bool>>& adj_matrix) {\n2\nstd::vector<bool> false);visited(adj_matrix.size(),\n3\nstd::stack<size_t> dfs;\n4\nint32_t network_count = 0;\n5\nfor (size_t i = 0; i < visited.size(); ++i) {\n6\nif (!visited[i]) {\n7\ntrue;visited[i] =\n8\ndfs.push(i);\n9\nwhile (!dfs.empty()) {\n10\nsize_t curr = dfs.top();\n11\ndfs.pop();\n12\nfor (size_t j = 0; j < adj_matrix[curr].size(); ++j) {\n13\nif (adj_matrix[curr][j] && !visited[j]) {\n14\ntrue;visited[j] =\n15\ndfs.push(j);\n16\n} // if\n17\n} // for j\n18\n} // while\n19\n++network_count;\n20\n} // if\n21\n} // for i\n22\nreturn network_count;\n23\n} // num_devices()\nExample 19.14 If you have worked with any image editing software before, you are probably familiar with the \"fill\" tool, which can be\nused to apply a color to a specific region of an image. In Microsoft Paint, for example, the fill tool is represented using a paint bucket, which\nis selected in the screenshot below. In this problem, we will implement a function that emulates the behavior of the fill tool on a given image.", "word_count": 501, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "296e52da-725a-5144-892e-06a0046a7ab4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 723, "real_page_number": null, "text": "19.3 Depth-First Search\n711\nimage image[i][j]You are given a vector of vectors with 𝑚rows and 𝑛columns, where represents the color of a pixel in the image.\nrow, column, color. colorYou are also given three integers, and Write a function that completes a fill of the image when the color is\nimage[row][column].applied on the pixel To perform a fill, all pixels that are 4-directionally adjacent to the starting pixel with the\nsame color (as well as any pixels of the same color that are adjacent to those pixels) are replaced with the new color. You may assume that\nthe provided row and column are not out of bounds. The function header is shown below:\nvoid apply_fill(std::vector<std::vector<int32_t>>& int32_t int32_t int32_timage, row, column, color);\nExample: Given the following image:\n1\n1\n1\n2\n2\n2\n1\n1\n1\n2\n1\n1\n0\n0\n0\n2\n1\n2\n1\n0\n2\n2\n1\n1\n1\nIf a fill is applied with a color of 3 on the pixel at row 1, column 2 (which currently has a color of 1, shown by the bolded cell below), then\nall adjacent pixels with a color of 1 are changed to have a color of 3.\n1\n1\n1\n2\n2\n2\n1\n1\n1\n2\n1\n1\n0\n0\n0\n2\n1\n2\n1\n0\n2\n2\n1\n1\n1\n3\n3\n3\n2\n2\n2\n3\n3\n3\n2\n3\n3\n0\n0\n0\n2\n3\n2\n1\n0\n2\n2\n1\n1\n1\nThis problem can be solved using a graph searching algorithm. We begin at the starting pixel (whose color is to be changed) and search for all\npixels that are reachable from this starting pixel that share the same color. This is done by changing the color of a pixel, and then checking its\nfour neighboring pixels to see if they share the same original color. If any pixel does share the same color, we update its color and search its\nneighboring pixels as well using the same process. This continues until no pixel reachable from the starting pixel contains the original color that\nwas changed. A recursive depth-first search solution is shown below:\n1\nvoid dfs_helper(std::vector<std::vector<int32_t>>& int32_t int32_timage, curr_row, curr_column,\n2\nint32_t int32_tstarting_color, new_color) {\n3\n// out of bounds check + check if color should be applied (if not, return)\n4\nif (curr_row < 0 || curr_row >= image.size() || curr_column < 0 ||\n5\ncurr_column >= image[curr_row].size() || image[curr_row][curr_column] == new_color ||\n6\nimage[curr_row][curr_column] != starting_color) {\n7\nreturn;\n8\n} // if\n9\n// if the code made it here, image[curr_row][curr_column] should be changed to a new color\n10\nimage[curr_row][curr_column] = new_color;\n11\n// perform a DFS to search adjacent pixels and update their colors if necessary\n12\ndfs_helper(image, curr_row - 1, curr_column, starting_color, new_color);\n13\ndfs_helper(image, curr_row + 1, curr_column, starting_color, new_color);\n14\ndfs_helper(image, curr_row, curr_column - 1, starting_color, new_color);\n15\ndfs_helper(image, curr_row, curr_column + 1, starting_color, new_color);\n16\n} // dfs_helper()\n17\n18\nvoid apply_fill(std::vector<std::vector<int32_t>>& int32_timage, row,\n19\nint32_t int32_tcolumn, color) {\n20\nint32_t starting_color = image[row][column];\n21\ndfs_helper(image, row, column, starting_color, color);\n22\n} // apply_fill()", "word_count": 529, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6825e742-6308-5cd9-b650-6d0fab6ad805", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 724, "real_page_number": null, "text": "712\nChapter 19. Graphs and Elementary Graph Algorithms\nThis problem can also be solved iteratively. An iterative DFS solution is provided below:\n1\nvoid apply_fill(std::vector<std::vector<int32_t>>& int32_timage, row,\n2\nint32_t int32_tcolumn, color) {\n3\n// if pixel already has the new color, no need to change anything\n4\nint32_t starting_color = image[row][column];\n5\nif (starting_color == color) {\n6\nreturn;\n7\n} // if\n8\n9\nint32_t num_rows = image.size();\n10\nint32_t num_columns = image[0].size();\n11\nstd::stack<std::pair<int32_t, int32_t>> dfs;\n12\n13\n// this allows us to easily visit the positions adjacent to the current pixel\n14\nstd::vector<std::pair<int32_t, int32_t>> dirs = {{1, 0}, {-1, 0}, {0, 1}, {0, -1}};\n15\n16\n// apply new color to pixel (this also marks it as \"visited\" for our search)\n17\nimage[row][column] = color;\n18\n19\n// perform a DFS of adjacent pixels\n20\ndfs.emplace(row, column);\n21\nwhile (!dfs.empty()) {\n22\nauto [curr_row, curr_column] = dfs.top();\n23\ndfs.pop();\n24\nfor (auto [x_increment, y_increment] : dirs) {\n25\nint32_t new_row = curr_row + x_increment;\n26\nint32_t new_column = curr_column + y_increment;\n27\n// out of bounds check\n28\nif (new_row < 0 || new_column < 0 || new_row >= image.size() || new_column >= image[0].size()) {\n29\ncontinue;\n30\n} // if\n31\n// if pixel has same color as starting color, update to new color and push into search stack\n32\nif (image[new_row][new_column] == starting_color) {\n33\nimage[new_row][new_column] = color;\n34\ndfs.emplace(new_row, new_column);\n35\n} // if\n36\n} // for [x_increment, y_increment]\n37\n} // while\n38\n} // apply_fill()\n19.4\nBreadth-First Search\n¸ 19.4.1\nBreadth-First Search\nThe breadth-first search (BFS) algorithm is another graph traversal algorithm that can be used to explore the vertices and edges of a graph that\nare reachable from a given source vertex. In a breadth-first search, the algorithm starts at a source vertex and explores vertices of the graph in a\nbreadthward fashion, node. The implementation of a breadth-first search relies on thein order of increasing edge distance from the source\nstd::queue<> data structure. An outline of the algorithm is as follows:\n1. Mark the source vertex as visited, and then push its neighbors (i.e., adjacent vertices) into a queue.\n2. Pop the vertex at the front of the queue and set it as the current vertex.\n3. Push the unvisited neighbors of the current vertex into the queue and mark them as visited.\n4. Repeat steps 2 and 3 until the queue is empty.\nAs an example, consider the following graph, which we will traverse using a breadth-first search starting at vertex 𝐴. Similar to the depth-first\nsearch example, discovered vertices will be given a light gray shading, while processed vertices will be given a dark gray shading. We start the\nstd::queue<>breadth-first search process by creating a that will be used to keep track of the vertices that we still need to explore.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nQueue\nCurrent Vertex", "word_count": 493, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f084fafb-1671-5389-820d-cd641e7f9492", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 725, "real_page_number": null, "text": "19.4 Breadth-First Search\n713\nThe first step is to mark the starting vertex as visited. In this case, our starting vertex is vertex 𝐴, so we will mark 𝐴as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nQueue\nCurrent Vertex\nA\nNext, we look at the vertices that vertex 𝐴is connected to, mark them as visited, and add them to the queue. The order we add these vertices\ndoes not matter, but for our example, we will add them in alphabetical order (i.e., 𝐵is inserted first, then 𝐷, then 𝐹).\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nB D F\nQueue\nCurrent Vertex\nA\nWe then take out the vertex at the front of the queue, vertex 𝐵, and set it as our current vertex. Since vertex 𝐵is our current vertex, we push all\nof its unvisited neighbors into the queue. Vertex 𝐵is connected to vertices 𝐴and 𝐶, but only vertex 𝐶is marked as unvisited, so vertex 𝐶gets\npushed into the queue and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nCD F\nQueue\nCurrent Vertex\nB\nVertex 𝐷is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐷’s unvisited neighbors\ninto the queue. Vertex 𝐷is connected to vertices 𝐴and 𝐶, but both 𝐴and 𝐶have been marked as visited. Thus, nothing is pushed into the\nqueue at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nCF\nQueue\nCurrent Vertex\nD\nVertex 𝐹is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐹’s unvisited neighbors\ninto the queue. Vertex 𝐹is connected to vertices 𝐴, 𝐺, 𝐻, and 𝐿, but only vertex 𝐴has been marked as visited. Thus, vertices 𝐺, 𝐻, and 𝐿are\npushed into the queue and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nC G H L\nQueue\nCurrent Vertex\nF\nVertex 𝐶is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐶’s unvisited neighbors\ninto the queue. Vertex 𝐶is connected to vertices 𝐵, 𝐷, and 𝐸, but only vertex 𝐸has not yet been marked as visited. Thus, vertex 𝐸is pushed\ninto the queue and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nG H L E\nQueue\nCurrent Vertex\nC", "word_count": 439, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e2bc96ac-7513-5ed4-9831-88df1c87c2f9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 726, "real_page_number": null, "text": "714\nChapter 19. Graphs and Elementary Graph Algorithms\nVertex 𝐺is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐺’s unvisited neighbors\ninto the queue. Vertex 𝐺is connected to vertices 𝐹, 𝐻, and 𝐼, but only vertex 𝐼has not yet been marked as visited. Thus, vertex 𝐼is pushed\ninto the queue and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nH L E I\nQueue\nCurrent Vertex\nG\nVertex 𝐻is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐻’s unvisited neighbors\ninto the queue. Vertex 𝐻is connected to vertices 𝐹, 𝐺, and 𝐽, but only vertex 𝐽has not yet been marked as visited. Thus, vertex 𝐽is pushed\ninto the queue and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nL E I\nJ\nQueue\nCurrent Vertex\nH\nVertex 𝐿is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐿’s unvisited neighbors\ninto the queue. Vertex 𝐿is connected to vertices 𝐹and 𝐾, but only vertex 𝐾has not yet been marked as visited. Thus, vertex 𝐾is pushed into\nthe queue and marked as visited.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nE I\nJ K\nQueue\nCurrent Vertex\nL\nVertex 𝐸is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐸’s unvisited neighbors\ninto the queue. Vertex 𝐸is connected to vertex 𝐶, which has already been marked as visited. Thus, nothing is pushed into the queue at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nI\nJ K\nQueue\nCurrent Vertex\nE\nVertex 𝐼is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐼’s unvisited neighbors into\nthe queue. Vertex 𝐼is connected to vertex 𝐺, which has already been marked as visited. Thus, nothing is pushed into the queue at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nJ K\nQueue\nCurrent Vertex\nI\nVertex 𝐽is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐽’s unvisited neighbors into\nthe queue. Vertex 𝐽is connected to vertices 𝐻and 𝐾, but both have already been visited. Thus, nothing is pushed into the queue at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nK\nQueue\nCurrent Vertex\nJ", "word_count": 477, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bdb3bd61-1c34-5c27-b931-9027ad97e048", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 727, "real_page_number": null, "text": "19.4 Breadth-First Search\n715\nVertex 𝐾is now at the front of the queue, so we pop it off and set it as our current vertex. We then push all of vertex 𝐾’s unvisited neighbors into\nthe queue. Vertex 𝐾is connected to vertices 𝐽and 𝐿, but both have already been visited. Thus, nothing is pushed into the queue at this step.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nQueue\nCurrent Vertex\nK\nThe queue is now empty, so our breadth-first search is complete, and we have successfully processed every vertex in the graph. Because a\nbreadth-first search relies on a queue to process nodes in a graph, the algorithm processes vertices in the graph in FIFO order. As a result, a\nbreadth-first search ends up visiting vertices in order of increasing edge distance from the source vertex: vertices that are closer to the source\nvertex are pushed into the queue vertices that are farther away, and are thus taken out of the queue and processed earlier as well! In ourbefore\nexample, the breadth-first search first processed vertices that were one edge away from the source vertex 𝐴: vertices 𝐵, 𝐷, and 𝐹(in this order,\nsince we pushed them into the queue in alphabetical order).\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nC\nQueue\nCurrent Vertex\nF\nThen, the breadth-first search processed vertices that were two edges away from the source vertex 𝐴:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nC\nQueue\nCurrent Vertex\nF\nThen, the breadth-first search processed vertices that were three edges away from the source vertex 𝐴:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nC\nQueue\nCurrent Vertex\nF\nHad the graph been larger, the breadth-first search would then have processed vertices four edges away, then five edges away, and so on. Because\nbreadth-first searches exhibit this behavior, they can always be used to find the shortest path between any two vertices in an unweighted graph,\nAs a result, breadth-first searches are often useful for solving shortest path graph problems,or a graph where all edge weights are the same.\nunweighted.3particularly for graphs that are\nSimilar to a depth-first search, a breadth-first search also visits all vertices that are accessible from any starting vertex, and thus can be used\nto determine if a simple path exists between any two vertices in a graph. If a vertex is never discovered during a breadth-first search before it\ncompletes (when the queue becomes empty), then that vertex is not accessible from the starting node. The pseudocode for determining if a path\nexists between two vertices using a breadth-first search is shown below:\n1\nAlgorithm BFS(source, destination):\n2\nmark source as visited\n3\npush source into queue\n4\nwhile queue is not empty:\n5\nget/pop candidate from front of queue\n6\nfor each neighbor of candidate:\n7\nif neighbor is unvisited:\n8\nmark neighbor as visited\n9\npush neighbor to top of queue\n10\nif neighbor is destination:\n11\nreturn true (success)\n12\nreturn false (failure)\n3Ifagraphisweighted,abreadth-firstsearchwill nolongerguaranteetheshortestpathbetweenanytwo vertices. Instead, wewillhavetouseashortestpath\nalgorithmsuchas insteadofBFS.WewillcoverDijkstra’sinalaterchapter.Dijkstra’salgorithm", "word_count": 560, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e3380f7a-478b-5e5b-8ef5-254b9a6fbe5f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 728, "real_page_number": null, "text": "716\nChapter 19. Graphs and Elementary Graph Algorithms\n¸ 19.4.2\nBreadth-First Search Time Complexity\nΘ(|𝑉|2)Θ(|𝑉|+|𝐸|)Similar to a depth-first search, the time complexity of a breadth-first search is worst-case on an adjacency list, and on\nan adjacency matrix. In an adjacency list, each vertex and edge in the graph is visited a constant number of times. However, in an adjacency\n|𝑉|matrix, iterating over all the edges of a vertex would require you to iterate over all vertices in the graph to check where an edge exists, which\nΘ(|𝑉|2) Θ(|𝑉|) |𝑉|results in a time complexity, as a is done times.\nIn summary, breadth-first searches provide a way to traverse over all of the vertices in a graph, and they can also be used to discover the\nshortest path between two vertices in an unweighted graph. If you think back to tree traversals, the level-order traversal is a breadth-first search\non a tree, since it uses a queue to determine the order in which nodes are processed. This is why a level-order traversal is able to visit nodes of a\ntree level by level; by performing a breadth-first search, it will always visit nodes that are closer to the root before nodes that are farther away!\nGraph Representation\nAdjacency List\nAdjacency Matrix\nBFS Time Complexity\nΘ(|𝑉|+|𝐸|)\nΘ(|𝑉|2)\n¸ 19.4.3\nSolving Problems Using Breadth-First Search\nExample 19.15 You are given an unweighted, undirected graph in the form of an adjacency list (where 𝑛vertices are labeled from 0\nto 𝑛−1) and two vertices: a source and a destination. Write a function that returns the path from the source vertex. If there areshortest\nmultiple shortest paths, you can return any. If there is no path, return an empty vector. The function header is shown below:\nstd::vector<int32_t> find_shortest_path(const std::vector<std::vector<int32_t>>& graph,\nint32_t int32_tsource, dest);\n[0, 2, 3].Example: Given the following graph, a source of 0, and a destination of 3, you should return\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n1\n2\n0\n4\n0\n3\n2\n4\n1\n3\nThis is a graph search problem, but unlike the previous example, we want the shortest path rather than any path between two vertices. Since\nthe graph is unweighted, the shortest path is the one that traverses the fewest nodes between the starting vertex and the destination. Thus, we\nwould want to use a breadth-first search, as this method visits vertices in order of distance from the source node (via a FIFO queue). Once you\nencounter the destination vertex along your breadth-first search, you have found the shortest path!\nTo fully implement a solution, you will need to and reproduce the path that brought you to the destination vertex along yourbacktrace\nsearch. For instance, if you wanted to find the shortest path from vertex 0 to vertex 3, you would need to identify the sequence of vertices that\nyou followed to get to vertex 3. This can be done by tracking the of each vertex encountered, starting from the source. As anpredecessor\nexample, let us complete a breadth-first search from vertex 0 to vertex 3 using the graph above. The first vertices we push into the queue are 1\nand 2, since they are the unvisited vertices that are directly reachable from 0. Since 1 and 2 are pushed into the queue while examining vertex 0,\nwe will set vertex 0 as the of vertices 1 and 2 (represented using the directional edge arrows below).predecessor\n0\n1\n2\n3\n4\n1 2\nQueue\nCurrent Vertex\n0\nVertex 1 is at the front of the queue, so we take it out and set it as our current vertex. The only other unvisited vertex directly connected to vertex\n1 is vertex 4, which we push into the queue. We will also set vertex 1 as the predecessor of 4.\n0\n1\n2\n3\n4\n2 4\nQueue\nCurrent Vertex\n1", "word_count": 652, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "00f99973-90e6-5731-9ede-7cfaa7d18391", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 729, "real_page_number": null, "text": "19.4 Breadth-First Search\n717\nVertex 2 is at the front of the queue, so we take it out and set it as our current vertex. Notice that our destination, vertex 3, is directly reachable\nfrom vertex 2. Since a breadth-first search explores nodes in increasing distance from the source node, we have therefore found the shortest path\nfrom source to destination. After setting vertex 2 as the predecessor of 3, we can use the predecessors to reconstruct the path that was taken (by\nlooking at 3’s predecessor, and then the predecessor’s predecessor, and so on until we reach the source vertex). In this case, we can see that the\nshortest path is 0 →2 →3 by following the predecessors from 3 back to 0.\n0\n1\n2\n3\n4\n4\nQueue\nCurrent Vertex\n2\nTo avoid running into cycles, we need a way to keep track of whether a vertex has been visited or not. However, notice that our above solution\ndoes not have an explicit container that keeps track of whether a vertex has been visited or not. This is because, by keeping track of the\npredecessors of each vertex along our search, we can use this information to identify whether a vertex has been visited, as only visited vertices\ncan have a predecessor! An implementation of this solution is shown below:\n1\nstd::vector<int32_t> find_shortest_path(const std::vector<std::vector<int32_t>>& graph,\n2\nint32_t int32_tsource, dest) {\n3\n// this map stores the predecessor of each vertex discovered along our BFS\n4\n// e.g., if backtrace[2] = 1, then vertex 1 directly precedes 2 along path\n5\nstd::unordered_map<int32_t, int32_t> backtrace;\n6\nbacktrace[source] = -1;\n// source vertex has no predecessor\n7\n8\n// perform BFS\n9\nstd::queue<int32_t> bfs;\n10\nbfs.push(source);\n11\nbool false;path_found =\n12\nwhile (!path_found && !bfs.empty()) {\n13\nint32_t curr = bfs.front();\n14\nbfs.pop();\n15\n// iterate over vertices directly connected with current vertex\n16\n// if any are unvisited (i.e., has no predecessor), push into queue\n17\nfor (int32_t neighbor : graph[curr]) {\n18\nif (backtrace.find(neighbor) == backtrace.end()) {\n19\nbacktrace[neighbor] = curr;\n20\n// if neighbor is solution, break out of loop\n21\nif (neighbor == dest) {\n22\ntrue;path_found =\n23\nbreak;\n24\n} // if\n25\nbfs.push(neighbor);\n26\n} // if\n27\n} // for neighbor\n28\n} // while\n29\n30\n// backtrace from dest back to source\n31\n// this is done by following the predecessors from dest to source\n32\nstd::vector<int32_t> path;\n33\nint32_t prev_vertex = dest;\n34\nif (path_found) {\n35\npath.push_back(prev_vertex);\n36\nwhile (backtrace[prev_vertex] != -1) {\n37\npath.push_back(backtrace[prev_vertex]);\n38\nprev_vertex = backtrace[prev_vertex];\n39\n} // while\n40\nstd::reverse(path.begin(), path.end());\n41\n} // if\n42\n43\nreturn path;\n44\n} // find_shortest_path()", "word_count": 453, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e676b509-9963-5595-a16b-a75cfe7c606e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 730, "real_page_number": null, "text": "718\nChapter 19. Graphs and Elementary Graph Algorithms\nExample 19.16 You have a job interview tomorrow, but your company just notified you today! As a result, you have to schedule a\n(origin), (dest),flight immediately. Given a function that takes in a starting airport a destination airport and a vector of available\n(flights, flights[i][0] i, flights[i][1] i,flights where represents the origin of flight represents the destination of flight\nflights[i][2]and represents the flight number), write a function that prints an itinerary for the sequence of flights that totalminimizes\nlayovers. Each line of the output should be of the form:\n[<NUMBER>] <ORIGIN> -> <DEST>\n<NUMBER> <ORIGIN> <DEST>where represents the flight number, represents the origin of that flight, and represents the destination of\nDTW ORD,that flight. For example, if you have to take flight 281 to fly from airport to airport then the output for that flight would be:\n[281] DTW -> ORD\nIf there are multiple sequences of flights that minimize layovers, you should prioritize flights with at each intermediatehigher flight numbers\nairport along your trip (e.g., if both flight 280 and flight 281 can be taken to yield a path that minimizes layovers, choose flight 281). If\ndest origin), originthere is no solution (i.e., is not reachable from print out all reachable airports from in the following format (the\nreachable airports can be printed out in any order):\nNo solution, <N> airports reachable\n<REACHABLE AIRPORT 1>\n<REACHABLE AIRPORT 2>\n<REACHABLE AIRPORT 3>\n<REACHABLE AIRPORT 4>\n<REACHABLE AIRPORT 5>\n...\n<REACHABLE AIRPORT N>\nThe function header is as follows:\nvoid print_itinerary(std::vector<std::vector<std::string>>& flights,\nconst conststd::string& origin, std::string& dest);\norigin = \"DTW\" dest = \"SEA\", flightsExample 1: Given and and the following connections in the vector:\nflights = {\n{\"ATL\", \"JFK\", 123},\n{\"ORD\", \"LAX\", 734},\n{\"MIA\", \"PHL\", 2152},\n{\"SEA\", \"ORD\", 203},\n{\"DTW\", \"ORD\", 281},\n{\"DTW\", \"ATL\", 230},\n{\"ORD\", \"ATL\", 12},\n{\"ATL\", \"LAX\", 280},\n{\"LAX\", \"SEA\", 3943},\n{\"LAX\", \"ATL\", 10}\n};\nthe output of the function should be\n[281] DTW -> ORD\n[734] ORD -> LAX\n[3943] LAX -> SEA\nDTW->ORD->LAX->SEA DTW SEA.This is because the path results in the fewest layovers to get from to\nSEA\nLAX\nORD\nATL\nDTW\nJFK\nMIA\nPHL\nDTW->ATL->LAX->SEA DTW->ORD DTW->ATLNote that is also a valid path that minimizes layovers, but was selected instead of\nbecause its flight number was larger (281 > 280).\nSEA\nLAX\nORD\nATL\nDTW\nJFK\nMIA\nPHL", "word_count": 404, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a8994e19-cdfd-5e68-b50c-d59363c2017c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 731, "real_page_number": null, "text": "19.4 Breadth-First Search\n719\ndest = PHL,Example 2: Given the same graph as above, but with the output of the function would be\nNo solution, 6 airports reachable\nSEA\nORD\nDTW\nATL\nLAX\nJFK\nDTW PHLThis is because there is no valid path from to using the flights given. The order of airports printed out is arbitrary.\nThis problem has several components, so let’s break it down step by step. The first thing to notice is that this is a graph search problem. Thus, we\nflightsshould first convert the vector into either an adjacency list or adjacency matrix to facilitate the searching process. For this problem,\nwe will be using an adjacency list (since they are a bit easier to work with for this problem — if you are ever given an exam problem where you\nhave to pick between the two, look at the time and space complexity constraints of the problem).\nLAXIn an adjacency list, each vertex of the graph should be mapped to its outgoing connections. For example, the vertex should be mapped\nSEA ATL, LAX.to the airports and since these are the airports that are reachable from\nLAX\nSEA ATL\nHowever, in this problem, we also have to keep track of flight numbers, since the number of a flight may be used to determine which path to take\nin the presence of a tie. Thus, we will also store the flight number of each edge in the adjacency list.\nLAX\nSEA\n3943\nATL\n10\nOne way to represent this adjacency list is by using a hash table that maps each airport to a vector of pairs, where each pair stores an outgoing\nairport with its flight number. The process of initializing such an adjacency list is shown in the code below:\n1\nint32_t>>>std::unordered_map<std::string, std::vector<std::pair<std::string, adj_list;\n2\nfor (auto& flight : flights) {\n3\nadj_list[flight[0]].emplace_back(flight[1], stoi(flight[2]));\n4\n} // for flight\nHowever, in this problem, we need to be able to access the flight numbers in order. Because of this, a vector of pairs is not the best waysorted\nto store information about each flight, since data values in a vector are not sorted. Instead, we should store the flight information in a sorted\nstd::map<>.container such as a This change is shown below (we want the flight number to be the key since maps are sorted by key):\n1\nstd::map<int32_t,std::unordered_map<std::string, std::string>> adj_list;\n2\nfor (auto& flight : flights) {\n3\nadj_list[flight[0]].emplace(std::stoi(flight[2]), flight[1]);\n4\n} // for flight\nATL\n123\nJFK\n280\nLAX\nORD\n12\nATL\n734\nLAX\nMIA\n2152\nPHL\nSEA\n203\nORD\nDTW\n230\nATL\n281\nORD\nLAX\n10\nATL\n3943\nSEA\norigin destNow that we have our graph representation, we want to find the path between and that goes through the fewest vertices. This is\na shortest-path problem, and since the graph is unweighted, we can solve this using a breadth-first search. However, there are two details we\nhave to pay attention to when implementing this problem: we want to print out the sequence of airports we visit along the shortest pathentire\nfrom our origin to our destination, and we want to prioritize flights with higher flight numbers in the case of a tie. How can address these two\nissues within our breadth-first search?\nTo print out every intermediary airport in our shortest path, we will need to keep track of the \"predecessor\" of each vertex whenever we\nLAX SEAdiscover it. For example, if we are iterating through the connections of and push into the queue, we have to remember that we reached\nSEA LAX.from This will allow us to \"backtrack\" after the completion of the algorithm to discover the path we took to reach the destination.\nSince this container should map each airport to the airport we came from (and its flight number), we can use an unordered map to represent it.\n// maps airport to previous airport and flight number\nint32_t>>std::unordered_map<std::string, std::pair<std::string, backtrace;\nNext, we want to ensure that flights with higher flight numbers are chosen over flights with lower flight numbers in the case of a tie. This may\nseem complicated to address at first, but it’s actually quite simple once you notice that breadth-first searches process vertices using a queue,\nwhich exhibits first-in, first-out behavior. As a result, to ensure that higher flight numbers are processed before lower ones, we just need to push\nflights with higher flight numbers into the queue those with lower flight numbers! For instance, consider the previous example, wherebefore\nDTW->ORD->LAX->SEA DTW->ATL->LAX->SEAand were both valid paths that minimized layovers. If we wanted to select the path with\nDTW->ORD DTW->ATL, ORD ATL DTW.instead of the one with we just need to push into the queue when processing the connections ofbefore\nDTW->ORD DTW->ATLThat way, the edge will be popped off the queue and processed before the edge is. This behavior can be accomplished\nby simply iterating over the vertex list of each airport in descending flight number order.", "word_count": 838, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "71dbf722-0802-50e6-b978-3bb4f30f2fdf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 732, "real_page_number": null, "text": "720\nChapter 19. Graphs and Elementary Graph Algorithms\nThis is shown in the code below:\n1\nstd::queue<std::string> bfs;\n2\nbfs.push(origin);\n3\nbacktrace[origin] = {\"\", -1};\n// also serves as \"visited\" container\n4\nwhile (!bfs.empty()) {\n5\nstd::string depart = bfs.front();\n6\nbfs.pop();\n7\nauto conn_it = adj_list.find(depart);\n8\nif (conn_it != adj_list.end()) {\n9\nstd::map<int32_t, std::string>& curr = conn_it->second;\n10\n// add the flights into queue in DESCENDING flight number order (this uses reverse iterators)\n11\nfor (auto it = curr.rbegin(); it != curr.rend(); ++it) {\n12\nconst std::string& arrive = it->second;\n13\nif (backtrace.find(arrive) == backtrace.end()) {\n14\nbacktrace[arrive] = {depart, it->first};\n15\nbfs.push(arrive);\n16\n...\n17\n} // if\n18\n} // for it\n19\n} // if\n20\n} // while\norigin dest.Lastly, we want to print out the shortest path we encountered from to This can be done by starting at the destination airport and\niteratively computing the previous airport (using the stored predecessors) until the starting airport is reached.\n1\nint32_t>>std::vector<std::pair<std::string, path = {{dest, 0}};\n2\nstd::string prev_flight = dest;\n3\nwhile (prev_flight != origin) {\n4\npath.push_back(backtrace[prev_flight]);\n5\nprev_flight = path.back().first;\n6\n} // while\n7\nfor (size_t i = path.size() - 1; i > 0; --i) {\n8\nstd::cout << \"[\" << path[i].second << \"] \" << path[i].first << \" -> \" << path[i - 1].first << '\\n';\n9\n} // for i\nbacktraceIn the no solution case, we want to print out all reachable airports. This can be done by printing out all the airports currently in the\nmap, since each airport is inserted into this map as soon as it is discovered to be reachable.\n1\nstd::cout << \"No solution, \" << backtrace.size() << \" airports reachable\\n\";\n2\nfor (auto it = backtrace.begin(); it != backtrace.end(); ++it) {\n3\nstd::cout << it->first << '\\n';\n4\n} // for\nPutting this all together, we have the following completed function:\n1\nvoid print_itinerary(std::vector<std::vector<std::string>>& flights,\n2\nconst conststd::string& origin, std::string& dest) {\n3\nstd::map<int32_t,std::unordered_map<std::string, std::string>> adj_list;\n4\nint32_t>>std::unordered_map<std::string, std::pair<std::string, backtrace;\n5\nfor (auto& flight : flights) {\n6\nadj_list[flight[0]].insert({std::stoi(flight[2]), flight[1]});\n7\n} // for\n8\n9\nstd::queue<std::string> bfs;\n10\nbfs.push(origin);\n11\nbacktrace[origin] = {\"\", -1};\n12\nbool false;finished =\n13\nwhile (!finished && !bfs.empty()) {\n14\nstd::string depart = bfs.front();\n15\nbfs.pop();\n16\nauto conn_it = adj_list.find(depart);\n17\nif (conn_it != adj_list.end()) {\n18\nstd::map<int32_t, std::string>& curr = conn_it->second;\n19\nfor (auto it = curr.rbegin(); it != curr.rend(); ++it) {\n20\nconst std::string& arrive = it->second;\n21\nif (backtrace.find(arrive) == backtrace.end()) {\n22\nbacktrace[arrive] = {depart, it->first};\n23\nif (arrive == dest) {\n24\ntrue;finished =\n25\nbreak;\n26\n} // if\n27\nbfs.push(arrive);\n28\n} // if\n29\n} // for it\n30\n} // if\n31\n} // while\n32\n33\n/* ... continued on next page ... */", "word_count": 475, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e67870db-b4d0-58ec-90d8-0c17add90dd8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 733, "real_page_number": null, "text": "19.4 Breadth-First Search\n721\n34\nif (!finished) {\n// no solution\n35\nstd::cout << \"No solution, \" << backtrace.size() << \" airports reachable\\n\";\n36\nfor (auto it = backtrace.begin(); it != backtrace.end(); ++it) {\n37\nstd::cout << it->first << '\\n';\n38\n} // for it\n39\n} // if\n40\nelse {\n// backtrace from dest back to origin\n41\nint32_t>>std::vector<std::pair<std::string, path = {{dest, 0}};\n42\nstd::string prev_flight = dest;\n43\nwhile (prev_flight != origin) {\n44\npath.push_back(backtrace[prev_flight]);\n45\nprev_flight = path.back().first;\n46\n} // while\n47\nfor (size_t i = path.size() - 1; i > 0; --i) {\n48\nstd::cout << \"[\" << path[i].second << \"] \" << path[i].first << \" -> \"\n49\n<< path[i - 1].first << '\\n';\n50\n} // for i\n51\n} // else\n52\n} // print_itinerary()\nExample 19.17 You are given an 𝑚×𝑛grid of PotatoBots, where each cell can have one of three enum values:\n1\nenum class Status {\n2\nEmpty,\n3\nFunctional,\n4\nCorrupted\n5\n};\nThere is a deadly virus currently spreading among these PotatoBots, and any PotatoBot who catches this virus will become corrupted (and\nsubsequently give terrible answers to Piazza questions). Every minute, any functional PotatoBot that is four-directionally adjacent (north,\ntime_to_corrupt()east, south, west) to a corrupted PotatoBot will also become corrupted. Implement the function, which takes in\n-1the 𝑚×𝑛grid and returns the total number of minutes that must elapse before all PotatoBots are corrupted (or if any PotatoBots can\nnever be corrupted). The function header is as follows:\nint32_t time_to_corrupt(std::vector<std::vector<Status>>& grid);\nExample: Given the grid on the left (where positions marked with an \"O\" indicate a functional PotatoBot, and positions marked with an\n3,\"X\" indicate a corrupted PotatoBot), you would return since 3 minutes will elapse before all the PotatoBots are corrupted:\nO\nO\nO\nO\nX\nO\nO\nO\nO\nO\nO\nX\nMinute 0\nX\nO\nO\nX\nX\nO\nX\nO\nX\nO\nO\nX\nMinute 1\nX\nX\nO\nX\nX\nX\nX\nX\nX\nO\nX\nX\nMinute 2\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nMinute 3\nThis problem can be solved using a breadth-first search starting with the corrupted PotatoBots, since a BFS visits elements outwards from a\ngiven starting vertex. This allows us to iteratively identify which PotatoBots are infected at each point in time. To start, we will first iterate over\nthe entire grid and do two things:\n• Count the number of functional PotatoBots (using a variable counter): this allows us to detect if any PotatoBots are still unaffected at the\nend of the algorithm.\n• Push the positions of all of the corrupted PotatoBots into a BFS queue.\nThen, as long as there remain corrupted PotatoBots in the queue, we take each corrupted PotatoBot out of the queue, examine its adjacent cells\nfor any functional PotatoBots, corrupt them, and then push these newly corrupted PotatoBots into the queue. However, we also have to keep\ntrack of the time at which each PotatoBot is corrupted, so that we can return the total time elapsed at the end of the function. This can be\nhandled by processing the PotatoBots on a time-by-time basis, where we first process PotatoBots that are corrupted at time 0, then PotatoBots\nwhilethat are corrupted at time 1, and so on. This is done by using an additional inner loop within the outer loop that runs based on the size\nof the queue (similar to the one used in example 18.8, when we wanted to find the maximum level sum of a tree). This structure makes it easy\nfor us to correctly keep track of total time using a simple counter.", "word_count": 618, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1096a5e1-3dc1-5fcc-9a4d-42619a42251d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 734, "real_page_number": null, "text": "722\nChapter 19. Graphs and Elementary Graph Algorithms\nA solution to the problem is shown below:\n1\nint32_t time_to_corrupt(std::vector<std::vector<Status>>& grid) {\n2\nstd::queue<std::pair<int32_t, int32_t>> bfs;\n3\nstd::vector<std::pair<int32_t, int32_t>> dirs = {{1, 0}, {-1, 0}, {0, 1}, {0, -1}};\n4\nint32_t functional_bots = 0;\n5\nint32_t time_elapsed = -1;\n6\n7\n// iterate over grid and push all initially corrupted PotatoBots into BFS queue\n8\nfor (size_t i = 0; i < grid.size(); ++i) {\n9\nfor (size_t j = 0; j < grid[i].size(); ++j) {\n10\nif (grid[i][j] == Status::Corrupted) {\n11\nbfs.push({i, j});\n12\n} // if\n13\nelse if (grid[i][j] == Status::Functional) {\n14\n++functional_bots;\n15\n} // else if\n16\n} // for j\n17\n} // for i\n18\n19\n// perform a BFS, using the queue size to distinguish between minutes\n20\nwhile (!bfs.empty()) {\n21\n++time_elapsed;\n22\nsize_t sz = bfs.size();\n23\nfor (size_t k = 0; k < sz; ++k) {\n24\nauto [curr_row, curr_col] = bfs.front();\n25\nbfs.pop();\n26\nfor (auto [x_increment, y_increment] : dirs) {\n27\nint32_t new_row = curr_row + x_increment;\n28\nint32_t new_col = curr_col + y_increment;\n29\nif (new_row < 0 || new_col < 0 || new_row >= grid.size() ||\n30\nnew_col >= grid[0].size() || grid[new_row][new_col] != Status::Functional) {\n31\ncontinue;\n32\n} // if\n33\ngrid[new_row][new_col] = Status::Corrupted;\n34\nbfs.push({new_row, new_col});\n35\n--functional_bots;\n36\n} // for [x_increment, y_increment]\n37\n} // for k\n38\n} // while\n39\n40\nreturn functional_bots == 0 ? std::max(0, time_elapsed) : -1;\n41\n} // time_to_corrupt()\nExample 19.18 Consider the following provided helper function, which returns a set of all prime numbers under one million, where each\nnumber is returned in the form of a string:\n1\nstd::vector<std::string> get_primes() {\n2\nconstexpr uint64_t million = 1000000;\n3\nstd::vector<bool> true);is_prime(million,\n4\nfalse;is_prime[0] = is_prime[1] =\n5\nfor (uint64_t n = 2; n n < million; ++n) {*\n6\nif (is_prime[n]) {\n7\nfor (uint64_t multiple = n n; multiple < million; multiple += n) {*\n8\nfalse;is_prime[multiple] =\n9\n} // for multiple\n10\n} // if\n11\n} // for n\n12\n13\nstd::vector<std::string> prime_strs;\n14\nfor (size_t i = 0; i < is_prime.size(); ++i) {\n15\nif (is_prime[i]) {\n16\nprime_strs.push_back(std::to_string(i));\n17\n} // if\n18\n} // for i\n19\n20\nreturn prime_strs;\n21\n} // get_primes()\nGiven two prime numbers, implement a function that returns the length of the shortest path between these numbers, provided that one of the\nfollowing rules must be applied when changing between two prime numbers:\n• A single digit is altered (i.e., changing from the prime 281 to the prime 271).\n• A single digit is added to the number (i.e., changing from the prime 281 to the prime 2281).\n• A single digit is removed from the number (i.e., changing from the prime 2819 to the prime 281).", "word_count": 482, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4234773a-0420-50e0-8461-f6edcc1fbddd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 735, "real_page_number": null, "text": "19.4 Breadth-First Search\n723\nget_primes()The prime number changed to must be less than one million (i.e., it must be in the set returned by the function), and\n\"0281\" -1.leading zeros are not valid (e.g., is not a valid string that can be encountered along the path). If there is no solution, return\nYou may assume that the input values are both valid prime numbers. The function header is shown below:\nint32_t shortest_path_between_primes(int32_t int32_tsource, dest);\nExample 1: If you are asked to convert the prime number 281 to the prime number 280183, you would return 5, since the shortest path\nbetween these two prime numbers is 5 if restricted to the rules above:\n→283→2083→20183→280183281\nExample 2: If you are asked to convert the prime number 281 to the prime number 619, you would return 5, since the shortest path between\nthese two prime numbers is 5 if restricted to the rules above:\n→181→11→19→619281\nExample 3: If you are asked to convert the prime number 281 to the prime number 340723, you would return 9, since the shortest path\nbetween these two prime numbers is 9 if restricted to the rules above:\n→241→641→643→6473→64793→640793→340793→340723281\nExample 4: If you are asked to convert the prime number 999979 to the prime number 2, you would return 8, since the shortest path\nbetween these two prime numbers is 8 if restricted to the rules above:\n→399979→39979→39929→3929→929→29→2999979\nTo solve this problem, we can think of the problem’s setup as a graph, where vertices are represented by the prime numbers that we can visit,\nand edges are represented by whether it is possible to change from one number to another. For example, the prime numbers reachable from 281\nusing the given rules are 181, 211, 241, 251, 271, 283, 881, 2081, 2281, 2381, 2801, 2819, 2851, 2861, 5281, and 9281. Thus, 281 would be\nconnected to these other numbers in our graph. We would then be able to perform a breadth-first search to find the shortest way to change from\none prime number to another.\nTo build our graph, we will need to know if it is possible to change from one prime number to another. This can be done by writing a\nseparate helper function that takes in two prime numbers and determines if it is valid to change from one to another. An implementation of this\nfunction is shown below:\n1\nbool is_valid_change(const conststd::string& prime1, std::string& prime2) {\n2\n// if the two numbers are off by more than one, it is not a valid change\n3\nint32_t length_diff = std::fabs(prime1.length() - prime2.length());\n4\nif (length_diff > 1) {\n5\nreturn false;\n6\n} // if\n7\n8\n// if the two numbers have the same length, a digit must be altered\n9\nint32_t digit_diff = 0;\n10\nif (length_diff == 0) {\n11\nfor (size_t i = 0; i < prime1.length(); ++i) {\n12\nif (prime1[i] != prime2[i]) {\n13\n// if more than one digit is different, it is not a valid change\n14\nif (++digit_diff > 1) {\n15\nreturn false;\n16\n} // if\n17\n} // if\n18\n} // for i\n19\nreturn digit_diff == 1;\n20\n} // if\n21\n22\n// if the two numbers differ in length by 1, a digit must be inserted or removed\n23\nconst std::string& longer_prime = prime1.length() > prime2.length() ? prime1 : prime2;\n24\nconst std::string& shorter_prime = prime1.length() > prime2.length() ? prime2 : prime1;\n25\nfor (size_t i = 0, j = 0; i < longer_prime.length(); ++i) {\n26\n// only increment i and j together if the digits match, else only increment for longer prime\n27\nif (longer_prime[i] == shorter_prime[j]) {\n28\n++j;\n29\n} // if\n30\n// there is a mismatch, so increment the diff counter and return false if it goes above 1\n31\nelse if (++digit_diff > 1) {\n32\nreturn false;\n33\n} // else if\n34\n} // for i, j\n35\n36\nreturn true;\n37\n} // is_valid_change()", "word_count": 692, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9636e866-1a2b-5a5e-968a-a675d33d8f9d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 736, "real_page_number": null, "text": "724\nChapter 19. Graphs and Elementary Graph Algorithms\nWe could use this method to construct an adjacency list, as shown:\n1\nstd::unordered_map<std::string, std::vector<std::string>> adj_list;\n2\nstd::vector<std::string> all_primes = get_primes();\n3\nfor (size_t i = 0; i < all_primes.size(); ++i) {\n4\nfor (size_t j = i + 1; j < all_primes.size(); ++j) {\n5\nconst std::string& prime1 = all_primes[i];\n6\nconst std::string& prime2 = all_primes[j];\n7\nif (is_valid_change(prime1, prime2)) {\n8\n// using the rules, if it is possible to change from prime1 to prime2,\n9\n// it must also be possible to change from prime2 back to prime1\n10\n// this is also why it is okay to start j at i + 1, since the graph is undirected\n11\nadj_list[prime1].push_back(prime2);\n12\nadj_list[prime2].push_back(prime1);\n13\n} // if\n14\n} // for j\n15\n} // for i\nHowever, this is more work than necessary, since it is unlikely for the search to consider all combinations of primes. Instead, it is more efficient\nto only consider whether a change is valid for the prime numbers you encounter along your search, rather than for every prime number under\none million (which can get quite computationally intensive).\nSince we only need to return the length of the shortest path between two primes, we can store the distance of each number from the source\nnumber, rather than the value of its predecessor. For example, if we change 281 to 881 along the search, we only need to know that 881 is one\nchange away from the source number of 281, and not that its predecessor is 281. An implementation of the solution is shown below:\n1\nint32_t shortest_path_between_primes(int32_t int32_tsource, dest) {\n2\nstd::vector<std::string> all_primes = get_primes();\n3\n4\nint32_t>std::unordered_map<std::string, dist_from_source;\n5\nstd::string source_str = std::to_string(source);\n6\nstd::string dest_str = std::to_string(dest);\n7\ndist_from_source[source_str] = 1;\n8\n9\nstd::queue<std::string> bfs;\n10\nbfs.push(source_str);\n11\nwhile (!bfs.empty()) {\n12\nstd::string curr = bfs.front();\n13\nbfs.pop();\n14\nfor (const std::string& prime : all_primes) {\n15\nif (dist_from_source.find(prime) == dist_from_source.end() && is_valid_change(curr, prime)) {\n16\ndist_from_source[prime] = dist_from_source[curr] + 1;\n17\nif (prime == dest_str) {\n18\nreturn dist_from_source[prime];\n19\n} // if\n20\nbfs.push(prime);\n21\n} // if\n22\n} // for neighbor\n23\n} // while\n24\n25\nreturn -1;\n// no solution\n26\n} // shortest_path_between_primes()\n19.5\nCycle Detection\nIn this section, we will discuss approaches that can be used to detect cycles in a graph. To recap, a cycle is a path with distinct edges in a graph\nwhere the starting and ending vertices are the same:\nA\nB\nC\nD\nE\nF", "word_count": 430, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c7bfadac-ef3d-5ea1-b87a-181560b7fddf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 737, "real_page_number": null, "text": "19.5 Cycle Detection\n725\nBy definition, if there exists a cycle in a graph, then there must be a way to access a node in the graph via more than one unique path. For\nexample, consider vertex C in the graph above. For any arbitrary vertex in the graph other than C, there are multiple paths we can follow to reach\nvertex C. For example, if we start from vertex D, we can reach vertex C along the path 𝐷→𝐶, 𝐷→𝐸→𝐴→𝐶, or 𝐷→𝐸→𝐴→𝐵→𝐶.\nA\nB\nC\nD\nE\nF\nA\nB\nC\nD\nE\nF\nA\nB\nC\nD\nE\nF\nHowever, if a graph were acyclic, then there exists only one way to reach each vertex in the graph from any starting vertex.\nA\nB\nC\nD\nE\nF\nWe can take advantage of this fact when implementing a cycle detection algorithm. Recall that DFS and BFS mark each node as visited as soon\nas it is discovered. However, because there exists only one way to reach each vertex in an acyclic graph, the search algorithm can only discover\neach vertex if a graph has no cycles! Thus, if the algorithm rediscovers a vertex after it has already been marked as visited, then there mustonce\nexist more than one way to access that node from the starting vertex, and a cycle must exist in the graph.\nExample 19.19 trueYou are given an undirected, connected graph in the form of an adjacency list. Write a function that returns if the\nfalsegraph has a cycle, and if it does not. Implement your cycle detection algorithm using a depth-first search.\nbool is_graph_cyclic(const std::vector<std::vector<int32_t>>& adj_list);\nu v, adj_list[u] v, adj_list[v]Since the graph is undirected, if an edge exists between vertices and then will contain and will\nu. u adj_list[u].contain You may assume that will not appear in\nadj_list[0])To solve this problem using a depth-first search, we will select an arbitrary node (in this case, we can just select and begin a\ndepth-first search, marking each vertex as visited as soon as it is discovered. Since the graph is connected, we know that the depth-first search\nwill eventually visit every vertex in the graph. If we ever come across a vertex that has already been visited, then there must be more than\none path that leads to that vertex from the starting vertex, which indicates the existence of a cycle in the graph. A recursive depth-first search\nimplementation is shown below:\n1\nbool dfs_helper(const std::vector<std::vector<int32_t>>& adj_list,\n2\nstd::vector<bool>& int32_t int32_tvisited, curr, prev) {\n3\ntrue;visited[curr] =\n4\nfor (int32_t neighbor : adj_list[curr]) {\n5\nif (neighbor != prev) {\n6\nif (visited[neighbor]) {\n7\nreturn true;\n// already seen vertex already, so cycle exists\n8\n} // if\n9\nif (dfs_helper(adj_list, visited, neighbor, curr)) {\n10\nreturn true;\n11\n} // if\n12\n} // if\n13\n} // for neighbor\n14\n// completed DFS without discovering same vertex twice, so no cycle exists\n15\nreturn false;\n16\n} // dfs_helper()\n17\n18\nbool is_graph_cyclic(const std::vector<std::vector<int32_t>>& adj_list) {\n19\nstd::vector<bool> false);visited(adj_list.size(),\n20\nreturn !adj_list.empty() && dfs_helper(adj_list, visited, 0, -1);\n21\n} // is_graph_cyclic()\ncurr prevNote that we check whether is equal to on line 5. This is important, because our algorithm would otherwise treat the path\n𝐴→𝐵→𝐴as a cycle, when it is just an edge. To fix this, we will keep track of the previous node at every vertex to ensure that we do not\nrevisit the node we came from and incorrectly assume that there exists a cycle in the graph.", "word_count": 598, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ac39814e-a5a6-5435-b48f-2488f5ff5a11", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 738, "real_page_number": null, "text": "726\nChapter 19. Graphs and Elementary Graph Algorithms\nAn iterative depth-first search implemention is shown below:\n1\nbool is_graph_cyclic(const std::vector<std::vector<int32_t>>& adj_list) {\n2\nif (adj_list.empty()) {\n3\nreturn false;\n4\n} // if\n5\nstd::vector<int32_t> parent(adj_list.size(), -1);\n6\nstd::stack<int32_t> dfs;\n7\nparent[0] = -2;\n// not -1 since -1 indicates unvisited (can use std::optional<> instead in C++17)\n8\ndfs.push(0);\n9\nwhile (!dfs.empty()) {\n10\nint32_t current = dfs.top();\n11\ndfs.pop();\n12\nfor (int32_t neighbor : adj_list[current]) {\n13\nif (neighbor != parent[current]) {\n14\nif (parent[neighbor] != -1) {\n15\nreturn true;\n16\n} // if\n17\nparent[neighbor] = current;\n18\ndfs.push(neighbor);\n19\n} // if\n20\n} // for neighbor\n21\n} // while\n22\nreturn false;\n23\n} // is_graph_cyclic()\nparentIn this implementation, the vector keeps track of each vertex’s previous node, so that we don’t mistakenly revisit the vertex we came\nfrom and immediately conclude that there exists a cycle.\nExample 19.20 Implement the same function in the previous example, but using a breadth-first search instead.\nThe breadth-first search solution is fairly similar to the iterative depth-first search approach, but using a queue instead of a stack. We pick an\narbitrary vertex in the graph and begin a breadth-first search. If we ever discover the same vertex more than once, then there must be a cycle;\notherwise, there is not. The code is shown below.\n1\nbool is_graph_cyclic(const std::vector<std::vector<int32_t>>& adj_list) {\n2\nif (adj_list.empty()) {\n3\nreturn false;\n4\n} // if\n5\nstd::vector<int32_t> parent(adj_list.size(), -1);\n6\nstd::queue<int32_t> bfs;\n7\nparent[0] = -2;\n8\nbfs.push(0);\n9\nwhile (!bfs.empty()) {\n10\nint32_t current = bfs.top();\n11\nbfs.pop();\n12\nfor (int32_t neighbor : adj_list[current]) {\n13\nif (neighbor != parent[current]) {\n14\nif (parent[neighbor] != -1) {\n15\nreturn true;\n16\n} // if\n17\nparent[neighbor] = current;\n18\nbfs.push(neighbor);\n19\n} // if\n20\n} // for neighbor\n21\n} // while\n22\nreturn false;\n23\n} // is_graph_cyclic()", "word_count": 322, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d71aaa34-0db7-5ab5-a7a8-aa79cab897a3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 739, "real_page_number": null, "text": "19.6 Topological Sort\n727\nIn addition to BFS and DFS, there is actually a shortcut that can be used to detect cycles in a graph, Givenprovided that the graph is connected.\n|𝑉| |𝑉|−1a connected graph with vertices, the number of edges in the graph must be equal to for the graph to be acyclic. If there are fewer\n|𝑉|−1than edges, then the graph would not be connected, as every vertex in the graph needs to touch an edge that connects it to the remaining\n|𝑉|−1elements. However, the addition of an any edge beyond would end up creating a cycle, since it would then be possible to reach some\nvertex in the graph along more than one unique path. As a result, we can detect the existence of a cycle in a connected graph by simply counting\n|𝑉|−1.up the edges and checking if it is equal to\nExample 19.21 Implement the same function as the previous two examples, but solve the problem by counting edges in the graph instead\nof conducting a graph search.\nSince the graph is undirected and connected, each edge appears twice in the adjacency list. Thus, if the number of edges in the graph is\n2(|𝑉|−1), 2(|𝑉|−1),then the graph has no cycles. If the number of edges exceeds then there must exist a cycle in the graph.\n1\nbool is_graph_cyclic(const std::vector<std::vector<int32_t>>& adj_list) {\n2\nsize_t num_vertices = adj_list.size();\n3\nsize_t num_edges = 0;\n4\nfor (size_t i = 0; i < num_vertices; ++i) {\n5\nnum_edges += adj_list[i].size();\n6\n} // for i\n7\nreturn !adj_list.empty() && (num_edges / 2) > (num_vertices - 1);\n8\n} // is_graph_cyclic()\n19.6\nTopological Sort (✽)\n¸ 19.6.1\n(✽)Topological Orderings\nDFS and BFS can also be applied to sort the vertices of a directed graph with no cycles. This is quite useful, since many problems in real life\ncan be modeled as directed graphs, where certain events must happen before others. For instance, consider the problem of class prerequisites: if\nyou wanted to take EECS 281, you would have to take all of its prerequisites first (EECS 183, EECS 203, EECS 280, etc.). We can represent\nthese prerequisites in the form of a graph, as shown below:\nEECS 183\nEECS 280\nEECS 203\nEECS 281\nEECS 370\nEECS 376\nEECS 485\nEECS 482\nEECS 477\nEECS 491\nIn this graph, there exists an to the vertices, such that certain vertices must come before others. For example, you can’t go and takeordering\nEECS 491 without first taking EECS 370 and EECS 482 (assuming that you have taken EECS 281 already).\nIn a directed graph, a valid ordering of its vertices is known as a topological ordering. In a topological ordering, the existence of a directed\nedge from vertex 𝑋to vertex 𝑌indicates that 𝑋comes before 𝑌in the ordering of vertices. Topological orderings only exist in directed graphs\nthat do contain cycles, known as directed acyclic graphs (DAGs). This is because it is impossible to order vertices in a directed cycle, as thenot\nlast vertex in any ordering would always point back to a vertex before it. The following is a valid topological ordering of the classes in the\nprevious graph:\n183\n203\n280\n281\n370\n376\n485\n477\n482\n491\nThe above ordering is valid because all of the directed edges point to the right. Topological orderings need be unique, and there may benot\nmultiple topological orderings that exist within a single graph. For instance, the following is another valid topological ordering of the classes:\n183\n280\n203\n281\n370\n485\n482\n376\n491\n477", "word_count": 602, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8d086fe5-3457-5ed7-8eb9-89a4a9a2a6b4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 740, "real_page_number": null, "text": "728\nChapter 19. Graphs and Elementary Graph Algorithms\n¸ 19.6.2\n(✽)Depth-First Search Topological Sort\nTofindavalidtopologicalorderingofadirectedacyclicgraph,wecanuseanalgorithmknownastopologicalsort. Onecommonimplementation\nof topological sort relies on a search. In this approach, an unvisited vertex is arbitrarily selected in the graph, and a recursivedepth-first\ndepth-first search is performed to explore all vertices that are reachable from that vertex. Every time a recursive call unrolls along the search, the\ncurrent vertex is added to the last available position of the current topological ordering. This process is repeated as long as there exist vertices\nthat remain unvisited. As an example, suppose we wanted to find a topological ordering of the following DAG:\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\nTo topologically sort this graph using the depth-first search approach, we first select an arbitrary vertex and begin a recursive depth-first search.\nIn this case, we will select vertex 𝐴.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\nTopological Ordering:\nA\nStack Frames\nCurrent Vertex\nA\nTo perform the depth-first search, we will make recursive calls on each of vertex 𝐴’s unvisited neighbors. To keep things simple, we will make\nthese recursive calls in alphabetical order. In this case, we will first make a recursive call on vertex 𝐸.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\nTopological Ordering:\nA\nE\nStack Frames\nCurrent Vertex\nE\nVertex 𝐸is now our current vertex. We then make a recursive call on vertex 𝐽, which is vertex 𝐸’s only unvisited neighbor.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\nTopological Ordering:\nA\nE\nJ\nStack Frames\nCurrent Vertex\nJ\nVertex 𝐽has no unvisited neighbors, so there are no recursive calls that need to be done. Thus, we are done processing vertex 𝐽, so we can\nunroll the recursion back to vertex 𝐸. Before we return from the recursive call, we will add 𝐽to the last available position of our current\ntopological ordering (since we know that no unvisited vertices depend on vertex 𝐽).\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\nTopological Ordering:\nJ\nA\nE\nStack Frames\nCurrent Vertex\nE", "word_count": 378, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d50a9dda-9cf7-5f34-b090-2b10d570705b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 741, "real_page_number": null, "text": "19.6 Topological Sort\n729\nThe recursion unrolls back to vertex 𝐸. Vertex 𝐸now has no more unvisited neighbors, so we are done processing it. Before the recursion\nreturns to vertex 𝐴, we will add 𝐸to the last available position of our current topological ordering.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\nTopological Ordering:\nE\nJ\nA\nStack Frames\nCurrent Vertex\nA\nThe recursion unrolls back to vertex A. Now that the recursive call on vertex 𝐸has been completed, we will now make a recursive call on the\nremaining unvisited neighbor, vertex 𝐼. We follow the same procedure as before: after making a recursive call on vertex 𝐼, we then make a\nrecursive call on vertex 𝐷. Since vertex 𝐷has no unvisited neighbors, the recursion unrolls from 𝐷back to 𝐼, and we add 𝐷to the last available\nposition of our current topological ordering. Similarly, after processing vertex 𝐷, all of vertex 𝐼’s neighbors have been visited, so we add 𝐼to\nthe last available position of our current topological ordering.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➊\nE\nJ\nA\nI\nCurrent\nI\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➋\nE\nJ\nA\nI\nD\nCurrent\nD\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➌\nD\nE\nJ\nA\nI\nCurrent\nI\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➍\nI\nD\nE\nJ\nA\nCurrent\nA\nAll of vertex 𝐴’s neighbors have been processed, so we add 𝐴to the front of our current topological ordering. The stack frame of vertex 𝐴\nunrolls, and our depth-first search on vertex 𝐴is complete.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\nTopological Ordering:\nA\nI\nD\nE\nJ\nStack Frames\nCurrent Vertex", "word_count": 303, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fe291bc1-bb03-5aa0-b948-ffa5f7440249", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 742, "real_page_number": null, "text": "730\nChapter 19. Graphs and Elementary Graph Algorithms\nOur topological sort, however, is not complete, since we still have vertices that still need to be visited. Therefore, we will need to select another\narbitrary unvisited vertex and begin a depth-first search. For our example, we will select vertex 𝐵next. During this depth-first search on vertex\n𝐵, we add vertices 𝐻, then 𝐹, then 𝐺, then 𝐵to the last available position of our topological ordering.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➊\nA\nI\nD\nE\nJ\nB\nCurrent\nB\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➋\nA\nI\nD\nE\nJ\nB\nF\nCurrent\nF\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➌\nA\nI\nD\nE\nJ\nB\nF\nH\nCurrent\nH\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➍\nH A\nI\nD\nE\nJ\nB\nF\nCurrent\nF\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➎\nF\nH A\nI\nD\nE\nJ\nB\nCurrent\nB\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➏\nF\nH A\nI\nD\nE\nJ\nB\nG\nCurrent\nG\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➐\nG\nF\nH A\nI\nD\nE\nJ\nB\nCurrent\nB\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➑\nB\nG\nF\nH A\nI\nD\nE\nJ\nCurrent\nWe still have vertices that need to be visited, so we will need to select another unvisited vertex to perform a depth-first search on. For this\nexample, we will select vertex 𝐶. During our depth-first search on vertex 𝐶, we first add vertex 𝐾, and then add vertex 𝐶to the last available\nposition of our topological ordering.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➊\nB\nG\nF\nH A\nI\nD\nE\nJ\nC\nCurrent\nC\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➋\nB\nG\nF\nH A\nI\nD\nE\nJ\nC\nK\nCurrent\nK\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➌\nK\nB\nG\nF\nH A\nI\nD\nE\nJ\nC\nCurrent\nC\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n➍\nC\nK\nB\nG\nF\nH A\nI\nD\nE\nJ\nCurrent", "word_count": 404, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9358f005-f053-5a62-b141-5e9ea7a66206", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 743, "real_page_number": null, "text": "19.6 Topological Sort\n731\nAll of the vertices in the graph have been added to the ordering, so our topological sort is complete. The result we get is a valid topological\nordering of the given graph. An implementation of the depth-first search approach to topological sort is shown below:\n1\nvoid dfs_helper(const std::vector<std::vector<int32_t>>& adj_list,\n2\nstd::unordered_set<int32_t>& std::deque<int32_t>& int32_tvisited, order, vertex) {\n3\nvisited.insert(vertex);\n4\nfor (int32_t neighbor : adj_list[vertex]) {\n5\nif (visited.find(neighbor) == visited.end()) {\n6\ndfs_helper(adj_list, visited, order, neighbor);\n7\n} // if\n8\n} // for neighbor\n9\norder.push_front(vertex);\n10\n} // dfs_helper()\n11\n12\nstd::deque<int32_t> topological_sort(const std::vector<std::vector<int32_t>>& adj_list) {\n13\nstd::deque<int32_t> order;\n14\nstd::unordered_set<int32_t> visited;\n15\nfor (size_t vertex = 0; vertex < adj_list.size(); ++vertex) {\n16\nif (visited.find(vertex) == visited.end()) {\n17\ndfs_helper(adj_list, visited, order, vertex);\n18\n} // if\n19\n} // for vertex\n20\nreturn order;\n21\n} // topological_sort()\nExample 19.22 There are a total of 𝑛classes you have to take to graduate, each labeled with an integer from 0 to 𝑛−1. Some courses may\nprereqs, prereq[i] = [a, b]have prerequisites. You are given a vector of all prerequisite pairs, where indicates that you must\nb a b a). ntake course course (i.e., is a prerequisite of Given the total number of courses and a vector of all prerequisite pairs, returnbefore\nan ordering of courses you should take to finish all courses. If there are multiple valid orderings, return any of them. You are guaranteed\nthat there exists at least one valid ordering.\nstd::vector<int> schedule_courses(int std::vector<std::vector<int>>&n, prereqs);\nn = 4 prereqs = [[1, 0], [2, 0], [3, 1], [3, 2]], [0, 1, 2, 3] [0, 2, 1, 3].Given and you would output OR This is\nbecause course 3 can only be taken after finishing both 1 and 2, and courses 1 and 2 can only be taken after finishing course 0.\nTo solve this problem, we will use a topological sort. Using the depth-first search approach, we get the following:\n1\nstd::vector<int32_t> schedule_courses(int32_t const std::vector<std::vector<int32_t>>&n, prereqs) {\n2\nstd::vector<std::vector<int32_t>> adj_list(n);\n3\nfor (auto& vec : prereqs) {\n4\nadj_list[vec[1]].push_back(vec[0]);\n5\n} // for vec\n6\nstd::vector<int32_t> order;\n7\nstd::vector<bool> false);visited(n,\n8\nfor (size_t vertex = 0; vertex < adj_list.size(); ++vertex) {\n9\nif (!visited[vertex]) {\n10\ndfs_helper(adj_list, visited, order, vertex);\n11\n} // if\n12\n} // for vertex\n13\nstd::reverse(order.begin(), order.end());\n14\nreturn order;\n15\n} // schedule_courses()\n16\n17\nvoid dfs_helper(const std::vector<std::vector<int32_t>>& adj_list,\n18\nstd::vector<bool>& std::vector<int32_t>& int32_tvisited, order, vertex) {\n19\ntrue;visited[vertex] =\n20\nfor (int32_t neighbor : adj_list[vertex]) {\n21\nif (!visited[neighbor]) {\n22\ndfs_helper(adj_list, visited, order, neighbor);\n23\n} // if\n24\n} // for neighbor\n25\norder.push_back(vertex);\n26\n} // dfs_helper()\nΘ(|𝑉|+|𝐸|)The time complexity of the topological sorting algorithm using depth-first search is on an adjacency list, since each vertex and\nΘ(|𝑉|2),edge is examined a constant number of times. The time complexity of this algorithm on an adjacency matrix is since finding all the\nΘ(|𝑉|) |𝑉|connections of a vertex takes time, which is done a total of times.", "word_count": 514, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4f65d3bd-daea-577a-ac99-fa4fe3783e98", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 744, "real_page_number": null, "text": "732\nChapter 19. Graphs and Elementary Graph Algorithms\n¸ 19.6.3\n(✽)Kahn’s Algorithm\nA topological sort can also be done iteratively rather than recursively. A common iterative implementation of topological sort is Kahn’s\nalgorithm, which relies on a and takes advantage of a vertex’s to find a topological ordering. Kahn’s algorithm isbreadth-first search in-degree\nactually rather intuitive, as it reflects how events are ordered in real life. Consider the graph of classes introduced previously:\nEECS 183\nEECS 280\nEECS 203\nEECS 281\nEECS 370\nEECS 376\nEECS 485\nEECS 482\nEECS 477\nEECS 491\nIf we wanted to build a schedule of classes that followed prerequisite rules, the logical decision would be to first look for classes that do not have\nany existing prerequisites (i.e., vertices with an in-degree of zero) and complete those first — in this case, we are able to take EECS 183 and\nEECS 203, so one of those classes should be first in the topological ordering.\nEECS 183\nEECS 280\nEECS 203\nEECS 281\nEECS 370\nEECS 376\nEECS 485\nEECS 482\nEECS 477\nEECS 491\nSuppose we choose EECS 183 as the first class in our topological ordering. After taking EECS 183, we would then be able to take EECS 280,\nsince we have satisfied all of its prerequisites. This is akin to \"removing\" EECS 183 from consideration in our prerequisite graph.\nEECS 183\nEECS 280\nEECS 203\nEECS 281\nEECS 370\nEECS 376\nEECS 485\nEECS 482\nEECS 477\nEECS 491\nKahn’s algorithm follows this idea to topologically sort a graph. At the beginning, all vertices with an in-degree of zero are pushed into a queue.\nThen, the vertex at the front of the queue is taken out, added to the topological ordering, and then removed from the graph. The neighbors of\nthis removed vertex are then updated, and any vertex with an updated in-degree of zero is pushed into the queue. This procedure is repeated\nuntil all the vertices are processed. For example, suppose we wanted to topologically sort the following graph using Kahn’s algorithm.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\nFirst, we will calculate the in-degree of all of the nodes.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n1\n0\n1\n2\n1\n1\n1\n2\n2\n2\nQueue\nCurrent Vertex\nNext, we will push all of the vertices with an in-degree of zero into the queue. Similar to before, we will push vertices into the queue in\nalphabetical order, so vertex 𝐵will be pushed in before vertex 𝐶.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n1\n0\n1\n2\n1\n1\n1\n2\n2\n2\nCB\nQueue\nCurrent Vertex\nTopological Ordering:", "word_count": 458, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5ddc9bbc-2b04-59b5-8be9-0d2e0c22f07a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 745, "real_page_number": null, "text": "19.6 Topological Sort\n733\nWe take vertex 𝐵out of the front of the queue and add it to the topological ordering. Then, we remove vertex 𝐵from the graph and update the\nin-degree of its neighbors. Vertices 𝐹and 𝐺now have an in-degree of zero, so we push them into the queue.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n1\n0\n0\n2\n1\n1\n0\n2\n2\n2\nC\nQueue\nCurrent\nB\nB\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n1\n0\n0\n2\n1\n1\n0\n2\n2\n2\nC GF\nQueue\nCurrent\nB\nB\nVertex 𝐶is next in the queue, so we take it out and add it to our topological ordering. We then remove vertex 𝐶from the graph and update the\nin-degree of its neighbor, vertex 𝐾. Vertex 𝐾now has an in-degree of zero, so we push 𝐾into the queue.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n2\n1\n1\n0\n2\n2\n2\nGF\nQueue\nCurrent\nC\nB\nC\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n2\n1\n1\n0\n2\n2\n2\nGF K\nQueue\nCurrent\nC\nB\nC\nVertex 𝐹is next in the queue, so we take it out and add it to our topological ordering. We then remove vertex 𝐹from the graph and update the\nin-degree of its neighbors, vertices 𝐴and 𝐻.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n1\n1\n1\n0\n1\n2\n2\nG K\nQueue\nCurrent Vertex\nF\nTopological Ordering:\nB\nC\nF\nNeither vertex 𝐴nor vertex 𝐻has an updated in-degree of zero, so nothing gets added to the queue just yet. Next, we take out vertex 𝐺from\nthe front of the queue and add it to our topological ordering. We then remove vertex 𝐺from the graph and update the in-degree of its neighbor,\nvertex 𝐻. Vertex 𝐻’s in-degree is now zero, so we push 𝐻into the queue.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n1\n1\n1\n0\n0\n2\n2\nK\nQueue\nCurrent\nG\nB\nC\nF\nG\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n1\n1\n1\n0\n0\n2\n2\nK H\nQueue\nCurrent\nG\nB\nC\nF\nG", "word_count": 406, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c6d923c9-3d26-509f-85fc-6a427a68b4d2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 746, "real_page_number": null, "text": "734\nChapter 19. Graphs and Elementary Graph Algorithms\nVertex 𝐾is next in the queue, so we take it out and add it to our topological ordering. We then remove vertex 𝐾from the graph and update the\nin-degree of its neighbor, vertex 𝐴. Vertex 𝐴’s in-degree is now zero, so we push 𝐴into the queue.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n1\n1\n0\n0\n2\n2\nH\nQueue\nCurrent\nK\nB\nC\nF\nG\nK\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n1\n1\n0\n0\n2\n2\nH A\nQueue\nCurrent\nK\nB\nC\nF\nG\nK\nVertex 𝐻is next in the queue, so we take it out and add it to our topological ordering. We then remove vertex H from the graph and update the\nin-degree of its neighbor, vertex 𝐸.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n2\nA\nQueue\nCurrent Vertex\nH\nTopological Ordering:\nB\nC\nF\nG\nK\nH\nVertex 𝐸’s in-degree is not yet zero, so we do not push it into the queue just yet. Next, we take out vertex 𝐴, which is at the top of the queue,\nand add it to the topological ordering. We then remove vertex 𝐴from the graph and update the in-degrees of its neighbors, vertices 𝐸and 𝐼.\nBoth 𝐸and 𝐼now have in-degrees of zero, so we push them into the queue.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n2\nQueue\nCurrent\nA\nB\nC\nF\nG\nK\nH\nA\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n2\nE I\nQueue\nCurrent\nA\nB\nC\nF\nG\nK\nH\nA\nVertex 𝐸is at the front of the queue, so we take it out and add it to our topological ordering. We then remove vertex 𝐸from the graph and\nupdate the in-degree of its neighbor, vertex 𝐽.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\nI\nQueue\nCurrent Vertex\nE\nTopological Ordering:\nB\nC\nF\nG\nK\nH\nA\nE\nVertex 𝐽’s updated in-degree is not yet zero, so we do not push it into the queue. Next, we take out vertex 𝐼, which is at the front of the queue,\nand add it to our topological ordering. We then remove vertex 𝐼from the graph and update the in-degrees of its neighbors, vertices 𝐷and 𝐽.\nBoth 𝐷and 𝐽now have in-degrees of zero, so we push them into the queue.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nQueue\nCurrent\nI\nB\nC\nF\nG\nK\nH\nA\nE\nI\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nJD\nQueue\nCurrent\nI\nB\nC\nF\nG\nK\nH\nA\nE\nI", "word_count": 535, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "83cf920f-244e-5cfd-ad24-035fcc0ff472", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 747, "real_page_number": null, "text": "19.6 Topological Sort\n735\nVertex 𝐷is next in the queue, so we take it out and add it to our topological ordering. We then remove vertex 𝐷from the graph. Since 𝐷has no\nneighbors, nothing is pushed into the queue.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nJ\nQueue\nCurrent Vertex\nD\nTopological Ordering:\nB\nC\nF\nG\nK\nH\nA\nE\nI\nD\nVertex 𝐽is next in the queue, so we take it out and add it to our topological ordering. We then remove vertex 𝐽from the graph. Similar to\nvertex 𝐷, vertex 𝐽has no neighbors, so nothing is pushed into the queue. All the vertices in the graph have been processed, so the algorithm\ncompletes, and we have our final topological ordering.\nA\nF\nC\nD\nE\nB\nG\nH\nI\nJ\nK\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nQueue\nCurrent Vertex\nJ\nTopological Ordering:\nB\nC\nF\nG\nK\nH\nA\nE\nI\nD\nJ\nThe code for Kahn’s algorithm is shown below. First, we calculate the in-degree of every vertex in the graph. Then, we explore vertices with an\nin-degree of zero using a breadth-first search, adding them to the topological ordering and removing them from the graph along the way. The\nΘ(|𝑉|+|𝐸|),time complexity of Kahn’s algorithm on an adjacency list is since most of the work comes from the breadth-first search. Similarly,\nΘ(|𝑉|2)the time complexity of Kahn’s algorithm is on an adjacency matrix.\n1\nstd::vector<int32_t> topological_sort(const std::vector<std::vector<int32_t>>& adj_list) {\n2\nstd::vector<int32_t> in_degrees(adj_list.size(), 0);\n3\nfor (auto& row : adj_list) {\n4\nfor (int32_t vertex : row) {\n5\n++in_degrees[vertex];\n6\n} // for vertex\n7\n} // for row\n8\nstd::vector<int32_t> result;\n9\nstd::queue<int32_t> bfs;\n10\nfor (size_t vertex = 0; vertex < adj_list.size(); ++vertex) {\n11\nif (in_degrees[vertex] == 0) {\n12\nbfs.push(vertex);\n13\n} // if\n14\n} // for vertex\n15\nwhile (!bfs.empty()) {\n16\nint32_t curr = bfs.front();\n17\nbfs.pop();\n18\nresult.push_back(curr);\n19\nfor (int32_t neighbor : adj_list[curr]) {\n20\nif (--in_degrees[neighbor] == 0) {\n21\nbfs.push(neighbor);\n22\n} // if\n23\n} // for\n24\n} // while\n25\nreturn result;\n26\n} // topological_sort()", "word_count": 376, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "28c72b34-ad41-5020-9cd2-7c043d0344af", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 748, "real_page_number": null, "text": "736\nChapter 19. Graphs and Elementary Graph Algorithms\nExample 19.23 There are a total of 𝑛classes you have to take to graduate, each labeled with an integer from 0 to 𝑛−1. Some courses may\nprereqs, prereq[i] = [a, b]have prerequisites. You are given a vector of all prerequisite pairs, where indicates that you must take\nb a b a). ncourse course (i.e., is a prerequisite of Given the total number of courses and a vector of all prerequisite pairs, return anbefore\nordering of courses you should take to finish all courses. If there are multiple valid orderings, return any of them. You are guaranteed that\nthere exists at least one valid ordering. This is the same problem as example 19.22, but use Kahn’s algorithm to topologically sort the graph.\nUsing the structure of Kahn’s algorithm, we can implement the function as follows:\n1\nstd::vector<int32_t> schedule_courses(int32_t std::vector<std::vector<int32_t>>&n, prereqs) {\n2\nstd::vector<std::vector<int32_t>> adj_list(n);\n3\nstd::vector<int32_t> in_degrees(adj_list.size(), 0);\n4\nfor (auto& prereq : prereqs) {\n5\nadj_list[prereq[1]].push_back(prereq[0]);\n6\n++in_degrees[prereq[0]];\n7\n} // for prereq\n8\nstd::vector<int32_t> result;\n9\nstd::queue<int32_t> bfs;\n10\nfor (int32_t course = 0; course < n; ++course) {\n11\nif (in_degrees[course] == 0) {\n12\nbfs.push(course);\n13\n} // if\n14\n} // for course\n15\nwhile (!bfs.empty()) {\n16\nint32_t curr = bfs.front();\n17\nbfs.pop();\n18\nresult.push_back(curr);\n19\nfor (int32_t neighbor : adj_list[curr]) {\n20\nif (--in_degrees[neighbor] == 0) {\n21\nbfs.push(neighbor);\n22\n} // if\n23\n} // for neighbor\n24\n} // while\n25\nreturn result;\n26\n} // schedule_courses()\n19.7\nAlgorithms for Strongly Connected Components (✽)\n¸ 19.7.1\n(✽)Strongly Connected Components\nIn a directed graph, a strongly connected component (SCC) is a subgraph such that, for every pair of vertices 𝑢and 𝑣in the subgraph, there\nexists a directed path from 𝑢to 𝑣and from 𝑣to 𝑢. That is, in a strongly connected component, it is possible to reach any vertex in the SCC from\nany other vertex in the same SCC using the directed edges of the graph. As an example, consider the following graph:\nA\nB\nC\nD\nE\nF\nG\nH\nThere are four strongly connected components in this graph, as shown:\nA\nB\nC\nD\nE\nF\nG\nH", "word_count": 370, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "83147e3b-52a9-5deb-8f56-8ae8e05933cf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 749, "real_page_number": null, "text": "19.7 Algorithms for Strongly Connected Components\n737\nEach of the vertices in a strongly connected component can reach all other vertices in the exact same strongly connected component. One\nparticular trait of strongly connected components is their ability to be condensed into a directed acyclic graph (DAG) by contracting each\ncomponent into a single vertex. For instance, if we take the above graph and treat each SCC as its own vertex, we would get the following DAG:\nAB\nCDH\nE\nFG\nNotice that this is always true because it is impossible to create a cycle between two distinct strongly connected components — otherwise, they\nwould be a part of the exact same strongly connected component!\nStrongly connected components show up in a lot of real-life examples, so they have several practical applications (for example, you could\nuse SCCs to identify groups of people on a social media page with similar interests). In this section, we will discuss a common algorithm that\ncan be used to solve strongly connected component problems: algorithm.Kosaraju’s\n¸ 19.7.2\n(✽)Kosaraju’s Algorithm\nKosaraju’s algorithm is an algorithm that can be used to find the strongly connected components of a graph. This algorithm relies on two\npasses of depth-first search: the first pass on the original graph, and the second pass on a or the original graph (which is the sametranspose\ngraph, but with the direction of every edge reversed). The steps of Kosaraju’s algorithm are as follows:\n1. Iterate over all the vertices of the original graph. If a vertex is unvisited, perform a recursive depth-first search starting from that vertex,\nand mark any vertex discovered during the search as visited. After all of a vertex’s neighbors are visited, push the vertex onto a stack.\n2. Reverse all of the edges of the graph to get its transpose.\nContinuously pop out vertices from the stack and perform a DFS on the transpose graph starting from each vertex if it is unvisited. All3.\nthe vertices encountered during each iteration of DFS forms a strongly connected component. Repeat this until all vertices are visited.\nTo illustrate this process, consider the graph provided previously. We begin by performing a DFS starting from vertex 𝐴, marking each vertex\nwe encounter as visited. Once all of a vertex’s neighbors are visited, we push it onto a stack.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nF\nF\nF\nF\nF\nF\nF\nA\nB\nC\nD\nE\nF\nG\nH\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nF\nF\nF\nF\nF\nF\nA\nB\nC\nD\nE\nF\nG\nH\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nF\nF\nF\nF\nF\nA\nB\nC\nD\nE\nF\nG\nH\nStack", "word_count": 467, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3553a678-e682-59f5-ad6e-9fedf20d7b9d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 750, "real_page_number": null, "text": "738\nChapter 19. Graphs and Elementary Graph Algorithms\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nF\nF\nF\nF\nA\nB\nC\nD\nE\nF\nG\nH\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nF\nF\nF\nT\nA\nB\nC\nD\nE\nF\nG\nH\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nF\nF\nF\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nF\nF\nF\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nF\nF\nF\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nStack", "word_count": 144, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fd96f936-c9f3-5159-91b5-6bc0fa0c9352", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 751, "real_page_number": null, "text": "19.7 Algorithms for Strongly Connected Components\n739\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nF\nF\nF\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nF\nF\nF\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nStack\nOur DFS from vertex 𝐴is now complete. Next, we look at vertex 𝐵. Vertex 𝐵is already visited, so no work needs to be done there. The same\napplies to vertices 𝐶and 𝐷. The next unvisited vertex we encounter is vertex 𝐸, so we perform a DFS on vertex 𝐸, adding vertices to the stack\nafter we are done visiting all of its neighbors.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nT\nF\nF\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nT\nT\nF\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nT\nT\nT\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nStack", "word_count": 224, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2b87f78e-5c31-5f58-9c0e-b1cb3a019568", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 752, "real_page_number": null, "text": "740\nChapter 19. Graphs and Elementary Graph Algorithms\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nT\nT\nT\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nG\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nT\nT\nT\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nG\nF\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nT\nT\nT\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nG\nF\nE\nStack\nAll the vertices have been visited and pushed into the stack, so we have completed our first pass of DFS over the graph. Our next step is to\ngenerate a transpose of the graph by reversing the directions of all the edges. The new transpose graph is shown below:\nA\nB\nC\nD\nE\nF\nG\nH\nNext, we begin another pass of DFS, this time on the transpose graph. During this second pass, we consider vertices in the order in which they\nare popped from the stack. In this case, 𝐸is at the top of the stack, so we start with a DFS beginning at vertex 𝐸.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nF\nF\nF\nF\nT\nF\nF\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nG\nF\nStack", "word_count": 248, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3ed22ad5-58be-5d81-b529-1229898bd902", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 753, "real_page_number": null, "text": "19.7 Algorithms for Strongly Connected Components\n741\nThere are no vertices that are reachable from 𝐸in this transpose graph, so our DFS completes. We can therefore conclude that 𝐸is part of its\nown strongly connected component.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nF\nF\nF\nF\nT\nF\nF\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nG\nF\nStack\nThe next vertex at the top of the stack is 𝐹. 𝐹is unvisited, so we begin a DFS starting from vertex 𝐹.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nF\nF\nF\nF\nT\nT\nF\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nG\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nF\nF\nF\nF\nT\nT\nT\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nG\nStack\nThere are no more unvisited vertices that are reachable starting from vertex 𝐹, so this iteration of DFS completes. The two vertices that we\nvisited during this DFS, vertices 𝐹and 𝐺, must therefore be a part of the same strongly connected component.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nF\nF\nF\nF\nT\nT\nT\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nA\nG\nStack", "word_count": 229, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "35c75458-cdd9-5d9a-ab3c-ddcb62fc5bd7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 754, "real_page_number": null, "text": "742\nChapter 19. Graphs and Elementary Graph Algorithms\nThe next vertex we pop off the stack is vertex 𝐺, but 𝐺has already been visited, so we do not need to do any additional work here. The vertex\nafter that is vertex 𝐴. Vertex 𝐴is unvisited, so we start a DFS from vertex 𝐴.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nF\nF\nF\nT\nT\nT\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nF\nF\nT\nT\nT\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nStack\nThere are no more unvisited vertices that are reachable starting from vertex 𝐴, so this iteration of DFS completes. The two vertices that we\nvisited during this DFS, vertices 𝐴and 𝐵, must therefore be a part of the same strongly connected component.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nF\nF\nT\nT\nT\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nC\nB\nStack\nThe next vertex we pop off the stack is vertex 𝐵, but 𝐵has already been visited, so we do not need to do any additional work here. The vertex\nafter that is vertex 𝐶. Vertex 𝐶is unvisited, so we start a DFS from vertex 𝐶.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nF\nT\nT\nT\nF\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nStack", "word_count": 258, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "797cbe7f-b229-53a9-85af-9d2836dbc58f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 755, "real_page_number": null, "text": "19.7 Algorithms for Strongly Connected Components\n743\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nF\nT\nT\nT\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nStack\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nT\nT\nT\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nStack\nThere are no more unvisited vertices that are reachable starting from vertex 𝐶, so this iteration of DFS completes. The three vertices that we\nvisited during this DFS, vertices 𝐶, 𝐷, and 𝐻, must therefore be a part of the same strongly connected component.\nA\nB\nC\nD\nE\nF\nG\nH\nVisited:\nT\nT\nT\nT\nT\nT\nT\nT\nA\nB\nC\nD\nE\nF\nG\nH\nH\nD\nStack\nThe next two vertices in the stack, 𝐷and 𝐻, have already been visited, so we can pop them off without doing any additional work. The stack is\nnow empty, meaning that our algorithm is complete, and we have found all the strongly connected components in the original graph.", "word_count": 183, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "660584bd-f8a6-5b78-8371-c8457e9a6ad8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 756, "real_page_number": null, "text": "744\nChapter 19. Graphs and Elementary Graph Algorithms\nAn implementation of Kosaraju’s algorithm is shown below (this implementation prints the SCCs to the console):\n1\nvoid dfs_helper(const std::vector<std::vector<int32_t>>& int32_tadj_list, vertex,\n2\nstd::vector<bool>& std::stack<int32_t>&visited, visit_order) {\n3\ntrue;visited[vertex] =\n4\n// iterate over all neighbors and begin a DFS if not visited\n5\nfor (int32_t neighbor : adj_list[vertex]) {\n6\nif (!visited[neighbor]) {\n7\ndfs_helper(adj_list, neighbor, visited, visit_order);\n8\n} // if\n9\n} // for\n10\n// push the vertices visited into the stack\n11\nvisit_order.push(vertex);\n12\n} // dfs_helper()\n13\n14\nstd::vector<std::vector<int32_t>> get_transpose(const std::vector<std::vector<int32_t>>& adj_list) {\n15\nstd::vector<std::vector<int32_t>> transpose_graph(adj_list.size());\n16\nfor (size_t vertex = 0; vertex < adj_list.size(); ++vertex) {\n17\nfor (int32_t neighbor : adj_list[vertex]) {\n18\ntranspose_graph[neighbor].push_back(vertex);\n19\n} // for neighbor\n20\n} // for vertex\n21\nreturn transpose_graph;\n22\n} // get_transpose()\n23\n24\nvoid dfs_helper_transpose(const std::vector<std::vector<int32_t>>& int32_ttranspose_graph, vertex,\n25\nstd::vector<bool>& visited) {\n26\ntrue;visited[vertex] =\n27\n// print out vertex (to identify which vertices belong to each strongly connected component)\n28\nstd::cout << vertex << \" \";\n29\n// iterate over all neighbors and begin a DFS if not visited\n30\nfor (int32_t neighbor : transpose_graph[vertex]) {\n31\nif (!visited[neighbor]) {\n32\ndfs_helper_transpose(transpose_graph, neighbor, visited);\n33\n} // if\n34\n} // for\n35\n} // dfs_helper_transpose()\n36\n37\nvoid kosaraju(const std::vector<std::vector<int32_t>>& adj_list) {\n38\nstd::stack<int32_t> visit_order;\n39\nstd::vector<bool> false);visited(adj_list.size(),\n40\n// first pass of DFS\n41\nfor (size_t vertex = 0; vertex < adj_list.size(); ++vertex) {\n42\nif (!visited[vertex]) {\n43\ndfs_helper(adj_list, vertex, visited, visit_order);\n44\n} // if\n45\n} // for vertex\n46\n// get transpose of graph\n47\nstd::vector<std::vector<int32_t>> transpose_graph = get_transpose(adj_list);\n48\n// reset all visited values to false before second pass of DFS\n49\nfalse);std::fill(visited.begin(), visited.end(),\n50\n// second pass of DFS, visit vertices in the order popped from the stack\n51\nwhile (!visit_order.empty()) {\n52\nint32_t next_vertex = visit_order.top();\n53\nvisit_order.pop();\n54\nif (!visited[next_vertex]) {\n55\ndfs_helper_transpose(transpose_graph, next_vertex, visited);\n56\nstd::cout << std::endl; // separates SCCs using a newline\n57\n} // if\n58\n} // while\n59\n} // kosaraju()\nIf the above implementation were run using our example graph (where vertex 𝐴= 0, 𝐵= 1, …, 𝐻= 7), we would get the following output that\nidentifies the four strongly connected components in the graph:\n4\n5 6\n0 1\n2 7 3\nBecause Kosaraju’s algorithm performs two passes of DFS on the given graph, the time complexity of the algorithm is also determined by the\nΘ(|𝑉|2)Θ(|𝑉|+|𝐸|)time complexity of a DFS; namely, on an adjacency list and on an adjacency matrix. Although the DFS is performed on\ntwo different graphs, the complexity of the first and second passes are identical since the transpose of any graph still contains the same number\nof vertices and edges.\nKosaraju’s Algorithm\nAdjacency List\nAdjacency Matrix\nTime Complexity\nΘ(|𝑉|+|𝐸|)\nΘ(|𝑉|2)", "word_count": 478, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "37779eb8-adf2-550e-8e8b-b5e81b64033f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 757, "real_page_number": null, "text": "19.7 Algorithms for Strongly Connected Components\n745\nWhy does Kosaraju’s algorithm work? To understand the intuition behind the algorithm, consider what happens when we perform a DFS on the\noriginal graph for the SCC containing vertices 𝐴and 𝐵. This DFS ends up visiting 𝐴and 𝐵, but it also visits several other vertices that are not\nin the same component (𝐶, 𝐷, and 𝐻). Because of this, the first DFS pass alone is not enough to determine the SCCs of a graph.\nA\nB\nC\nD\nE\nF\nG\nH\nHowever, when we reverse the edges of the graph, we are able to filter out these extraneous vertices. In the transpose graph, the edge between 𝐵\nand 𝐶is flipped, making it impossible for us to visit the component containing 𝐶, 𝐷, and 𝐻from the component containing 𝐴and 𝐵.\nA\nB\nC\nD\nE\nF\nG\nH\nHowever, reversing the graph brings out a new issue: a DFS on the transpose graph starting from the SCC containing 𝐴and 𝐵also visits\nvertices we did not in the original graph, namely 𝐸, 𝐹, and 𝐺. However, 𝐸, 𝐹, and 𝐺are not in the same SCC as 𝐴and 𝐵.\nA\nB\nC\nD\nE\nF\nG\nH\nThis issue is addressed using the order in which we visit vertices in the transpose graph. Recall that we push each vertex onto a stack weafter\nhave visited all of its neighbors in the original graph. This means that, for any pair of vertices 𝑢and 𝑣in the original graph such that there is a\npath from 𝑢to 𝑣but not from 𝑣to 𝑢, we must have pushed 𝑢into the stack 𝑣. This also means that 𝑢is visited 𝑣in the transposeafter before\ngraph since the vertices are visited in last in, first out (LIFO) order via the stack. Thus, when we begin a DFS on 𝑣in the transpose graph, we do\nnot consider 𝑢as part of the same SCC even though there is a path from 𝑣to 𝑢: this is because 𝑢must have already been marked as visited\nbefore the DFS on 𝑣(and thus is ignored during the DFS on 𝑣). As a result, this implies that each iteration of DFS in the transpose graph\nidentifies a distinct SCC in the graph, which in turn also identifies a distinct SCC in the original graph (since any graph has the exact same\nstrongly connected components as its transpose).\nIn our example above, the ordering of the vertices in the stack ensures that vertices 𝐸, 𝐹, and 𝐺are already visited before we process\nvertices 𝐴and 𝐵. Thus, when we process vertices 𝐴and 𝐵in the transpose graph, we do not end up considering 𝐸, 𝐹, or 𝐺as part of the same\nSCC; instead, all the vertices encountered during the same iteration of DFS must also be part of the same SCC.\nIn summary, for any vertex 𝑢in the original graph, any vertex 𝑣that from 𝑢in the original graph but not the other way aroundis reachable\nis filtered out due to the reversed edges in the transpose graph, and any vertex 𝑣that 𝑢in the original graph but not the other wayis able to reach\naround is filtered out due to the LIFO ordering used to visit vertices in the transpose graph. As a result, if 𝑢is able to visit 𝑣in both the original\ngraph and the transpose graph, then the two vertices must be part of the same strongly connected component.\nExample 19.24 You are running a promotion for an ad campaign, and you want to share a promotion message to a specific group of users\non a social media page. Assume that, if a user shares a message, then every one of that user’s followers will also share the message to their\nown followers. Given an adjacency list of users and the people that follow them, implement a function that returns the minimum number of\npeople that you need to reach out to so that your promotion message can be spread across the entire network of users.\nExample: Given the following graph of users, you would return 2, since you would only need to request two users to send your promotion\nmessage for the entire network of users to receive it (user 𝐷, and one of either user 𝐿or 𝑀):\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM", "word_count": 724, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1f9e85a7-3cb7-576e-8a5d-a62d41d28313", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 758, "real_page_number": null, "text": "746\nChapter 19. Graphs and Elementary Graph Algorithms\nAt first glance, it may seem that the solution is to send a message to all the users whose in-degree is zero (i.e., vertices in the graph that have\nno edges directed at it), since these are the users that cannot be reached via a direct connection with another user they follow. However, this\napproach would fail in the provided example, as user 𝐷is the only vertex with an in-degree of zero. If a message were only sent to user 𝐷, then\nusers 𝐿and 𝑀would not be able to get it through the network.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nEven though users 𝐿and 𝑀do not have an in-degree of zero, they are not able to get the message from user 𝐷because they are a part a\nself-containedcyclethatisnotreachablefromanyoftheothervertices. Asaresult,wecannotsimplyconsidereachuserinthegraphindividually,\nas users with an in-degree greater than zero may still be disjoint from other users in the network (as was the case with 𝐿and 𝑀). Instead, a\nbetter solution would be to treat the graph as a collection of strongly connected components. Why is this so? Notice that if a single vertex has an\nin-degree greater than zero, we conclude that the vertex is reachable from another arbitrary vertex since the positive in-degree may becannot\ncaused by the presence of self-contained cycles. However, if a strongly connected component has an in-degree greater than zero, then we can\nconclude that is reachable from another strongly connected component. This is because a graph whose strongly connected components are each\ncondensed into a single vertex cannot have any cycles!\nA\nBC\nD\nEFIJK\nLM\nGH\nIn this condensed graph, only two vertices have an in-degree of zero: 𝐷and 𝐿𝑀. Since the graph is also a directed acyclic graph, all the other\nvertices must be reachable from some other strongly connected component in the graph. Therefore, you would only need to request user 𝐷and\none of either user 𝐿or 𝑀to send your promotion message for it to eventually reach everyone in the network. From this, we can see that the\nsolution to the problem is to simply condense the users into individual SCCs, and then count the number of SCCs with an in-degree of zero.\nThis can be done using Kosaraju’s algorithm, where a possible implementation is shown below:\n1\nvoid dfs_helper(const std::vector<std::vector<int32_t>>& int32_tadj_list, vertex,\n2\nstd::vector<bool>& std::stack<int32_t>&visited, visit_order) {\n3\ntrue;visited[vertex] =\n4\n// iterate over all neighbors and begin a DFS if not visited\n5\nfor (int32_t neighbor : adj_list[vertex]) {\n6\nif (!visited[neighbor]) {\n7\ndfs_helper(adj_list, neighbor, visited, visit_order);\n8\n} // if\n9\n} // for\n10\n// push the vertices visited into the stack\n11\nvisit_order.push(vertex);\n12\n} // dfs_helper()\n13\n14\nstd::vector<std::vector<int32_t>> get_transpose(const std::vector<std::vector<int32_t>>& adj_list) {\n15\nstd::vector<std::vector<int32_t>> transpose_graph(adj_list.size());\n16\nfor (size_t vertex = 0; vertex < adj_list.size(); ++vertex) {\n17\nfor (int32_t neighbor : adj_list[vertex]) {\n18\ntranspose_graph[neighbor].push_back(vertex);\n19\n} // for neighbor\n20\n} // for vertex\n21\nreturn transpose_graph;\n22\n} // get_transpose()\n23\n24\nvoid dfs_helper_transpose(const std::vector<std::vector<int32_t>>& int32_ttranspose_graph, vertex,\n25\nstd::vector<int32_t>& int32_trepresentatives, rep_vertex) {\n26\nrepresentatives[vertex] = rep_vertex;\n27\n// iterate over all neighbors and begin a DFS if not visited; since this recurses on all vertices\n28\n// in the same SCC as the passed in 'vertex', we can set all of them to have a rep of 'vertex'\n29\nfor (int32_t neighbor : transpose_graph[vertex]) {\n30\nif (representatives[neighbor] == -1) {\n31\ndfs_helper_transpose(transpose_graph, neighbor, representatives, rep_vertex);\n32\n} // if\n33\n} // for\n34\n} // dfs_helper_transpose()", "word_count": 619, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f51ad5f8-8af8-5e14-8266-8217aab47caa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 759, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n747\n36\nint32_t min_users_to_spread_message(const std::vector<std::vector<int32_t>>& adj_list) {\n37\nstd::stack<int32_t> visit_order;\n38\nstd::vector<bool> false);visited(adj_list.size(),\n39\n40\n// first pass of DFS\n41\nfor (size_t vertex = 0; vertex < adj_list.size(); ++vertex) {\n42\nif (!visited[vertex]) {\n43\ndfs_helper(adj_list, vertex, visited, visit_order);\n44\n} // if\n45\n} // for vertex\n46\n47\n// get transpose of graph\n48\nstd::vector<std::vector<int32_t>> transpose_graph = get_transpose(adj_list);\n49\n50\n// create vector of representatives to keep track of distinct SCCs\n51\nstd::vector<int32_t> representatives(adj_list.size(), -1);\n52\n53\n// second pass of DFS, visit vertices in the order popped from the stack\n54\nwhile (!visit_order.empty()) {\n55\nint32_t next_vertex = visit_order.top();\n56\nvisit_order.pop();\n57\nif (representatives[next_vertex] == -1) {\n58\n// 'next_vertex' is part of an SCC we have not visited yet, so set 'next_vertex' as the\n59\n// representative of every vertex we visit during this recursive call\n60\ndfs_helper_transpose(transpose_graph, next_vertex, representatives, next_vertex);\n61\n} // if\n62\n} // while\n63\n64\n// count in-degrees of condensed SCC graph, then return number of reps with an in-degree of zero\n65\nstd::vector<int32_t> in_degrees(adj_list.size(), 0);\n66\nfor (int32_t i = 0; i < adj_list.size(); ++i) {\n67\nfor (int32_t j : adj_list[i]) {\n68\nif (representatives[i] != representatives[j]) {\n69\n// directed edge from SCC of i to SCC of j, so increment in-degree of SCC of j\n70\n++in_degrees[representatives[j]];\n71\n} // if\n72\n} // for j\n73\n} // for i\n74\n75\nint32_t count = 0;\n76\nfor (size_t k = 0; k < in_degrees.size(); ++k) {\n77\nif (representatives[k] == k && in_degrees[k] == 0) {\n78\n++count;\n79\n} // if\n80\n} // for k\n81\n82\nreturn count;\n83\n} // min_users_to_spread_message()\nΘ(|𝑉|+|𝐸|)This solution runs one iteration of Kosaraju’s algorithm (which takes time on an adjacency list), and then loops over the edges to\nΘ(|𝐸|)count the in-degrees (which takes time), and then loops over the vertices to determine which components have an in-degree of zero\nΘ(|𝑉|) Θ(|𝑉|+|𝐸|).(which takes time). Since the Kosaraju’s step has the dominant time complexity, the overall time complexity is\n19.8\nAlgorithms for Articulation Points and Bridges (✽)\n¸ 19.8.1\n(✽)Articulation Points and Bridges\nArticulation points and bridges are two additional concepts that are important when analyzing graphs. An articulation point (or vertex)cut\nin a graph is a vertex whose removal would increase the number of connected components in the graph. For instance, vertices 𝐷and 𝐸are\narticulation points in the following graph, since removing either of these vertices would disconnect the graph.\nA\nB\nC\nD\nE\nF\nG\nH\nA\nB\nC\nE\nF\nG\nH\nD Removed\nA\nB\nC\nD\nF\nG\nH\nE Removed", "word_count": 454, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dafea358-7d66-58c5-af7a-4397ea8fe8e4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 760, "real_page_number": null, "text": "748\nChapter 19. Graphs and Elementary Graph Algorithms\nSimilarly, a bridge (or edge) in a graph is an edge whose removal would increase the number of connected components in the graph. In thecut\nfollowing graph, edges 𝐷𝐸and 𝐷𝐻are bridges, since removing either of these edges would disconnect the graph.\nA\nB\nC\nD\nE\nF\nG\nH\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷𝐸Removed\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷𝐻Removed\nAlgorithms that work with articulation points and bridges play an important role in graph theory and have several practical applications in\ncomputer science. For example, the ability to identify articulation points and bridges in a network will allow you to determine which vertices\nand connections are critical to the function of the network and may be vulnerable to failure or disruption. In this section, we will discuss one\nalgorithm that can be used to find articulation points and bridges: algorithm.Tarjan’s bridge-finding\n¸ 19.8.2\n(✽)Tarjan’s Bridge-Finding Algorithm\nTarjan’s bridge-finding algorithm is an algorithm that can be used to identify articulation points and bridges in a graph in linear time. To\nunderstand how Tarjan’s bridge-finding algorithm works, let us first look at how we can determine whether a vertex is an articulation point\nor not. Consider vertex 𝐷in our previous example, which we will illustrate as a connection between two disjoint subgraphs: one containing\nvertices 𝐴to 𝐶, and one containing vertices 𝐸to 𝐻:\nABC\nD\nEFGH\nWe know that 𝐷is an articulation point because there is no edge that connects the subgraph 𝐴𝐵𝐶to the subgraph 𝐸𝐹𝐺𝐻. Had there been an\nedge between these two subgraphs, then 𝐷would not be an articulation point, as the remaining subgraphs would still be connected via that edge.\nABC\nD\nEFGH\nThus, to identify articulation points, we would need a way to efficiently determine if this condition is met for every vertex in the graph. This\nforms the basis of Tarjan’s bridge-finding algorithm. To start, the algorithm picks an arbitrary vertex (the root) and performs a depth-first search\non the graph, marking each vertex with a number that indicates the order in which it was discovered (we will denote this number as a vertex’s\ntime). An example is shown below, where a DFS is initiated at vertex 𝐴, and adjacent vertices are visited in alphabetical order.discovery\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷=0\n𝐷=1\n𝐷=2\n𝐷=3\n𝐷=4\n𝐷=5\n𝐷=6\n𝐷=7\nDuring the DFS, we also keep track of the of each vertex, which is lowest discovery time that is reachable from that vertex by onlylow time\ntraversing at most one in the graph. A back edge is an edge in the graph that travels from one vertex to another vertex with a smallerback edge\ndiscovery time (with the exception of the direct parent of the vertex, i.e., if you discover vertex 𝐵from vertex 𝐴during the DFS, then the edge\nfrom 𝐵directly back to 𝐴cannot be considered). In the graph above, the low time of vertex 𝐴is 0, since that is trivially the lowest discovery\ntime that is reachable from vertex 𝐴(which has a discovery time of 0 itself). The low time of vertex 𝐵is also 0, since that is the lowest discovery\ntime reachable from 𝐵using only one back edge (depicted using the dashed line).\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷=0\n𝐿=0\n𝐷=1\n𝐿=0\n𝐷=2\n𝐷=3\n𝐷=4\n𝐷=5\n𝐷=6\n𝐷=7", "word_count": 587, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3879c7d0-98bc-5ed0-be76-4be325709435", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 761, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n749\nThe low time of vertex 𝐶is 0, since that is the lowest discovery time reachable from 𝐶using only one back edge.\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷=0\n𝐿=0\n𝐷=1\n𝐿=0\n𝐷=2\n𝐿=0\n𝐷=3\n𝐷=4\n𝐷=5\n𝐷=6\n𝐷=7\nThe low time of vertex 𝐷is 0, since that is the lowest discovery time reachable from 𝐷using only one back edge.\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷=0\n𝐿=0\n𝐷=1\n𝐿=0\n𝐷=2\n𝐿=0\n𝐷=3\n𝐿=0\n𝐷=4\n𝐷=5\n𝐷=6\n𝐷=7\nThe low time of vertex 𝐸is 4, since that is the lowest discovery time reachable from 𝐸(note that we cannot travel directly back to 𝐷, since 𝐷\nwas the direct parent of 𝐸during the DFS, and thus 𝐸𝐷is not considered to be a back edge).\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷=0\n𝐿=0\n𝐷=1\n𝐿=0\n𝐷=2\n𝐿=0\n𝐷=3\n𝐿=0\n𝐷=4\n𝐿=4\n𝐷=5\n𝐷=6\n𝐷=7\nThe low time of vertex 𝐹is 4, since that is the lowest discovery time reachable from 𝐹using only one back edge.\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷=0\n𝐿=0\n𝐷=1\n𝐿=0\n𝐷=2\n𝐿=0\n𝐷=3\n𝐿=0\n𝐷=4\n𝐿=4\n𝐷=5\n𝐿=4\n𝐷=6\n𝐷=7\nThe low time of vertex 𝐺is 4, since that is the lowest discovery time reachable from 𝐺using only one back edge.\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷=0\n𝐿=0\n𝐷=1\n𝐿=0\n𝐷=2\n𝐿=0\n𝐷=3\n𝐿=0\n𝐷=4\n𝐿=4\n𝐷=5\n𝐿=4\n𝐷=6\n𝐿=4\n𝐷=7", "word_count": 307, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "803eab99-44c7-51ef-a94a-a8310fc6b0bc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 762, "real_page_number": null, "text": "750\nChapter 19. Graphs and Elementary Graph Algorithms\nLastly, the low time of vertex 𝐻is 7, since that is the lowest discovery time reachable from 𝐻(again, since we discovered 𝐻from 𝐷during\nour initial DFS, we cannot count the edge from 𝐻back to 𝐷as a back edge).\nA\nB\nC\nD\nE\nF\nG\nH\n𝐷=0\n𝐿=0\n𝐷=1\n𝐿=0\n𝐷=2\n𝐿=0\n𝐷=3\n𝐿=0\n𝐷=4\n𝐿=4\n𝐷=5\n𝐿=4\n𝐷=6\n𝐿=4\n𝐷=7\n𝐿=7\nWith this information, we are now able to determine the articulation points of the graph. Observe that, for any two adjacent vertices 𝑢and 𝑣in\nthe graph such that 𝑢has a lower discovery time than 𝑣(except the special case where 𝑢is the root, which we will cover later), then 𝑢must be an\n𝐿𝑣≥𝐷𝑢).articulation point if 𝑣has a low time that is greater than or equal to 𝑢(i.e., We can see why using the same visualization we used\nearlier. Assuming 𝑢is not the root of a DFS, we know that 𝑢is connected to two subgraphs — one with vertices with discovery times less than 𝑢\n(denoted as 𝑡), and one with vertices with discovery times greater than 𝑢(denoted as 𝑣):\n𝑡\n𝑢\n𝑣\nIf there exists a vertex in 𝑣with a low time that is than the discovery time of 𝑢, that means it is impossible for that vertex to reach anynot lesser\nvertex in 𝑡via a back edge (otherwise, the low time of 𝑣would be less than the discovery time of 𝑢, since 𝑣would be able to reach a vertex with\na discovery time less than that of 𝑢using that back edge). As a result, we know that vertex 𝑢must be an articulation point.\n𝑡\n𝑢\n𝑣\n𝐿𝑣≥𝐷𝑢, 𝑣to 𝑡, 𝑢mustIf then there cannot be a back edge from so be an articulation point.\nThere is one separate case that is not covered by this explanation: what if the vertex 𝑢were the root vertex of our depth-first search? Luckily,\ndetermining whether the root is an articulation point is relatively straightforward: we just need to check if the root is connected to at least two\nseparate components that are disconnected from each other. If it is, then the root must be an articulation point. For instance, the root vertex 𝑟is\nconnected to two subgraphs 𝑎and 𝑏in the illustration below. If 𝑎and 𝑏are disjoint from each other, then 𝑟must be an articulation point since its\nremoval would disconnect the graph. To keep track of the number of disjoint components connected to the root, we can use a counter that is\nincremented whenever we encounter a vertex adjacent to the root that has not been visited within any previous component explored during the\nDFS (see the solution to example 19.12 if you want to review how this would work).\n𝑟\n𝑎\n𝑏\nIn our initial graph, vertex 𝐷has direct connections with 𝐸and 𝐻, which are both discovered after 𝐷during our DFS. If we look at the low\ntimes of 𝐸and 𝐻, we can see that they are both greater than or equal to the discovery time of 𝐷. As a result, we can conclude that 𝐷is an\narticulation point, since there must not exist a back edge that connects 𝐸and 𝐻to any vertex that was discovered before 𝐷during our DFS. The\nsame logic applies to vertex 𝐸— the adjacent vertices 𝐹and 𝐺are discovered after 𝐸and both have a low time greater than or equal to the\ndiscovery time of 𝐸, so 𝐸is also an articulation point, as there must be no back edge that connects 𝐹and 𝐺with vertices discovered before 𝐸.\nTo summarize, we can use Tarjan’s bridge-finding algorithm to identify articulation points in a graph by following these steps:\n1. Select an arbitrary vertex (which becomes the root) and perform a depth-first search starting from this root. Keep track of a counter that\nindicates the number of disjoint components that are connected to the root.\n2. During the depth-first search, assign each visited vertex with the following information:\n• A time, which indicates the order in which the vertices of the graph are visited during the depth-first search.discovery\n• A time, which indicates the lowest discovery time that is reachable from that vertex using at most one back edge.low\n3. If a vertex 𝑢is encountered such that there is a vertex 𝑣with a larger discovery time where the low time of 𝑣is greater than or equal to\nthe discovery time of 𝑢, then 𝑢must be an articulation point.\n4. After the DFS is complete, check if the root is connected to more than one disjoint component. If so, then the root is also an articulation\npoint in the graph. At this point, all articulation points will have been discovered.\nWhat is the time complexity of this algorithm? Notice that we are simply performing a depth-first search with additional steps to keep track\nof discovery times and low times (both of which can be computed in constant time during the DFS). Because of this, the time complexity of\nΘ(|𝑉|+|𝐸|)Tarjan’s bridge-finding algorithm for finding articulation points is the same as that of a depth-first search: on an adjacency list, and\nΘ(|𝑉|2) on an adjacency matrix.", "word_count": 872, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ba926dc5-1a8f-5766-8806-c64cc61ee9c4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 763, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n751\nNow, what if you want to find bridges instead of articulation points? The algorithm would be pretty much the same, with one notable exception:\nan edge 𝑢𝑣is a bridge only if the low time of 𝑣is greater than the discovery time of 𝑢(i.e., 𝐷𝑢).𝐿𝑣>strictly\nWhy does the equal case no longer apply? If the low time of 𝑣were equal to the discovery time of 𝑢, and we were working with articulation\npoints, we could ensure that removing 𝑢would also break the the graph into additional components since 𝑣cannot reach anything before 𝑢.\nHowever, when working with bridges, removing 𝑢𝑣is not guaranteed to do the same if the edge were part of a cycle. Consider the graph below:\nA\nB\nC\nD\nIf we perform our depth-first search starting from vertex 𝐴, we would get the following discovery times and low times:\nA\nB\nC\nD\n𝐷=0\n𝐿=0\n𝐷=1\n𝐿=1\n𝐷=2\n𝐿=1\n𝐷=3\n𝐿=1\n𝐿𝐶≥𝐷𝐵).Consider the edge 𝐵𝐶. In this case, 𝐷𝐵, which indicates that vertex 𝐵is an articulation point (since the condition to satisfy is𝐿𝐶=\nHowever, 𝐵𝐶is not a bridge, which would require 𝐿𝐶to be strictly greater than 𝐷𝐵.\nA\nB\nC\nD\n𝐵𝐶doesRemoving not disconnect the graph\nNow, consider the edge 𝐴𝐵. In this case, 𝐷𝐴, which indicates that 𝐴𝐵is a bridge in the graph.𝐿𝐵>\nA\nB\nC\nD\n𝐴𝐵disconnectsRemoving the graph\nTo summarize, we can use Tarjan’s bridge-finding algorithm to identify bridges in a graph by following these steps:\n1. Select an arbitrary vertex (which becomes the root) and perform a depth-first search starting from this root. (Since we want to identify\nbridges, we do not need to count the number of disjoint components connected to the root, since the edges connected to the root are\nalready handled by the following procedure.)\n2. During the depth-first search, assign each visited vertex with the following information:\n• A time, which indicates the order in which the vertices of the graph are visited during the depth-first search.discovery\n• A time, which indicates the lowest discovery time that is reachable from that vertex using at most one back edge.low\n3. If an edge 𝑢𝑣is encountered such that the low time of 𝑣is strictly greater than the discovery time of 𝑢, then 𝑢𝑣must be a bridge.\n4. After the DFS is complete, all bridges will have been discovered.\nΘ(|𝑉|2)Θ(|𝑉|+|𝐸|)Like before, the time complexity of this algorithm is the same as that of a depth-first search: on an adjacency list, and on\nan adjacency matrix.", "word_count": 439, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9d0f98d1-fa5c-5974-957c-3ce8ff4977b5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 764, "real_page_number": null, "text": "752\nChapter 19. Graphs and Elementary Graph Algorithms\nExample19.25 Thereareatotalof𝑛serversinaroom,numberedfrom0to𝑛−1. Theseserversareconnectedbyundirectedserver-to-server\nconnections that form a network; any server can reach another server in the room directly or indirectly through the network.\n[a,b]Youaregivenavectorofconnections, suchthat representsaconnectionfromserver𝑎toserver𝑏. Returnallthecriticalconnections\nin the network, or connections that, when removed, would prevent some servers from reaching other servers in the original network.\nstd::vector<std::vector<int32_t>> critical_connections(int32_t const std::vector<std::vector<int32_t>>&n, connections);\n[[0, 1], [1, 2], [2, 3], [1, 3]], [[0, 1]],Example: Given servers and the following connections: you would return𝑛= 4\nsince the edge from server 0 to server 1 is a critical connection.\n0\n1\n2\n3\nThis is a bridge-finding problem, so we can use Tarjan’s bridge-finding algorithm. First, we will need to convert our input into an adjacency list.\nThen, we will apply the algorithm discussed above, adding each encountered bridge to a container that is returned. One possible implementation\nof the solution is shown below:\n1\nvoid dfs(int32_t int32_t const std::vector<std::vector<int32_t>>&curr, parent, adj_list,\n2\nstd::vector<int32_t>& std::vector<int32_t>& int32_t&discovery_times, low_times, time,\n3\nstd::vector<std::vector<int32_t>>& bridges) {\n4\nlow_times[curr] = discovery_times[curr] = ++time;\n5\nfor (int32_t neighbor : adj_list[curr]) {\n6\nif (neighbor == parent) {\n7\ncontinue;\n8\n} // if\n9\nif (discovery_times[neighbor] == 0) {\n10\ndfs(neighbor, curr, adj_list, discovery_times, low_times, time, bridges);\n11\nif (low_times[neighbor] > discovery_times[curr]) {\n12\nbridges.push_back({curr, neighbor});\n13\n} // if\n14\nlow_times[curr] = std::min(low_times[curr], low_times[neighbor]);\n15\n} // if\n16\nelse {\n17\nlow_times[curr] = std::min(low_times[curr], discovery_times[neighbor]);\n18\n} // else\n19\n} // for neighbor\n20\n} // dfs()\n21\n22\nstd::vector<std::vector<int32_t>> critical_connections(\n23\nint32_t const std::vector<std::vector<int32_t>>&n, connections) {\n24\nstd::vector<std::vector<int32_t>> adj_list(n);\n25\nfor (const auto& connection : connections) {\n26\nint32_t u = connection[0];\n27\nint32_t v = connection[1];\n28\nadj_list[u].push_back(v);\n29\nadj_list[v].push_back(u);\n30\n} // for connection\n31\n32\nint32_t time = 0;\n33\nstd::vector<int32_t> discovery_times(n, 0);\n34\nstd::vector<int32_t> low_times(n, 0);\n35\nstd::vector<std::vector<int32_t>> bridges;\n36\n37\nfor (int32_t i = 0; i < n; ++i) {\n38\nif (discovery_times[i] == 0) {\n39\ndfs(i, i, adj_list, discovery_times, low_times, time, bridges);\n40\n} // if\n41\n} // for i\n42\n43\nreturn bridges;\n44\n} // critical_connections()\nΘ(|𝑉|+|𝐸|), |𝑉| |𝐸|The time complexity of this solution is for the number of vertices and edges in the input graph.", "word_count": 415, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "aeaefdfb-6039-5014-8f27-a440d643812f", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 765, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n753\nChapter 19 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. A complete graph has 10 vertices. How many edges does it have?\nA) 10\nB) 30\nC) 45\nD) 55\nE) 90\n2. True or false? For a weighted, undirected graph, the cost of going from points A to B will always be the same regardless of how you get there.\nA) True\nB) False\n3. For which of the following types of graphs would an adjacency matrix be preferred over an adjacency list?\nA) Dense\nB) Sparse\nC) Directed\nD) Undirected\nE) Weighted\n4. Consider two social media platforms, Facebook and Twitter. On Facebook, if you are friends with another user, that user must also be friends\nwith you. On Twitter, if you follow another user, that user does not need to follow you back. Knowing this, friends on Facebook can be best\nrepresented as a _____________ graph, while followers on Twitter can be best represented as a _____________ graph.\nA) Directed, directed\nB) Directed, undirected\nC) Undirected, directed\nD) Undirected, undirected\nE) None of the above\n5. Which one of the following graphs is directed?\nA) A graph where the vertices represent people at an event, and the edges represent handshakes that have occurred between two people\nB) A graph where the vertices represent all intersections in New York City, and the edges represent all two-way streets that pass through\nthese intersections\nC) A graph where the vertices represent all students at the University of Michigan, and the edges represent whether a student shares a\nclass with another student\nD) A graph where the vertices represent all students and staff in EECS 281, and the edges represent whether someone has gotten help\nfrom someone else during office hours\nE) A graph where the vertices represent all EECS students, and the edges represent whether a student has ever collaborated with another\nstudent on an EECS assignment\nSuppose you have a graph with 100 edges and 100 vertices. Is the graph sparse or dense, and should you represent this graph as an adjacency6.\nlist or an adjacency matrix?\nA) This graph is sparse, and an adjacency list should be used\nB) This graph is sparse, and an adjacency matrix should be used\nC) This graph is dense, and an adjacency list should be used\nD) This graph is dense, and an adjacency matrix should be used\nE) None of the above\n7. Suppose you have a binary tree of height 281, where the leaf nodes have height 1, and you want to search for an element 𝑘. You know that 𝑘\nexists as a distinct element in this tree, and that it is a leaf node. Which of the following statements is FALSE?\nA) If you conduct a depth-first search, you’ll never have to store more than 281 nodes in memory\nB) Conducting a breadth-first search would likely require much more memory than a depth-first search\nC) Using a stack to implement this search is preferable to using a queue\nD) The path from the root to element 𝑘returned by a BFS is shorter than the path returned by a DFS\nE) None of the above", "word_count": 576, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "2975c91a-101d-5f58-9d5c-1a14d5711e96", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 766, "real_page_number": null, "text": "754\nChapter 19. Graphs and Elementary Graph Algorithms\nConsider questions 8 and 9 independently.\n8. You are performing a depth-first search on a binary tree with depth 281. You are using a deque to conduct this search, and you want to\nfind an element 𝑘. You do not know where this element 𝑘is, but you do know that 𝑘exists as a unique value in this tree. Once 𝑘is first\nencountered, the search terminates immediately (without pushing it into the deque). You start the search by pushing the root into the deque.\nAfter running the DFS for a while, you notice that an element at a depth of 121 was pushed into your deque. Knowing this information,\nwhich of the following statements is FALSE?definitely\nA) The element 𝑘can be found at a depth of 120\nB) The element 𝑘can be found at a depth of 121\nC) The element 𝑘can be found at a depth of 122\nD) More than one of A, B, or C\nE) None of the above\n9. You are performing a breadth-first search on a binary tree with depth 281. You are using a deque to conduct this search, and you want to\nfind an element 𝑘. You do not know where this element 𝑘is, but you do know that 𝑘exists as a unique value in this tree. Once 𝑘is first\nencountered, the search terminates immediately (without pushing it into the deque). You start the search by pushing the root into the deque.\nAfter running the BFS for a while, you notice that an element at a depth of 121 was pushed into your deque. Knowing this information,\nwhich of the following statements is FALSE?definitely\nA) The element 𝑘can be found at a depth of 120\nB) The element 𝑘can be found at a depth of 121\nC) The element 𝑘can be found at a depth of 122\nD) More than one of A, B, or C\nE) None of the above\n10. Consider the following adjacency list:\nWhich of the following statements is FALSE?\nA) This adjacency list represents a directed graph\nB) Ann Arbor has a direct connection with New York\nC) New York has a direct connection with Seattle\nD) Chicago has a direct connection with New York\nE) More than one of the above\n11. The partially-filled table below represents the distances between six locations. The graph associated with this table is simple and undirected.\nA\nB\nC\nD\nE\nF\nA\n258\n447\n−𝐱−\nB\n611\nC\n114\n745\nD\n−𝐲−\n331\nE\n583\nF\n252\nWhat is the value of 𝑥+𝑦?\nA) 583\nB) 778\nC) 863\nD) 997\nE) Not enough information is given to answer this question", "word_count": 451, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7f4e0c25-7a73-5391-8238-d99fbd35b71a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 767, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n755\nFor questions 12-13, consider the following scenario.\nYou currently have a graph that represents several airports (vertices) and the flights that connect each of them (edges), weighted by the distance\nof each flight. You decide to implement this graph using an adjacency list. The number of vertices is represented by 𝑉, and the number of edges\nis represented by 𝐸.\n12. Given an airport 𝑋, what is the time complexity of finding the closest airport to airport 𝑋?average-case\nA) Θ(1)\nB) Θ(𝐸)\nC) Θ(𝑉)\nD) Θ(1+𝐸∕𝑉)\nΘ(𝑉2)E)\n13. Given an airport 𝑋, what is the time complexity of finding if flights depart from airport 𝑋?worst-case any\nA) Θ(1)\nB) Θ(𝐸)\nC) Θ(𝑉)\nD) Θ(1+𝐸∕𝑉)\nΘ(𝑉2)E)\n14. You are given a graph where vertices represent current students at the University of Michigan and edges represent whether two students\nhave shared a class together. If you want to model this graph in a computer algorithm, which type of graph representation should you prefer,\nand why?\nA) Adjacency list, because this graph is sparse\nB) Adjacency list, because this graph is dense\nC) Adjacency matrix, because this graph is sparse\nD) Adjacency matrix, because this graph is dense\nE) Both the adjacency list and adjacency matrix are equally preferable\n15. You are given a graph that represents a localized network of servers, where each server has a direct connection with every other server in\nthe network. If you want to model this graph in a computer algorithm, which type of graph representation should you prefer, and why?\nA) Adjacency list, because this graph is sparse\nB) Adjacency list, because this graph is dense\nC) Adjacency matrix, because this graph is sparse\nD) Adjacency matrix, because this graph is dense\nE) Both the adjacency list and adjacency matrix are equally preferable\n16. You are conducting a breadth-first search on the graph below. The graph is unweighted. If you start your search at vertex 𝑆, which one of\nthe following vertices would you visit last?\nA) Vertex 𝐴\nB) Vertex 𝐵\nC) Vertex 𝐶\nD) Vertex 𝐷\nE) Vertex 𝐸\n17. For which of the following graphs would a breadth-first search always find the lowest weighted path between two vertices in the graph?\nI. A tree with weighted edges\nII. A weighted graph where all edges have the same weight\nIII. A weighted graph where all paths from the source to destination vertex have edge weights that are in strictly increasing order as\nthe distance from the source vertex increases\nA) I only\nB) II only\nC) I and II only\nD) II and III only\nE) I, II, and III", "word_count": 448, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "df37e18e-d701-5f84-ac23-814e3d7a0765", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 768, "real_page_number": null, "text": "756\nChapter 19. Graphs and Elementary Graph Algorithms\n18. Which of the following statements is FALSE regarding a breadth-first search?\nA) A breadth-first search will always find the shortest weighted path between two vertices in a weighted graph, but only if that path\nconsists of the fewest possible number of edges\nB) A breadth-first search will always find the shortest path between two vertices in an unweighted graph, if a path exists\nΘ(|𝑉|2)Θ(|𝑉|+|𝐸|) |𝑉|C) The time complexity of a breadth-first search is on an adjacency list and on an adjacency matrix, where is\n|𝐸|the number of vertices and is the number of edges\nD) The time complexity of a breadth-first search is the same on both a directed graph and an undirected graph, provided that they share\nthe same underlying graph representation\nE) None of the above\n19. Consider the following graph:\nA\nB\nC\nD\nE\nF\nG\nWhich of the following set of terms best describes this graph?\nA) Weighted, undirected, acyclic\nB) Unweighted, undirected, acyclic\nC) Unweighted, directed, acyclic\nD) Weighted, directed, cyclic\nE) Unweighted, directed, cyclic\n20. Consider the following adjacency matrix:\nA\nB\nC\nD\nE\nF\nA\n0\n0\n1\n0\n0\n1\nB\n0\n0\n0\n0\n1\n0\nC\n0\n0\n0\n1\n0\n0\nD\n0\n1\n1\n0\n1\n0\nE\n0\n0\n0\n1\n0\n1\nF\n1\n0\n0\n1\n1\n0\nWhich of the following search (i.e., discovery) orders are possible with a depth-first search?\nI. A, C, B, E, D, F\nII. A, F, D, E, B, C\nIII. A, F, E, D, B, C\nA) I only\nB) II only\nC) III only\nD) II and III only\nE) I, II, and III\n21. Which of the following correctly matches the graph searching algorithm with its corresponding data structure?\nA) BFS: Queue\nDFS: Stack\nB) BFS: Stack\nDFS: Queue\nC) BFS: Queue\nDFS: Queue\nD) BFS: Stack\nDFS: Stack\nE) None of the above\n22. True or false? Given the same graph, a recursive depth-first search will always visit vertices in the same order as an iterative depth-first\nsearch.\nA) True\nB) False\n23. Which of the following techniques can be used to detect whether a cycle exists in an undirected graph?\nI. Breadth-first search\nII. Depth-first search\nIII. Union-find\nA) I only\nB) II only\nC) I and II only\nD) I and III only\nE) I, II, and III", "word_count": 409, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8c63d286-b575-5993-93c4-583c449d8adb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 769, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n757\n24. An in a graph is a vertex whose removal would increase the number of connected components in the graph. For instance,articulation point\nvertices 𝐷and 𝐸are articulation points in the following graph, since removing either of these vertices would disconnect the graph.\nA\nB\nC\nD\nE\nF\nG\nH\nA\nB\nC\nE\nF\nG\nH\nD Removed\nA\nB\nC\nD\nF\nG\nH\nE Removed\nConsider a brute force approach for finding all articulation points of a graph that individually removes each node and then checks if the\nnumber of connected components has increased or not using a depth-first search. If the graph is represented as an adjacency list, and the\n|𝑉| |𝐸|,number of nodes and edges are and respectively, what is the time complexity of this brute force approach?\nΘ(|𝑉|)A)\nΘ(|𝑉|+|𝐸|)B)\nΘ(|𝑉|2)C)\nΘ(|𝑉|2+|𝑉||𝐸|)D)\nΘ(|𝑉|3)E)\n25. You are given a matrix of 0’s and 1’s. Implement a function that finds the distance of the nearest 0 for each cell of the matrix. The distance\nbetween two adjacent cells is 1.\nExample: Given the following matrix:\nmatrix = [ [0, 0, 0],\n[0, 1, 0],\n[1, 1, 1] ]\nyou should return the following:\nresult = [ [0, 0, 0],\n[0, 1, 0],\n[1, 2, 1] ]\nYou may assume that there is at least one 0 in the given matrix. Cells are only adjacent in four directions: up, down, left, and right. You\nsolution should run in time and auxiliary space, where 𝑀and 𝑁are the dimensions of the matrix.𝑂(𝑀𝑁) 𝑂(𝑀𝑁)\nstd::vector<std::vector<int32_t>> distance_to_zero(const std::vector<std::vector<int32_t>>& matrix);\n26. You are given an integer 𝑛representing the number of vertices in a directed graph where the vertices are numbered from 0 to 𝑛−1. Each\nred_edgesedge in this graph is colored either red or blue, and there could be self-edges and parallel edges. Given two vectors and\nblue_edges, red_edges[i] = [a, b] a bwhere indicatesthatthereisadirectedrededgefromfromvertex tovertex (andsimilar\nblue_edges), result result[x] xfor return a vector of length 𝑛such that is the length of the shortest path from vertex 0 to vertex\n-1such that the edge colors alternate between red and blue along the path, or if such a path does not exist.\nExample: Given the following inputs:\nred_edges\n= [[0, 1], [1, 2]]\nblue_edges = [[3, 1], [2, 3]]\n[0, 1, -1, 2].you would return the output This is the because the shortest path with alternating colors has length 0 from vertex 0 to\n0 (trivially), length 1 from vertex 0 to 1 (0 →1), impossible from vertex 0 to 2, and length 2 from vertex 0 to 3 (0 →1 →3).\nstd::vector<int32_t> shortest_color_alternating_path(int32_t n,\nconst std::vector<std::vector<int32_t>>& red_edges,\nconst std::vector<std::vector<int32_t>>& blue_edges);", "word_count": 471, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dab254b7-f407-5af8-a9b6-a3a2abfd7433", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 770, "real_page_number": null, "text": "758\nChapter 19. Graphs and Elementary Graph Algorithms\n'0', '1', '2', '3', '4', '5', '6', '7', '8',27. You have a lock in front of you with four circular wheels. Each wheel has ten slots:\n'9'. '9' '0'The wheels can rotate freely and wrap around (e.g., can become in a single turn, and vice versa). Each move consists of a\n\"0000\",turning one wheel one slot. The lock initially starts at a string representing the state of the four wheels.\ndeadends,You are given a vector of and if the lock displays any of these dead ends, the wheels of the lock will stop turning and you will\ntargetbe unable to open it. Given a representing the value that will unlock the lock, return the minimum number of turns needed to open\n-1the lock, or if it is impossible.\ndeadends = [\"0201\", \"0101\", \"0102\", \"1212\", \"2002\"] target = \"0202\",Example: Given and the function would\n6, \"0202\" \"0000\" →\"1000\" →\"1100\" →\"1200\"return since six moves are needed to get to without hitting any of the dead ends:\n→\"1201\" →\"1202\" →\"0202\".\norig_seq:Youaregiventhefollowingfunction,whichreturnsallsequencesthatcanbeattainedinasingleturnfromtheoriginalsequence\n1\nsingle_turn_seqs(conststd::vector<std::string> std::string& orig_seq) {\n2\nstd::vector<std::string> result;\n3\nfor (int32_t i = 0; i < 4; ++i) {\n4\nstd::string temp = orig_seq;\n5\ntemp[i] = (orig_seq[i] - '0' + 1) % 10 + '0';\n6\nresult.push_back(temp);\n7\ntemp[i] = (orig_seq[i] - '0' - 1 + 10) % 10 + '0';\n8\nresult.push_back(temp);\n9\n} // for i\n10\nreturn result;\n11\n} // single_turn_seqs()\nsingle_turn_seqs()You may use the function in your solution.\nint32_t min_turns_to_open_lock(const conststd::vector<std::string>& deadends, std::string& target);\nmatrix 'a' 'z'.28. You are given a 2D matrix of size 𝑚×𝑛in the form of a vector of vectors, where each cell can store a character from to\nmatrix.Implement a function that returns whether there exists any cycle consisting of the same value in A cycle is a path of length 4\nor more in the matrix of the same value that starts and ends at the same cell. From any given cell, you can move to any one of the cells\nadjacent to it in the four directions of up, down, left, and right. However, you cannot move to the cell that you visited in your last move\ntrue(e.g., do not consider as a cycle). For example, you would return if given this matrix, due to the cycle of b’s:(1,1)→(1,2)→(1,1)\na\nb\nb\nb\nb\na\nb\nb\na\nb\na\nb\na\nb\nb\na\nb\nb\nb\na\na\nb\na\na\nb\nbool contains_cycle(const std::vector<std::vector<char>>& matrix);\n29. You are given an integer 𝑛representing the number of vertices in an undirected graph, where each vertex is numbered from 0 to 𝑛−1. You\nedges edges[i] = [a, b] a b.arealsogivenavector where indicatestheexistenceofanundirectededgefromvertex to Implement\na function that returns the number of connected components in the graph. A connected component is a subgraph within a graph incomplete\nwhich there exists a path between any two vertices, and no vertex in the subgraph shares an edge with a vertex outside the subgraph. A\ncomplete connected component is one where there exists an edge between every pair of vertices.\nedges = [[0, 1], [0, 2], [1, 2], [3, 4], [3, 5]], 1.Example: Given and you would return This is because𝑛= 6\nthere is only one connected component, consisting of elements 0, 1, and 2. The component containing 3, 4, and 5 is not complete,complete\nsince there is no connection between 4 and 5, and thus is not counted in our solution.\n0\n1\n2\n3\n4\n5\nint32_t num_complete_components(int32_t const std::vector<std::vector<int32_t>>&n, edges);", "word_count": 644, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4992e573-cf74-5843-aaa3-b3de55c0ccd9", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 771, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n759\nChapter 19 Exercise Solutions\n1. The correct answer is (C). The number of edges can be calculated using the following equation:\n𝑉(𝑉−1)𝐸=\n2\n10×9=\n2\n=45\n2. The correct answer is (B). This statement is not always true: parallel edges may exist that have different weights.The distance required to\ngo to a certain vertex may be different depending on the path you take.\n3. The correct answer is (A). Adjacency matrices are good for dense graphs, as they provide constant time edge lookup without having to\nworry about additional overhead that could be incurred while exploring edges that do not exist (which is what an adjacency list is designed\nto handle, for sparse graphs).\n4. The correct answer is (C). Friends on Facebook are bidirectional: being friends with someone on Facebook means that the other person\nmust also be friends with you. Thus, a graph representing Facebookd friends is undirected. On the other hand, Twitter followers have a\n\"direction\": you can follow someone without forcing them to follow you back. Because instances exist where one person may be connected\nto another person but not the other way around, this graph is directed.\n5. The correct answer is (D). If person A got help from person B during office hours, it does not mean that person B also got help from\nperson A. Thus, a graph representing help during office hours would be directed. All the other examples are undirected, where a connection\nfrom A to B also implies a connection from B to A.\n6. The correct answer is (A). The number of vertices is on the order of the number of edges (𝐸≈𝑉), so the graph is sparse, and an adjacency\nlist should be used.\n7. The correct answer is (D). For a binary tree, there is only one way to get to each node. Because of this, the path returned by a breadth-first\nsearch would be the same as the path returned by a depth-first search. The other statements are all true. For option (A), a DFS explores\nvertices in the graph one branch at a time, so the most nodes that would need to be stored in memory at any point in time is bounded by the\nlength of the longest possible path between two vertices (in this case, 281). For option (B), a BFS visits the tree one level at a time, so all\nthe nodes at a single level may need to be stored in the underlying queue at the same time (which is a lot greater than 281 for the final level,\nsince 𝑘is a leaf node). For option (C), a stack is preferable because a DFS is preferable to a BFS, for the reasons discussed above.\n8. The correct answer is (E). Since a depth-first search looks down each path individually (and does not look at every element in a level\nbefore moving down), you cannot conclude anything about where the element 𝑘may be.\n9. The correct answer is (A). Since a breadth-first search explores every element at a level before moving down to the next level, and the\nsearch terminates immediately when the target element is discovered (without pushing it into the deque), if you reached a depth of 121, you\nknow that 𝑘must not have been found at a depth of 120: otherwise, the search would have terminated before reaching depth 121.\n10. The correct answer is (C). The adjacency list tells us that (1) Chicago has a direct connection with Los Angeles, Ann Arbor, and New\nYork, and (2) Ann Arbor has a direct connection with New York and Seattle. Because Chicago has a direct connection to Ann Arbor, but\nAnn Arbor does not have a connection back to Chicago, we know that this graph is directed and that choice (A) is true. The only false\nstatement is choice (C): since New York does not have an adjacency list entry, there are no connections from New York to anywhere in this\ngraph, even though there are direct connections from Chicago and Ann Arbor to New York.\n11. The correct answer is (C). Because the graph is simple and undirected, you know that the weight from A to B is equal to the weight from\nB to A. 𝑥is equal to the weight from A to F, which is the same as the weight from F to A (252). 𝑦is equal to the weight from D to B, which\nis equal to the weight from 𝐵to 𝐷(611). Thus, 863.𝑥+𝑦=252+611=\n12. The correct answer is (D). The average length of a linked list in an adjacency list is 𝐸∕𝑉, where 𝑉is the number of airports and 𝐸is the\ntotal number of connections that exist among these airports. In the average case, the total time complexity is the time required to access the\nvertex list (a operation) and all the elements in the list (a operation), which comes out to Θ(1+𝐸∕𝑉).Θ(𝐸∕𝑉)Θ(1)\n13. The correct answer is (A). To see if any flight departs from an airport, simply check if there is an element in the list corresponding to that\nairport. This only takes time.Θ(1)\n14. The correct answer is (A). The number of students that any one student has shared a class with at the university is a relatively small\nconstant in comparison with the total number of students at the university (e.g., it’s infeasible for a single student to have shared a class with\nevery student at the university), so the graph is sparse, and an adjacency list should be used.\n15. The correct answer is (D). Since every server has a direct connection with every other server in the network, the graph is complete and\ntherefore dense, and an adjacency matrix should be used.\n16. The correct answer is (D). A breadth-first search encounters vertices in order of distance from the source node. In this case, the path\nlength from 𝑆to 𝐷is longer than the path length from 𝑆to any other vertex, so vertex 𝐷would be discovered last during the search.\n17. The correct answer is (C). A breadth-first search will find a path from source to destination that traverses the fewest number of edges, so\nif that path also has the lowest weight, then the breadth-first search would have also found the lowest weighted path. This is true for I,\nsince there is only one path between any two vertices in a tree, and also for II, since any path with the fewest edges must also be the lowest\nweighted path (because all edges have the same weight, more edges = more total weight). However, for III, it is still possible for a shorter\npath to have a larger weight, which would cause BFS to fail to find the lowest weighted path (e.g., a path with three edges of increasing\nweight 1, 2, and 3, vs. a path of two edges of increasing weight 4 and 5).\n18. The correct answer is (A). There can be multiple paths to the target vertex that consist of the same number of edges. BFS will always find\none of these paths, but there is no guarantee that this path will be the shortest weighted path (e.g., if there are two paths with the fewest\npossible number of edges, but with different total weights, you cannot guarantee which path BFS will discover).", "word_count": 1235, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "37b48911-a46c-52cc-a7f2-0f8db23a83e1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 772, "real_page_number": null, "text": "760\nChapter 19. Graphs and Elementary Graph Algorithms\n19. The correct answer is (E). This graph is unweighted since the edges have no weight associated with them, directed since some edges are\none-directional (e.g., F goes to B, but B does not go back to F), and cyclic, since there are cycles in the graph (e.g., 𝐴→𝐸→𝐹→𝐴).\n20. The correct answer is (D). This is what the graph looks like:\nF\nA\nC\nD\nE\nB\nThe search order in I is not possible, since there is no way for the search to discover 𝐵if only 𝐴and 𝐶were discovered previously. The\nsearch order in II is possible if the DFS first explores the path 𝐴→𝐹→𝐷→𝐸, and then the path 𝐴→𝐹→𝐷→𝐵, and then the path\n𝐴→𝐶. The search order in III is possible if the DFS first explores the path 𝐴→𝐹→𝐸→𝐷→𝐵, and then the path 𝐴→𝐶.\n21. The correct answer is (A). A breadth-first processes vertices in first-in first-out (FIFO) order, which uses a queue. A depth-first search\nprocesses vertices in last-in first-out (LIFO) order, which uses a stack (or recursion, which uses the call stack).\nThe correct answer is (B). There is no guarantee that a recursive and iterative DFS would visit all potential branches in the same order,22.\nsince that depends on the order in which neighboring vertices are visited. See section 19.3.1 and 19.3.2 for an example where the branches\nare visited in different orders.\n23. The correct answer is (E). All three strategies can be used to detect cycles in an undirected graph. With a BFS and DFS, we know a cycle\nexists if there are multiple ways to access a node from the starting vertex, which we can identify by checking if each vertex encountered\nduring the search had been discovered before (see section 19.5). We can also use a similar strategy with union-find: in such an approach,\nwe would start by having every vertex as its own disjoint set. Then, for each edge from two vertices 𝑖and 𝑗in the graph, we check if 𝑖and 𝑗\nare part of the same disjoint set. If there are not, we union them together. Otherwise, there must be a cycle, since 𝑖and 𝑗already being in\nthe same disjoint set implies that there must exist some other way to get from 𝑖to 𝑗that was discovered previously, and that the new edge\nunder consideration would add another path from 𝑖to 𝑗. Note that this union-find approach would only work if the graph were undirected,\nsince we cannot represent a directed graph using this data structure.\n24. The correct answer is (D). This brute force approach individually removes each of the 𝑛nodes and performs a DFS after each removal.\nΘ(|𝑉|+|𝐸|), |𝑉|Since an adjacency list is used, the time complexity of each DFS is which is run times. Multiplying these two terms\ntogether gives us the time complexity in option (D).\n25. Since we want to find the closest distance to the nearest 0 for each cell of the matrix, this can be treated as a shortest-path problem, and we\ncan try using a breadth-first search. One solution is to iterate over the entire matrix, and whenever we encounter a 0, we push it into a queue.\nThen, we begin a BFS using the queue, and every unvisited cell we discover in the matrix must have a shortest distance that is one larger\nthan that of the neighboring cell we encountered directly before it during our search. An implementation of this is shown below:\n1\nstd::vector<std::vector<int32_t>> distance_to_zero(\n2\nconst std::vector<std::vector<int32_t>>& matrix) {\n3\n// we will iterate over this to easily access neighboring cells (right, up, left, down)\n4\nstd::vector<std::pair<int32_t, int32_t>> dirs = {{1, 0}, {0, 1}, {-1, 0}, {0, -1}};\n5\n// queue of coordinates to use for BFS\n6\nstd::queue<std::pair<int32_t, int32_t>> bfs;\n7\n// push all existing zeros into the queue, and then start a BFS using these\n8\n// coordinates to discover the closest distance to all other cells\n9\nint32_t m = matrix.size(), n = matrix[0].size();\n10\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(n,dist(m, -1));\n11\nfor (int32_t i = 0; i < m; ++i) {\n12\nfor (int32_t j = 0; j < n; ++j) {\n13\nif (matrix[i][j] == 0) {\n14\nbfs.emplace(i, j);\n15\ndist[i][j] = 0;\n16\n} // if\n17\n} // for j\n18\n} // for i\n19\nwhile (!bfs.empty()) {\n20\nauto [curr_row, curr_col] = bfs.front();\n21\nbfs.pop();\n22\nfor (auto [row_offset, col_offset] : dirs) {\n23\nint32_t next_row = curr_row + row_offset;\n24\nint32_t next_col = curr_col + col_offset;\n25\nif (!(next_row == m || next_col == n || next_row < 0 || next_col < 0) &&\n26\ndist[next_row][next_col] == -1) {\n27\nbfs.emplace(next_row, next_col);\n28\ndist[next_row][next_col] = dist[curr_row][curr_col] + 1;\n29\n} // if\n30\n} // for offset\n31\n} // while\n32\nreturn dist;\n33\n} // distance_to_zero()", "word_count": 809, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "38ad13eb-cf2f-542d-b8b4-3f1c99af2ce9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 773, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n761\n26. This is a standard shortest-path BFS problem, with the added twist that the path discovered must have alternating colors. This can be\nhandled by performing a normal BFS, but when exploring the unvisited neighbors of a vertex, we only push them into the queue if the edge\ncolor differs from the previous edge we traversed to reach the current vertex. One solution is shown below; here, we build an adjacency list\nthat also keeps track of the color of each edge for each vertex. Then, when performing our BFS, we only push a vertex into the queue if its\nedge color is different from the preceding edge color we encountered.\n1\nenum class EdgeColor { None, Red, Blue };\n2\n3\nstd::vector<int32_t> shortest_color_alternating_path(int32_t n,\n4\nconst std::vector<std::vector<int32_t>>& red_edges,\n5\nconst std::vector<std::vector<int32_t>>& blue_edges) {\n6\nstd::vector<int32_t> solution(n, -1);\n7\nstd::vector<std::vector<std::pair<int32_t, EdgeColor>>> adj_list(n);\n8\nstd::queue<std::pair<int32_t, EdgeColor>> bfs{{{0, EdgeColor::None}}};\n9\n// can use std::unordered_set as well, but will need custom hash function\n10\nstd::set<std::pair<int32_t, EdgeColor>> visited;\n11\n12\n// populate adjacency list\n13\nfor (const std::vector<int32_t>& edge : red_edges) {\n14\nadj_list[edge[0]].emplace_back(edge[1], EdgeColor::Red);\n15\n} // for edge\n16\n17\nfor (const std::vector<int32_t>& edge : blue_edges) {\n18\nadj_list[edge[0]].emplace_back(edge[1], EdgeColor::Blue);\n19\n} // for edge\n20\n21\nint32_t len = 0;\n22\nwhile (!bfs.empty()) {\n23\nsize_t init_queue_size = bfs.size();\n24\nfor (size_t i = 0; i < init_queue_size; ++i) {\n25\nauto [curr_vertex, prev_color] = bfs.front();\n26\nbfs.pop();\n27\nif (solution[curr_vertex] == -1) {\n28\nsolution[curr_vertex] = len;\n29\n} // if\n30\nfor (auto [neighbor, next_color] : adj_list[curr_vertex]) {\n31\nif (visited.find({neighbor, next_color}) != visited.end() || prev_color == next_color) {\n32\ncontinue;\n33\n} // if\n34\nbfs.emplace(neighbor, next_color);\n35\nvisited.emplace(neighbor, next_color);\n36\n} // for neighbor\n37\n} // for i\n38\n++len;\n39\n} // while\n40\n41\nreturn solution;\n42\n} // shortest_color_alternating_path()", "word_count": 314, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d0f4b43d-9545-595a-8a6f-2f702a97abf2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 774, "real_page_number": null, "text": "762\nChapter 19. Graphs and Elementary Graph Algorithms\n27. While it may not seem like it initially, this problem can be converted into a graph problem if we view each state of the lock as a vertex and\neach possible turn as an edge (e.g., there is an edge from the state \"0000\" to \"0001\"). By doing this, the problem essentially becomes \"what\nis the shortest path from \"0000\" to the target combination?\", which can be solved using a breadth-first search. The method for determining\nsingle_turn_seqs()which combinations we can attain from any single source combination is already given to use in the function,\nso we can use that to determine the neighbors of any vertex of our graph. One implementation of this solution is shown below:\n1\nint32_t min_turns_to_open_lock(const std::vector<std::string>& deadends,\n2\nconst std::string& target) {\n3\nstd::unordered_set<std::string> deadends_set(deadends.begin(), deadends.end());\n4\nstd::unordered_set<std::string> visited;\n5\nstd::queue<std::string> bfs;\n6\nstd::string init = \"0000\";\n7\nif (deadends_set.find(init) != deadends_set.end()) {\n8\nreturn -1;\n9\n} // if\n10\nvisited.insert(init);\n11\nbfs.push(init);\n12\nint32_t result = 0;\n13\nwhile (!bfs.empty()) {\n14\n// same strategy as level order traversal, we keep track of size of the queue at start of loop\n15\n// to identify all vertices that are the same distance away from the source vertex\n16\nsize_t curr_layer_size = bfs.size();\n17\nfor (size_t i = 0; i < curr_layer_size; ++i) {\n18\nstd::string next = bfs.front();\n19\nbfs.pop();\n20\nif (next == target) {\n21\nreturn result;\n22\n} // if\n23\nstd::vector<std::string> possible_seqs = single_turn_seqs(next);\n24\nfor (const std::string& seq : possible_seqs) {\n25\nif (visited.find(seq) == visited.end() && deadends_set.find(seq) == deadends_set.end()) {\n26\nbfs.push(seq);\n27\nvisited.insert(seq);\n28\n} // if\n29\n} // for seq\n30\n} // for i\n31\n++result;\n32\n} // while\n33\nreturn -1;\n34\n} // min_turns_to_open_lock()\nThis is just a variation of the cycle detection problem, except that the graph is represented in the form of a 2D matrix. We can perform DFS28.\nor BFS on the matrix from each cell, and if we encounter a cell we have already visited, we know there must be a cycle. A DFS solution is\nshown below (note that BFS and union-find are also acceptable, see details in the cycle detection section of the chapter):\n1\nbool contains_cycle(const std::vector<std::vector<char>>& matrix) {\n2\nint32_t m = matrix.size(), n = matrix[0].size();\n3\nstd::vector<std::vector<bool>> std::vector<bool>(n, false));visited(m,\n4\nfor (int32_t i = 0; i < m; ++i) {\n5\nfor (int32_t j = 0; j < n; ++j) {\n6\nif (!visited[i][j] && is_cyclic(matrix, visited, i, j, -1, -1)) {\n7\nreturn true;\n8\n} // if\n9\n} // for j\n10\n} // for i\n11\nreturn false;\n12\n} // contains_cycle()\n13\n14\nbool is_cyclic(const std::vector<std::vector<char>>& std::vector<std::vector<bool>>&matrix, visited,\n15\nint32_t int32_t int32_t int32_tcurr_row, curr_col, prev_row, prev_col) {\n16\n// we will iterate over this to easily access neighboring cells (right, up, left, down)\n17\nstatic const std::vector<std::pair<int32_t, int32_t>> dirs = {{1, 0}, {0, 1}, {-1, 0}, {0, -1}};\n18\ntrue;visited[curr_row][curr_col] =\n19\nfor (auto [row_offset, col_offset] : dirs) {\n20\nint32_t next_row = curr_row + row_offset;\n21\nint32_t next_col = curr_col + col_offset;\n22\nbool is_next_valid = next_row != matrix.size() && next_col != matrix[0].size() &&\n23\nnext_row >= 0 && next_col >= 0;\n24\nif (is_next_valid && matrix[curr_row][curr_col] == matrix[next_row][next_col] &&\n25\n!(next_row == prev_row && next_col == prev_col)) {\n26\nif (visited[next_row][next_col] ||\n27\nis_cyclic(matrix, visited, next_row, next_col, curr_row, curr_col)) {\n28\nreturn true;\n29\n} // if\n30\n} // if\n31\n} // for\n32\nreturn false;\n33\n} // is_cyclic()", "word_count": 603, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9fcb6d94-e116-5512-a7df-56d1fc627f4d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 775, "real_page_number": null, "text": "19.8 Algorithms for Articulation Points and Bridges\n763\n29. To solve this problem, we can use a DFS (or BFS) to identify the connected components in the graph (similar to example 19.13). While\n𝑛(𝑛−1)identifying each components, we can count the number of nodes and edges and use the equation\n2\nto determine if the component is\n𝑛(𝑛−1)complete (see section 19.1 for more details, but a complete graph with 𝑛vertices has\n2\nedges). A DFS solution is provided below:\n1\nint32_t num_complete_components(int32_t const std::vector<std::vector<int32_t>>&n, edges) {\n2\n// convert to adjacency list\n3\nstd::vector<std::vector<int32_t>> adj_list(n);\n4\nfor (const auto& edge : edges) {\n5\nadj_list[edge[0]].push_back(edge[1]);\n6\nadj_list[edge[1]].push_back(edge[0]);\n7\n} // for edge\n8\nstd::vector<bool> visited(n);\n9\nint32_t count = 0;\n10\nfor (int32_t i = 0; i < n; ++i) {\n11\nint32_t num_vertices = 0, num_edges = 0;\n12\nif (!visited[i]) {\n13\ndfs(adj_list, i, visited, num_vertices, num_edges);\n14\nif (num_vertices (num_vertices - 1) == num_edges) {*\n15\n++count;\n16\n} // if\n17\n} // if\n18\n} // for\n19\nreturn count;\n20\n} // num_complete_components()\n21\n22\nvoid dfs(const std::vector<std::vector<int32_t>>& int32_tadj_list, curr_vertex,\n23\nstd::vector<bool>& int32_t& int32_t&visited, num_vertices, num_edges) {\n24\ntrue;visited[curr_vertex] =\n25\nnum_vertices += 1;\n26\nnum_edges += adj_list[curr_vertex].size();\n27\nfor (int32_t neighbor : adj_list[curr_vertex]) {\n28\nif (!visited[neighbor]) {\n29\ndfs(adj_list, neighbor, visited, num_vertices, num_edges);\n30\n} // if\n31\n} // for\n32\n} // dfs()\nHere is an iterative DFS solution (a BFS is similar, just with a queue instead of a stack):\n1\nint32_t num_complete_components(int32_t const std::vector<std::vector<int32_t>>&n, edges) {\n2\n// convert to adjacency list\n3\nstd::vector<std::vector<int32_t>> adj_list(n);\n4\nfor (const auto& edge : edges) {\n5\nadj_list[edge[0]].push_back(edge[1]);\n6\nadj_list[edge[1]].push_back(edge[0]);\n7\n} // for edge\n8\nstd::vector<bool> visited(n);\n9\nstd::stack<int32_t> dfs;\n10\nint32_t count = 0;\n11\nfor (int32_t i = 0; i < n; ++i) {\n12\nif (!visited[i]) {\n13\nint32_t num_vertices = 0, num_edges = 0;\n14\ntrue;visited[i] =\n15\ndfs.push(i);\n16\nwhile (!dfs.empty()) {\n17\nint32_t curr = dfs.top();\n18\ndfs.pop();\n19\nnum_vertices += 1;\n20\nnum_edges += adj_list[curr].size();\n21\nfor (int32_t neighbor : adj_list[curr]) {\n22\nif (!visited[neighbor]) {\n23\ntrue;visited[neighbor] =\n24\ndfs.push(neighbor);\n25\n} // if\n26\n} // for neighbor\n27\n} // while\n28\nif (num_vertices (num_vertices - 1) == num_edges) {*\n29\n++count;\n30\n} // if\n31\n} // if\n32\n} // for\n33\nreturn count;\n34\n} // num_complete_components()", "word_count": 411, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4515a2bf-5915-5b4b-b7b5-57d62dee42de", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 776, "real_page_number": null, "text": "764\nChapter 19. Graphs and Elementary Graph Algorithms\nThis page has been intentionally left blank.", "word_count": 15, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cb42ea12-34b0-5a53-bc7b-3e02e7632a8a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 777, "real_page_number": null, "text": "Chapter 20\nMinimum Spanning Trees\n20.1\nIntroduction to Minimum Spanning Trees\n𝐸′A spanning tree of a graph is a subset of edges that connects all vertices 𝑉in 𝐺with no cycles. This also implies that a spanning𝐺=(𝑉,𝐸)\ntree of 𝐺connects all vertices in 𝐺using the edges. For example, consider the following graph:fewest number of\nA\nB\nE\nC\nD\nThe following are all valid spanning trees of the above graph. Notice that a graph can have more than one valid spanning tree, as long as all its\nvertices are connected with no cycles (as shown below).\nA\nB\nE\nC\nD\nA\nB\nE\nC\nD\nA\nB\nE\nC\nD\n|𝑉| |𝑉|−1Because spanning trees are connected and acyclic, any spanning tree of a graph with vertices must have edges. If there are fewer\n|𝑉|−1 |𝑉|−1than edges, the graph would not be fully connected. If there are more than edges, the graph would have a cycle, since every\n|𝑉|−1additional edge beyond would allow a vertex to be visited along more than one unique path. Try it yourself: add an additional fifth edge\nto any of the three spanning trees above — no matter where you add a connection, you will always end up creating a cycle!\nIn this chapter, we will focus on a special type of spanning tree, known as a minimum spanning tree (MST). Given an edge-weighted,\nundirected graph (𝑉,𝐸), the minimum spanning tree of 𝐺is the spanning tree of 𝐺with weight. Since MSTs are a𝐺= the lowest total edge\nspecial category of spanning trees, the MST of any graph will always be connected and acyclic.", "word_count": 275, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "27792662-7e9a-5385-a4a7-a944e7834b03", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 778, "real_page_number": null, "text": "766\nChapter 20. Minimum Spanning Trees\nAs an example, consider the following weighted, undirected graph:\nA\nB\nE\nC\nD\n8\n3\n7\n2\n5\n6\n4\nA spanning tree that minimizes total edge weight is shown below, with a total weight of 2 + 3 + 5 + 7 = 17. Since there is no way to connect all\nthe vertices in the graph with a total edge cost less than 17, this is a valid MST of the previous graph.\nA\nB\nE\nC\nD\n3\n7\n2\n5\nNote that it is possible for a graph to have multiple valid MSTs if more than one unique spanning tree minimizes total edge weight. For instance,\nif we adjusted the weight of edge 𝐵𝐸to 7, there would be two valid MSTs, which are shown below. This is because there are two different\nedges we can include to connect vertex E to our MST, both of which share the same edge weight.\nA\nB\nE\nC\nD\n3\n7\n2\n7\n5\n6\n4\nA\nB\nE\nC\nD\n3\n7\n2\n5\nA\nB\nE\nC\nD\n3\n7\n2\n5\nExample 20.1 Prove or disprove that a shortest edge in a graph must be included in its MST.unique\nTo show that a unique shortest edge will always be included in a MST, we can use proof by contradiction. Suppose that the unique shortest edge\nis included in the MST. If this were the case, we would be able to add this unique shortest edge to the MST to create a cycle. Then, if wenot\nwere to remove some other edge from this cycle, our new spanning tree would have a lower weight than before. This results in a contradiction:\nbecause we were able to create a spanning tree with a lower weight, our initial spanning tree without the unique shortest edge must not have\nbeen a valid MST!\nMinimum spanning trees have significant practical applications in many computer science problems. For example, a MST can be used to\nefficiently connect different data centers using heavy duty fiber-optic cables (which can be quite expensive). To find out which pairs of data\ncenters you should connect so that all data centers are reachable from each other using the lowest cost possible, you can represent these data\ncenters as a graph and calculate its MST. Spanning trees also show up in the field of networking: Ethernet networking systems, for instance, rely\non spanning trees to ensure that data in a network do not get stuck in cycles, which wastes network resources and consumes bandwidth at the\nexpense of other network traffic (known as a storm). Lastly, as you will see in chapter 22, minimum spanning trees can be usedbroadcast\nto approximate solutions to complex problems such as the (TSP), which, given a graph of cities and distancestraveling salesperson problem\nbetween pairs of cities, seeks to find the shortest route that visits each city exactly once and starts and ends at the same origin city.\nBecause of how useful minimum spanning trees are in computer science, it would be remiss of us not to explore MSTs and MST algorithms\nin this course. In this chapter, we will focus on two important algorithms that can be used to find a minimum spanning tree of a weighted,\nundirected graph: and algorithm.Prim’s algorithm Kruskal’s\n20.2\nPrim’s Algorithm\nPrim’s algorithm is an algorithm that can be used to find a MST of a weighted, connected, and undirected graph. This algorithm works by\ngrowing a tree from a single vertex, greedily selecting vertices to add to the tree based on the edge weights of the graph. The steps of Prim’s\nalgorithm are as follows:\n1. A tree is initialized starting from a single vertex that is arbitrarily chosen from the graph. The vertex chosen does not matter, as Prim’s\nwill always return a valid MST regardless of which vertex is used as a starting point. However, if there are multiple MSTs that exist\nwithin a graph, the choice of starting vertex could affect the MST found by the algorithm.\n2. Then, of the edges that connect the tree to vertices not yet in the tree, the algorithm finds the edge with the lowest weight and adds it to\nthe tree. This ends up growing the tree by connecting a new vertex that was previously not in the tree.\n3. Step 2 is then repeated until all the vertices are in the tree. After the algorithm completes, the final tree is a valid minimum spanning tree\nof the entire graph.", "word_count": 767, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8198b826-0e9c-5208-877b-9f52bc9d8c5d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 779, "real_page_number": null, "text": "20.2 Prim’s Algorithm\n767\nAs an example, consider the graph below. We will run Prim’s algorithm on this graph, using vertex A as our starting vertex. To make explaining\na bit easier, we will use the term to describe a vertex that is within the partial MST at any point during the algorithm, and the term toinnie outie\ndescribe a vertex that has not yet been added to our partial MST. In the figures below, innies are colored black and outies are colored white.\nB\nE\nC\nD\nF\nA\n3\n5\n7\n4\n9\n6\n10\n8\n2\nAt every step of Prim’s algorithm, we find the outie with the and add it to the partial MST. Of the edges connectedsmallest distance to any innie\nto the partial MST (shaded using a gray border), edge 𝐴𝐵has the lowest weight, so we will add vertex B to our partial MST using edge 𝐴𝐵.\nThis converts vertex B from an outie to an innie.\nB\nE\nC\nD\nF\nA\n3\n5\n7\n4\n9\n6\n10\n8\n2\nE\nC\nD\nF\nA\nB\n3\n5\n7\n4\n9\n6\n10\n8\n2\nThere are still outies remaining, so we continue this procedure. Again, we will look for the outie with the smallest distance to any innie and add\nit to our partial MST. In this case, this is vertex E, which can be connected using a weight of 3 via edge 𝐴𝐸.\nE\nC\nD\nF\nA\nB\n3\n5\n7\n4\n9\n6\n10\n8\n2\nC\nD\nF\nA\nB\nE\n5\n7\n4\n9\n6\n10\n3\n8\n2\nNext, the outie with the smallest distance to any innie is vertex C, which can be connected using a weight of 5 via edge 𝐴𝐶.\nC\nD\nF\nA\nB\nE\n5\n7\n4\n9\n6\n10\n3\n8\n2\nD\nF\nA\nB\nE\nC\n7\n4\n9\n6\n10\n3\n5\n8\n2\nThe outie with the smallest distance to any innie is now vertex D, which can be connected using a weight of 4 via edge 𝐶𝐷.\nD\nF\nA\nB\nE\nC\n7\n4\n9\n6\n10\n3\n5\n8\n2\nF\nA\nB\nE\nC\nD\n7\n9\n6\n10\n3\n5\n4\n8\n2\nThe outie with the smallest distance to any innie is now vertex F, which can be connected using a weight of 8 via edge 𝐷𝐹.\nF\nA\nB\nE\nC\nD\n7\n9\n6\n10\n3\n5\n4\n8\n2\nA\nB\nE\nC\nD\nF\n7\n9\n6\n10\n3\n5\n4\n2\n8", "word_count": 440, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "95ef9f7a-5236-5b3c-9343-73602debf86d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 780, "real_page_number": null, "text": "768\nChapter 20. Minimum Spanning Trees\nThere are no more outies left, so every vertex has been added to our tree. Prim’s algorithm is now complete, and this is our MST.\nA\nB\nE\nC\nD\nF\n3\n5\n4\n2\n8\nHow can we turn this algorithm into code? In terms of data structures, each vertex 𝑣will need to keep track of three things, which we will\ndenote as 𝑘𝑣, 𝑑𝑣and 𝑝𝑣:\n• 𝑘𝑣: whether vertex 𝑣has been added to the MST (i.e., whether it is an innie).\n• 𝑑𝑣: the minimum edge weight that connects vertex 𝑣to the partial MST (𝑑𝑣=∞if there is no edge that directly connects vertex 𝑣to the\npartial MST yet).\n• 𝑝𝑣: the vertex that connects vertex 𝑣to the MST, also known as the parent of 𝑣(initially unknown for all 𝑣).\n𝑣.1One way to store this information is to use a vector of objects that track these three values for each vertex An outline of this is shown in the\nPrimData prim_tablecode below. Here, each object in the vector stores the values of 𝑘𝑣, 𝑑𝑣, and 𝑝𝑣for each vertex 𝑣in the graph (i.e.,\nprim_table[0], prim_table[1],vertex 0’s data is stored in vertex 1’s data is stored in etc.).\n1\nstruct PrimData {\n2\ndouble d;\n// lowest distance to MST\n3\nint32_t p;\n// index of vertex's parent in MST\n4\nbool k;\n// whether or not vertex already in MST\n5\n6\nPrimData()\n7\nstd::numeric_limits<double>::infinity() false: d{ }, p{ -1 }, k{ } {}\n8\n};\n9\n10\nstd::vector<PrimData> prim_table;\nAfter this data structure is set up, Prim’s algorithm can be implemented using the following procedure:\n1. Set the 𝑑value of the starting vertex to 0.\nfalse,2. From the set of vertices for which 𝑘𝑣is select the vertex 𝑣that has the smallest value of 𝑑𝑣.\ntrue.3. Set the value of 𝑘𝑣for this vertex to\nfalse,For each vertex 𝑤adjacent to 𝑣for which 𝑘𝑤is check whether 𝑑𝑤is greater than the edge weight that connects 𝑣and 𝑤. If it is,4.\nset 𝑑𝑤to the weight of the edge that connects 𝑣and 𝑤, and set 𝑝𝑤to vertex 𝑣.\n5. Repeat steps 2-4 until 𝑘𝑣is true for every vertex (i.e., every vertex has been added to the tree).\nprim_tableLet’s go through this process using the graph from above. Here, the data in the vector is represented using a table, where each\nrow holds the values of 𝑘𝑣, 𝑑𝑣, and 𝑝𝑣for each vertex 𝑣. Since we will select vertex A as our starting vertex, the value of 𝑑𝐴is initially set to 0.\nA\nB\nE\nC\nD\nF\n3\n5\n7\n4\n9\n6\n10\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nF\n0\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\nNow, we will begin our loop. We first look through all the vertices for which 𝑘𝑣is false and find the vertex with the smallest value of 𝑑𝑣; this\ntrue.ends up being vertex A. Thus, we will add vertex A to our tree and set 𝑘𝐴to\nB\nE\nC\nD\nF\nA\n3\n5\n7\n4\n9\n6\n10\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n1Itispreferabletouseasinglevectorofobjectsthantousethreeseparatevectorsforeachof𝑘𝑣,𝑑𝑣,and𝑝𝑣. Notonlyisthiscleanerandmoremaintainable,itis\nalsofastersinceyoudonotneedtoperformmultiplevectoraccessestoretrievethedataofasinglevertex.", "word_count": 600, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7cb3a433-d543-5bbc-889e-e0ca477d274e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 781, "real_page_number": null, "text": "20.2 Prim’s Algorithm\n769\nfalseThen, we iterate over the neighbors of vertex A whose values of 𝑘are (i.e., neighbors that are currently outies). For each neighbor 𝑤,\nwe compare 𝑑𝑤with the weight of the edge connecting 𝑤with vertex A. If the edge weight is lower, we set 𝑑𝑤to this value and update 𝑝𝑤to\nvertex A. In this graph:\n• < 𝑑𝐵(∞), so set 𝑑𝐵to 2 and 𝑝𝐵to A.𝐴𝐵(2)\n• < 𝑑𝐶(∞), so set 𝑑𝐶to 5 and 𝑝𝐶to A.𝐴𝐶(5)\n< 𝑑𝐷(∞), so set 𝑑𝐷to 6 and 𝑝𝐷to A.• 𝐴𝐷(6)\n• < 𝑑𝐸(∞), so set 𝑑𝐸to 3 and 𝑝𝐸to A.𝐴𝐸(3)\nB\nE\nC\nD\nF\nA\n3\n5\n7\n4\n9\n6\n10\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nF\n2\nA\nF\n5\nA\nF\n6\nA\nF\n3\nA\n∞-F\nWe will repeat this process until all the vertices are added to our MST. Again, we will select the vertex 𝑣with the smallest 𝑑𝑣among the vertices\nfalse. true.whose 𝑘𝑣is In this case, vertex B has the smallest value of 𝑑𝑣among the unadded vertices, so we add B to our tree and set 𝑘𝐵to\nE\nC\nD\nF\nA\nB\n3\n5\n7\n4\n9\n6\n10\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nF\n5\nA\nF\n6\nA\nF\n3\nA\n∞-F\nfalseWe then iterate over the neighbors of vertex B whose values of 𝑘are and update their distance 𝑑in the table if the edge connecting them\nfalseto vertex B has a lower weight. The only neighbor of B whose 𝑘is is vertex E, but the weight of edge 𝐵𝐸is worse than the current\ndistance of via edge 𝐴𝐸. Thus, nothing is updated at this step.𝑑𝐸=3\nE\nC\nD\nF\nA\nB\n3\n5\n7\n4\n9\n6\n10\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nF\n5\nA\nF\n6\nA\nF\n3\nA\n∞-F\nfalse.The next vertex we add to the tree is vertex E, whose 𝑑value of 3 is smallest among the vertices whose 𝑘values are\nC\nD\nF\nA\nB\nE\n5\n7\n4\n9\n6\n10\n3\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nF\n5\nA\nF\n6\nA\nT\n3\nA\n∞-F\nfalseWe then iterate over the neighbors of vertex E whose values of 𝑘are and update their distance 𝑑in the table if the edge connecting them\nfalseto vertex E has a lower weight. The only neighbor of E whose 𝑘is is vertex D, but the weight of edge 𝐸𝐷is worse than the current\ndistance of via edge 𝐴𝐷. Thus, nothing is updated at this step.𝑑𝐷=6\nC\nD\nF\nA\nB\nE\n5\n7\n4\n9\n6\n10\n3\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nF\n5\nA\nF\n6\nA\nT\n3\nA\n∞-F", "word_count": 512, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7ef2f480-e9bf-5612-8c0b-5869ef98c617", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 782, "real_page_number": null, "text": "770\nChapter 20. Minimum Spanning Trees\nfalse.The next vertex we add to the tree is vertex C, whose 𝑑value of 5 is smallest among the vertices whose 𝑘values are\nD\nF\nA\nB\nE\nC\n7\n4\n9\n6\n10\n3\n5\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nT\n5\nA\nF\n6\nA\nT\n3\nA\n∞-F\nfalseWe then iterate over the neighbors of vertex C whose values of 𝑘are and update their distance 𝑑in the table if the edge connecting them\nto vertex C has a lower weight. In this graph:\n• < 𝑑𝐷(6), so set 𝑑𝐷to 4 and 𝑝𝐷to C.𝐶𝐷(4)\n• < 𝑑𝐹(∞), so set 𝑑𝐹to 9 and 𝑝𝐶to C.𝐶𝐹(9)\nD\nF\nA\nB\nE\nC\n7\n4\n9\n6\n10\n3\n5\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nT\n5\nA\nF\n4\nC\nT\n3\nA\nF\n9\nC\nfalse.The next vertex we add is vertex D, whose 𝑑value of 4 is the smallest among the vertices whose 𝑘values are\nF\nA\nB\nE\nC\nD\n7\n9\n6\n10\n3\n4\n5\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nT\n5\nA\nT\n4\nC\nT\n3\nA\nF\n9\nC\nfalseWe then iterate over the neighbors of vertex D whose values of 𝑘are and update their distance 𝑑in the table if the edge connecting them\nfalseto vertex D has a lower weight. The only neighbor of D whose 𝑘is is vertex F. The weight of edge 𝐷𝐹, or 8, is better than the current\nweight of 9, so we update 𝑑𝐹to 8 and set 𝑝𝐹to D.𝑑𝐹=\nF\nA\nB\nE\nC\nD\n7\n9\n6\n10\n3\n4\n5\n8\n2\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nT\n5\nA\nT\n4\nC\nT\n3\nA\nF\n8\nD\nfalse.The next vertex we add is vertex F, whose 𝑑value of 8 is the smallest among the vertices whose 𝑘values are\nA\nB\nE\nC\nD\nF\n7\n9\n6\n10\n3\n4\n5\n2\n8\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n2\nA\nT\n5\nA\nT\n4\nC\nT\n3\nA\nT\n8\nD\nAll of the vertices have been added, so Prim’s algorithm completes, and we have our final MST (comprised of the bolded edges above). The\ntotal weight of the MST is the sum of all the values in the 𝑑𝑣column (in this case, 2 + 5 + 4 + 3 + 8 = 22). To determine which edges exist in\nthe MST, simply look at the parents of each of the vertices in the table. Here, the parent of B is A, which means that the edge 𝐴𝐵in the MST\n(the same applies for the remaining entries in the table).", "word_count": 499, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "34c1fa49-af0e-5efb-9717-a24301371d29", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 783, "real_page_number": null, "text": "20.2 Prim’s Algorithm\n771\nThe time complexity of Prim’s algorithm depends on the data structure that is used to represent the graph, as well as the process by which\nthe outie with the smallest 𝑑𝑣is found. For instance, consider an implementation of Prim’s algorithm that conducts a of all thelinear search\nvertices to find the outie with the smallest 𝑑𝑣. The pseudocode for this implementation is shown below:\n1\nAlgorithm PrimLinearSearch():\n2\nk, d, pinitialize Prim table (which stores and for each vertex v)\n3\ndset of starting vertex to 0\n4\nfor each vertex v in graph:\n5\nd kfind vertex v_min with smallest value among vertices whose is false\n6\nkset of vertex v_min to true\n7\nfor each neighbor n of vertex v_min in graph:\n8\nk dif of vertex n is false and edge weight between n and v_min < of n:\n9\ndset of vertex n to weight of edge between n and v_min\n10\npset of vertex n to index of v_min\nv_min,Since a linear search is completed to find or the outie with the smallest 𝑑𝑣, the time complexity of completing line 5 of the pseudocode\nΘ(|𝑉|), |𝑉|above is where is the number of vertices in the graph. The time complexity of performing the loop on line 7 is also linear on the\n|𝑉|fornumber of vertices in the graph. Since all of this is nested in a loop on line 4 that runs times, the overall time complexity of Prim’s\nΘ(|𝑉|2).algorithm using linear search is worst-case\nAnother common implementation of Prim’s algorithm uses a to store the vertices of the graph, where the priority of a vertex 𝑣ismin-heap\ndetermined by 𝑑𝑣, or the smallest edge weight that connects vertex 𝑣to the partial MST. This removes the need to conduct a linear search to find\nthe vertex with the smallest 𝑑𝑣, as such a vertex can be obtained by popping values off the top of the heap. Every time a vertex 𝑣is popped out\nof the heap and added to the partial MST, we go through the outies that are directly connected to 𝑣— if an outie 𝑤has a value of 𝑑𝑤that is\ngreater than the edge weight connecting it to 𝑣, the value of 𝑑𝑤is updated to this edge weight, which increases the priority of 𝑤in the min-heap.\nThe code for this implementation is shown below:\n1\nstruct PrimData {\n2\ndouble d;\n3\nint32_t p;\n4\nbool k;\n5\nPrimData()\n6\nstd::numeric_limits<double>::infinity() false: d{ }, p{ -1 }, k{ } {}\n7\n};\n8\n9\nusing std::vector<std::vector<std::pair<int32_t, int32_t>>>;AdjList =\n10\nusing std::pair<int32_t, int32_t>;DistPair =\n// <d_v, v>\n11\n12\nprim_heap(const int32_tstd::vector<PrimData> AdjList& adj_list, start) {\n13\nstd::vector<PrimData> prim_table(adj_list.size());\n14\nstd::priority_queue<DistPair, std::vector<DistPair>, std::greater<DistPair>> pq;\n15\nprim_table[start].d = 0;\n16\npq.emplace(prim_table[start].d, start);\n17\nwhile (!pq.empty()) {\n18\nint32_t parent = pq.top().second;\n19\npq.pop();\n20\ntrue;prim_table[parent].k =\n21\nfor (auto& neighbor_dist_pair : adj_list[parent]) {\n22\nauto [neighbor, dist] = neighbor_dist_pair;\n23\nif (!prim_table[neighbor].k && dist < prim_table[neighbor].d) {\n24\nprim_table[neighbor].d = dist;\n25\nprim_table[neighbor].p = parent;\n26\npq.emplace(dist, neighbor);\n27\n} // if\n28\n} // for neighbor_dist_pair\n29\n} // while\n30\nreturn prim_table;\n31\n} // prim_heap()\nIn this code:\n• Line 9 provides an alias for the adjacency list object, which is represented using a 2-D vector of pairs, where each pair stores a connection\nwith its weight.\n• Line 10 provides an alias for the object we will store in the priority queue, which is a pair that stores 𝑑𝑣with each vertex 𝑣.\n• Lines 13-14 initialize the Prim table and priority queue, and lines 15-16 set the 𝑑value of the starting vertex to 0 and pushes it into the\npriority queue.\n• Lines 17-29 perform the following loop, as long as the priority queue is not empty:\ntrue .second– We take out the vertex at the top of the queue and set its 𝑘to (lines 18-20). We use on line 18 because each vertex\n.firstis stored as part of a pair, and stores its value of 𝑑(used to determine its priority).\n– We iterate over all the edges of this vertex in its adjacency list (line 21). Note: since the adjacency list stores of vertices andpairs\nweights, a structured binding was used to retrieve both components on one line (see section 11.13.1 for how they work). It’s just a\nanother way of writing:\n1\nauto neighbor = neighbor_dist_pair.first;\n2\nauto dist = neighbor_dist_pair.second;", "word_count": 755, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "70e171cf-a9c7-596f-b974-b477fcbbf2d6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 784, "real_page_number": null, "text": "772\nChapter 20. Minimum Spanning Trees\nfalse),– As long as a neighbor of the parent vertex has not been added to the partial MST (i.e., 𝑘is check and update its 𝑑\nvalue if necessary. If an update is made, the priority of the vertex should be updated in the priority queue as well. Since there is\nstd::priority_queue<>,no way to directly update the priority of an element in a the above implementation pushes the\nvertex back into the priority queue so that its priority uses the updated 𝑑value (duplicates in the priority queue are okay, as any\nduplicates of lower priority are ignored if its vertex has already been seen; i.e., the first check on line 23). However, there exist\nheap implementations, such as the heap, that support direct priority modification in amortized constant time (see theFibonacci\nremark below on what this specific type of heap, but you will not be required to know about it for the class).\nWhat is the time complexity of the heap implementation of Prim’s algorithm? If a binary heap is used as the underlying structure of the\nΘ(log(|𝑉|))std::priority_queue<>),priority queue (which it is for a then the cost of finding the next vertex to add to the MST is (from\n|𝑉| Θ(|𝑉|log(|𝑉|)).retrieving popping the vertex at the top of the heap). This is done a total of times, for a total complexity of However, this is\nnot all: we also need to iterate over the neighbors of each vertex (line 21) and update their distance values if necessary. The cost of updating the\nΘ(log(|𝑉|)), Θ(|𝐸|)priority of a vertex is since we need to push the updated vertex into the priority queue (line 26). This work is done at most\ntimes, since it is executed within a loop that iterates over all edges in the graph. Thus, the overall cost of updating the distance of vertices in the\nΘ(|𝐸|log(|𝑉|)).algorithm is worst-case Putting both steps together, the worst-case time complexity of the heap implementation of Prim’s\nΘ((|𝑉|+|𝐸|)log(|𝑉|)), Θ(|𝐸|log(|𝑉|)). |𝐸|algorithm is or simply Note that we can make this simplification because the number of edges\n|𝑉|−1 |𝐸|will always be larger than in a connected graph — thus, the contribution of toward the time complexity will be at least as dominant\n|𝑉|.as the contribution of\nRemark: The time complexity of Prim’s algorithm actually depends on the type of heap that serves as the underlying structure of the\npriority queue. If you use a binary heap as the implementation (which the STL does), the worst-case time complexity of Prim’s algorithm\nΘ(|𝑉|log(|𝑉|)+|𝐸|log(|𝑉|)) Θ(|𝐸|log(|𝑉|)).is However, there exists another type of heap known as a heap, which allowsFibonacci=\nyou to decrease the priority of an element in amortized time. This drops the worst-case time complexity of Prim’s algorithm fromΘ(1)\nΘ(|𝑉|log(|𝑉|)+|𝐸|log(|𝑉|)) Θ(|𝑉|log(|𝑉|)+|𝐸|), Θ(log(|𝑉|))to since the cost of updating a vertex’s priority no longer takes time.\nΘ(|𝐸|log(|𝑉|))Is the heap implementation of Prim’s algorithm always more efficient than the linear search implementation? A complexity of\nΘ(|𝑉|2)might appear faster than the time required in a linear search, but this actually depends on the density of the underlying graph. If the\n|𝐸| 𝑂(|𝑉|), Θ(|𝐸|log(|𝑉|)) Θ(|𝑉|log(|𝑉|)),heap implemention is used on a sparse graph, so the worst-case time complexity becomes= =\nΘ(|𝑉|2). Θ(|𝑉|2),|𝐸|which is indeed better than However, if the heap implementation were used on a dense graph, so the worst-case time=\nΘ(|𝑉|2 Θ(|𝑉|2).Θ(|𝐸|log(|𝑉|)) log(|𝑉|)),complexity becomes which is than Thus, if you want to run Prim’s algorithm on a graph,worse=\ncheck to see if it is dense or sparse first. If the graph is dense, the linear search implementation of Prim’s algorithm is preferred; if the graph is\nsparse, the heap implementation of Prim’s algorithm is preferred.\nSummary of Prim’s Algorithm Time Complexities\nImplementation Method\nTime Complexity\nAdjacency matrix, linear search\nΘ(|𝑉|2)\nAdjacency list, binary heap\nΘ(|𝐸|log(|𝑉|))\nAdjacency list, Fibonacci heap\nΘ(|𝐸|+|𝑉|log(|𝑉|))\n20.3\nKruskal’s Algorithm\nKruskal’s algorithm is another algorithm that can be used to find the MST of a weighted, connected, and undirected graph. This algorithm\ngreedily selects edges one at a time and adds it to a growing subgraph, which produces a \"forest\" of smaller, disjoint trees that are eventually\nmerged into a single minimum spanning tree. Kruskal’s algorithm can be summarized using the following two steps:\n1. Sort the edges of the tree in order of increasing edge weight.\n2. Iterate over all the edges in sorted order, and add each edge to the partial MST only if it does produce a cycle. If the inclusion of annot\n|𝑉|−1edge creates a cycle, discard that edge. This step is repeated until edges are added to the MST.\nBecause Kruskal’s algorithm grows a partial MST by greedily adding edges in order of increasing edge weight, the initial edges of the partial\nMST may be disjoint. However, all of these disjoint components will eventually be linked together once Kruskal’s algorithm runs to completion.\nAs an example, consider the same graph that we used to demonstrate Prim’s algorithm. When running Kruskal’s algorithm, we first sort the\nedges of the graph in order of increasing edge weight:\nA\nB\nE\nC\nD\nF\n3\n5\n7\n4\n9\n6\n10\n8\n2\n𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10", "word_count": 896, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "26713a11-0fa8-5e75-bcaa-e4b8f6f8174c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 785, "real_page_number": null, "text": "20.3 Kruskal’s Algorithm\n773\nThen, we go through each of these edges and add them to the MST, as long as the edge does not result in a cycle. First, we will add edge 𝐴𝐵to\nour MST, since it has the lowest weight.\nE\nC\nD\nF\nA\nB\n3\n5\n7\n4\n9\n6\n10\n8\n2\n➔𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10\nThe next edge we consider is edge 𝐴𝐸. The addition of 𝐴𝐸does not produce a cycle, so we add it to the MST.\nC\nD\nF\nA\nB\nE\n5\n7\n4\n9\n6\n10\n3\n8\n2\n➔\n𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10\nThe next edge we consider is edge 𝐶𝐷. The addition of 𝐶𝐷does not produce a cycle, so we add it to the MST.\nF\nC\nD\nA\nB\nE\n5\n7\n9\n6\n10\n3\n4\n8\n2\n➔\n𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10\nThe next edge we consider is edge 𝐴𝐶. The addition of 𝐴𝐶does not produce a cycle, so we add it to the MST.\nF\nA\nB\nE\nC\nD\n7\n9\n6\n10\n5\n3\n4\n8\n2\n➔\n𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10\nThe next edge we consider is edge 𝐴𝐷. The addition of edge 𝐴𝐷would produce a cycle 𝐴→𝐶→𝐷→𝐴, so this edge is discarded and not\nadded to the MST.\nF\nA\nB\nE\nC\nD\n7\n9\n10\n6\n5\n3\n4\n8\n2\n➔\n𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10", "word_count": 309, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7967993b-6742-58b3-9214-5eb99735832b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 786, "real_page_number": null, "text": "774\nChapter 20. Minimum Spanning Trees\nThe next edge we consider is edge 𝐷𝐸. The addition of edge 𝐷𝐸would produce a cycle 𝐴→𝐶→𝐷→𝐸→𝐴, so this edge is also discarded\nand added to the MST.not\nF\nA\nB\nE\nC\nD\n6\n9\n10\n7\n5\n3\n4\n8\n2\n➔\n𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10\nThe next edge we consider is edge 𝐷𝐹. The addition of 𝐷𝐹does not produce a cycle, so we add it to the MST.\nA\nB\nE\nC\nD\nF\n7\n6\n9\n10\n5\n3\n4\n8\n2\n➔\n𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10\n|𝑉|−1A total of edges have been added to the MST, so Kruskal’s algorithm is complete. Any additional edge that remains would produce a\ncycle, so edges 𝐶𝐹and 𝐵𝐸are both discarded. This gives us our completed MST.\nA\nB\nE\nC\nD\nF\n5\n3\n4\n8\n2\n𝐴𝐵- 2\n𝐴𝐸- 3\n𝐶𝐷- 4\n𝐴𝐶- 5\n𝐴𝐷- 6\n𝐷𝐸- 7\n𝐷𝐹- 8\n𝐶𝐹- 9\n𝐵𝐸- 10\nTo determine if adding an edge would create a cycle, Kruskal’s algorithm relies on the union-find data structure. This is because an edge is only\nsafe to add if it connects two vertices belonging to disjoint sets. If two vertices are already part of the same disjoint set, there must existdifferent\na path from one to the other, and any new edge between the two vertices would create a cycle.\n𝐵and 𝐸part of same disjoint set, so 𝐵𝐸creates a cycle\nF\nC\nD\nA\nB\nE\n10\n5\n7\n9\n6\n3\n4\n8\n2\nElement\nRepresentative\nA\nB\nC\nD\nE\nF\nA\nA\nC\nC\nA\nF\nfind_set(B) == find_set(E)\n𝐴and 𝐶part of different disjoint sets, so 𝐴𝐶safe to add\nF\nC\nD\nA\nB\nE\n5\n7\n9\n6\n10\n3\n4\n8\n2\nElement\nRepresentative\nA\nB\nC\nD\nE\nF\nA\nA\nC\nC\nA\nF\nfind_set(A) != find_set(C)", "word_count": 347, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2e3e729d-7682-52bd-9439-8d9a7d0b4cfb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 787, "real_page_number": null, "text": "20.3 Kruskal’s Algorithm\n775\nAn implementation of Kruskal’s algorithm is shown in the code below (the function itself begins on line 19). This implementation returns a\nvector containing all the edges in the discovered MST.\n1\nstruct EdgeData {\n2\ndouble weight;\n3\nstd::pair<int32_t, int32_t> endpoints;\n4\nbool operator<(const EdgeData& rhs) {\n5\nreturn weight < rhs.weight;\n6\n}\n7\n};\n8\n9\nclass UnionFind {\n10\n// ... see chapter 13 for full implementation\n11\npublic:\n12\nUnionFind(int32_t size);\n13\nint32_t find_set(int32_t x);\n14\nvoid union_set(int32_t int32_tx, y);\n15\n};\n16\n17\nusing std::vector<std::vector<std::pair<int32_t, double>>>;AdjList =\n18\n19\nkruskal(conststd::vector<EdgeData> AdjList& adj_list) {\n20\nstd::vector<EdgeData> edge_list;\n21\nfor (int32_t v = 0; v < adj_list.size(); ++v) {\n22\nfor (auto& neighbor_dist_pair : adj_list[v]) {\n23\nauto [neighbor, dist] = neighbor_dist_pair;\n24\nif (v < neighbor) {\n25\nedge_list.push_back({dist, {v, neighbor}});\n26\n} // if\n27\n} // for neighbor_dist_pair\n28\n} // for v\n29\nstd::sort(edge_list.begin(), edge_list.end());\n30\nUnionFind uf(adj_list.size());\n31\nstd::vector<EdgeData> in_mst;\n32\nfor (auto& edge : edge_list) {\n33\ndouble dist = edge.weight;\n34\nauto [v1, v2] = edge.endpoints;\n35\nint rep_v1 = uf.find_set(v1);\n36\nint rep_v2 = uf.find_set(v2);\n37\nif (rep_v1 != rep_v2) {\n38\nin_mst.push_back(edge);\n39\nif (in_mst.size() == adj_list.size() - 1) {\n40\nbreak;\n41\n} // if\n42\nuf.union_set(v1, v2);\n43\n} // if\n44\n} // for edge\n45\nreturn in_mst;\n46\n} //kruskal()\nIn this code:\nLines 21-28 use the adjacency list to create a vector of edges that can be sorted based on edge weight. The check on line 24 is needed•\nbecause the graph is undirected, and we don’t want to add the same edge twice.\n• The edges are then sorted by increasing edge weight on line 29.\n• Line 30 initializes the union-find container, and line 31 initializes the result vector.\n• Lines 32-44 use the union-find container to determine whether an edge should be added to the MST or not. An edge is only added if it\n|𝑉|−1connects two vertices in different disjoint sets (i.e., their ultimate representatives are unequal), and the algorithm exits once edges\nare added to the MST.\nWhat is the time complexity of Kruskal’s algorithm? If you analyze the steps of the algorithm, you will discover that the cost of sorting the\nΘ(|𝐸|log(|𝐸|))edges is the bottleneck of the entire algorithm. Sorting takes time, while all of the other steps have a lower time complexity: the\n|𝐸| Θ(|𝐸|)cost of looping over edges takes time, and the process of checking for a cycle can be treated as a constant time operation, assuming\nunion-by-size.2that the union-find data structure is efficiently implemented using path compression and either union-by-rank or Thus, the\nΘ(|𝐸|log(|𝐸|)).sorting step is the dominant term in the complexity class, and the overall time complexity of Kruskal’s algorithm is\nΘ(|𝐸|log(|𝑉|)) Θ(|𝐸|log(|𝐸|)).Remark: You will often see the time complexity of Kruskal’s algorithm expressed as instead of This\nΘ(|𝐸|log(|𝐸|)) Θ(|𝐸|log(|𝑉|))doesn’t mean that our previous analysis was incorrect! In fact, and are part of the same complexity class,\n≤|𝐸| ≤log(|𝐸|)|𝑉|2, log(|𝑉|2)|𝑉| log(|𝑉|) 2log(|𝑉|) Θ(log(|𝑉|)).since which implies that When working with graph< <−1 = =\n|𝐸| |𝑉|, Θ(|𝐸|log(|𝑉|))complexities, it may make sense to include both and and denoting the complexity of Kruskal’s algorithm as makes\nit easier to compare it with other algorithms, such as Prim’s algorithm. That being said, the two complexities are interchangeable, and both\ncan be used to describe the runtime of Kruskal’s algorithm.\n2More Θ(𝛼(|𝑉|))accurately,asinglecyclechecktakes time,where𝛼representstheinverseAckermannfunction. However,thisfunctiongrowssoslowlythatits\ncostcanessentiallybetreatedasaconstant(seechapter13formoredetails).", "word_count": 621, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d6d970a8-45cc-584c-aa31-f5ab12e7fc8b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 788, "real_page_number": null, "text": "776\nChapter 20. Minimum Spanning Trees\nExample 20.2 Given the following adjacency matrix, what is the total weight of its MST?\nA\nB\nC\nD\nE\nA\n-\n6\n8\n5\n-\nB\n6\n-\n-\n4\n2\nC\n8\n-\n-\n7\n-\nD\n5\n4\n7\n-\n3\nE\n-\n2\n-\n3\n-\nWith an adjacency matrix like this, you may be tempted to draw everything out. However, there is no need to do so! All of the information\nyou need to calculate the weight of the MST is provided in the table — you just need to sort the edges in order of increasing weight, and then\ngreedily select edges that do not produce a cycle. The edges in sorted order are:\n2, 3, 4, 5, 6, 7,𝐵𝐸= 𝐷𝐸= 𝐵𝐷= 𝐴𝐷= 𝐴𝐵= 𝐶𝐷= 𝐵𝐶=8\nFirst, we add edges 𝐵𝐸and 𝐷𝐸to the MST, since they have the smallest weights and do not produce a cycle. Edge 𝐵𝐷is omitted because\nvertices B and D are already part of the same disjoint set, which means the addition of 𝐵𝐷would produce a cycle. Edge 𝐴𝐷is added to the\nMST next, which adds A to the same disjoint set as vertices B, D, and E. Then, the next edge, edge 𝐴𝐵, is omitted. Lastly, edge 𝐶𝐷is added to\n|𝑉|−1connect vertex C to the rest of the vertices in the MST. A total of edges have now been added to the MST, so any remaining edges must\nnot be included. The total weight of the MST is therefore the combined weights of edges 𝐵𝐸, 𝐷𝐸, 𝐴𝐷, and 𝐶𝐷, or 17.2+3+5+7=\nThe tree, when drawn out, looks like this:\nA\nB\nC\nD\nE\n2\n5\n6\n7\n3\n8\n4\n20.4\nComparing Prim’s and Kruskal’s Algorithms\nIn this chapter, we covered two algorithms that can be used to identify a minimum spanning tree for a weighted, connected, and undirected\ngraph. grows a minimum spanning tree from an arbitrarily selected vertex by iteratively adding the outie that is closest toPrim’s algorithm\nΘ(|𝑉|2)the current tree. The worst-case time complexity of Prim’s algorithm is if a linear search is used to search for the closest vertex, and\nΘ(|𝐸|log(|𝑉|)) if a binary heap is used instead.\n➀\nB\nC\nD\nE\nF\nA\n2\n3\n2\n4\n3\n3\n4\n4\n➁\nB\nD\nE\nF\nA\nC\n3\n2\n4\n2\n3\n3\n4\n4\n➂\nB\nD\nF\nA\nC\nE\n3\n4\n2\n2\n3\n3\n4\n4\n➃\nD\nF\nA\nC\nE\nB\n3\n4\n2\n2\n3\n4\n4\n3\n➄\nF\nA\nC\nE\nB\nD\n4\n2\n2\n3\n4\n4\n3\n3\n➅\nA\nC\nE\nB\nD\nF\n4\n2\n2\n3\n4\n3\n3\n4", "word_count": 467, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d36fdff9-aeee-5873-8a31-ba2a8ecafee9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 789, "real_page_number": null, "text": "20.4 Comparing Prim’s and Kruskal’s Algorithms\n777\nOn the other hand, grows a tree by traversing the edges in order of increasing weight and iteratively adding the next cheapestKruskal’s algorithm\nΘ(|𝐸|log(|𝑉|)),edge, provided that the edge does not create a cycle. The time complexity of Kruskal’s algorithm is as long as the sorting\nalgorithm and union-find container are implemented efficiently.\n➀\nB\nD\nE\nF\nA\nC\n2\n3\n2\n4\n3\n3\n4\n4\n➁\nB\nD\nF\nA\nC\nE\n3\n4\n2\n2\n3\n3\n4\n4\n➂\nD\nF\nA\nC\nE\nB\n3\n4\n2\n2\n3\n4\n4\n3\n➃\nF\nA\nC\nE\nB\nD\n4\n3\n2\n2\n4\n4\n3\n3\n➄\nA\nC\nE\nB\nD\nF\n4\n3\n2\n2\n4\n3\n4\n3\nNotice that the two MSTs generated by Prim’s and Kruskal’s algorithm are for the graph above – this is entirely possible if a graph hasdifferent\nmultiple valid MSTs. It is also possible for an algorithm to return different MSTs for the same graph depending on how it is implemented. For\ninstance, if you start Prim’s algorithm on a different vertex, or if you modify the order in which Kruskal’s algorithm handles duplicate weights,\nyou could end up with an entirely new MST.\nWhenshouldyouusePrim’salgorithm,andwhenshouldyouuseKruskal’salgorithm? Iftheunderlyinggraphisdense,thenthelinearsearch\nΘ(|𝑉|2),|𝐸|implementation of Prim’s algorithm is the best choice. This is because the number of edges in a dense graph is which balloons the\nΘ(|𝑉|2Θ(|𝐸|log(|𝑉|)) log(|𝑉|)).worst-case time complexity of Kruskal’s algorithm (and the heap implementation of Prim’s algorithm) to =\nΘ(|𝑉|2)This is worse off than the time complexity of linear search Prim’s.\n|𝐸| 𝑂(|𝑉|).However, if the underlying graph is sparse, the number of edges is Therefore, the worst-case time complexity of Kruskal’s\nΘ(|𝑉|2)Θ(|𝑉|log(|𝑉|)),algorithm (and the heap implementation of Prim’s algorithm) becomes which is asymptotically faster than the time\ncomplexity of linear search Prim’s. Thus, Kruskal’s algorithm and the heap implementation of Prim’s algorithm work best on sparse graphs. A\ncomparison between the two algorithms is shown below:\nPrim’s Algorithm\nKruskal’s Algorithm\n• Builds a MST by iteratively adding the outie that is closest to the\ncurrent tree.\n• Can be implemented several ways:\n1. Adjacency matrix (dense graph) with linear search, worst-\nΘ(|𝑉|2)case time complexity\n2. Adjacency list (sparse graph) with binary heap, worst-case\nΘ(|𝐸|log(𝑉))time complexity\n3. Adjacency list (sparse graph) with Fibonacci heap, worst-\nΘ(|𝐸|+|𝑉|log(𝑉))case time complexity\nThe linear search implementation of Prim’s algorithm is ideal for•\ndense graphs.\nThe heap implementation of Prim’s works better on sparse graphs•\n(among the heap implementations, a Fibonacci heap works better\nthan a binary heap, since it supports priority updates in amortized\nconstant time).\n• Builds a MST by greedily adding edges that do not produce a\ncycle, in order of edge weight.\nImplemented using a sorting algorithm and a union-find (disjoint•\nΘ(|𝐸|log(|𝐸|))set) container, has worst-case time complexity =\nΘ(|𝐸|log(𝑉)). Reliesonanefficientimplementationofthesorting\nalgorithm and union-find.\n• Kruskal’s algorithm is best suited for sparse graphs.\nBoth Prim’s and Kruskal’s algorithms work even if the underlying graph contains negative edges, as the logic used by these algorithms is not\naffected by the sign of an edge (a negative edge simply lowers the total cost of the MST but does not change it, as long as the relative ordering of\nthe other edge weights are the same).\nHowever, these algorithms do work on graphs. Prim’s algorithm makes the assumption that every node is reachable from everynot directed\nother node, which is not true for directed graphs. Thus, you could run into a situation where there is no directed edge that connects an innie with\nan outie, which would cause Prim’s algorithm to get stuck and fail even if there are vertices that remain unexplored. Similarly, directed edges\ncould prevent Kruskal’s algorithm from properly detecting cycles using union-find, since two vertices may be in the same disjoint set even if\nthere is no way to travel between the two vertices due to edge direction. With directed edges, it may also be needed for cycle-inducing edges to\nbe added to minimize the weight of the solution, which is something that Kruskal’s algorithm cannot do.\nLastly, it should also be mentioned that the concept of a \"minimum spanning tree\" does not make much sense for directed graphs. For\ndirected graphs, a similar concept known as a is used instead. You will not need to know this for this class, butminimum spanning arborescence\nan is a directed graph where, for each vertex 𝑢, there is exactly one directed path from 𝑢to any other vertex 𝑣. An algorithmarborescence\nknown as can be used to find the spanning arborescence of a directed graph that minimizes edge weight, with the sameEdmonds’ algorithm\ntime complexity as that of Prim’s algorithm.", "word_count": 833, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4ade7031-a4e4-5258-9c17-e319aacafad6", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 790, "real_page_number": null, "text": "778\nChapter 20. Minimum Spanning Trees\nChapter 20 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n|𝑉|1. Which of the following statements must be TRUE regarding the minimum spanning tree (MST) of a graph with vertices?\nI. The MST is acyclic.\n|𝑉|II. The MST has edges.\nIII. The MST cannot include the edge with the largest weight in the original graph.\nA) I only\nB) II only\nC) I and II only\nD) II and III only\nE) I, II, and III\n|𝑉| |𝐸|2. Given a sparse graph with vertices and edges that is represented using an adjacency list, what is the time complexity of running the\nheap implementation of Prim’s algorithm on this graph, provided that you use a binary heap as the underlying structure of your heap?\nΘ(|𝑉|+|𝐸|)A)\nΘ(|𝐸|log(|𝑉|))B)\nΘ(|𝑉||𝐸|log(|𝑉|))C)\nΘ(|𝑉|2)D)\nΘ(|𝑉|3)E)\n3. Which of the following statements is FALSE?\n|𝐸| Θ(|𝐸|log(|𝐸|))A) The time complexity of running Kruskal’s algorithm on a graph with edges is\nB) In terms of time complexity, the sorting algorithm involved in Kruskal’s algorithm is the bottleneck of the entire algorithm\nC) The efficiency of Kruskal’s algorithm relies on the efficiency of the union-find data structure\nD) For certain graphs, Kruskal’s and Prim’s algorithms may produce different MSTs\nE) Kruskal’s algorithm does not work for graphs with negative edge weights\nFor questions 4-5, consider the following graph:\n𝐶\n𝐴\n𝐷\n𝐵\n𝐸\n2\n19\n11\n12\n14\n16\n6\n4. Using Prim’s algorithm on the graph above (starting at vertex 𝐴), which vertex is added last?\nA) Vertex 𝐴\nB) Vertex 𝐵\nC) Vertex 𝐶\nD) Vertex 𝐷\nE) Vertex 𝐸\n5. Using Prim’s algorithm on the graph above (starting at vertex 𝐴), what is the total weight of the MST?\nA) 31\nB) 33\nC) 45\nD) 80\nE) None of the above\n6. Given a graph and its MST, consider the following four scenarios:\nI. You increase the weight of an edge that is in the MST.\nII. You decrease the weight of an edge that is in the MST.\nIII. You increase the weight of an edge that is not in the MST.\nIV. You decrease the weight of an edge that is not in the MST.\nFor which of these scenarios could the MST change?\nA) I and III only\nB) I and IV only\nC) II and III only\nD) II and IV only\nE) I, II, III, and IV", "word_count": 450, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "b93c768a-6121-51a8-b10c-5f4e476dc6a3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 791, "real_page_number": null, "text": "20.4 Comparing Prim’s and Kruskal’s Algorithms\n779\n7. Consider the following graph:\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\n𝐿\n52\n42\n41\n19\n34\n24\n46\n44\n25\n20\n35\n22\n47\n18\n36\n37\n12\n32\n21\n31\n26\n48\n30\n29\n27\n28\n26\nHow many edges does this graph’s MST have?\nA) 10\nB) 11\nC) 12\nD) 13\nE) More than 13\n8. Suppose you are given the following distance matrix for five different vertices.\n𝐕𝟏\n𝐕𝟐\n𝐕𝟑\n𝐕𝟒\n𝐕𝟓\n𝐕𝟏\n-\n9\n12\n10\n7\n𝐕𝟐\n9\n-\n14\n6\n8\n𝐕𝟑\n12\n14\n-\n13\n11\n𝐕𝟒\n10\n6\n13\n-\n5\n𝐕𝟓\n7\n8\n11\n5\n-\nWhat is the total weight of the MST that connects these five vertices?\nA) 26\nB) 27\nC) 28\nD) 29\nE) None of the above\n9. For which of the following graphs is it possible for Prim’s and Kruskal’s algorithms to return different MSTs? Select all that apply.\nA)\n𝐶\n𝐴\n𝐷\n𝐵\n5\n4\n3\n6\n7\nB)\n𝐶\n𝐴\n𝐷\n𝐵\n3\n4\n4\n5\n6\nC)\n𝐶\n𝐴\n𝐷\n𝐵\n3\n4\n5\n6\n4\nD)\n𝐶\n𝐴\n𝐷\n𝐵\n5\n3\n5\n4\n4\nE) None of the above", "word_count": 213, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "be6461da-3c37-5c88-b886-4c3a9e4f25b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 792, "real_page_number": null, "text": "780\nChapter 20. Minimum Spanning Trees\n10. Suppose you are given the following distance matrix for five different vertices.\n𝐕𝟏\n𝐕𝟐\n𝐕𝟑\n𝐕𝟒\n𝐕𝟓\n𝐕𝟏\n-\n21\n22\n14\n17\n𝐕𝟐\n21\n-\n11\n20\n13\n𝐕𝟑\n22\n11\n-\n18\n15\n𝐕𝟒\n14\n20\n18\n-\n19\n𝐕𝟓\n17\n13\n15\n19\n-\nIf you run Prim’s algorithm on this graph, starting from vertex 𝑉1, what is the order in which the edges of the graph are added to the MST?\nA) −𝑉4, −𝑉5, −𝑉5,𝑉1 𝑉1 𝑉2 𝑉2−𝑉3\nB) −𝑉4, −𝑉4, −𝑉3,𝑉1 𝑉3 𝑉2 𝑉2−𝑉5\nC) −𝑉4, −𝑉3, −𝑉5,𝑉1 𝑉2 𝑉2 𝑉1−𝑉5\nD) −𝑉5, −𝑉5, −𝑉3,𝑉1 𝑉2 𝑉2 𝑉1−𝑉4\nE) None of the above\n11. What is the total weight of the MST for the graph provided in question 10?\nA) 53\nB) 54\nC) 55\nD) 56\nE) None of the above\n12. Suppose you are given the following distance matrix for six different vertices.\n𝐕𝟏\n𝐕𝟐\n𝐕𝟑\n𝐕𝟒\n𝐕𝟓\n𝐕𝟔\n𝐕𝟏\n-\n12\n22\n17\n21\n10\n𝐕𝟐\n12\n-\n18\n20\n19\n11\n𝐕𝟑\n22\n18\n-\n15\n14\n24\n𝐕𝟒\n17\n20\n15\n-\n13\n23\n𝐕𝟓\n21\n19\n14\n13\n-\n16\n𝐕𝟔\n10\n11\n24\n23\n16\n-\nIf you run Kruskal’s algorithm on this graph, in what order are the edges added to the MST?\nA) −𝑉6, −𝑉6, −𝑉2, −𝑉5,𝑉1 𝑉2 𝑉1 𝑉4 𝑉3−𝑉4\nB) −𝑉6, −𝑉6, −𝑉5, −𝑉5,𝑉1 𝑉2 𝑉4 𝑉3 𝑉1−𝑉4\nC) −𝑉6, −𝑉6, −𝑉5, −𝑉5,𝑉1 𝑉2 𝑉4 𝑉3 𝑉5−𝑉6\nD) −𝑉6, −𝑉6, −𝑉5, −𝑉4,𝑉1 𝑉2 𝑉3 𝑉3 𝑉5−𝑉6\nE) None of the above\n13. What is the total weight of the MST for the graph provided in question 12?\nA) 60\nB) 64\nC) 65\nD) 66\nE) None of the above\n14. You are given a simple, connected graph 𝐺with 30 vertices. All of the edges in 𝐺initially have a weight of 1. Suppose you randomly\nselect 20 edges in 𝐺and cut each of their weights in half. After doing so, what is the lowest possible weight a spanning tree in 𝐺can have?\nA) 9\nB) 10\nC) 19\nD) 20\nE) None of the above\n15. Suppose you are given two weighted graphs 𝐴and 𝐵with identical vertices and edges, but with different edge weights. You are given\nanother weighted graph 𝐶, which also has identical vertices and edges, but with edge weights equal to the sum of 𝐴and 𝐵. Let’s denote the\ntotal weights of the MSTs of 𝐴, 𝐵, and 𝐶as 𝑀𝑆𝑇𝐴, 𝑀𝑆𝑇𝐵, and 𝑀𝑆𝑇𝐶. Which of the following expressions could potentially be true?\nI. 𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵>𝑀𝑆𝑇𝐶\nII. 𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵=𝑀𝑆𝑇𝐶\nIII. 𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵<𝑀𝑆𝑇𝐶\nA) II only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III", "word_count": 476, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e9c2dc42-c43f-5653-adb1-49396fa83c55", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 793, "real_page_number": null, "text": "20.4 Comparing Prim’s and Kruskal’s Algorithms\n781\nChapter 20 Exercise Solutions\n1. The correct answer is (A). By definition, minimum spanning trees are acyclic, so statement I is true. Statement II is false because a MST\n|𝑉|−1 |𝑉|has edges, not edges. Statement III is false because it is possible for the largest weighted edge in the graph to connect a vertex\nthat cannot be reached from anywhere else, which necessitates it to be in the MST (also if every edge has the same weight). One example\nthat disproves statement III is shown below (the edge from 𝐷to 𝐹of weight 8 must be in the MST):\nA\nB\nE\nC\nD\nF\n3\n5\n4\n2\n8\n6\n7\nΘ(|𝐸|log(|𝑉|)2. The correct answer is (B). The time complexity of Prim’s algorithm using a min-binary heap on an adjacency list is (see\nthe section on Prim’s algorithm for a detailed explanation).\n3. The correct answer is (E). Kruskal’s algorithm greedily selects the edges with the lowest weight, so negative weights are not a problem.\nΘ(|𝐸|log(|𝐸|))Choice (A) is true because sorting the edges dominates with time. Choice (B) is true since sorting is indeed the bottleneck\nΘ(|𝐸|log(|𝐸|))of the algorithm (adding edges and testing for cycles both take less than time). Choice (C) is true because Kruskal’s\nrelies on the union-find data structure to determine whether an edge can be added to the MST without introducing a cycle. Choice (D) is\ntrue because, if multiple MSTs exist for the same graph, the two algorithms may end up discovering different MSTs depending on their\nimplementations (e.g., which vertex Prim’s algorithm is initiated on, how equal weights are tiebroken in Kruskal’s algorithm, etc.).\n4. The correct answer is (E). Vertex 𝐸is added last. The process is shown below:\n5. The correct answer is (B). The total weight of the MST is 2 + 11 + 14 + 6 = 33.\n𝐶\n𝐴\n𝐷\n𝐵\n𝐸\n19\n12\n16\n2\n11\n14\n6\n6. The correct answer is (B). In scenario I, increasing the weight of an edge in the MST makes the MST worse, so the MST may change if\nthere is another edge that can be used to replace the edge that was modified. In scenario II, decreasing the weight of an edge in the MST\nonly makes the MST better, so the MST would not change. In scenario III, increasing the weight of an edge outside the MST makes the\nalternatives worse, so the MST would not change. In scenario IV, decreasing the weight of an edge outside the MST makes the alternatives\nbetter, so the MST may change.", "word_count": 439, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "1d6ebc11-3057-5783-a17f-8c888b48e3f2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 794, "real_page_number": null, "text": "782\nChapter 20. Minimum Spanning Trees\n|𝑉| |𝑉|−17. The correct answer is (B). A graph with vertices has edges in its MST, which is 11 in this case. For those adventurous enough\nto compute the MST of this graph, this is what the MST looks like:\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\n𝐿\n52\n42\n34\n46\n44\n35\n47\n36\n37\n32\n31\n26\n48\n30\n29\n27\n28\n26\n41\n19\n24\n25\n20\n22\n18\n12\n21\n8. The correct answer is (D). The simplest strategy to solve this by hand is to use Kruskal’s algorithm. Pick the smallest edge, check for\ncycles, and add to a running total if no cycles are created. First, we select the shortest edge of 5 between and and add it to the MST.𝑉4 𝑉5\nThen, we look at the next shortest edge of 6 between and 𝑉4. No cycle is involved, so we add this edge to the MST for a running total of𝑉2\n5 + 6 = 11. Next, we look at the edge with a weight of 7, which connects and 𝑉5. Again, no cycle is involved, so we add this edge to the𝑉1\nMST for a running total of 11 + 7 = 18. Next, we look at the edge with a weight of 8, which connects and 𝑉5. Adding this edge would𝑉2\nproduce the cycle →𝑉2, so we ignore it. Next, we look at the edge with a weight of 9, which connects and 𝑉2. Adding𝑉2 →𝑉4 →𝑉5 𝑉1\nthis edge would produce the cycle →𝑉1, so we ignore it. Next, we look at the edge with weight 10, which connects𝑉1 →𝑉2 →𝑉4 →𝑉5 𝑉1\nand 𝑉4. Adding this edge would produce the cycle →𝑉1, so we ignore it. Next, we look at the edge with weight 11, which𝑉1 →𝑉4 →𝑉5\nconnects and 𝑉5. No cycle is involved, so we add this edge to the MST for a running total of 18 + 11 = 29. Since all vertices are now in𝑉3\nthe MST, we are done. The final tree is shown below:\n𝑉3\n𝑉1\n𝑉2\n𝑉5\n𝑉4\n7\n6\n5\n11\n9. The correct answer is (C). Only this graph has two potential MSTs (the other three graphs have a single unique MST).\n𝐶\n𝐴\n𝐷\n𝐵\n6\n4\n3\n4\n5\n𝐶\n𝐴\n𝐷\n𝐵\n6\n4\n3\n5\n4\n10. The correct answer is (A). We start at and add the external vertex with the shortest connection to 𝑉1, which is with a weight of 14.𝑉1 𝑉4\nThen, we add the external vertex with the shortest connection to 𝑉1,𝑉4, which is with a weight of 17 to 𝑉1. Then, we add the external𝑉5\nvertex with the shortest connection to 𝑉1,𝑉4,𝑉5, which is vertex with a weight of 13 to 𝑉5. Then, we add the external vertex with the𝑉2\nshortest connection to 𝑉1,𝑉2,𝑉4,𝑉5, which is vertex with a weight of 11 to 𝑉2.𝑉3\n11. The correct answer is (C). The total weight of the MST is 11 + 13 + 14 + 17 = 55.\n12. The correct answer is (C). The edges in sorted order are 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22. We first add 10 (𝑉1−𝑉6) and 11\n(𝑉2−𝑉6). We cannot add 12 (𝑉1−𝑉2) because it would cause a cycle →𝑉6. Next, we add 13 (𝑉4−𝑉5) and 14 (𝑉3−𝑉5). We𝑉1 →𝑉2\ncannot add 15 (𝑉3−𝑉4) because it would cause a cycle →𝑉5. Next, we add 16 (𝑉5−𝑉6), and our MST is complete.𝑉3 →𝑉4\n13. The correct answer is (B). The total weight of the MST is 10 + 11 + 13 + 14 + 16 = 64.", "word_count": 638, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b227c8ee-7e1b-55c9-bf13-223824d78845", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 795, "real_page_number": null, "text": "20.4 Comparing Prim’s and Kruskal’s Algorithms\n783\n14. Thecorrectansweris(C).Aspanningtreeoftheoriginalgraph𝐺musthave29edgessincethereare30vertices. Withoutanymodifications,\nthe total weight of the spanning tree must also be 29, since every edge has a weight of 1. If we select 20 of these edges and cut their weights\nin half from 1 to 0.5, we reduce the total weight of the MST by 20 0.5 = 10. This yields a new minimum possible weight of 29 - 10 = 19.×\n15. The correct answer is (D). The case where 𝑀𝑆𝑇𝐶can occur when 𝐴and 𝐵share the same edges in the MST, which𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵=\nmeans that 𝐶must also have the same MST with a total weight that is the sum of the MSTs of 𝐴and 𝐵. An example is shown below:\n𝐴1\n𝐴2\n𝐴3\n1\n2\n3\n𝐵1\n𝐵2\n𝐵3\n4\n5\n6\n𝐵1\n𝐵2\n𝐵3\n5\n7\n9\n𝑀𝑆𝑇𝐴=3\n𝑀𝑆𝑇𝐵=9\n𝑀𝑆𝑇𝐶=12\nThe case where 𝑀𝑆𝑇𝐶can occur when 𝐴and 𝐵share different edges in the MST. One example is shown below:𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵<\n𝐴1\n𝐴2\n𝐴3\n1\n2\n3\n𝐵1\n𝐵2\n𝐵3\n5\n4\n2\n𝐵1\n𝐵2\n𝐵3\n6\n6\n5\n𝑀𝑆𝑇𝐴=3\n𝑀𝑆𝑇𝐵=6\n𝑀𝑆𝑇𝐶=11\nThe case where 𝑀𝑆𝑇𝐶is a bit more interesting, since this implies that you can sum the edges of 𝐴and 𝐵to obtain a𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵>\ngraph whose MST is better than the sums of the MSTs of 𝐴and 𝐵. Let’s suppose that some arbitrary vertex 𝐴𝑥in graph 𝐴is initially\nconnected to some other arbitrary vertex 𝐴𝑦in the MST. There are two possibilities here: the corresponding vertex 𝐵𝑥in graph 𝐵could\nalso be connected to 𝐵𝑦in the MST, or it could be connected to some other arbitrary vertex 𝐵𝑧in the MST.\nScenario 1: 𝐵𝑥is initially connected to the MST of 𝐵using a matching edge.\n𝐴𝑥\n𝐴𝑦\n𝐴𝑧\n𝑎𝑂𝑃𝑇\n𝐵𝑥\n𝐵𝑦\n𝐵𝑧\n𝑏𝑂𝑃𝑇\n𝐶𝑥\n𝐶𝑦\n𝐶𝑧\n𝑎𝑂𝑃𝑇+𝑏𝑂𝑃𝑇\nAs shown above, when a vertex is connected to the MST using the same edge in both graphs 𝐴and 𝐵, we know that the cost to connect\n𝐶𝑥to the MST of graph 𝐶must be 𝑎𝑂𝑃𝑇+𝑏𝑂𝑃𝑇(the shaded vertices are already part of the MST, i.e., \"innies\" using the terminology in\nsection 20.2). If all MST edges match between 𝐴and 𝐵, then we get the case where 𝑀𝑆𝑇𝐶.𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵=\nScenario 2: 𝐵𝑥is initially connected to the MST of 𝐵using the some other non-matching edge.\n𝐴𝑥\n𝐴𝑦\n𝐴𝑧\n𝑎𝑂𝑃𝑇\n𝐵𝑥\n𝐵𝑦\n𝐵𝑧\n𝑏𝑂𝑃𝑇\n𝐶𝑥\n𝐶𝑦\n𝐶𝑧\n?\n?\nIn this case, 𝐵𝑥in graph 𝐵is not connected to 𝐵𝑦in the MST, but instead through some other arbitrary vertex 𝐵𝑧. What happens now to\nthe cost of adding 𝐶𝑥to the MST of 𝐶? In turns out that the cost of adding 𝐶𝑥to the MST of 𝐶in this scenario than thecannot be better\ncost when 𝐴𝑥and 𝐵𝑥are connected to the MST using the same matching edge. We know that the edge 𝐶𝑥−𝐶𝑦or the edge 𝐶𝑥−𝐶𝑧must\nbe in the MST of 𝐶, since any other edge would imply that 𝐴𝑥−𝐴𝑦or 𝐵𝑥−𝐵𝑧would not be the cheapest way to connect 𝐴𝑥and 𝐵𝑥to the\nMSTs of 𝐴and 𝐵, respectively. However, we also know that the edges 𝐴𝑥−𝐴𝑧and 𝐵𝑥−𝐵𝑦(denoted as 𝑎𝐴𝐿𝑇and 𝑏𝐴𝐿𝑇) must have a\nweight that is no better than 𝑎𝑂𝑃𝑇and 𝑏𝑂𝑃𝑇, since they are not in the MST. As a result, both edges that can be used to connect 𝐶𝑥to the\nMST of 𝐶are either equal or worse off when compared to the scenario where the optimal edges of the MSTs were aligned.\n𝐴𝑥\n𝐴𝑦\n𝐴𝑧\n𝑎𝑂𝑃𝑇\n𝑎𝐴𝐿𝑇(≥𝑎𝑂𝑃𝑇)\n𝐵𝑥\n𝐵𝑦\n𝐵𝑧\n𝑏𝑂𝑃𝑇\n𝑏𝐴𝐿𝑇(≥𝑏𝑂𝑃𝑇)\n𝐶𝑥\n𝐶𝑦\n𝐶𝑧\n𝑎𝑂𝑃𝑇+𝑏𝐴𝐿𝑇\n𝑎𝐴𝐿𝑇+𝑏𝑂𝑃𝑇\nThis brings us to the following conclusion: if all the MST edges match between 𝐴and 𝐵, then we get the case where 𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵=\n𝑀𝑆𝑇𝐶(statement II). If there are any mismatching edges, then we could get the case where 𝑀𝑆𝑇𝐶, since the MST of𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵<\nthe combined graph 𝐶might be summing up and including inferior edge weights that are not in the MSTs of either 𝐴or 𝐵(statement\nIII). However, there is no case where the combined graph may yield a better MST than the sums of the MSTs of 𝐴and 𝐵, since 𝑀𝑆𝑇𝐶\nequaling 𝑀𝑆𝑇𝐴+𝑀𝑆𝑇𝐵occurs in the best case where all the optimal edges align. Thus, only statements II and III can potentially be true.", "word_count": 717, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "546622d3-1e42-5e22-9e24-30e4c3bd36b3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 796, "real_page_number": null, "text": "784\nChapter 20. Minimum Spanning Trees\nThis page has been intentionally left blank.", "word_count": 13, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "da862a0e-3fdc-5ab4-a0e3-b4c7ff89a8fa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 797, "real_page_number": null, "text": "Chapter 21\nGreedy Algorithms and Divide-and-Conquer\n21.1\nIntroduction to Algorithm Families\nRecall that an algorithm is a well-defined set of instructions that can be followed to solve a problem. Algorithms typically take in some form of\ninput and then perform steps on this input to transform it into some form of output. For instance, consider the assortment of sorting algorithms\ncovered previously. Each of these algorithms performs a sequence of operations on an input container to produce an output container with its\ncontents in sorted order. However, the sequence of operations performed is drastically different across different sorting algorithms, even if the\nend goal is the same. Some sorting algorithms, such as bubble sort and selection sort, utilize a sequence of greedy decisions to sort a container.\nOther sorting algorithms, such as quicksort and mergesort, divide the input container into smaller segments and perform operations on these\nsmaller chunks before recombining them to obtain the sorted output.\nIt turns out that many algorithms can be categorized into different algorithm families, each of which share a similar approach or pattern\ntoward solving a problem. Some algorithms, like Prim’s and Kruskal’s algorithms, solve a problem by making locally optimal decisions that\nlead to a globally optimal solution. We consider these algorithms as part of the algorithm family. Other algorithms, such as quicksortgreedy\nand the Karatsuba multiplication algorithm, recursively split a problem into smaller subproblems until each subproblem is simple enough to be\nsolved trivially, before combining the results together to obtain a solution to the original problem. We denote these algorithms as part of the\nalgorithm family. By categorizing algorithms into different families, we are better able to identify and analyze methods thatdivide-and-conquer\ncan be used to tackle problems that share similar characteristics. For instance, if a problem involves optimizing a value given a set of constraints,\nwe can show that a greedy approach will always find an optimal solution as long as certain conditions are met. In addition, problems that are\nsuited for certain algorithm families may not be suited for others; as you will see later, problems that involve overlapping subproblems can be\nsolved using dynamic programming, but not divide-and-conquer.\nAs a preview of what’s to come, the topic of algorithm families will be split into several chapters. In this chapter, we will focus on the\nalgorithm families of force, greedy, and divide-and-conquer. In chapter 22, we will discuss the algorithm families of andbrute backtracking\nbound. Lastly, in chapter 23, we will discuss the algorithm family of programming. We will then apply all of thesebranch and dynamic\nalgorithm families in chapter 24 to solve the problem, a combinatorial optimization problem important to the field of computer science.knapsack", "word_count": 452, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "da54cdce-16d6-5e9a-b045-a9bc55124d7b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 798, "real_page_number": null, "text": "786\nChapter 21. Greedy Algorithms and Divide-and-Conquer\n21.2\nBrute Force\nA brute force algorithm is one that solves a problem in the most simple, direct, or obvious way. Brute force algorithms are not typically\ndistinguishable by structure or form, and two different brute force algorithms can be implemented completely differently. Regardless of how\na brute force algorithm is implemented, however, the goal of such an algorithm is to try out every possible solution, often relying on sheer\ncomputing power to do so. Because brute force algorithms check every possible answer to a problem, they are guaranteed to get the right answer\nfor the problem they are trying to solve.\nFor certain types of problems, brute force is the only way to go. For example, if you wanted to guess someone’s password, where each\nguess does not give you any information other than whether you are right or wrong, then you have no choice but to try everything. However, in\nmany cases, brute force does more work than necessary, and there are more efficient ways to solve a given problem, even if such solutions are\nnot immediately straightforward or obvious.\nTo highlight an example of the brute force process, suppose you are a cashier with forty coins — one quarter, three dimes, six nickels, and\nthirty pennies — and you want to use these coins to return 30¢ of change using as few coins as possible. If you rely on brute force to solve\nthis problem, you will have to check all possible subsets of coins, determine if they sum up to 30¢, and then return the subset that uses the\nfewest number of coins. A table depicting the subsets you would need to check is shown below (note that this table is abbreviated, since it only\ncontains subsets that sum to 30¢ — in reality, you would need to check possible subsets that can be created using the forty coins you have,all\neven those that do not sum up to the target amount).\n# Quarters\n# Dimes\n# Nickels\n# Pennies\n# Coins\n0\n0\n0\n30\n30\n0\n0\n1\n25\n26\n0\n0\n2\n20\n22\n0\n0\n3\n15\n18\n0\n0\n4\n10\n14\n0\n0\n5\n5\n10\n0\n0\n6\n0\n6\n0\n1\n0\n20\n21\n0\n1\n1\n15\n17\n0\n1\n2\n10\n13\n0\n1\n3\n5\n9\n0\n1\n4\n0\n5\n0\n2\n0\n10\n12\n0\n2\n1\n5\n8\n0\n2\n2\n0\n4\n0\n3\n0\n0\n3\n1\n0\n0\n5\n6\n1\n0\n1\n0\n2\nThe last row of the table — one quarter and one nickel — allows you to obtain 30¢ using the fewest number of coins, so this would be the\n240 240solution to the problem. However, we had to check possible subsets of coins before we could come up with this answer! The was\nobtained using the fundamental counting principle: since there are forty coins total and two possibilities for each of these coins (either included\n240.or excluded from a subset), the total number of possible subsets is 2×2×…×2=\n2𝑛possibleIn general, if there are 𝑛total coins, there are a total of subsets that could potentially be our solution. If we use the brute force\n2𝑛subsetsapproach to solve the coin change problem, we would then have to check the sum of all to determine if it sums to the desired amount,\nand then choose the feasible subset that consists of the fewest number of coins. It takes time to count the number of coins in a subset andΘ(𝑛)\n2𝑛possibledetermine if it sums to the desired amount; since these operations are performed for all subsets, the time complexity of the brute\nΘ(𝑛2𝑛).force solution to the coin change problem is bounded above by This is computationally expensive and is pretty much infeasible for\nΘ(𝑛2𝑛)large values of 𝑛! Luckily, there are ways to do better than by using algorithm families that can be used to improve the efficiency of\noptimization problems. One such family is the approach, which will be explored in the following section.greedy\n21.3\nGreedy Algorithms\n¸ 21.3.1\nThe Greedy Approach and Optimization Problems\nGreedy algorithms can be used to solve certain types of problems in a manner that is often asymptotically faster than brute force. Before we\nbegin our discussion on greedy algorithms, we will first introduce a category of problems known as optimization problems. Optimization\nproblems involve minimizing or maximizing an given a set of constraints. We consider solutions that satisfy our constraintsobjective function\nas solutions, and the optimal solution is the best solution among the possible solutions in the feasible solution set. It is entirely valid forfeasible\nan optimization problem to have no constraints, and it is also valid for a problem to have more than one constraint.\nFor example, the coin change problem is an optimization problem, where the objective function is the number of coins returned, and the\nconstraint is that the change must sum up to 30¢. Other examples of optimization problems are shown below:\n1. Determine the time allocation to each of your final exams to maximize total points earned, given that you only have 24 hours left to study.\n• maximization problemType of optimization problem:\n• total points earned across all examsObjective function:\n• you only have 24 hours left to studyConstraint(s):", "word_count": 906, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "da32b168-69cd-523a-b6fe-2a321a354f8c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 799, "real_page_number": null, "text": "21.3 Greedy Algorithms\n787\n2. Find a path from Pierpont Commons to the François-Xavier Bagnoud (FXB) building that minimizes total distance.\n• minimization problemType of optimization problem:\n• distance of path from Pierpont to FXBObjective function:\n• noneConstraint(s):\n3. Given a set of items (each with a weight and a value) and a knapsack of capacity 𝐶, determine which items you should take to maximize\ntotal value without exceeding the weight capacity of the knapsack.\n• maximization problemType of optimization problem:\n• total value of items takenObjective function:\n• total weight of all items taken cannot exceed 𝐶Constraint(s):\n4. You are currently on a road trip, and your car can run for 𝑀miles on a full tank of gas. There are 𝑁gas stations along your route, and\nyou are given the distances of each gas station from your starting point. Identify which gas stations you should refuel at if you want to\nmake the fewest number of stops for gas without ever running out.\n• minimization problemType of optimization problem:\n• number of stops you need to make for gasObjective function:\n• your car must run for 𝑀miles without ever running out of gasConstraint(s):\nWhen dealing with optimization problems, there are four algorithm families that are typically used:\n• Brute Force\n• Greedy\n• Branch and Bound (chapter 22)\n• Dynamic Programming (chapter 23)\nIn a greedy approach, we solve a problem by making a sequence of choices: at each step of a greedy algorithm, we make the bestlocally optimal\nchoice we can at the current moment toward our goal, as long as that choice is feasible. Once a choice is made, we decision.never reconsider that\nEventually, after making all of our decisions, we hope to end up with a globally optimal solution. However, as we will see later on, the greedy\napproach is guaranteed to find an optimal solution for all optimization problems, unlike the other three algorithm families listed above!not\nFor example, let’s return to the coin change problem. If we wanted to minimize the total number of coins needed to return any change\namount, the locally optimal decision would be to consider the coins in order of decreasing denomination and pick the highest-valued coin that is\nfeasible at every step. In our example, we would keep on adding quarters to our change until we cannot add any more, at which point we then\nadd dimes, then nickels, then pennies, until we obtain our desired amount.\nExample 21.1 You want to make change for 30¢, and your current coin denominations are 25¢, 10¢, 5¢, and 1¢. What coins would you\nuse to make change if you use the greedy approach to minimize the total number of coins you need?\nUsing the greedy approach, you will always select the highest-value coin that does not bring you over the desired change amount. In this case,\nsince we want to make change for 30¢, the highest-value coin that does not go over is the quarter, so we add a quarter to our solution. We now\nhave 5¢ left to return, so the highest-value coin that does not cause us to go over is the nickel. After adding a nickel to our solution, we now\nhave our desired amount of change, so the solution of the greedy approach would be to return 30¢ in the form of one quarter and one nickel.\nExample 21.2 You want to make change for 94¢, and your current coin denominations are 25¢, 10¢, 5¢, and 1¢. What coins would you\nuse to make change if you use the greedy approach to minimize the total number of coins you need?\nWe first select as many quarters as possible without going over, so we would add three quarters to our solution, for a total of 75¢. We now have\n19¢ left to return, so we select as many dimes as possible without going over. In this case, we add one dime to our solution, leaving us with\n9¢ of change remaining. Continuing this process, we would then add one nickel to our solution (leaving us with 4¢ remaining), followed by four\npennies. This gives us our desired change amount, so the solution of the greedy approach would be to return 94¢ in the form of three quarters,\none dime, one nickel, and four pennies.\nExample 21.3 You want to make change for 30¢, but you do have any nickels! Thus, your current coin denominations are 25¢, 10¢,not\nand 1¢. What coins would you use to make change if you use the greedy approach to minimize the total number of coins you need?\nWe first select as many quarters as possible without going over, so we would add one quarter to our solution. We now have 5¢ left to return, so\nthe only coins we can add to our solution without going over are pennies (since we have no nickels), of which we need to add five. Thus, the\nsolution of the greedy approach would be to return 30¢ in the form of one quarter and five pennies.\n¸ 21.3.2\nProving the Correctness of a Greedy Approach\nWhen used correctly, the greedy approach can be used to solve an optimization problem very quickly: if we are given 𝑛coins in the coin change\nproblem above, we can obtain a greedy solution in time if the coin denominations aren’t already sorted, and time if they are.Θ(𝑛log(𝑛)) Θ(𝑛)\nHowever, you must be careful: the greedy approach is guaranteed to return an optimal solution for all optimization problems! In examplesnot\n21.1 and 21.2, the greedy approach did return the correct solution. However, in example 21.3, the greedy approach failed; we ended up returning\nsix coins to make change for 30¢, when we only needed three (using three dimes). Just by changing up the coin denominations, we ended up\nnegating the correctness of the greedy approach for the coin change problem. This is the pitfall of the greedy approach: it can be used to solve\nmustproblems efficiently, but you prove the correctness of the greedy approach before you can apply it toward solving an optimization problem!", "word_count": 1028, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d4eed82b-d4d3-51f3-942b-bac2ea326b66", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 800, "real_page_number": null, "text": "788\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nHow can we prove that a greedy approach works? Optimization problems that are solvable using a greedy approach exhibit two key traits:\nan and the property. A problem with an optimal substructure is one whose optimal solution can beoptimal substructure greedy-choice\nconstructed using the optimal solutions of its subproblems (a is a version of the original problem, typically with a smaller input size,subproblem\nthat is solved on the way toward solving the original problem). The following illustrates the optimal substructure of the coin change problem,\nwhere the optimal solution of any change amount can be built using the optimal solutions of smaller change amounts (the example uses 41¢, but\nthis structure holds for any change amount desired). As an example, the optimal solution for 41¢ can be constructed by adding a 10¢ coin to the\noptimal solution for 31¢. In the context of greedy algorithms, we can prove optimal substructure by showing that, if we are given an optimal\nsolution to the subproblem that excludes the greedy choice, we can always combine this solution with the greedy choice to obtain an optimal\nsolution to the original problem. If a problem has an optimal substructure, we can always obtain an optimal solution if we perform a greedy\nchoice, and then combine this choice with the optimal solution of the subproblem that remains.\n41¢\n25¢\n10¢\n5¢\n1¢\n40¢\n25¢\n10¢\n5¢\n36¢\n25¢\n10¢\n1¢\n31¢\n25¢\n5¢\n1¢\n16¢\n10¢\n5¢\n1¢\nA problem satisfies the greedy-choice property if there exists at least one optimal solution that contains the first greedy choice. That is, if we\nmake a greedy choice on a problem that satisfies the greedy-choice property for a given greedy algorithm, there is guaranteed to be at least one\noptimal solution that includes the choice we just made. This ensures that a greedy choice can be safely made without ever preventing us from\nfinding an optimal solution.\nThese two characteristics give us a method for proving the efficacy of a greedy approach on an optimization problem. First, we want to\nframe the problem in a way such that only one subproblem remains after a greedy choice is made. Then, we want to show that the problem\nsatisfies the greedy-choice property for our greedy algorithm — this ensures that the greedy choice is always safe to make. Lastly, we want to\ndemonstrate that the problem has an optimal substructure, where an optimal solution for the entire problem can be obtained by combining the\ngreedy choice with the optimal solution of the subproblem that remains.\nIf we can prove that a problem has an optimal substructure and satisfies the greedy-choice property for a specific greedy algorithm, then we\nhave also successfully proven the correctness of the greedy algorithm on that problem. Why is this so? If a problem satisfies the greedy-choice\nproperty and has an optimal substructure, we can use to show that the greedy choice must always lead to an optimalmathematical induction\nsolution. This process is shown below.\nA Proof Using Mathematical Induction\nLet be the claim that a greedy algorithm always finds an optimal solution after 𝑛greedy choices (where 𝑛is the number of choices𝑃(𝑛)\nneeded to obtain a solution) for problems that satisfy the greedy-choice property and have an optimal substructure.\n𝑃(𝑛) 𝑛.Base Step: We want to show that is true for some initial value of\nIn this case, we can trivially show that is true. This is because the problem satisfies the greedy-choice property, which means the first𝑃(1)\ngreedy choice must be a part of the optimal solution. Therefore, if a solution can be obtained after a single choice, then that solution must be\nthe optimal solution.\n∀𝑘≥1, 𝑃(𝑘)→𝑃(𝑘+1). 𝑃(𝑘) 𝑃(𝑘+1)Inductive Step: We want to show that That is, being true implies that is also true.\nSuppose we are given input for which greedy choices are needed before a solution is obtained. Via the greedy-choice property, we𝑘+1\nknow that the first greedy choice is part of some optimal solution. In addition, once we make a greedy choice, we are left with another\nsubproblem. Because our original problem has an optimal substructure, we know that the optimal solution for this remaining subproblem\ncan be combined with the greedy choice we just made to obtain an optimal solution for the entire problem. We just made a choice for the\nproblem that required choices to obtain a solution, so there are 𝑘choices left to make in the remaining subproblem. Since is true𝑘+1 𝑃(𝑘)\nvia the inductive hypothesis, the greedy algorithm must return an optimal solution for the remaining subproblem with 𝑘choices. Thus, we\ncan combine this solution with the greedy choice to obtain an optimal solution for the original problem with choices, successfully𝑘+1\nproving that is true.𝑃(𝑘+1)\n𝑛≥1.Thus, we can conclude using mathematical induction that is true for all This implies that a greedy algorithm will always produce𝑃(𝑛)\nan optimal solution for problems with an optimal substructure that satisfy the greedy-choice property for that greedy algorithm.\nTo demonstrate this procedure, let’s consider the coin change problem for denominations of 25¢, 10¢, 5¢, and 1¢. To prove that greedily\nselecting the coin with the highest-denomination always leads to an optimal solution, we first want to frame the problem as one in which we\nmake a greedy choice and then are left with one subproblem to solve. This is relatively straightforward for the coin change problem. If we want\nto make change for 𝑛cents, and the coin we greedily add to our solution has a value of 𝑚cents, we are left with the subproblem of making\nchange for 𝑛−𝑚cents.\nProblem:\nMake change for 𝑛cents\n𝑚¢\n+\nSubproblem:\nMake change for 𝑛−𝑚cents", "word_count": 960, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "269a20e2-5c24-5f72-8f56-7b7f5ff0fe19", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 801, "real_page_number": null, "text": "21.3 Greedy Algorithms\n789\nFor example, suppose we wanted to make change for 41¢ using only quarters, dimes, nickels, and pennies. If we greedily select the feasible coin\nwith the highest denomination at every choice, we would add a quarter to our solution first. This leaves us with 16¢ of change to fulfill.\nProblem:\nMake change for 41¢\n25¢\n+\nSubproblem:\nMake change for 16¢\nNotice that this structure allows us to apply the greedy choice recursively to obtain a greedy solution, since the remaining subproblem has the\nsame form as the original problem. In this case, after we make a greedy choice, we are essentially left with the same coin change problem as\nbefore, but for 16¢ rather than 41¢ (this process can be recursively applied until all choices are made and a potential solution is obtained).\n25¢\n+\nSubproblem:\nMake change for 16¢\n25¢\n+\n10¢\n+\nSubproblem:\nMake change for 6¢\nNext, we want to prove that the greedy-choice property holds, and that the greedy choice is always safe to make. This is done by ensuring that\nthe first greedy choice we make does not ever invalidate the optimal solution. One common strategy for proving that an optimal solution always\ncontains the greedy choice is to use an argument, which exhibits the following pattern:exchange\n1. Assume that an arbitrary optimal solution (which we will denote as 𝑂) does include the first choice made by the greedy algorithmnot\n(which we will denote as 𝑔).\n𝑂′2. Modify the optimal solution 𝑂to create a new solution that includes 𝑔.\n𝑂′3. Show that is a valid solution that is no worse than 𝑂.\nBy using the exchange argument, we can show that any optimal solution that does contain the first choice made by a greedy algorithm cannot\nbe modified to create a solution that include this first choice, and that this new solution is guaranteed to be no worse (and perhaps evendoes\nbetter) than the optimal solution we had before. This would prove that there always exists an optimal solution that includes the first greedy\nchoice, which implies that the greedy-choice property holds.\nExample 21.4 Prove the greedy-choice property holds for the coin change problem for denominations 25¢, 10¢, 5¢, and 1¢.\nTo show this, we will use a proof by cases, where the change we want to make is either greater than 24¢, between 10¢ and 24¢, between 5¢ and\n9¢, or less than 5¢. Note that an optimal solution must follow these rules:\n• The number of pennies in an optimal solution must be less than five. Otherwise, we can replace these pennies with a single nickel to\nimprove the solution.\n• The number of nickels in an optimal solution must be less than two. Otherwise, we can replace these nickels with a single dime to\nimprove the solution.\n• The combined number of dimes and nickels in an optimal solution must be less than three. Otherwise,\n– If we have three dimes, we can improve the solution by replacing them with a quarter and a nickel.\n– If we have two dimes and one nickel, we can improve the solution by replacing them with a single quarter.\n– From our previous rule, we cannot have more than one nickel in our optimal solution.\nIf the amount of change we want to make is greater than 24¢, then the greedy choice would be to add a quarter to our solution. Assume that\nthere exists an optimal solution 𝑂that does not include a quarter. If this were the case, we would have to make change for at least 25¢ using\ndimes, nickels, and pennies. This would require 𝑂to include at least three coins worth of dimes and nickels. However, from the rules above, we\n𝑂′can modify this solution to get another solution that replaces any combination of two dimes and one nickel with a single quarter, and any\n𝑂′combination of three dimes with one quarter and one nickel. This would imply that returns fewer coins than 𝑂, contradicting our initial\nassumption that 𝑂is optimal. Thus, if we have to make change for more than 24¢, there must exist an optimal solution that contains a quarter.\nIf the amount of change we want to make is between 10¢ and 24¢, then the greedy choice would be to add a dime to our solution. Assume that\nthere exists an optimal solution 𝑂that does not include a dime. If this were the case, we would have to make change for at least 10¢ worth\nof change using nickels and pennies, which would require 𝑂to use at least two nickels. However, we can modify this solution to get another\n𝑂′ 𝑂′solution that replaces any combination of two nickels with a single dime. This would imply that returns fewer coins than 𝑂, contradicting\nour initial assumption that 𝑂is optimal. Thus, if we have to make change for 10-24¢, there must exist an optimal solution that contains a dime.\nIf the amount of change we want to make is between 5¢ and 9¢, then the greedy choice would be to add a nickel to our solution. Assume that\nthere exists an optimal solution 𝑂that does not include a nickel. If this were the case, we would have to make change for at least 5¢ worth of\n𝑂′change using only pennies, which would require 𝑂to use at least five pennies. However, we can modify this solution to get another solution\n𝑂′that replaces any combination of five pennies with a single nickel. This would imply that returns fewer coins than 𝑂, contradicting our initial\nassumption that 𝑂is optimal. Thus, if we have to make change for 5-9¢, there must exist an optimal solution that contains a nickel.\nIf the amount of change we want to make is under 5¢ , then we only have one type of coin that can be added to our solution. This trivially\nimplies the optimal solution must contain a penny. We have successfully completed our proof.", "word_count": 1004, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c3812653-e414-52bc-a98e-500f28870d9e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 802, "real_page_number": null, "text": "790\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nAfter showing that the problem satisfies the greedy-choice property, we want to demonstrate that it also exhibits an optimal substructure, where\nthe greedy choice can be combined with the optimal solution of the remaining subproblem to obtain an optimal solution for the entire problem.\nOnce we have identified an optimal substructure for the problem, we can then apply induction on the solution size to prove that the greedy\nchoice will always find an optimal solution, as shown previously.\nExample 21.5 Show that the greedy coin change approach exhibits optimal substructure for coin denominations of 25¢, 10¢, 5¢, and 1¢.\nTo show this, recall the illustration of the algorithm from before. Since our greedy algorithm makes a greedy choice and is then left with one\nsubproblem to solve, we can prove optimal substructure by showing we can always combine the greedy choice of an 𝑚-cent coin with an optimal\nsolution for 𝑛−𝑚cents to obtain an optimal solution for 𝑛cents.\nProblem:\nMake change for 𝑛cents\n𝑚¢\n+\nSubproblem:\nMake change for 𝑛−𝑚cents\nWe can do this using a proof by contradiction. Assume that the optimal solution for 𝑛−𝑚cents returns 𝑘coins (for some arbitrary natural\nnumber 𝑘), and that combining this optimal solution with an 𝑚-cent coin does not yield an optimal solution for 𝑛cents. This would mean that\nthere exists a way to make change for 𝑛cents using fewer than coins. We know via the greedy-choice property that there must exist one𝑘+1\noptimal solution that includes the greedy choice of an 𝑚-cent coin. Therefore, we can take this optimal solution and remove the 𝑚-cent coin to\nreturn change for 𝑛−𝑚cents. Since the optimal solution for 𝑛cents uses fewer than coins, removing the 𝑚-cent coin would produce a𝑘+1\nsolution for 𝑛−𝑚cents that uses fewer than 𝑘coins. This results in a contradiction, since the optimal solution for 𝑛−𝑚cents returns 𝑘coins.\nThus, an optimal solution for 𝑛cents cannot use fewer than coins if the optimal solution of the remaining subproblem uses 𝑘coins. This𝑘+1\nimplies that the greedy choice can always be combined with the optimal solution of the remaining subproblem to obtain an optimal solution for\nthe entire problem, and our proof is now complete.\nWe will not go too deep into the weeds with proofs in this class, but it is good to understand how greedy algorithms are designed, and how their\ncorrectness can be proven. The greedy approach plays an important role in several of the algorithms that we have already discussed in this\ncourse, such as Prim’s and Kruskal’s algorithms, which uses the greedy property to efficiently construct a minimum spanning tree for weighted,\nconnected, and undirected graphs.\nIn the next few subsections, we will look at some additional examples of problems that can be solved using a greedy approach. You won’t\nneed to fully understand the proofs for why the greedy approach works in each of these examples; rather, the goal is to expose you to a few types\nof problems where a greedy algorithm can be successfully applied.\n¸ 21.3.3\nActivity Selection Problem\nIn the activity selection problem, you are given a set of activities that share a common resource. Each of these activities has a specified starting\nand ending time, and only one activity can be scheduled on the shared resource at any time. Once an activity begins using the shared resource, it\nmust be allowed to run to completion without any preemption. Your goal is to maximize the total number of activities that are scheduled.\nThe activity selection problem is an important problem in computer science, since its design can be applied to many different real-life\napplications, from scheduling a series of lectures that share the same lecture hall to assigning computational tasks to a shared processor. As an\nexample, let’s consider the following set of jobs, with the following start and end times:\nJob\nStart Time\nEnd Time\nA\n0\n16\nB\n1\n5\nC\n4\n7\nD\n6\n10\nE\n10\n14\nF\n11\n13\nG\n13\n17\nH\n14\n15\nI\n15\n17\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐼\n𝐻\n𝐺\n𝐹\n𝐸\n𝐷\n𝐶\n𝐵\n𝐴", "word_count": 705, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "55171a1d-401a-597f-a76b-8ada24906983", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 803, "real_page_number": null, "text": "21.3 Greedy Algorithms\n791\nHow would we go about approaching this problem? One way would be to check all possible subsets of activities, determine if each subset is\nfeasible, and keep track of the feasible subset that maximizes the total number of activities scheduled. This brute force approach, however,\nwould take exponential time. Is there a way to do better?\nThe key to notice here is that we have an optimization problem, where we want to maximize total activities scheduled under the constraint\nthat only one activity can use the shared resource at a time. As a result, there may exist a greedy method that can be used to solve the problem.\nHowever, finding the right greedy approach (if there even is one) can be quite tricky — what metric should we use to determine what the locally\noptimal decision is at each step?\nInstinctively, it might make sense to greedily select activities with the earliest start time. That is, the greedy algorithm would pick the\navailable job that starts first, remove all jobs that conflict with the chosen job, and repeat until all jobs have been considered. However, this\nmethod does not work. If we used this approach to select jobs in the given example, we would end up scheduling job 𝐴and nothing else — this\nis clearly not an optimal solution!\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐴\nAnother possibility would be to greedily select activities with the shortest duration (as long as they do not conflict with any activities already\nchosen). However, this approach also fails to discover an optimal solution in all cases. If we used this greedy method on our example, we would\nschedule the following jobs:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐶\n𝐹\n𝐻\n𝐼\nThis is not optimal, since we can improve our solution by scheduling jobs 𝐵and 𝐷instead of job 𝐶:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐵\n𝐷\n𝐹\n𝐻\n𝐼\nFortunately, there does exist a greedy strategy that discovers the optimal solution in all cases, even if it is not the most obvious approach. In\norder to solve the activity selection problem, the correct strategy is to greedily select available activities with the first. Thisearliest finish time\nensures that every activity we add to our schedule leaves open as much time as possible for any activities that may occur later down the line. In\nour example, job 𝐵has the earliest finish time, so we would add job B to our schedule first and discard activities that overlap (jobs 𝐴and 𝐶).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐵\nThe next available job with the earliest finish time is job 𝐷, so we add job 𝐷to our schedule and discard any jobs that overlap (in this case, job\n𝐷overlaps with jobs 𝐴and 𝐶, but these two jobs have already been discarded, so nothing more needs to be done).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐵\n𝐷\nThe next available job with the earliest finish time is job 𝐹, so we add job 𝐹to our schedule and discard job 𝐸, which overlaps.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐵\n𝐷\n𝐹", "word_count": 592, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "97a23aa9-4b4c-57d0-847d-9ca152f2f63f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 804, "real_page_number": null, "text": "792\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nThe next available job with the earliest finish time is job 𝐻, so we add job 𝐻to our schedule and discard job 𝐺, which overlaps.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐵\n𝐷\n𝐹\n𝐻\nThe next available job with the earliest finish time is job 𝐼, so we add job 𝐼to our schedule. All of the jobs have either been added or discarded,\nso we are now done. This is the final result, with an optimal solution of five scheduled jobs.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐵\n𝐷\n𝐹\n𝐻\n𝐼\nIn our example, greedily selecting activities in order of earliest finish time got us the optimal solution. However, can we ensure that this greedy\napproach returns the optimal solution, for any subset of jobs? Before we employ a greedy algorithm, we must make sure that it alwaysalways\nreturns an optimal solution, especially since the effectiveness of this approach for the activity selection problem is not immediately obvious. To\nprove that greedily selecting jobs by finish time works in all cases, we will follow the proof framework that was established earlier.\nFrame the problem as one in which only one subproblem remains after a greedy choice is made.\nThis step is relatively straightforward for this problem. If our greedy algorithm selects some arbitrary job 𝑘that has the earliest finish time\namong the jobs still available, we are then left with the subproblem of maximizing the number of jobs we can schedule among the jobs that begin\nafter job 𝑘ends. We do not need to worry about any other subproblems, since any other jobs must have already been considered (either added or\ndiscarded) if they begin before job 𝑘ends, since we know job 𝑘has the earliest finish time among the jobs still available. In our example, after\nmaking the greedy choice of scheduling job 𝐵, we are left with the subproblem of solving the activity selection problem for jobs 𝐷through 𝐼.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n𝐼\n𝐻\n𝐺\n𝐹\n𝐸\n𝐷\n𝐵\nShow that the greedy-choice property is satisfied, such that at least one optimal solution to the original problem includes the first greedy choice\nmade by the greedy algorithm.\nLet 𝑎𝑘represent the activity with the earliest finish time, and thus the activity chosen by the first greedy choice. Assume that an optimal solution\n𝑂′𝑂doesnot include𝑎𝑘. Ifthis were thecase, we canconstruct anewsolution byremovingthe activityin𝑂with theearliestfinish time(which\n𝑂′we will denote as 𝑎𝑓). Since no activities in 𝑂conflict with 𝑎𝑓, all of the remaining activities in must begin after 𝑎𝑓finishes. We know that\nactivity 𝑎𝑘is the activity with the earliest finish time, so 𝑎𝑘must finish at the same time or before 𝑎𝑓finishes. This implies that all the remaining\n𝑂′ 𝑂′. 𝑂′activities in must also begin after 𝑎𝑘finishes, and that 𝑎𝑘is compatible with all the activities currently in Thus, it is safe to add 𝑎𝑘to\n𝑂′to obtain an optimal solution that schedules the same number of jobs as 𝑂. Since 𝑂is optimal, must also be optimal. We have successfully\nproven that there must exist an optimal solution that contains the activity with the earliest finish time, and that the greedy-choice property holds.\nShow that the problem exhibits an optimal substructure, where the optimal solution of the remaining subproblem can be combined with the\ngreedy choice to obtain an optimal solution for the original problem.\n𝑆′Let 𝑆represent the original activity selection problem we are trying to solve, and let represent the subproblem that remains after a greedy\n𝑆′choice is made. Assume that the optimal solution for schedules 𝑛jobs, for some arbitrary natural number 𝑛. We then claim that the problem\n𝑆′does not exhibit an optimal substructure, and that the optimal solution for cannot be combined with the greedy choice 𝑎𝑘to obtain an optimal\nsolution for 𝑆. This implies that the optimal solution for 𝑆must schedule more than jobs. We know via the greedy-choice property that𝑛+1\nthere must exist an optimal solution 𝑂that includes the activity that finishes first. If we remove this activity from 𝑂, we would end up with a\n𝑆′solution for that schedules more than 𝑛jobs, since 𝑂schedules more than jobs. This results in a contradiction! Thus, if the subproblem𝑛+1\n𝑆′ optimally schedules 𝑛jobs, the optimal solution for 𝑆must schedule exactly jobs… otherwise, we would be able to devise a solution𝑛+1\n𝑆′ 𝑆′for that schedules more than 𝑛jobs, contradicting our initial claim that optimally schedules 𝑛jobs. We have successfully proven that the\nproblem exhibits an optimal substructure.", "word_count": 808, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "51afd3fa-7d91-5ce0-985e-ba202b259c22", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 805, "real_page_number": null, "text": "21.3 Greedy Algorithms\n793\nWe have successfully shown that the greedy approach of selecting activities in order of increasing finish time satisfies the greedy-choice property,\nand that the activity selection problem exhibits optimal substructure. At this point, we can use mathematical induction to prove that this greedy\napproach will always discover the optimal solution in all cases (the inductive approach will not be explicitly shown since it is not specific to this\nproblem; rather, the template provided on page 788 can be applied to all greedy algorithms once you prove that the greedy-choice property and\noptimal substructure are satisfied).\nThe activity scheduling problem can thus be solved using the following procedure:\n• Sort the activities in order of increasing finish time.\n• Greedily select the available activity with the earliest finish time and add it to the solution. Once an activity is scheduled, remove any\nconflicting activities from consideration.\n• Repeat until all activities are either scheduled or discarded.\nThe time complexity of solving the activity selection problem is Θ(𝑛log(𝑛)), where the sorting process is the bottleneck of the entire algorithm.\nHowever, if the activities are already sorted in order of finish time, the time complexity would become Θ(𝑛), since only a linear pass would then\nbe needed to schedule all the activities. Regardless, this is exponentially better than the exponential time complexity of a brute force approach!\n¸ 21.3.4\n(✽)Breakpoint Selection Problem\nIn this problem, you are on a planned route with 𝑛gas stations, which we will denote as 𝑏0, 𝑏1, …, 𝑏𝑛−1. Given that your car can only go at\nmost 𝑚miles on a full tank, you want to identify a refueling schedule that minimizes the total number of stops you will need to make to travel\nbetween two locations without ever running out of gas (assuming you start with a full tank). An example is shown below:\nCapacity: 5 miles on a full tank\nDestination: 17 miles\nGas Station\nDistance (miles)\nA\n1\nB\n3\nC\n4\nD\n7\nE\n8\nF\n10\nG\n13\nH\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nt\n𝐴\nt\n𝐵\nt\n𝐶\nt\n𝐷\nt\n𝐸\nt\n𝐹\nt\n𝐺\nt\n𝐻\n,\nSince you want to minimize the number of stops you have to make, a reasonable greedy approach would be drive as far as you can before\nrefueling. More formally, if you previously refueled at gas station 𝑏𝑖, and your car has a capacity 𝑚, the greedy choice would be to refuel at the\n≤𝑚.gas station 𝑏𝑗with the greatest distance satisfying dist(𝑏𝑗)−dist(𝑏𝑖)\nLet’s consider the previous example. Since your car can drive up to 5 miles on a full tank, you can only reach gas stations 𝐴, 𝐵, and 𝐶before\nyou run out of gas. Using the above greedy method, you would choose to refuel at gas station 𝐶since it is the reachable gas station that is\nfarthest from your current position.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nt\n𝐴\nt\n𝐵\nt\n𝐶\nt\n𝐷\nt\n𝐸\nt\n𝐹\nt\n𝐺\nt\n𝐻\n,\nunreachable\nreachable\nYour car would then have a full tank at position 4. At this point, you can reach all gas stations up to position 9, or stations 𝐷and 𝐸. Similar to\nbefore, the greedy approach would choose to refuel at gas station 𝐸since it is the reachable gas station that is farthest from your current position.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nt\n𝐴\nt\n𝐵\nt\n𝐶\nt\n𝐷\nt\n𝐸\nt\n𝐹\nt\n𝐺\nt\n𝐻\n,\nunreachable\nreachable", "word_count": 627, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7f0f08ce-38c7-590a-8683-760c2ed1a7c4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 806, "real_page_number": null, "text": "794\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nYour car would then have a full tank at position 8. Gas stations 𝐹and 𝐺are reachable, so the greedy choice refuels at station 𝐺.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nt\n𝐴\nt\n𝐵\nt\n𝐶\nt\n𝐷\nt\n𝐸\nt\n𝐹\nt\n𝐺\nt\n𝐻\n,\nunreachable\nreachable\nThe destination is now reachable. Thus, the solution returned by the greedy approach would refuel at stations 𝐶, 𝐸, and 𝐺.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nt\n𝐶\nt\n𝐸\nt\n𝐺\n,\nIs it possible to do better? You can try brute forcing all possible combinations of gas stations, but there exists no solution that involves fewer\nthan three refueling stops. It turns out the greedy approach of choosing the farthest reachable gas station discovered an optimal solution for this\nexample. However, one example is not enough to prove the efficacy of a greedy approach on all possible inputs! To prove that this greedy\nmethod works in all cases, we will apply the proof strategy covered earlier.\nFrame the problem as one in which only one subproblem remains after a greedy choice is made.\nIf our greedy algorithm selects the farthest gas station that is reachable from the current position, we are left with the subproblem of minimizing\nthe number of stops we need among the remaining gas stations along our trip. For example, after making the greedy choice of gas station 𝐶, we\nare left with the subproblem of solving the breakpoint selection problem for gas stations 𝐷through 𝐻with an initial starting position of 4.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nt\n𝐶\nt\n𝐷\nt\n𝐸\nt\n𝐹\nt\n𝐺\nt\n𝐻\n,\nShow that the greedy-choice property is satisfied, such that at least one optimal solution to the original problem includes the first greedy choice\nmade by the greedy algorithm.\nLet 𝑏𝑘represent the farthest gas station that is reachable from our current position, and thus the greedy choice. Assume that an optimal solution\n𝑂does not include 𝑏𝑘. If this were the case, 𝑂must instead include some other gas station 𝑏𝑗as the first refueling stop. We know that the\nposition of 𝑏𝑗must be before 𝑏𝑘, since 𝑏𝑘is the farthest reachable gas station. This implies that any gas station reachable from 𝑏𝑗must also be\n𝑂′ 𝑂′reachable from 𝑏𝑘. Thus, we can safely construct a new solution by replacing 𝑏𝑗with 𝑏𝑘, as the next stop in 𝑂is also reachable from 𝑏𝑘.\n𝑂′involves the same number of refueling stops as 𝑂, and since 𝑂is optimal, must also be optimal. We have now successfully proven that\nthere must exist an optimal solution that contains the farthest gas station that is reachable from our starting position, and that the greedy-choice\nproperty holds.\nShow that the problem exhibits an optimal substructure, where the optimal solution of the remaining subproblem can be combined with the\ngreedy choice to obtain an optimal solution for the original problem.\n𝑆′Let 𝑆represent the original breakpoint selection problem we are trying to solve, and let represent the subproblem that remains after a greedy\n𝑆′choice is made. Assume that the optimal solution for stops at 𝑛gas stations, for some arbitrary natural number 𝑛. We then claim that the\n𝑆′problem does not exhibit an optimal substructure, and that the optimal solution for cannot be combined with the greedy choice 𝑏𝑘to obtain\nan optimal solution for 𝑆. This implies that the optimal solution for 𝑆must visit fewer than gas stations. We know via the greedy-choice𝑛+1\nproperty that there must exist an optimal solution 𝑂that includes the farthest gas station that is reachable from our current position. If we\n𝑆′remove this gas station from 𝑂, we would end up with a solution for that involves fewer than 𝑛stops, since 𝑂involves fewer than stops.𝑛+1\n𝑆′This results in a contradiction! Thus, if the subproblem optimally visits 𝑛gas stations, the optimal solution for 𝑆must visit exactly 𝑛+1\n𝑆′ 𝑆′gas stations… otherwise, we would be able to devise a solution for that makes fewer than 𝑛stops, contradicting our initial claim that\noptimally makes 𝑛stops. We have successfully proven that the problem exhibits an optimal substructure.\nWe have successfully shown that the greedy approach of selecting the farthest reachable gas station satisfies the greedy-choice property, and that\nthe breakpoint selection problem exhibits an optimal substructure. Thus, we can conclude via mathematical induction that this greedy approach\nwill always discover the optimal solution.", "word_count": 777, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c30accbf-018b-59d7-a4c9-b079b132bd71", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 807, "real_page_number": null, "text": "21.3 Greedy Algorithms\n795\nThe breakpoint selection problem can therefore be solved using the following procedure:\n• Sort the gas stations in order of distance away from your starting position.\n• Greedily refuel at the farthest gas station that is reachable from your current position, given your car’s capacity.\n• Repeat until the destination is reached (or until you run out of reachable gas stations, which indicates that the destination is unreachable).\nThe time complexity of solving the breakpoint selection problem is if the gas stations are not already sorted in order of distance, andΘ(𝑛log(𝑛))\nif they are. Similar to the activity selection problem, once we can show that a greedy approach works on all inputs for a problem, we canΘ(𝑛)\nsolve the problem using an algorithm that is often significantly better than brute force.\n¸ 21.3.5\n(✽)Huﬀman Coding\nAs our last example, we will look at another practical application of greedy algorithms. This isn’t a topic that will be covered in this class, so\nthese notes won’t go into too much detail here. However, this is knowledge that is good to know, especially as it relates to greedy algorithms and\ntheir usefulness in solving important problems we deal with on a daily basis.\nSuppose you wanted to compress an ASCII text file to reduce its size on your hard drive, or to upload it to the internet faster. How would\n(char)you do this? Recall that ASCII characters are represented as one byte (or eight bits) in memory. For example, the string \"hello world\"\nwould be represented like this in binary, where each character takes up eight bits (8 11 = 88 bits total):×\n01101000 01100101 01101100 01101100 01101111 00100000 01110111 01101111 01110010 01101100 01100100\nHere, the character ‘h’ has a value of 104 in ASCII (01101000 in binary), the letter ‘e’ has a value of 101 in ASCII (01100101 in binary), the\nletter ‘l’ has a value of 108 in binary (01101100 in binary), and so on. After receiving this sequence of bits, the computer then translates it into\nthe string \"hello world\". However, this raises an interesting question: if each character must take up at least 8 bits of space in order to be uniquely\nidentifiable, how is it possible to compress a text file even further? Wouldn’t data compression be a physically impossible task for text files?\nWell, obviously text compression must be possible, or else we wouldn’t be able to compress text files without losing data! So how is it done?\nOne potential way of compressing a text file is to assign a new based on which characters appear in the file. For instance,fixed-length code\nconsider a file that only has the characters ‘a’, ‘b’, ‘c’, ‘d’, ‘e’, and ‘f’, each appearing the following number of times:\nCharacter\na\nb\nc\nd\ne\nf\nFrequency\n20\n5\n12\n14\n39\n10\nSince we know there are only six unique characters in this file, there is no need to use eight bits to represent each character. Instead, we can\n⌈log2(6)⌉=safely create a new mapping that can represent all possible characters using a fixed size of bits. The following is an example of a3\npotential mapping that can be used for these six characters.\nCharacter\na\nb\nc\nd\ne\nf\nCode\n000\n001\n010\n011\n100\n101\nHow effective is this method of compression? Previously, without any compression involved, the file’s contents would take up a total of 100\ncharacters 8 bits = 800 bits. After converting each character to a new three-bit code, the size of the file’s contents drops to 100 characters 3× ×\nbits = 300 bits.\nThat being said, the fixed-length approach obviously doesn’t work in all cases. What if every possible ASCII character were present in the\nfile we wanted to compress? In such a scenario, there would be no way to map each character to a new value that reduces the number of bits\nused per character. Since a fixed-length code isn’t always ideal, a better way to compress the file would be to create a code,variable-length\nwhere each character in the file may be mapped to a variable number of bits. Here is one potential mapping that uses variable-length codes:\nsome characters use up one bit, while others use up two.\nCharacter\na\nb\nc\nd\ne\nf\nCode\n0\n1\n00\n01\n10\n11\nUsing this compression approach, the total size of the file’s contents drops to\n(20 ‘a’ 1 bit) + (5 ‘b’ 1 bit) + (12 ‘c’ 2 bits) + (14 ‘d’ 2 bits) + (39 ‘e’ 2 bits) + (10 ‘f’ 2 bits) = 175 bits× × × × × ×\nHowever, notice that the specific character assignments matter! If we instead changed the mappings around so that ‘a’ and ‘e’ were mapped to\none bit, we end up dropping the file content size to\n(20 ‘a’ 1 bit) + (5 ‘b’ 2 bits) + (12 ‘c’ 2 bits) + (14 ‘d’ 2 bits) + (39 ‘e’ 1 bit) + (10 ‘f’ 2 bits) = 141 bits× × × × × ×\nBy assigning smaller values to more frequent characters, we ended up compressing our file even more.\nIn an ideal world, we could just map the most frequent characters to values that take up the fewest bits. However, this is not possible when\nworking with variable-length codes. This is because, when a computer looks at the binary of a file, it can only see a sequence of 0’scontinuous\nand 1’s. When our codes had a fixed-length, it was easy to determine where one letter ended and another began. However, we don’t have this\nluxury with variable-length encoding; for example, if a computer sees the binary sequence \"1010\", the underlying character string could be\n\"baba\" (1 0 1 0), \"bae\" (1 0 10), \"eba\" (10 1 0), or \"ee\" (10 10).\nThus, to correctly develop an algorithm that uses variable-length codes, we also need to ensure that the characters in our encoding can be\nuniquely distinguishable within a sequence of 0’s and 1’s. To do so, we can only assign codes such that no code is a prefix of any other code\n(codes that satisfy this property are known as codes) — otherwise, the character that begins an encoded file may be ambiguous.prefix", "word_count": 1061, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f586c63c-8d59-5c80-921b-5a1bba300d5a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 808, "real_page_number": null, "text": "796\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nUpon first glance, this problem seems challenging; we want to minimize the total size of a text file after compression, but we can only assign\ncodes that do not share a common prefix with any other previously assigned code. In fact, this was very much an open problem in the early 20th\ncentury, one that puzzled even the greatest of computer scientists at the time. However, in 1951, MIT graduate student David Huffman solved\nthis problem by inventing the concept of Huffman coding, a greedy approach for constructing a uniquely-identifiable, variable-length code\ninputs.1system that guarantees optimal compression on all possible The steps of this algorithm are as follows:\n1. Sort the characters in the file in order of frequency.\n2. Greedily select the two smallest frequency values and merge them together into a single node whose combined frequency is equal to the\nsum of its children.\n3. Repeat step 2 until a single tree remains.\n4. Assign variable-length codes to each character based on its position in the tree. If you need to travel down a left branch to get to the\ncharacter, append a 0 to its prefix code; if you need to travel down a right branch, append a 1 to its prefix code.\nTo illustrate an example of how Huffman coding works, consider a file with the same character frequencies as before:\nCharacter\na\nb\nc\nd\ne\nf\nFrequency\n20\n5\n12\n14\n39\n10\nFirst, the characters are sorted in order of frequency:\nb:5\nf:10\nc:12\nd:14\na:20\ne:39\nThen, the two smallest values are merged into a single node whose value is equal to the sum of its children. Here, ‘b’ and ‘f’ have the smallest\nfrequencies of 5 and 10, so they are merged together into a node whose frequency value is 5 + 10 = 15.\nb:5\nf:10\n15\nc:12\nd:14\na:20\ne:39\nThis merging process is repeated until a single tree remains. The characters ‘c’ and ‘d’ have the next smallest frequencies of 12 and 14, so they\nare merged into a single node whose frequency value is 12 + 14 = 26.\nb:5\nf:10\n15\nc:12\nd:14\n26\na:20\ne:39\nThe next two smallest frequency values are 15 for the \"bf\" node and 20 for ‘a’. Thus, these two components are merged into a single node whose\nfrequency value is 15 + 20 = 35.\nb:5\nf:10\n15\na:20\n35\nc:12\nd:14\n26\ne:39\nThe next two smallest frequency values are 26 for the \"cd\" node and 35 for the \"bfa\" node. We merge these two components together to get a\nsingle node whose frequency value is 26 + 35 = 61.\nb:5\nf:10\n15\na:20\n35\nc:12\nd:14\n26\n61\ne:39\n1Foroneofhisclasses,Huffman’sprofessorgavestudentstheoptionto eithertakethefinalexamorwriteatermpaperonfinding themostoptimalbinarycode.\nHuffman didn’t want to take the final exam, so he decided to write the paper instead. This led to the discovery of Huffman coding, which actually solved a\nproblemthatHuffman’sownprofessorhadstruggledwith—unbeknownsttoHuffmanatthetime,hisprofessorhadactuallyassignedanopenproblemthathe\nwasalsotryingtosolve! SurelysomethingtothinkaboutifyourEECS376professoreverchallengesyoutoprove 𝑁𝑃insteadoftakingthefinalexam…𝑃=", "word_count": 569, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "de0a9b90-a6af-57d5-bd6f-69a67a82f363", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 809, "real_page_number": null, "text": "21.3 Greedy Algorithms\n797\nNow, the two smallest frequency values are 39 for ‘e’ and 61 for the remaining characters. We merge these two together to get a single node\nwhose frequency value is 61 + 39 = 100. Our Huffman tree is now complete.\nb:5\nf:10\n15\na:20\n35\nc:12\nd:14\n26\n61\ne:39\n100\nAt this point, variable-length codes are assigned to each character based on their position in the tree. Whenever we travel down a left branch, we\nadd a 0 to the character’s binary code. Whenever we travel down a right branch, we add a 1 to the character’s binary code. For example, the\nbinary mapping of the character ‘f’ in this file would be 0001, since we travel down a left branch three times, and a right branch once.\nb:5\nf:10\n15\n0\n1\na:20\n35\n0\n1\nc:12\nd:14\n26\n0\n1\n61\n0\n1\ne:39\n100\n0\n1\nThe other binary mappings are shown in the table below. This is an optimal assignment of characters to bits that minimizes file size while also\nensuring that characters are individually distinguishable within a sequence of bits (each bit indicates a direction to move down the tree, and we\nknow that a new character begins once a character, or root node, is encountered).\nCharacter\na\nb\nc\nd\ne\nf\nCode\n001\n0000\n010\n011\n1\n0001\nUsing the example from earlier, the size of the file contents using this variable-code system is:\n(20 ‘a’ 3 bits) + (5 ‘b’ 4 bits) + (12 ‘c’ 3 bits) + (14 ‘d’ 3 bits) + (39 ‘e’ 1 bit) + (10 ‘f’ 4 bits) = 237 bits× × × × × ×\nHuffman coding guarantees a mapping that maximizes compression for variable-length coding, where the frequency of each symbol is known.\nBecause of this, the algorithm forms the basis of many widely used compression formats, including JPEG, MP3, and gzip (which has file\nextension .gz, the compression format used to compress files before submission to the autograder).", "word_count": 340, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ac1fa1a3-9f53-5594-8ff6-6ac4d950a418", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 810, "real_page_number": null, "text": "798\nChapter 21. Greedy Algorithms and Divide-and-Conquer\n¸ 21.3.6\nSolving Problems Using a Greedy Approach\nIn this section, we will look at a few interview-style questions that can be solved using a greedy approach. For these problems, proving that a\ngreedy approach works allows you to implement a solution that is more performant than if you tried to implement the solution in any other way.\nExample 21.6 Your company has two office locations in two different cities, which we will denote as offices 𝐴and 𝐵. The company wants\nto interview 2𝑛candidates for a job opening, and they want to make sure an equal number of people are interviewed at each office.\n𝑖𝑡ℎpersoncosts, costs[i] costs[i] =You are given a vector where stores the cost of flying the to cities 𝐴and 𝐵(in the form\n[cost_a, cost_b]. Return the minimum cost required to fly every candidate to an office such that exactly 𝑛people end up in each\noffice. The function header is shown below:\nint32_t min_interview_cost(std::vector<std::vector<int32_t>>& costs);\ncosts = [[13, 42], [33, 65], [17, 29], [70, 68]],Example: Given the optimal strategy is to send candidates 1 and 2 to\noffice 𝐴and candidates 3 and 4 to office 𝐵, for a total cost of 13 + 33 + 29 + 68 = 143. Thus, your should return 143 for this input.\nThe simplest solution would be to go through the list of costs and brute force your way to the optimal solution. However, that is certainly not an\noptimal strategy. Since this is an optimization problem, let’s first try to see if a greedy approach can be used to derive a better solution.\nThere are several ways we can make a greedy choice. One way would be to greedily assign candidates in order of increasing cost. For\ninstance, the lowest cost is 13, so we would first assign candidate 1 to office 𝐴. The next lowest cost among the remaining candidates is 17, so\nwe then assign candidate 3 to office 𝐴. Since we now have two candidates at office 𝐴, the remaining two candidates must be assigned to office\n𝐵. However, this approach clearly does not work: we end up with a cost 13 + 17 + 65 + 68 = 163, which is not optimal. This is because the\nhigh expenses of assigning candidates 2 and 4 to office 𝐵far outweigh the value of cheaply assigning candidates 1 and 3 to office 𝐴.\nInstead of looking purely at the absolute cheapest costs to assign each candidate to an office, a better method would be to consider the total\n(i.e., the difference in costs) of sending a candidate to one office rather than the another. By doing so, we can avoid situations like thesavings\none we ran into above. For instance, consider a candidate whose cost to office 𝐴is low, but whose cost to office 𝐵is not significantly higher.\nEven if sending this candidate to office 𝐴may be the better decision in isolation, it may be worthwhile to send them to office 𝐵if this allows us\nto reserve a position in office 𝐴for another candidate whose cost to office 𝐵is much higher.\nLet’s look at this strategy in action using the example provided. The following are the savings associated with each candidate (we are using\na signed number here to highlight which office is better: if the number is negative, there is an advantage in sending that candidate to office 𝐴; if\nthe number of positive, there is an advantage in sending that candidate to office 𝐵):\n• 13 - 42 = -29Candidate 1:\n• 33 - 65 = -32Candidate 2:\n• 17 - 29 = -12Candidate 3:\n• 70 - 68 = +2Candidate 4:\nIf we sort these numbers, we can see that the two smallest numbers (-32 and -29) belong to candidates 1 and 2. This means that sending these\ncandidates to office 𝐴yields us the best advantage. On the other hand, the candidates with the larger two numbers (-12 and +2) belong to\ncandidates 3 and 4. Using similar reasoning, it is best to send these two candidates to office 𝐵. The total cost of this arrangement ends up being\n13 + 33 + 29 + 68, which is our solution of 143.\nIf greedily selecting candidates by cost difference always yields an optimal solution, then the problem’s implementation becomes trivial: we\ncan just sort the candidates in order of cost difference and then assign the first half of candidates to office 𝐴and the second half of candidates to\noffice 𝐵. However, is it safe to assume that this greedy approach always works? It turns out that it is, which we will show in the following proof\nusing the exchange argument and a bit of math. Let us consider our candidates, sorted in order of cost difference 𝑑(defined as cost 𝑎- cost 𝑏):\n[𝑑1, 𝑑2, …, 𝑑𝑛, 𝑑𝑛+1, …, 𝑑2𝑛]\nLet candidate 𝑋be any arbitrary candidate in the first half of the sorted array (which we greedily assigned to office 𝐴), and let candidate 𝑌be\n𝑑𝑋≤𝑑𝑌.any arbitrary candidate in the second half of the sorted array (which we greedily assigned to office 𝐵). By definition, we know that\nFurthermore, suppose that the solution to the greedy approach is some value 𝐶.\nNow, consider what happens if we send 𝑋to office 𝐵and 𝑌to office 𝐴(essentially swapping the two candidates). The net cost we incur\nfrom sending 𝑋to office 𝐵instead of office 𝐴is 𝑏𝑋−𝑎𝑋, where 𝑎𝑋is the cost of sending 𝑋to office 𝐴, and 𝑏𝑋is the cost of sending 𝑋\nto office 𝐵. Similar logic can be applied to determine the cost incurred from sending 𝑌to office 𝐴instead of office 𝐵, which ends up being\n𝐶′:𝑎𝑌−𝑏𝑌. We will denote our new cost after swapping 𝑋and 𝑌as\n𝐶′ 𝐶+ (𝑏𝑋−𝑎𝑋)=\n⏟⏞⏞⏟⏞⏞⏟\nnetcostfrom\nsending toX B\n(𝑎𝑌−𝑏𝑌)+\n⏟⏞⏞⏟⏞⏞⏟\nnetcostfrom\nsending toY A\nNotice that is equal to −𝑑𝑋, and is equal to 𝑑𝑌. Thus, we can rewrite our new cost as:(𝑏𝑋−𝑎𝑋) (𝑎𝑌−𝑏𝑌)\n𝐶′ 𝐶−𝑑𝑋+𝑑𝑌=\n𝑑𝑋≤𝑑𝑌, 𝐶′Since the expression 𝐶−𝑑𝑋+𝑑𝑌cannot possibly be less than 𝐶! Thus, our new cost of cannot be optimal, and 𝑋and 𝑌should\nnot be swapped. Since 𝑋and 𝑌are arbitrary candidates chosen from offices 𝐴and 𝐵(using our greedy solution), we have also proven that there\nis no way to improve our cost by switching two candidates in the greedy solution. Therefore, our greedy approach must always be optimal!any", "word_count": 1078, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c96635f4-0b19-553c-9e0a-258f53422ea4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 811, "real_page_number": null, "text": "21.3 Greedy Algorithms\n799\nAn implementation of this greedy solution is shown below:\n1\nstruct CostDiffCompare {\n2\nbool operator() (const std::vector<int32_t>& const std::vector<int32_t>&v1, v2) {\n3\nreturn v1[0] - v1[1] < v2[0] - v2[1];\n4\n} // operator()()\n5\n};\n6\n7\nint32_t min_interview_cost(std::vector<std::vector<int32_t>>& costs) {\n8\nCostDiffCompare comp;\n9\nstd::sort(costs.begin(), costs.end(), comp);\n10\n11\nint32_t total_cost = 0;\n12\nfor (size_t i = 0; i < costs.size() / 2; ++i) {\n13\ntotal_cost += costs[i][0] + costs[i + (costs.size() / 2)][1];\n14\n} // for i\n15\n16\nreturn total_cost;\n17\n} // min_interview_cost()\nCostDiffCompare costsThe object is a custom comparator that can be used to sort the vector in order of increasing cost difference, as\nshown on line 9. The loop on line 12 then adds the costs to a running counter that is returned. Notice the optimization in the body of the\nloop: because we know that an equal number of candidates are assigned to each office, we can add the costs of the candidates in the first half\ncosts.size()the costs of the candidates in the second half. This allows us to reduce the number of loop iterations from toalongside\ncosts.size() / 2. We can also replace the comparator with a lambda expression, as shown:\n1\nint32_t min_interview_cost(std::vector<std::vector<int32_t>>& costs) {\n2\nstd::sort(costs.begin(), costs.end(),\n3\n[](const std::vector<int32_t>& const std::vector<int32_t>&v1, v2) {\n4\nreturn v1[0] - v1[1] < v2[0] - v2[1];\n5\n});\n6\n7\nint32_t total_cost = 0;\n8\nfor (size_t i = 0; i < costs.size() / 2; ++i) {\n9\ntotal_cost += costs[i][0] + costs[i + (costs.size() / 2)][1];\n10\n} // for i\n11\n12\nreturn total_cost;\n13\n} // min_interview_cost()\nThe time complexity of this solution is Θ(𝑛log(𝑛)), since the sorting step acts as the bottleneck of the entire algorithm. This complexity class is\nmuch better than any other solution we could have come up with, had we not pursued a greedy approach!\nExample 21.7 You are playing a video game that requires you to go on side quests to gain experience before starting an important mission.\nDue to time constraints, you are only allowed to complete 𝑘side quests before beginning the mission. Each side quest requires a specific\namount of experience (in the form of a positive integer), and you can only begin a side quest if you have the necessary experience beforehand.\nYou can gain experience by completing side quests, each of which award a predetermined amount of experience. Because you want to be as\nprepared as possible for the important mission, you want to complete the side quests that allow you to gain the most experience.\nGiven a vector of 𝑛side quests, your initial experience, and the number of missions you must complete (i.e., 𝑘), implement a function that\ncomputes the maximum experience possible after completing 𝑘distinct side quests. The function header is shown below:\n1\nstruct SideQuest {\n2\nint32_t exp_required;\n// minimum experience required to start this side quest\n3\nint32_t exp_rewarded;\n// experience gained by completing this side quest\n4\n};\n5\n6\nint32_t int32_t int32_tmax_experience(std::vector<SideQuest>& side_quests, init_exp, k);\nExample: Given 4, an initial experience of 3, and the following ten side quests to choose from:𝑘=\n• Experience Required: 1, Experience Rewarded: 6Side Quest A:\n• Experience Required: 2, Experience Rewarded: 7Side Quest B:\n• Experience Required: 4, Experience Rewarded: 9Side Quest C:\n• Experience Required: 7, Experience Rewarded: 3Side Quest D:\n• Experience Required: 10, Experience Rewarded: 2Side Quest E:\n• Experience Required: 14, Experience Rewarded: 5Side Quest F:\n• Experience Required: 20, Experience Rewarded: 50Side Quest G:\n• Experience Required: 21, Experience Rewarded: 51Side Quest H:\n• Experience Required: 22, Experience Rewarded: 52Side Quest I:\n• Experience Required: 26, Experience Rewarded: 53Side Quest J:\nyou would select side quests A, B, C, and I to maximize your experience to a value of 3 (initial) + 6 (side quest A) + 7 (side quest B) + 9\n(side quest C) + 52 (side quest I) = 77.", "word_count": 678, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8866321e-2084-57d2-9727-a26f8f037f4d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 812, "real_page_number": null, "text": "800\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nThis is another problem that can be solved using a greedy approach. In fact, you may have noticed that this problem is very similar to the gas\nstation example we covered earlier: much like with the gas stations, greedily selecting the attainable side quest that rewards the most experience\nwill get you to the optimal solution. For example, consider the side quests in the example, each rewarding a differing amount of experience.\nexperience required for side quest\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n6\n7\n9\n3\n2\n5\n50\n51\n52\n53\nAt the beginning, you start off with 3 experience. This gives you access to the first two side quests, as shown:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n6\n7\n9\n3\n2\n5\n50\n51\n52\n53\nThe greedy solution is to select the side quest that yields the highest experience among the quests that you are able to complete. In this case,\nyou would choose the side quest that yields 7 experience, bringing your total experience up to 3 + 7 = 10. This gives you access to several\nadditional side quests up to an experience of 10.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n6\n7\n9\n3\n2\n5\n50\n51\n52\n53\nAmong these side quests, the one that yields the most experience rewards 9, bringing you up to a total experience of 10 + 9 = 19.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n6\n7\n9\n3\n2\n5\n50\n51\n52\n53\nAmong these side quests, the one that yields the most experience rewards 6, bringing you up to a total experience of 19 + 6 = 25.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n6\n7\n9\n3\n2\n5\n50\n51\n52\n53\nAmong these side quests, the one that yields the most experience rewards 52, bringing you up to a total experience of 25 + 52 = 77.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n6\n7\n9\n3\n2\n5\n50\n51\n52\n53\nA total of side quests have been completed, so we are done. The most experience you can gain is therefore 77.𝑘=4\nThe reason why this greedy approach works is similar to the reasoning behind the gas station problem covered earlier. Picking the side\nquest that gives you the most experience maximizes the choices you have for later side quests, giving you more opportunities to unlock side\nquests that may reward more experience in the future. As a result, choosing any side quest other than the one that yields the most experience can\nnever lead to an outcome better than that of the greedy approach.", "word_count": 573, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b8894c04-e42e-5d4c-a5ed-339545be31d4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 813, "real_page_number": null, "text": "21.4 Divide-and-Conquer\n801\nThere are several ways to implement this problem, one of which is shown below. Here, we first sort the vector of side quests in order of\nexperience required. Then, we insert the attainable projects into a max-priority queue based on the experience rewarded by each side quest.\nAs long as this priority queue is not empty, we continuously pop off the side quest at the top, add its experience to our total, and push in any\nadditional quests that are now attainable with our new experience. After 𝑘quests are completed, we return our total experience as the solution.\n1\nstruct SideQuestCompare {\n2\nbool operator() (const constSideQuest& q1, SideQuest& q2) {\n3\nreturn q1.exp_required < q2.exp_required;\n4\n} // operator()()\n5\n};\n6\n7\nint32_t int32_t int32_tmax_experience(std::vector<SideQuest>& side_quests, init_exp, k) {\n8\nSideQuestCompare comp;\n9\nstd::sort(side_quests.begin(), side_quests.end(), comp);\n10\n11\nint32_t curr_exp = init_exp;\n12\nstd::priority_queue<int32_t> quest_pq;\n13\nfor (int32_t i = 0; k > 0; --k) {\n14\nwhile (i < side_quests.size() && curr_exp >= side_quests[i].exp_required) {\n15\nquest_pq.push(side_quests[i++].exp_rewarded);\n16\n} // while\n17\nif (!quest_pq.empty()) {\n18\ncurr_exp += quest_pq.top();\n19\nquest_pq.pop();\n20\n} // if\n21\n} // for i\n22\n23\nreturn curr_exp;\n24\n} // max_experience()\nBoth sorting the side quests (line 9) and pushing and popping the quests from the priority queue (lines 13-21) are bounded by a time complexity\nside_questsof Θ(𝑛log(𝑛)), where 𝑛is the number of side quests you are given in the vector. Since these steps serve as the bottleneck of the\nalgorithm, the overall time complexity of this solution is also Θ(𝑛log(𝑛)).\n21.4\nDivide-and-Conquer\n¸ 21.4.1\nDivide-and-Conquer and Independent Subproblems\nSuppose you are given a group assignment consisting of multiple questions that can be answered independently. If we assume that everyone in\nyour group is equally available and competent, what is the fastest way to get this assignment done? A reasonable approach would be to split the\nassignment up among the group members, have each member complete their assigned questions, and combine all of your answers at the end.\nThis is much faster than having someone complete all the questions on their own.\nThe same idea can be applied when developing algorithms for certain types of problems. As long as a given problem can be split up into\nsubproblems2different (where the answer to one subproblem does not depend on the answers to other subproblems), we canindependent\ndivide the problem into smaller components and solve each component separately, and then combine the solutions of these components to\nobtain a solution to the original problem. This is the foundation of the divide-and-conquer algorithmic approach. Divide-and-conquer is often\nimplemented recursively, with the following three steps applied at each level of the recursion:\n• Divide a larger problem into smaller versions of the same problem that can be solved independently.\nConquer the subproblems by solving them individually (either by using recursion, or by using a straightforward approach if the input•\nsize is small enough).\n• Combine the solutions of the subproblems in a meaningful way to construct the solution for the original, larger problem.\nOriginal Problem\nSubproblem 1\nSubproblem 2\n…\nSubproblem 𝑛\nSolution to\nSubproblem 1\nSolution to\nSubproblem 2\nSolutions to\nOther Subproblems\nSolution to\nSubproblem 𝑛\nSolution to Original Problem\nDivide\nConquer\nCombine\n2What if we have subproblems, where the answer to one subproblem may depend on another? In that case, we would usedependent dynamic programming\ninstead,whichwillbecoveredinchapter23.", "word_count": 582, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ca5fa19b-d318-52c3-8d06-4b3be995b3af", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 814, "real_page_number": null, "text": "802\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nThe combining step is trivial for problems that only examine one smaller subproblem at every step, since there isn’t anything to \"combine\"\nthe solution of this subproblem with. An example of an algorithm that divides its input into only one subproblem is binary search, which was\ncovered in chapter 15.\nRemark: Problems such as binary search are often considered an \"edge case\" of the divide-and-conquer paradigm, since they only divide a\nproblem into smaller subproblem, and thus there is no combining work to do. To differentiate this special case from situations whereone\nthere are two or more subproblems (and thus may involve combining work), some have proposed a new category of algorithms known as\ndecrease-and-conquer, comprised of algorithms that reduce a problem to a smaller subproblem with each recursive call. However, thissingle\nalgorithm family is not consistently applied, and many sources still use the divide-and-conquer paradigm to label algorithms that divide its\ninput into one subproblem. For these notes, we will not introduce any additional algorithm families that aren’t defined in the class or most\nestablished algorithm textbooks, such as CLRS. Therefore, we will follow the convention of defining algorithms that only examine one\nsubproblem at each recursive depth, such as binary search, as part of the divide-and-conquer algorithm family.\nThe recursive nature of divide-and-conquer algorithms — where the original input size is split into several independent subproblems — makes\nthem ideal candidates for the Master Theorem. If a divide-and-conquer algorithm\n(𝑎≥1,• separates a problem of input size 𝑛into 𝑎smaller subproblems whose input size is 1∕𝑏the size of the original problem 1)𝑏>\n• the work to divide/combine the input takes time𝑓(𝑛)\nwe can represent the algorithm’s runtime as:\n𝑛𝑇(𝑛) 𝑎𝑇(=\n𝑏)+𝑓(𝑛)\nThen, we can apply the Master Theorem to obtain the time complexity of the algorithm:\n𝑇(𝑛)=\n⎧\n⎪\n⎨\n⎪⎩\nΘ(𝑛log𝑏(𝑎)),\n𝑏𝑐if 𝑎>\nΘ(𝑛𝑐log(𝑛)),\n𝑏𝑐if 𝑎=\nΘ(𝑛𝑐),\n𝑏𝑐if 𝑎<\nA common example of the divide-and-conquer paradigm is top-down recursive mergesort, since the dividing, conquering, and combining steps\nare well-established. First, mergesort continuously the input into smaller subarrays, until the input size of each subarray is small enoughdivides\nto be solved trivially. Then, the algorithm the subproblems by sorting them individually, and then the results in sorted orderconquers combines\nmerge()using the function. A diagram of this process is shown below:\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n1\n3\n2\n8\n4\n7\n1\n3\n5\n6\n2\n4\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\nDivide\nConquer\nCombine\nQuicksort is another algorithm that uses divide-and-conquer. At each step, quicksort partitions the input based on a pivot value, then thedivides\ninput into smaller subarrays based on the position of the pivot. The algorithm then the subproblems by sorting the values to the leftconquers\nand right of the pivot, and then the results to obtain the sorted result for the original input.combines\n1\n4\n2\n3\n7\n9\n8\n6\n5\n1\n4\n2\n3\n5\n9\n8\n6\n7\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9\nDivide\nConquer\nCombine", "word_count": 573, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4794ceb4-c057-50f6-ab77-10040fbb9cdb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 815, "real_page_number": null, "text": "21.4 Divide-and-Conquer\n803\nGenerally, both mergesort and quicksort are accepted as part of the divide-and-conquer algorithm family. However, there is a slight difference\nbetween the two with respect to where most of the work is done, even if both algorithms share the same structure. With quicksort, the\nimplementation is top-heavy, with the work primarily done during the dividing step (i.e., partitioning with the pivot value). However, this allows\nthe combining step to be completed trivially, as all the values end up in their correct sorted positions after each subproblem is conquered.\n1\n4\n2\n3\n7\n9\n8\n6\n5\n1\n4\n2\n3\n5\n9\n8\n6\n7\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9\nDivide\nConquer\nCombine\nMore work done here\nLess work done here\nThe opposite is true for mergesort. In the case of mergesort, the implementation is bottom-heavy, as the work is primarily done during the\ncombiningstep(i.e.,mergingtheindividualsubcomponents). Thisworkallowsthedividingsteptobecompletedtrivially,sincenopreprocessing\nwork needs to be done before the input is split into several subproblems.\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n3\n1\n8\n2\n4\n7\n5\n6\n1\n3\n2\n8\n4\n7\n1\n3\n5\n6\n2\n4\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\nDivide\nConquer\nCombine\nLess work done here\nMore work done here\nTo differentiate between these two types, you may see the term combine-and-conquer in this class to describe algorithms that break a problem\ninto smaller independent subproblems, but rely primarily on the combining step to obtain the solution of the original problem. Combine-and-\nconquer algorithms start with the smallest subdomain possible, then combine increasingly larger subdomains until the original, larger problem\nis solved. Under this definition, mergesort can be treated as a \"combine-and-conquer\" algorithm, since much of its work is spent combining\ntogether individual subproblems, rather than dividing the input (which is done trivially). On the contrary, algorithms that primarily rely on the\ndividing step to solve a problem, such as quicksort and binary search, retain the \"divide-and-conquer\" moniker under this definition.\nDivide-and-conquer can be used to improve the time complexity of problems whose input can be divided into smaller, independent\nsubproblems. A few examples are provided below.\n¸ 21.4.2\nPower Function\nSuppose you wanted to write a simple power function that can be used to take the power of any integer.\nuint32_t power(uint32_t uint32_tx, n);\n// returns x^n\nx n - 1A naïve solution would be to multiply a total of times:\nx^n = x x x x ... x* * * * *\nHowever, this approach would take time, since multiplications have to be done. We can reduce the number of multiplications weΘ(𝑛) 𝑛−1\n𝑥𝑛∕2, 𝑥𝑛(note:need by recursively solving the subproblem of and then multiplying the result with itself to obtain the solution for if 𝑛is odd,\n𝑥⌊𝑛∕2⌋,solve for multiply it with itself, and then multiply again by an additional factor of 𝑥). The recurrence relation is shown below:\n𝑥𝑛=\n⎧\n⎪\n⎨\n⎪⎩\n1,\nif 𝑛=0\n𝑥𝑛∕2 ∗𝑥𝑛∕2,\nif 0,even𝑛>\n𝑥∗𝑥⌊𝑛∕2⌋∗𝑥⌊𝑛∕2⌋,\nif 0,odd𝑛>", "word_count": 564, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "59eca555-9a73-53a9-916d-89db2ab881c9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 816, "real_page_number": null, "text": "804\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nThe divide-and-conquer implementation is shown below (for simplicity, we will ignore overflow):\n1\nuint32_t power(uint32_t uint32_tx, n) {\n2\nif (n == 0) {\n3\nreturn 1;\n4\n} // if\n5\nint result = power(x, n / 2);\n6\nresult *= result;\n7\nif (n % 2 != 0) {\n// if n is odd\n8\nresult *= x;\n9\n} // if\n10\nreturn result;\n11\n} // power()\nWe can make this implementation tail recursive by adding an accumulator argument (covered in section 5.3) to perform the multiplication\nbefore the recursive call returns:\n1\nuint32_t power(uint32_t uint32_t uint32_tx, n, result = 1) {\n2\nif (n == 0) {\n3\nreturn result;\n4\n} // if\n5\nelse if (n % 2 == 0) {\n// even\n6\nreturn power(x x, n / 2, result);*\n7\n} // else if\n8\nelse {\n// odd\n9\nreturn power(x x, n / 2, result x);* *\n10\n} // else\n11\n} // power()\nThe divide-and-conquer solution to the power function can be expressed using the following recurrence relation:\n𝑇(𝑛) 𝑇(𝑛∕2)+Θ(1)=\nUsing the Master Theorem with 1, 2, and 0, we can conclude that the time complexity of this new solution is with respect𝑎= 𝑏= 𝑐= Θ(log(𝑛))\n𝑥𝑛intoto the power value 𝑛. By splitting independent subproblems with powers less than 𝑛and using the solutions to these subproblems to\n𝑥𝑛,obtain the solution for we were able to bring the time complexity of our implementation down from to Θ(log(𝑛)).Θ(𝑛)\n¸ 21.4.3\n(✽)Integer Multiplication and the Karatsuba Algorithm\nAnother prominent application of divide-and-conquer can be demonstrated using the Karatsuba algorithm, which is an optimized method that\ncan be used to multiply two integers. To understand the usefulness of Karatsuba, let’s first consider the standard integer multiplication strategy\ntaught in grade school. Here, we first multiply the top number (1337) with the last digit in the final number (1). Then, we multiply the top\nnumber (1337) with the second-to-last digit of the bottom number (8), but with the result shifted one position to the left. Then, we multiply the\ntop number with the third-to-last digit, then the fourth-to-last digit, etc. shifting the result one position to the left each time. Once the top number\nis multiplied for all the digits of the bottom number, we add the results (in this case, 1337 + 106960 + 267400 = 375697) to get the final answer.\n1337\n×\n281\n1337\n10696\n+ 2674\n375697\nWhat is the time complexity of this algorithm, in terms of the number of digits in the bigger input number, 𝑛? Each multiplication would take\nΘ(𝑛2)time, and since we have to multiply the top digit once for every digit in the bottom number, computing the partial results takes time.Θ(𝑛)\nThen, we have to add these 𝑛numbers, each of which have digits. Adding two digit numbers also takes time, so adding 𝑛ofΘ(𝑛) Θ(𝑛) Θ(𝑛)\nΘ(𝑛2)them takes a total of time. Since both of these steps occur sequentially, the time complexity of entire algorithm in terms of the number of\nΘ(𝑛2)+Θ(𝑛2) Θ(𝑛2). Θ(𝑛2)?digits 𝑛is Is there a way to do better than=\nThe Karatsuba algorithm reduces this time complexity by using a divide-and-conquer approach. To understand Karatsuba, first notice what\nhappens if we split a given number into two halves. We will denote the number in the left half as 𝑎and the number in the right half as 𝑏(for\nexample, after splitting the number 1337 in half, we end up with and 37).𝑎= 𝑏=13\n1337\n13\n⏟⏟⏟\n𝑎\n37\n⏟⏟⏟\n𝑏", "word_count": 613, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "44144901-35f7-57ef-88a6-5543a6437fd2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 817, "real_page_number": null, "text": "21.4 Divide-and-Conquer\n805\nNotice that there is a relationship between these two numbers and the original number:\n1337=13×100+37\n13×102= +37\n13×10𝑛∕2= +37\n𝑎×10𝑛∕2In fact, we can split any 𝑛-digit number 𝑁into two halves 𝑎and 𝑏, such that +𝑏(here, the number is in base-10, but you can𝑁=\nreplace 10 with any base you are working with). We can use this fact to simplify the multiplication of two numbers, which we will denote as 𝑁1\nand 𝑁2. If we split into 𝑎and 𝑏, and into 𝑐and 𝑑, we get:𝑁1 𝑁2\n𝑎×10𝑛∕2𝑁1 +𝑏=\n𝑐×10𝑛∕2𝑁2 +𝑑=\nWe can then compute as:𝑁1×𝑁2\n(𝑎×10𝑛∕2+𝑏)×(𝑐×10𝑛∕2𝑁1×𝑁2 +𝑑)=\n(𝑎×𝑐×10𝑛)+(𝑎×𝑑×10𝑛∕2)+(𝑏×𝑐×10𝑛∕2)+(𝑏×𝑑)=\n(𝑎×𝑐×10𝑛)+[(𝑎×𝑑+𝑏×𝑐)×10𝑛∕2]+(𝑏×𝑑)=\nInstead of calculating (𝑎×𝑐), (𝑎×𝑑), and (𝑏×𝑐), and directly, the Karatsuba algorithm only computes the following three values, which(𝑏×𝑑)\nwe will denote as 𝑚1, 𝑚2, and 𝑚3:\n𝑚1 (𝑎+𝑏)×(𝑐+𝑑)=\n𝑚2 𝑎×𝑐=\n𝑚3 𝑏×𝑑=\nThis is because these three values are the only ones we need to know to be able to solve ×𝑁2, allowing us to minimize the total number of𝑁1\nmultiplications required. Observe that, when written out, is equal to:𝑚1\n𝑚1 (𝑎+𝑏)×(𝑐+𝑑)=\n(𝑎×𝑐)+(𝑎×𝑑)+(𝑏×𝑐)+(𝑏×𝑑)=\nIf we subtract and from this result, we get:𝑚2 𝑚3\n𝑚1−𝑚2−𝑚3 (𝑎×𝑐)+(𝑎×𝑑)+(𝑏×𝑐)+(𝑏×𝑑)−(𝑎×𝑐)−(𝑏×𝑑)=\n(𝑎×𝑑)+(𝑏×𝑐)=\nWe can therefore substitute these values into our initial equation for ×𝑁2, as shown below. This completes the Karatsuba algorithm.𝑁1\n(𝑎×𝑐×10𝑛)+[(𝑎×𝑑+𝑏×𝑐)×10𝑛∕2]+(𝑏×𝑑)𝑁1×𝑁2 =\n×10𝑛)+[(𝑚1 −𝑚3)×10𝑛∕2]+𝑚3(𝑚2 −𝑚2=\nHow efficient is Karatsuba? Instead of performing 𝑛multiplications (as with the grade school multiplication method), we only need to solve for\n𝑚1, 𝑚2, and 𝑚3, and substitute them into the equation for ×𝑁2. This leaves us with the following computations:𝑁1\nComputing performs two additions of 𝑛∕2-digit numbers, which takes time. The two 𝑛∕2-digit numbers we get from the addition• 𝑚1 Θ(𝑛)\nare then multiplied recursively (which contributes a term of to our final recurrence relation).𝑇(𝑛∕2)\n• Computing and each requires a multiplication of two 𝑛∕2-digit numbers, which is done by recursively applying Karatsuba twice𝑚2 𝑚3\nwith an input size of 𝑛∕2. Each of these contribute a term of to our final recurrence relation.𝑇(𝑛∕2)\n×10𝑛)+[(𝑚1 −𝑚3)×10𝑛∕2]+𝑚3.• After computing these three intermediary values, we combine the values by solving for This(𝑚2 −𝑚2\nrequires two additions and two subtractions, which take time. It also requires two multiplications with powers of ten, but these canΘ(𝑛)\n10𝑛simplyalso be done in time — this is because a number multiplied with appends 𝑛zeros to the end of the number (this can beΘ(𝑛)\ndone using a left shift, which was briefly introduced in chapter 17).\nTo summarize, the overall algorithm requires us to perform three recursive multiplications witn 𝑛∕2-digit numbers, work to add twoΘ(𝑛)\n×10𝑛)+[(𝑚1 −𝑚3)×10𝑛∕2]+𝑚3numbers when computing 𝑚1, and work to solve for after computing 𝑚1, 𝑚2, and 𝑚3. ThisΘ(𝑛) (𝑚2 −𝑚2\nresults in the following recurrence relation:\n𝑇(𝑛) 3𝑇(𝑛∕2)+Θ(𝑛)=\nΘ(𝑛log𝑏𝑎)Using the Master Theorem with 3, 2, and 1, we can conclude that the time complexity of the Karatsuba algorithm is𝑎= 𝑏= 𝑐= =\n≈Θ(𝑛1.585).Θ(𝑛log23) By using divide-and-conquer, we have devised an integer multiplication algorithm whose time complexity is better than\nΘ(𝑛2)the time complexity produced by a standard grade school multiplication approach!", "word_count": 554, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "30527666-564f-5f19-a4ed-b8c36fc14e4c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 818, "real_page_number": null, "text": "806\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nJust to illustrate an example of the Karatsuba algorithm using actual numbers, consider the multiplication of 1337×281, as covered earlier.\nWith Karatsuba, we would break these two numbers into 13, 37, 02, and 81, where (the number of digits in 1337). Then,𝑎= 𝑏= 𝑐= 𝑑= 𝑛=4\nwe solve for the following three values:\n𝑚1 (𝑎+𝑏)×(𝑐+𝑑)= =(13+37)×(02+81)=50×83=4150\n𝑚2 𝑎×𝑐== 13×2=26\n𝑚3 𝑏×𝑑== 37×81=2997\nWe can then use these three values to solve for 1337×281, as shown:\n×10𝑛)+[(𝑚1 −𝑚3)×10𝑛∕2]+𝑚3(𝑚2 −𝑚21337×281=\n(26×104)+[(4150−26−2997)×102]+2997=\n(26×104)+(1127×102)+2997=\n=260000+112700+2997\n=375697\nThis gives us a solution of 375697, which is the same solution we got using the less efficient grade school method.\n¸ 21.4.4\n(✽)The Maximum Subarray Problem Using Divide-and-Conquer\nIn this problem, you are given a one-dimensional array that contains both positive and negative integers, and you want to find the largest sum\nthat can be obtained from a subarray of numbers in the array. For example, given the following array, the maximum subarray spanscontiguous\nfrom index 4 to index 8 (inclusive), with a sum of 9 - 8 + 2 - 1 + 8 = 10.\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nThe most obvious approach would be to use brute force and check every contiguous sequence, keeping track of the one with the largest sum.\nThis can be done by looping through the array and considering all possible subsequences that begin at each index.\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSum: 5\nBest so far: 5\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSum: 6\nBest so far: 6\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSum: 3\nBest so far: 6\n…\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSum: 5\nBest so far: 9", "word_count": 437, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "680f0779-93fd-5480-9b1c-3fb18758d662", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 819, "real_page_number": null, "text": "21.4 Divide-and-Conquer\n807\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSum: 1\nBest so far: 9\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSum: -2\nBest so far: 9\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSum: -6\nBest so far: 9\n…\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSum: 7\nBest so far: 10\nThe code for the brute force approach is shown below:\n1\nint32_t max_subarray_sum(const std::vector<int32_t>& nums) {\n2\nint32_t std::numeric_limits<int32_t>::min();best =\n3\nfor (size_t i = 0; i < nums.size(); ++i) {\n4\nint32_t current_subarray_sum = 0;\n5\nfor (size_t j = i; j < nums.size(); ++j) {\n6\ncurrent_subarray_sum += nums[j];\n7\nbest = std::max(current_subarray_sum, best);\n8\n} // for j\n9\n} // for i\n10\nreturn best;\n11\n} // max_subarray_sum()\nΘ(𝑛2).The time complexity of this solution is Can we do better?\nWe can indeed do better if we use a divide-and-conquer approach. The key to notice here is that we can split the original input array into\nseveral subarrays whose individual solutions can be solved independently of other subarrays. That is, if we divide the initial array in half and\nrecursively identify the maximum subset sum of these halves (which can each be solved independently, without knowing on the maximum subset\nsum of the other half), we can combine this information to obtain the maximum subset sum of the entire array. An example is shown below:\n-4\n6\n3\n-7\n-6\n2\n5\n-8\n-4\n6\n3\n-7\n-6\n2\n5\n-8\n-4\n6\n3\n-7\nMax Subarray Sum: 9\n-6\n2\n5\n-8\nMax Subarray Sum: 7\n-4\n6\n3\n-7\n-6\n2\n5\n-8\nMax Subarray Sum: max(9, 7) = 9", "word_count": 384, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bcbdef83-bbb3-5e55-bbd3-008a40cf634b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 820, "real_page_number": null, "text": "808\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nIf you paid close attention, you may have noticed that this procedure does fully work. This is because it fails to consider the case where thenot\nmaximum subarray sum includes values from both subarrays. However, this issue can be resolved with a simple fix: instead of only finding the\nmaximum subarray sum of the left and right halves, we will also find the maximum sum of any subarray that crosses the boundary between the\nsubarrays. This \"crossing sum\" can be calculated by finding the maximum contiguous sum in the left subarray that includes itsleft and right\nrightmost element, as well as the maximum contiguous sum in the right subarray that includes its leftmost element, and then summing these two\nvalues together. An example is shown below:\n-4\n6\n3\n-7\n8\n2\n5\n-8\n-4\n6\n3\n-7\n8\n2\n5\n-8\n-4\n6\n3\n-7\nMax Subarray Sum: 9\nMax Sum w/ Last Value: 2\n8\n2\n5\n-8\nMax Subarray Sum: 15\nMax Sum w/ First Value: 15\n-4\n6\n3\n-7\n8\n2\n5\n-8\nMax Subarray Sum: max(9, 7, 2 + 15) = 17\nTherefore, the maximum subarray problem can be solved using a divide-and-conquer approach as follows:\n1. Divide the given input array into two halves.\n2. Return the maximum of the following three values:\n• The maximum subarray sum of the left subarray.\n• The maximum subarray sum of the right subarray.\n• The maximum subarray sum of any subarray that includes elements from both sides.\nAn implementation of this solution is shown below:\n1\nint32_t max_subarray_sum(const std::vector<int32_t>& nums) {\n2\nreturn max_sum_helper(nums, 0, nums.size());\n3\n} // max_subarray_sum()\n4\n5\n// inclusive left, exclusive right\n6\nint32_t max_sum_helper(const std::vector<int32_t>& int32_t int32_tnums, left, right) {\n7\nif (left == right - 1) {\n8\nreturn nums[left];\n9\n} // if\n10\nint32_t mid = left + (right - left) / 2;\n11\nint32_t current_sum = 0;\n12\nint32_t std::numeric_limits<int32_t>::min();best_left_sum_with_mid =\n13\nint32_t std::numeric_limits<int32_t>::min();best_right_sum_with_mid =\n14\nfor (int32_t i = mid - 1; i >= left; --i) {\n15\ncurrent_sum += nums[i];\n16\nbest_left_sum_with_mid = std::max(best_left_sum_with_mid, current_sum);\n17\n} // for i\n18\ncurrent_sum = 0;\n19\nfor (int32_t j = mid; j < right; ++j) {\n20\ncurrent_sum += nums[j];\n21\nbest_right_sum_with_mid = std::max(best_right_sum_with_mid, current_sum);\n22\n} // for j\n23\nint32_t best_crossing_sum = best_left_sum_with_mid + best_right_sum_with_mid;\n24\nint32_t max_left_sum = max_sum_helper(nums, left, mid);\n25\nint32_t max_right_sum = max_sum_helper(nums, mid, right);\n26\nreturn std::max({max_left_sum, max_right_sum, best_crossing_sum});\n27\n} // max_sum_helper()", "word_count": 428, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "360ba83e-02f5-5605-9362-d6d01a19b3d5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 821, "real_page_number": null, "text": "21.4 Divide-and-Conquer\n809\nWhat is the time complexity of this divide-and-conquer approach? The process of finding the maximum sum that includes values from both\nsubarrays (lines 14-23) requires a worst-case iteration of the subarray, starting from the boundary edge. Then, two recursive calls areΘ(𝑛)\ncompleted (lines 24-25) with half the input size. All remaining work takes constant time. Putting this all together, the total work done can be\nexpressed using the following recurrence relation:\n𝑇(𝑛) 2𝑇(𝑛∕2)+Θ(𝑛)=\nΘ(𝑛𝑐log(𝑛))Using the Master Theorem with 2, 2, and 1, we can conclude that the time complexity of this new solution is𝑎= 𝑏= 𝑐= =\nΘ(𝑛1 Θ(𝑛2)withrespecttothesizeoftheoriginalarray𝑛. Thisisanimprovementoverthe timecomplexityofthepreviouslog(𝑛)) Θ(𝑛log(𝑛))=\nbrute force approach.\n¸ 21.4.5\n(✽)The Maximum Subarray Problem Using Kadane’s Algorithm\nHowever, there is actually a way to do better than the time complexity of a divide-and-conquer approach. The optimal solution to theΘ(𝑛log(𝑛))\nmaximum subarray problem utilizes an algorithm known as Kadane’s algorithm. Kadane’s algorithm incrementally calculates the maximum\nsubarray sum ending at each index of the original input array. That is, it calculates the maximum sum attainable from any subarray whose last\nelement is at index 0, then the maximum sum of any subarray whose last element is at index 1, then the maximum sum of any subarray whose\nlast element is at index 2, and so on. The largest of these values would then be the solution to the entire maximum subarray problem.\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nFor example, the maximum subarray ending at index 6 is [9, -8, 2], since this subarray yields a larger sum than all other subarrays that end at\nindex 6, as illustrated below:\n• sum([5, 1, -3, -4, 9, -8, 2]) = 2\n• sum([1, -3, -4, 9, -8, 2]) = -3\n• sum([-3, -4, 9, -8, 2]) = -4\n• sum([-4, 9, -8, 2]) = -1\n• sum([9, -8, 2]) = 3 →best!\n• sum([-8, 2]) = -6\n• sum([2]) = 2\nAt this point, you may be wondering: isn’t this just a rehash of the brute force approach, since you have to generate and compare all the subarrays\nΘ(𝑛2)that end at each index? Although the ideas are similar, Kadane’s algorithm is not the same as the brute force approach, as it takes\nadvantage of a key optimization: if you already know the maximum subarray sum of any subarray ending at index 𝑘, you can calculate the\nmaximum subarray sum ending at index in time.𝑘+1 constant\nTo understand why this works, consider the following example. Suppose we are given an array, and we want to find the maximum subarray\n𝑘≥0.sum of any subarray that ends at an arbitrary index 𝑘+1, where Assume we know that the value at index is some number 𝑉𝑘+1, and𝑘+1\nthat the maximum subarray sum that ends at index 𝑘is some number 𝑆𝑘.\n𝑉𝑘+1\n𝑘+1\n𝑆𝑘\n⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞\n⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞\nWe make the claim that the maximum subarray sum ending at index 𝑘+1, or 𝑆𝑘+1, must be the larger of:\n• 𝑉𝑘+1\n• 𝑆𝑘+𝑉𝑘+1\nWhymustthisbetrue? Assumethatthemaximumsubarraysumendingatindex isneither nor𝑆𝑘+𝑉𝑘+1, butrathersomeothervalue𝑘+1 𝑉𝑘+1\n≠𝑉𝑘+1,𝑇𝑘+1. Weknowthat so mustbeacombinationof andsomeothersubarraysumendingatindex𝑘,whichwewilldenoteas𝑇𝑘+1 𝑇𝑘+1 𝑉𝑘+1\n𝑇𝑘≠𝑆𝑘, ≠𝑆𝑘+𝑉𝑘+1.𝑇𝑘. However,wealsoknowthat or wouldbeequalto𝑆𝑘+𝑉𝑘+1,andweinitiallyassumedthat Thismeansthat𝑇𝑘+1 𝑇𝑘+1\n𝑇𝑘is a subarray sum ending at index 𝑘, but the maximum subarray sum ending at index 𝑘. This implies that 𝑆𝑘+𝑉𝑘+1.𝑇𝑘+1 𝑇𝑘+𝑉𝑘+1 <=not\nSince is always less than 𝑉𝑘+1, it would be impossible for to be the maximum subarray sum ending at index a𝑇𝑘+1 𝑆𝑘+ 𝑇𝑘+1 𝑘+1 …\ncontradiction! Thus, if the maximum subarray sum ending at index is not 𝑉𝑘+1, it must be the sum of and the maximum subarray𝑘+1 𝑉𝑘+1\nsum ending at index 𝑘, or 𝑆𝑘+𝑉𝑘+1.", "word_count": 712, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "46b53cea-a8a7-568c-86f2-72fa7b96452f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 822, "real_page_number": null, "text": "810\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nTherefore, to calculate the maximum subarray sum ending at index 𝑘+1, we only need to compare the values of and 𝑆𝑘+𝑉𝑘+1. This can𝑉𝑘+1\nbe done in constant time if we already know the value of 𝑆𝑘. Consider the same array as before:\n5\n1\n-3\n-4\n9\n-8\n2\n-1\n8\n-5\n-6\n2\n3\n-4\n-1\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nUsing the example above, we would iterate over the array and calculate the following values using Kadane’s algorithm:\n• The maximum subarray sum ending at index 0 is 5 (the first value).\n• The maximum subarray sum ending at index 1 is max(1, 5 + 1) = 6.\n• The maximum subarray sum ending at index 2 is max(-3, 6 - 3) = 3.\n• The maximum subarray sum ending at index 3 is max(-4, 3 - 4) = -1.\n• The maximum subarray sum ending at index 4 is max(9, -1 + 9) = 9.\n• The maximum subarray sum ending at index 5 is max(-8, 9 - 8) = 1.\n• The maximum subarray sum ending at index 6 is max(2, 1 + 2) = 3.\n• The maximum subarray sum ending at index 7 is max(-1, 3 - 1) = 2.\n• The maximum subarray sum ending at index 8 is max(8, 2 + 8) = 10 →best!\n• The maximum subarray sum ending at index 9 is max(-5, 10 - 5) = 5.\n• The maximum subarray sum ending at index 10 is max(-6, 5 - 6) = -1.\n• The maximum subarray sum ending at index 11 is max(2, -1 + 2) = 2.\n• The maximum subarray sum ending at index 12 is max(3, 2 + 3) = 5.\n• The maximum subarray sum ending at index 13 is max(-4, 5 - 4) = 1.\n• The maximum subarray sum ending at index 15 is max(-1, 1 - 1) = 0.\n• The maximum subarray sum ending at index 16 is max(7, 0 + 7) = 7.\nThe maximum subarray sum of the entire input is the largest of these values, or 10. The code is shown below:\n1\nint32_t max_subarray_sum(const std::vector<int32_t>& nums) {\n2\nint32_t max_current_index = nums[0];\n3\nint32_t best_so_far = max_current_index;\n4\nfor (size_t i = 1; i < nums.size(); ++i) {\n5\nmax_current_index = std::max(nums[i], max_current_index + nums[i]);\n6\nif (best_so_far < max_current_index) {\n7\nbest_so_far = max_current_index;\n8\n} // if\n9\n} // for i\n10\nreturn best_so_far;\n11\n} // max_subarray_sum()\nSince Kadane’s algorithm performs a constant time operation for each index of the original input array, its runtime is Θ(𝑛).\nOur discussion of divide-and-conquer concludes here. However, this will not be the last you will see of this algorithm family in this class: we\nwill revisit this paradigm in chapter 26 when we discuss the problem.closest pair of points\nIn summary, brute force, greedy, and divide-and-conquer are just a few algorithmic patterns that can be used to approach different problems.\nHowever, this is not all; we have merely scraped the surface of all the algorithm families that will be explored in this course! In the next\nchapter, we will examine two new algorithm families that at times have similar implementations, but are used to solve two different categories of\nproblems: the algorithm families of and bound.backtracking branch and", "word_count": 576, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fa00c6a0-4110-5dc3-ab4e-847e5cb949b4", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 823, "real_page_number": null, "text": "21.4 Divide-and-Conquer\n811\nChapter 21 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Which of the following statements is/are TRUE regarding the brute force algorithmic approach?\nI. A correctly implemented brute force algorithm may still fail to find the optimal solution to an optimization problem.\nΘ(2𝑛).II. Given any problem with an input size of 𝑛, a brute force solution for that problem can never exceed a time complexity of\nIII. Brute force algorithms may need to rely on immense computational power to work and can take significant time to run.\nA) II only\nB) III only\nC) I and III only\nD) II and III only\nE) I, II, and III\n2. Which of the following is an optimization problem?\nA) Finding the number of ways to schedule rooms for final exams without any conflicts\nB) Finding the ideal number of gadgets that should be produced at a factory to maximize profits\nC) Finding a path out of a complicated maze\nD) Finding the MST of a connected graph\nE) More than one of the above\n3. A greedy approach can always be used to solve an optimization problem as long as:\nI. The problem has an optimal substructure.\nII. The problem has a unique solution.\nIII. The problem satisfies the greedy-choice property.\nA) III only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n4. Consider the greedy solution to the coin-change problem, in which the minimal number of coins needed to return any change amount is\ncomputed by greedily selecting the highest-value coin that does not bring you over the desired amount. For which of the following coin\ndenominations would the greedy approach succeed in finding the optimal solution to this problem?always\nI. {1¢, 5¢, 10¢, 25¢}\nII. {1¢, 8¢, 16¢, 20¢}\nIII. {1¢, 3¢, 9¢, 27¢}\nA) I only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n5. Which of the following statements is/are TRUE?\nI. If an optimization problem can be solved with a greedy approach, and the first greedy choice adds an element 𝑥to the solution,\nthen there is still a chance that no optimal solution to the problem includes the element 𝑥.\nII. If an optimal solution to an optimization problem can be constructed efficiently from optimal solutions of its subproblems, then a\ngreedy approach is guaranteed to work on that problem.\nIII. Mathematicalinductionisausefultechniquethatcanbeusedtoprovetheefficacyofagreedyapproachwhensolvingoptimization\nproblems.\nA) II only\nB) III only\nC) I and III only\nD) II and III only\nE) I, II, and III", "word_count": 498, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "e8f431d2-5568-53c8-8e4b-ea967fcf55db", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 824, "real_page_number": null, "text": "812\nChapter 21. Greedy Algorithms and Divide-and-Conquer\n6. Suppose you decided to help EECS 281 students by offering special 1-on-1 tutoring sessions on the day before the final exam. You send out\na sign-up form to every EECS 281 student in the class that allows them to sign up for these tutoring sessions. On this form, each student can\nsubmit their ideal start time and the expected duration of their 1-on-1 session (e.g., a student requesting an 11 AM start time and a duration\nof 45 minutes would like to have a session that lasts from 11 AM to 11:45 AM). Given multiple student requests, which of the following\ngreedy algorithms would allow you to help the greatest number of students? You may assume that all requests are restricted betweenalways\n8 AM and 8 PM on the day before the final exam.\nA) Greedily selecting the available student requests with the earliest desired start time\nB) Greedily selecting the available student requests with the earliest desired end time\nC) Greedily selecting the available student requests with the latest desired end time\nD) Greedily selecting the available student requests with the smallest desired duration\nE) More than one of the above\n7. Suppose you are beginning a road trip in a car that can go at most 𝑚miles on a full tank. Given an vector of 𝑛integers representingunsorted\nthe distances of gas stations on your route (in miles) from your starting position, what is the worst-case time complexity of identifying the\nminimum number of stops you can make to travel to a destination 𝑑miles away without running out of gas (assuming you start with a full\ntank), if you use the most efficient algorithm?\nA) Θ(𝑛)\nB) Θ(𝑑+𝑛)\nC) Θ(𝑑𝑛)\nD) Θ(𝑛log(𝑛))\nE) Θ(𝑑𝑛log(𝑛))\n8. You decide to use a brute force algorithm to calculate the shortest distance between two vertices in a graph. Using this strategy, you obtain a\nsolution of 281. If your brute force algorithm is implemented correctly, which of the following can you safely assume?\nA) The optimal distance is exactly 281\nB) The optimal distance may be less than 281\nC) The optimal distance may be greater than 281\nD) All paths between the two vertices have a distance of 281\nE) We cannot safely assume any of the above statements\n9. Your friend is trying to solve the coin change problem to minimize the number of coins needed to make change given a set of coin\ndenominations. They have written two algorithms to solve the problem: one that uses brute force, and one that uses the greedy approach of\nalways selecting the highest denomination coin that is available. Assuming that both algorithms are implemented correctly, which of the\nfollowing outcomes is/are NOT possible when solving for the same target change amount with the same coin denominations?\nA) The brute force solution returns 281, and the greedy solution returns 280\nB) The brute force solution returns 281, and the greedy solution returns 281\nC) The brute force solution returns 281, and the greedy solution returns 282\nD) Both (A) and (C)\nE) All of (A), (B), and (C)\n10. Which of the following statements is FALSE regarding the divide-and-conquer algorithm family?\nA) Divide and conquer algorithms often have time complexities that involve a logarithmic term (e.g., log(𝑛))\nB) The Master Theorem can be a useful tool for analyzing the time complexity of divide-and-conquer algorithms\nC) Divide-and-conquer works best if a problem can be split up into multiple subproblems whose solutions depend on each other\nD) Divide-and-conquer algorithms can often be parallelized, where work is distributed to different processors and done simultaneously\nE) None of the above\n11. Which of the following problems can be solved using divide-and-conquer?\nA) Given a one-dimensional array of both positive and negative numbers, identify the subarray of numbers that produces the largest sum\nB) Given an array of integers, sort the array in ascending order\n𝑚𝑛C) Given two integers 𝑚and 𝑛, compute the value of\nD) Given multiple points on a two-dimensional plane, find the two points that are closest to each other\nE) All of the above\n12. Which of the following sorting algorithms utilize a divide-and-conquer (or combine-and-conquer) approach?\nI. Mergesort\nII. Quicksort\nIII. Heapsort\nA) I only\nB) II only\nC) I and II only\nD) I and III only\nE) I, II, and III", "word_count": 730, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ca5cc54c-1e5b-5539-94ab-e2c4a26b7cff", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 825, "real_page_number": null, "text": "21.4 Divide-and-Conquer\n813\n𝑖thduration duration[i]13. You are given a list of 𝑛tasks, as well as a vector where is the duration of the task in minutes. Given you\ntotal_timehave minutes to work on the tasks, implement a function that returns the maximum number of distinct tasks that you can\ntotal_timecomplete with minutes. The tasks can be completed in any order.\nint32_t max_tasks(const std::vector<int32_t>& int32_tduration, total_time);\nduration = [1, 4, 2, 3, 1] total_time = 7,Example: Given and you would return 4, since you can complete all but the\njob with duration 4 in the given time (1 + 2 + 3 + 1 = 7).\nLecture14. Consider the following entity, which stores the start and end time of a given lecture:\nstruct Lecture {\nint32_t begin_time;\nint32_t end_time;\n};\n𝑛LectureYou are given a vector of entities. Implement a function that returns the minimum number of classrooms needed to schedule all\nthe lectures such that no two lectures occur at the same time in the same room. Note that it is valid for two lectures to happen back-to-back\n(e.g., if one lecture ends at time 5 and another starts at time 5, they can be sent to the same classroom). You may assume that the begin and\nend times are all valid (i.e., begin time < end time).\nint32_t min_classrooms_needed(const std::vector<Lecture>& lectures);\nExample: Given lectures with the following beginning and ending times: [0, 2], [0, 2], [0, 3], [2, 3], [2, 5], [4, 5], you would return 3, since\nit is possible to schedule all these lectures using just 3 classrooms. One potential arrangement is shown below:\n0\n1\n2\n3\n4\n5\n15. You are attending a multi-day programming conference where many guest lecturers are scheduled to give presentations. Consider the\nPresentationfollowing entity, which stores the start and end dates of a given presentation:\nstruct Presentation {\nint32_t begin_date;\nint32_t end_date;\n};\nIn this case, a lecturer will give the same presentation on all days between the begin date and end date, inclusive. For example, if begin date\nis 2 and end date is 4 for a given presentation, that means the presentation will be given on days 2, 3, and 4. You may only attend one\nPresentationpresentation on any given day. Given a vector of entities, implement a function that returns the maximum number of\ndistinct presentations that you will be able to attend at the conference.\nint32_t max_presentations(const std::vector<Presentation>& presentations);\nExample: Given presentations with the following beginning and ending dates: [1, 2], [2, 2], [2, 3], [3, 4], [4, 4], you would return 4, since\nit is possible for you to attend at most 4 of these presentations (the shaded section represents the day you attend a presentation):\n1\n2\n3\n4", "word_count": 464, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3d9205ce-ce29-5bf5-bc2c-5c73ccc4aece", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 826, "real_page_number": null, "text": "814\nChapter 21. Greedy Algorithms and Divide-and-Conquer\nChapter 21 Exercise Solutions\n1. The correct answer is (C). A brute force algorithm, when implemented correctly, would check all possible solutions before deciding on the\nsolution that solves the problem. This means that a brute force approach can ensure that the optimal solution is found, so statement I is false.\n2𝑛,Statement II is not guaranteed to be true either depending on the problem to be solved — if the number of possible solutions exceeds\nΘ(2𝑛)then the time complexity of brute force can exceed (one example is the Traveling Salesperson Problem (TSP), which has a brute\nforce time complexity of Θ(𝑛!), which we will discuss in more detail in the following chapter). Statement III is true: since brute force needs\nto check the entire solution space, it is computationally intensive and may take significant time to run (and if a machine is not powerful\nenough, a brute force algorithm may end up taking an infeasible amount of time to complete).\n2. The correct answer is (E). Both (B) and (D) are optimization problems: (B) seeks to find the maximum profits under a set of constraints,\nwhile (D) seeks to find the minimum total weight required to connect all vertices in a graph.\n3. The correct answer is (C). If a problem has an optimal substructure and satisfies the greedy-choice property, we can use mathematical\ninduction to prove the correctness of a greedy approach.\n4. The correct answer is (C). The denominations in I are the just the standard coin denominations in real life, which we proved can be solved\nusing a greedy approach in example 21.4. The denominations in II do not work for 24¢, since the greedy solution would return 4 coins (20¢,\n1¢, 1¢, 1¢) when the optimal solution is 3 coins (8¢, 8¢, 8¢). We can prove that III is optimal using the same exchange argument that we\nused to prove I: we know the number of 1¢ coins in our solution must be less than 3, since otherwise we can replace these coins with a\nsingle 3¢ coin. We also know that the number of 3¢ and 9¢ coins are each less than 3 as well, for the same reason. Using a proof by cases,\nwe can show that:\n• If the amount of change is greater than 27¢, then the greedy choice adds this coin to the solution. If this is not optimal, there must be\na way to return a change amount above 27¢ without a 27¢ coin. This involves at least 3 coins worth of 9¢, 3¢, or 1¢ coins, which\nwould not be optimal.\n• If the amount of change is between 9¢ and 26¢, then the greedy choice would be to add a 9¢ coin to our solution. If this is not optimal,\nthere must be a way to return this change amount without a 9¢ coin. This would require at least 3 coins worth of 3¢ or 1¢ coins,\nwhich would not be optimal.\n• If the amount of change is between 3¢ and 9¢, then the greedy choice would be to add a 3¢ coin to our solution. If this is not optimal,\nthere must be a way to return this change amount without a 3¢ coin. This would require at least 3 coins worth of 1¢ coins, which\nwould not be optimal.\nIf the amount of change is less than 3¢, we only have one type of coin that can be used, which trivially implies that the optimal•\nsolution must contain this coin.\n5. The correct answer is (B). Statement I is false because a greedy approach does not look back and undo any choices, so if a greedy approach\nis known to work and 𝑥is added to the solution, there must be at least one optimal solution that includes 𝑥. Statement II is false because an\noptimal substructure alone cannot prove that greedy always works: we also need to show that the greedy-choice property holds, and that\nevery greedy choice we make has the potential to be a valid solution. Statement III is true, since mathematical induction can be used to\nshow that a greedy approach is valid after identifying that the greedy-choice property and an optimal substructure holds for a given problem.\n6. The correct answer is (B). This is a variation of the activity selection problem introduced in section 21.3.3. The correct greedy approach\nfor this type of problem is to select the request with the earliest end time.\n7. The correct answer is (D). This is the breakpoint selection problem introduced in section 21.3.4: to solve this problem, we sort the gas\nstations in ascending order and greedily select the last gas station we can reach before running out of gas. The bottleneck of this algorithm\nis the sorting step, which takes time.Θ(𝑛log(𝑛))\n8. The correct answer is (A). A brute force solution examines all possible solutions, so if returns 281 as the solution, then 281 must be the\noptimal solution (otherwise it would have discovered a solution better than 281).\n9. The correct answer is (A). The brute force approach examines all possible solutions, so it is guaranteed to return the best solution if\nimplemented correctly. Thus, it is not possible for a greedy approach to do better, so choice (A) is not possible.\n10. The correct answer is (C). Divide-and-conquer works best when a problem can be split into subproblems that are independent from each\nother, which allows the input to be broken up, solved individually, and combined together without having to worry about dependencies\nbetween subproblems. If there is a dependency between different subproblems, then another algorithmic approach my work better (e.g.,\ndynamic programming, which will be discussed in a later chapter).\n11. The correct answer is (E). All of the above can be solved using divide-and-conquer. Option (A) was demonstrated in section 21.4.4, option\n(B) can be solved using a divide-and-conquer sorting algorithm like mergesort or quicksort, option (C) was demonstrated in section 21.4.2,\nand option (D) was briefly mentioned at the end of the chapter (and will be covered in section 26.3).\n12. The correct answer is (C). Mergesort and quicksort are two divide-and-conquer sorting algorithms, since they break up the input into\nsmaller subproblems, solve those subproblems independently, and then combine the subproblems to obtain the final solution (i.e., a fully\nsorted array).", "word_count": 1066, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "331c794a-ce7e-5745-a9a6-7715e6c0d24b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 827, "real_page_number": null, "text": "21.4 Divide-and-Conquer\n815\n13. The greedy solution to this problem is to sort all the tasks in order from least to most time required, and then greedily select the tasks that\nrequire the least amount of time. The proof for this is pretty intuitive — if somehow the optimal solution does not include the task that\ntakes the least amount of time, we can obtain a solution that is no worse by swapping out a longer task with the one that takes the least\namount of time (a contradiction). One possible solution is implemented below:\n1\nint32_t max_tasks(const std::vector<int32_t>& int32_tduration, total_time) {\n2\nint32_t num_tasks = 0;\n3\nstd::vector<int32_t> sorted_durations(duration);\n4\nstd::sort(sorted_durations.begin(), sorted_durations.end());\n5\nfor (int32_t curr_duration : sorted_durations) {\n6\nif (curr_duration > total_time) {\n7\nbreak;\n8\n} // if\n9\ntotal_time -= curr_duration;\n10\n++num_tasks;\n11\n} // for i\n12\nreturn num_tasks;\n13\n} // max_tasks()\nThe greedy solution to this problem is to sort all the lectures in order of starting time, and then iterate over the lectures and assign them to14.\nan available classroom (or allocate a new one if all the existing classrooms are full). This is guaranteed to identify the maximum number of\nlectures in conflict at any given time, since a room will only be assigned under this approach if 𝑑other classrooms are occupied. Note𝑑+1\nthat if the maximum number of lectures that conflict with each other at any given time is 𝑑, then all possible solutions to the problem must\nrequire at least 𝑑classrooms (since anything less than 𝑑would cause two lectures to overlap), which makes 𝑑the optimal solution. One\npossible implementation is shown below: here, we use a priority queue to keep track of the end times of all the lectures at any point in time.\nWhenever we want to add a new lecture, we remove any lectures that may have completed and add the new lecture in, keeping track of the\nlargest size the priority queue was able to reach.\n1\nstruct LectureCompare {\n2\nbool operator() (const constLecture& lhs, Lecture& rhs) {\n3\nreturn lhs.begin_time < rhs.begin_time;\n4\n} // operator()()\n5\n};\n6\n7\nint32_t min_classrooms_needed(const std::vector<Lecture>& lectures) {\n8\nstd::vector<Lecture> sorted_lectures(lectures);\n9\nLectureCompare comp;\n10\nstd::sort(sorted_lectures.begin(), sorted_lectures.end(), comp);\n11\n12\nint32_t min_classrooms = 0;\n13\nstd::priority_queue<int32_t, std::vector<int32_t>, std::greater<int32_t>> pq;\n14\nfor (const Lecture& lecture : sorted_lectures) {\n15\nwhile (!pq.empty() && pq.top() <= lecture.begin_time) {\n16\npq.pop();\n17\n} // while\n18\npq.push(lecture.end_time);\n19\nstatic_cast<int32_t>(pq.size()));min_classrooms = std::max(min_classrooms,\n20\n} // for\n21\n22\nreturn min_classrooms;\n23\n} // min_classrooms_needed()", "word_count": 429, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7ee16c5c-bd41-5846-a6ad-76cc65d7e577", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 828, "real_page_number": null, "text": "816\nChapter 21. Greedy Algorithms and Divide-and-Conquer\n15. The greedy solution to this problem is to sort all the presentations in order of starting time, and then iterate over the presentations to identify\nwhich one you should attend for each day. If there are multiple presentations that you can attend on a single day, you would greedily select\nthe presentation with the earliest end date. Why does this work? Suppose there are two presentations that you attend on any given day,\ndenoted as and 𝑃2, where ends before 𝑃2. Let’s say that the optimal solution is to select instead of on this date. However, in𝑃1 𝑃1 𝑃2 𝑃1\nthis case, we can always construct another schedule that chooses instead of that is no worse than the schedule that chose 𝑃2.𝑃1 𝑃2\n• If can be chosen on some later day, we can always swap and and still be able to attend both presentations (since if we can𝑃1 𝑃1 𝑃2\nchoose 𝑃1, we can also choose since we know it ends later).𝑃2\n• If cannot be chosen on some later day, we can replace with when making our choice and still end up with a solution that is𝑃1 𝑃2 𝑃1\nno worse than the outcome we obtained from making the opposite choice.\nThus, it is optimal to always choose the presentation that ends sooner, if there are multiple presentations that you can attend on a single day.\nThe implementation to this problem can be done very similarly to the previous problem: we sort the presentations in order of start time, and\nthen for each day you can attend a presentation, we push in the available presentation end times into a min-priority queue and then choose\nthe available presentation at the top of the priority queue. An implementation is shown below:\n1\nstruct PresentationCompare {\n2\nbool operator() (const constPresentation& lhs, Presentation& rhs) {\n3\nreturn lhs.begin_date < rhs.begin_date;\n4\n} // operator()()\n5\n};\n6\n7\nint32_t max_presentations(const std::vector<Presentation>& presentations) {\n8\nstd::vector<Presentation> sorted_presentations(presentations);\n9\nPresentationCompare comp;\n10\nstd::sort(sorted_presentations.begin(), sorted_presentations.end(), comp);\n11\nstd::priority_queue<int32_t, std::vector<int32_t>, std::greater<int32_t>> pq;\n12\n13\nint32_t max_day = 0;\n14\nfor (const Presentation& presentation : sorted_presentations) {\n15\nmax_day = std::max(max_day, presentation.end_date);\n16\n} // for\n17\n18\nsize_t idx = 0;\n19\nint32_t solution = 0;\n20\nfor (int32_t day = 1; day <= max_day; ++day) {\n21\nwhile (!pq.empty() && pq.top() < day) {\n22\npq.pop();\n23\n} // while\n24\nwhile (idx < sorted_presentations.size() && sorted_presentations[idx].begin_date == day) {\n25\npq.push(sorted_presentations[idx].end_date);\n26\n++idx;\n27\n} // while\n28\nif (!pq.empty()) {\n29\npq.pop();\n30\n++solution;\n31\n} // if\n32\n} // for day\n33\n34\nreturn solution;\n35\n} // max_presentations()", "word_count": 453, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6b5b677d-b1d7-5238-b52b-8fb357dc5c92", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 829, "real_page_number": null, "text": "Chapter 22\nBacktracking and Branch and Bound\n22.1\nBacktracking\n¸ 22.1.1\nIntroduction to Backtracking\nBacktracking is an algorithm family that can be used to solve problems. Unlike optimization problems, which wereconstraint satisfaction\nintroduced in the previous chapter, constraint satisfaction problems are only concerned with whether there a solution that satisfies allexists\nconstraints, and not what the best solution is. The backtracking approach can often be applied to solve problems that ask you to either find a\nsingle solution or enumerate over all solutions that satisfy a given set of constraints.\nSimilar to the brute force approach, backtracking algorithms are designed to explore all possible solutions to determine if any of them\nsatisfy a given set of constraints. However, unlike brute force, backtracking stops checking a partial solution constraint.as soon as it violates any\nThis process of removing partial solutions that cannot lead to a valid solution is known as pruning.\nTo demonstrate how backtracking works, let’s start with a simple example with no constraints. Suppose you have three marbles that you\nhave to place in a straight line: a red marble (𝑅), and blue marble (𝐵), and a yellow marble (𝑌). Your goal is to write a function that prints out\nall the different ways these marbles can be placed, assuming they can be placed in any order.\nThe simplest way to solve this problem is to consider all possible choices that can be made at each step. For the first marble, we can either\nchoose 𝑅, 𝐵, or 𝑌. If we choose 𝑅as our first marble, we can choose 𝐵or 𝑌as our second marble. If we choose 𝐵as our first marble, we can\nchoose 𝑅or 𝑌as our second marble. If we choose 𝑌as our first marble, we can choose 𝑅or 𝐵as our second marble (and so on). Since we are\nmaking a series of decisions that bring us toward a solution, we can represent the solution set to the problem in the form of a tree, where each\nbranch represents a choice that was made to reach a solution (this type of tree is known as a tree).state-space", "word_count": 353, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1f93e864-fb0b-5f20-a4e4-d7571552c502", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 830, "real_page_number": null, "text": "818\nChapter 22. Backtracking and Branch and Bound\nThe state-space tree for the marble problem is shown below:\n𝑅\n𝐵\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nSince we have no constraints on how the marbles can be placed, we can obtain all orderings of these three marbles by performing a depth-first\nsearch on this tree. Every time we hit a leaf node, we know that we have reached a valid outcome of our problem.\nRemark: Note that the state-space tree is not actually a physical tree that is stored in memory, but rather an abstract representation of the\nsolution space of our problem. The backtracking algorithm does not necessarily involve physical tree objects; rather, the state-space tree is\nonly used to demonstrate that the behavior of backtracking is similar to a depth-first search over a solution space, as if the solution space were\nrepresented as a tree where nodes represent partial input states and branches represent choices.\nNow, let’s add a constraint to this problem. Consider the same scenario as before, but this time, suppose the red marble must be in thealways\nmiddle position. Notice how our tree changes when this constraint is added — certain branches no longer lead us to a valid solution! As a result,\nthere is no need to explore these branches during our depth-first search. This is the core difference between backtracking and brute force —\nbrute force would check every branch, even ones that do not work, while backtracking would stop checking a branch as soon as it realizes the\nbranch cannot lead to a viable solution.\n𝑅\n𝐵\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nThis idea forms the foundation of the backtracking approach, which performs a depth-first search on the solution space and branches thatprunes\ncannot lead to a valid solution. Backtracking algorithms essentially boil down to the following four steps:\n1. Make a choice that leads you toward a solution (which moves you down a level of the state-space tree).\n2. Check if the choice is promising (that is, if it can lead you to a valid solution that satisfies the constraints). If it is, fix the choice you just\nmade and recurse on the subproblem that remains — this allows you to explore further down the branch until you either obtain a valid\nsolution or discover that the path no longer satisfies the given constraints.\n3. Check if the current partial solution is a valid complete solution to the problem. If it is, you have a solution. If the problem only asks for\none solution, you are done. If you are asked for all solutions, keep track of the solution you obtained.\n4. Undo the choice you most recently made (i.e., backtrack) so that you can explore other branches.\nTo demonstrate this process, consider these steps when applied to the marble problem, under the constraint that the red marble must be in the\nmiddle. First, our algorithm will make a choice. It doesn’t matter what choice we make as long as it brings us closer to a solution, so for the\nsake of simplicity, we will always choose a marble for the leftmost position available.\n𝑅\n𝐵\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nThere are three choices we can make here: we can either place the red marble, the blue marble, or the yellow marble in the first position. Each\nof these choices represents a branch of our state-space tree. For our example, we will choose the red marble first (you can choose any marble\nhere since all the branches will need to be explored eventually, but for consistency, we will go with the order 𝑅, then 𝐵, then 𝑌):\n𝑅\n𝐵\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅", "word_count": 719, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7fb5d872-6567-5256-bb5c-957189accf8c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 831, "real_page_number": null, "text": "22.1 Backtracking\n819\nAfter making a choice, we then check if the choice is promising. In this case, the choice is promising, as it violates the constraint that the rednot\nmarble must be in the middle. Thus, we can all the solutions down this path, since we know that none of them will yield a valid solution.prune\n𝑅\n✕\n𝐵\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nNow, we backtrack by undoing the choice we just made. This allows us to explore another branch by making a different choice.\n𝑅\n✕\n𝐵\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nWe then repeat this process until we have considered all viable branches of the tree. Since we have already considered the choice of the red\nmarble, we will now choose the blue marble for the leftmost position.\n𝑅\n✕\n𝐵\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nAgain, we will check if the choice is promising. Here, the choice is indeed promising, as it does not violate any constraints. Thus, we will\nrecurse down this branch by fixing the blue marble in place and making a choice for the marble in the middle. There are two choices we can\nmake: the red marble and the yellow marble. Here, we will first consider the red marble.\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nThis choice is promising, so we recurse down the branch by making a choice for the final marble. Only the yellow marble is left, so we will\nchoose the yellow marble for this position.\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nWe have made a choice for all the positions and thus have a full solution. This solution does not violate any constraints, so it is a valid solution\nfor our problem. If our goal was to simply discover if a solution exists, then we would be done! However, since we want to enumerate over all\nsolutions, we will continue searching. To do so, we keep track of this solution (usually in a container of solutions) and then undo the choice of\nthe final marble so that we can explore other branches.\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅", "word_count": 515, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5c475b1d-9402-56f3-9a54-fcdaa84b1b5b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 832, "real_page_number": null, "text": "820\nChapter 22. Backtracking and Branch and Bound\nWe have considered all possible choices for the third marble, so there are no more branches we have to consider for the case where 𝐵and 𝑅take\nup the first two positions. We then undo the choice of the second marble to move up a level of the tree.\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝐵\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nSince we have finished considering the red marble in the middle position, we will now consider the yellow marble.\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝐵\n𝑌\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nHowever, this choice is promising, so we will prune off all solutions that begin with the blue and yellow marbles.not\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝐵\n𝑌\n✕\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nWe then undo the choice and return back to the state where only the blue marble has been placed. At this point, we have considered all possible\nscenarios where the blue marble is in the first position, so we can undo that choice as well.\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝐵\n𝑌\n✕\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅\nThe last marble we still need to consider for the first position is the yellow marble, so we will select it to explore our final branch.\n𝑌\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝐵\n𝑌\n✕\n𝐵\n𝑅\n𝑌\n𝑅\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n𝑅", "word_count": 340, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d221de3e-aad1-5667-ba54-b14175bcc99a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 833, "real_page_number": null, "text": "22.1 Backtracking\n821\nThis choice does not break any constraints, so we will continue recursing down its branch. Using a similar approach as before, we will consider\nboth 𝑅and 𝐵as the second marble. The branch with 𝑅as the second marble leads to a valid solution, while the branch with 𝐵as the second\nmarble gets pruned since it breaks the given constraint.\n𝑌\n𝐵\n𝑅\n𝑅\n✕\n𝐵\n𝐵\n𝑌\n✕\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝐵\n✕\n𝑅\n𝐵\n𝑅\n𝑌\n𝑅\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝐵\n𝑌\n𝑅\n𝑌\n𝐵\n𝑅\nThus, there are two valid solutions to the marble problem that satisfy the constraint that the red marble must be in the middle:\n𝐵\n𝑅\n𝑌\n𝑌\n𝑅\n𝐵\n¸ 22.1.2\nBacktracking Structure\nThe good news about backtracking solutions is that they often share a similar structure, so the logic that is used to solve one backtracking\nproblem can often be applied to solve other backtracking problems. When writing a backtracking algorithm, you typically want to do the\nfollowing four things:\n1. Identify the choice you need to make at each step to bring you closer to a solution.\nIdentifying the allows you to conceptualize the state-space tree for a backtracking problem. This step is usually relatively straightforwardchoice\nand can be inferred from the type of problem you are given (such as adding a number to a running collection, or placing a piece down on a grid).\nA rule of thumb is to think about the choices you would need to make to solve the problem if you were brute forcing it by hand, and then apply\nthat choice in a backtracking approach.\n2. Devise a mechanism for checking whether a partial solution is a valid full solution.\nYou will need a way to determine whether a partial solution could potentially be a complete solution to the entire problem. For example, in the\nmarble example from before, you would know that a solution is complete once all three marbles have been placed. If any position is empty, then\nthe partial solution cannot be a full valid solution. This check is needed so that your algorithm knows when to store a solution or exit the search.\n3. Devise a mechanism for checking whether a partial solution is promising.\nYou will need a way to determine whether a partial solution is promising, so that you can prune branches that cannot lead to a valid solution.\nThis is often done by checking a partial solution against the given constraints. Implementing a promising check is not always trivial, and can\neven be the most complicated component of a backtracking algorithm. However, this process is also the most important, since a backtracking\nalgorithm without any promising checks essentially degenerates to brute force.\n4. Using the three features above, implement a function for exploring the solution space.\nFor backtracking problems, this is often done by writing a recursive function that performs the choices needed to extend a partial solution and\nthen checks that partial solution against the given constraints. If you have all of the things above, you can typically implement this function\ncheck_node()using one of the following two templates. Here, checks the current partial solution (the function is named that way because\neach partial solution is a node of a backtracking problem’s state-space tree).\nTemplate 1 (Promising Check Before Recursive Call):\n1\nAlgorithm check_node(partial_solution):\n2\nif partial_solution is a valid full solution:\n3\nstore solution\n4\nelse:\n5\nif (promising(partial_solution)):\n6\nfor each choice that can be made to extend partial_solution:\n7\npartial_solution = partial_solution + choice\n8\ncheck_node(partial_solution)\n9\npartial_solution = partial_solution - choice\nTemplate 2 (Promising Check After Recursive Call):\n1\nAlgorithm check_node(partial_solution):\n2\nif partial_solution is not promising:\n3\nreturn\n4\nif partial_solution is a valid full solution:\n5\nstore solution\n6\nelse:\n7\nfor each choice that can be made to extend partial_solution:\n8\npartial_solution = partial_solution + choice\n9\ncheck_node(partial_solution)\n10\npartial_solution = partial_solution - choice", "word_count": 665, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3cb29203-6845-5aaf-91ff-b1c0e242e74e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 834, "real_page_number": null, "text": "822\nChapter 22. Backtracking and Branch and Bound\nBoth templates accomplish in the same thing; the only difference is that the first template checks if a partial solution is promising abefore\nrecursive call is made, while the second template checks if a partial solution is promising a recursive call is made. Either version can beafter\nused, but depending on the problem you are trying to solve, one may be easier to implement than the other. To demonstrate this strategy in\naction, we will look at a few examples of backtracking problems over the next few pages.\n¸ 22.1.3\nSolving Problems Using Backtracking: Combinations\nExample 22.1 nums, target.You are given an vector of distinct positive integers, as well as a target integer Your goal is to write a\nnums targetfunction that returns a list of all combinations of numbers in that sum to (you can return the solutions in any order).unique\nnums = [2, 3, 5] target = 8, [2, 2, 2, 2], [2, 3, 3],For example, if you are given the array and you would return\n[3, 5], numsand since these are the unique combinations of numbers in that sum up to 8.\nSince this is a constraint satisfaction problem that asks you to enumerate over all solutions that satisfy a given set of constraints, backtracking\ncan be used to implement a solution. To implement a backtracking solution, we first want to think about the we should make at each stepchoice\nnumsto bring us closer to a solution. In this case, the choice is to add a number from to a running combination of numbers, and there are three\npossible choices that we can make at each step: we can either add a 2, a 3, or a 5 to our running total. To ensure that we aren’t considering\n[2, 3] [3, 2]), numsduplicates (i.e., both and we will not add any values whose index position in is less than the index of any value\nalready in our running collection. The state-space tree for this problem is shown below:\n[]\n[5]\n[5,5]\n[5,5,5]\n⋮\n[3]\n[3,5]\n[3,5,5]\n⋮\n[3,3]\n[3,3,5]\n⋮\n[3,3,3]\n⋮\n[2]\n[2,5]\n[2,5,5]\n⋮\n[2,3]\n[2,3,5]\n⋮\n[2,3,3]\n[2,3,3,5]\n⋮\n[2,3,3,3]\n⋮\n[2,2]\n[2,2,5]\n[2,2,5,5]\n⋮\n[2,2,3]\n[2,2,3,5]\n⋮\n[2,2,3,3]\n⋮\n[2,2,2]\n[2,2,2,5]\n⋮\n[2,2,2,3]\n⋮\n[2,2,2,2]\n⋮\nIn addition to the choice, we also need a way to check if a partial solution is a valid complete solution, and if a partial solution is promising. To\ndetermine if a partial solution is complete, we just need to check if its sum is equal to our target value of 8. Similarly, because the values in\nnums are all positive, we can check if a choice is promising by looking at the sum of the new partial solution once the new value is added — a\nbranch can be pruned as soon as its sum exceeds the target value of 8.\n[]\n[5]\n[5,5]\n✕\n[3]\n[3,5]\n[3,5,5]\n✕\n[3,3]\n[3,3,5]\n✕\n[3,3,3]\n✕\n[2]\n[2,5]\n[2,5,5]\n✕\n[2,3]\n[2,3,5]\n✕\n[2,3,3]\n[2,3,3,5]\n✕\n[2,3,3,3]\n✕\n[2,2]\n[2,2,5]\n[2,2,5,5]\n✕\n[2,2,3]\n[2,2,3,5]\n✕\n[2,2,3,3]\n✕\n[2,2,2]\n[2,2,2,5]\n✕\n[2,2,2,3]\n✕\n[2,2,2,2]\n⋮\nNowthatweknowhowtocheckforwhetherapartialsolutioniscompleteorpromising, wealsoneedawaytoimplementthesechecksefficiently.\nA naïve approach to implement our promising check would be to individually sum up every partial solution we encounter. However, this method\nis quite inefficient, since each summation takes linear time on the size of the partial solution, and we end up doing a summation for every partial\nsolution we encounter in the tree. To prevent our algorithm from performing a summation at every step, we can keep track of a separate integer\nremain that stores how much we can add to a partial solution before it becomes too large. For example, if we are currently considering the\n[2, 3], remainpartial solution then would have a value of 3, since this is the most we can add to this partial solution without going over our\nremaintarget of 8. This allows us to complete a promising check in constant time, since we only need to check if is non-negative. Similarly,\nremainthis additional variable allows us to easily check if a solution is complete, since a solution is complete only if is exactly 0.", "word_count": 835, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3f09d0cc-aec8-563f-8a03-2478dced6b09", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 835, "real_page_number": null, "text": "22.1 Backtracking\n823\nNow that we have identified the choice we need to make and the procedure for checking whether the current solution is promising or complete,\nwe can use the backtracking template to write a solution for the combination sum problem.\n1\nstd::vector<std::vector<int32_t>> combination_sum(const std::vector<int>& int32_tnums, target) {\n2\nstd::vector<std::vector<int32_t>> solutions;\n3\nstd::vector<int32_t> current_partial_solution;\n4\ncheck_sum(nums, target, solutions, current_partial_solution, 0);\n5\nreturn solutions;\n6\n} // combination_sum()\n7\n8\nvoid check_sum(const std::vector<int32_t>& int32_tnums, remain,\n9\nstd::vector<std::vector<int32_t>>& solutions,\n10\nstd::vector<int32_t>& size_tcurrent_partial_solution, start_idx) {\n11\nif (remain == 0) {\n12\nsolutions.push_back(current_partial_solution);\n13\n} // if\n14\nelse {\n15\nfor (size_t i = start_idx; i < nums.size(); ++i) {\n16\ncurrent_partial_solution.push_back(nums[i]);\n17\nif (nums[i] <= remain) {\n18\ncheck_sum(nums, remain - nums[i], solutions,\ncurrent_partial_solution, i);\n19\n} // if\n20\ncurrent_partial_solution.pop_back();\n21\n} // for i\n22\n} // else\n23\n} // check_sum()\nExample 22.2 nums,Given a vector of unique integers implement a function to return all possible subsets that can be constructed using\n[1, 2, 3],this vector of integers (i.e., the power set). For example, given the vector you would return\n[ [], [1], [2], [1, 2], [3], [1, 3], [2, 3], [1, 2, 3] ]\nThis problem is similar to the combination sum problem, where we add values to a running collection (using the same rules to ensure we do not\ncheck the same subset twice). However, in this case, we do not want to add values that have already been added, since a valid subset cannot\ncontain duplicate elements. The state-space tree for this problem is shown below:\n[ ]\n[3]\n[2]\n[2, 3]\n[1]\n[1, 3]\n[1, 2]\n[1, 2, 3]\nNotice that every node of the state-space tree is a subset of the original input array — thus, we will need to keep track of every partial solution\nin our final output container. Furthermore, there are no constraints that allow us to prune off any branches in the tree, so we will need to treat\nevery branch as promising. The solution is shown below:\n1\nstd::vector<std::vector<int32_t>> power_set(const std::vector<int32_t>& nums) {\n2\nstd::vector<std::vector<int32_t>> solutions;\n3\nstd::vector<int32_t> current_partial_solution;\n4\ncheck_node(nums, solutions, current_partial_solution, 0);\n5\nreturn solutions;\n6\n} // power_set()\n7\n8\nvoid check_node(const std::vector<int32_t>& std::vector<std::vector<int32_t>>nums, &solutions,\n9\nstd::vector<int32_t>& size_tcurrent_partial_solution, start_idx) {\n10\nsolutions.push_back(current_partial_solution);\n11\nfor (size_t i = start_idx; i < nums.size(); ++i) {\n12\ncurrent_partial_solution.push_back(nums[i]);\n13\ncheck_node(nums, solutions, current_partial_solution, i + 1);\n14\ncurrent_partial_solution.pop_back();\n15\n} // for i\n16\n} // check_node()", "word_count": 416, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ba836abb-7c97-5b8f-a240-67ccf5fe0b19", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 836, "real_page_number": null, "text": "824\nChapter 22. Backtracking and Branch and Bound\nExample 22.3 Given a positive integer 𝑛, implement a function that returns all combinations of well-formed sequences that can be\nconstructed using 𝑛pairs of parentheses. For example, given 3, you would return𝑛=\n[\"((()))\", \"(()())\", \"(())()\", \"()(())\", \"()()()\"]\nSince this is a constraint satisfaction problem, it can be solved using a backtracking approach. Here, each choice would be to add a parenthesis\n(either left or right, as long as we have one available) to a running sequence of parentheses. The state-space tree for is shown below:𝑛=3\n\"\"\n\")\"\n\"))\"\n\")))\"\n\")))(\"\n\")))((\"\n\")))(((\"\n\"))(\"\n\"))()\"\n\"))()(\"\n\"))()((\"\n\"))((\"\n\"))(()\"\n\"))(()(\"\n\"))(((\"\n\"))((()\"\n\")(\"\n\")()\"\n\")())\"\n\")())(\"\n\")())((\"\n\")()(\"\n\")()()\"\n\")()()(\"\n\")()((\"\n\")()(()\"\n\")((\"\n\")(()\"\n\")(())\"\n\")(())(\"\n\")(()(\"\n\")(()()\"\n\")(((\"\n\")((()\"\n\")((())\"\n\"(\"\n\"()\"\n\"())\"\n\"()))\"\n\"()))(\"\n\"()))((\"\n\"())(\"\n\"())()\"\n\"())()(\"\n\"())((\"\n\"())(()\"\n\"()(\"\n\"()()\"\n\"()())\"\n\"()())(\"\n\"()()(\"\n\"()()()\"\n\"()((\"\n\"()(()\"\n\"()(())\"\n\"((\"\n\"(()\"\n\"(())\"\n\"(()))\"\n\"(()))(\"\n\"(())(\"\n\"(())()\"\n\"(()(\"\n\"(()()\"\n\"(()())\"\n\"(((\"\n\"((()\"\n\"((())\"\n\"((()))\"\nJust by looking at the tree, it is easy to determine which branches are promising and which are not. However, when we actually implement this\nproblem, we do not want to individually calculate whether a sequence of parentheses is balanced for every partial solution we encounter! Is\nthere a faster way to determine whether a partial solution is promising, preferably in constant time?\nTo address this, notice that a sequence of parentheses is promising only if the number of left parentheses is greater than or equal to the\nnumber of right parentheses. In addition, the number of left parentheses in the sequence cannot exceed 𝑛. Thus, to efficiently determine if a\npartial solution is promising, we just need to keep track of the number of left and right parentheses present in any partial solution. If the number\nof right parentheses exceeds the number of left parentheses, or if the number of left parentheses exceeds 𝑛, we can prune the branch of the tree.\n\"\"\n\")\"\n✕\n\"(\"\n\"()\"\n\"())\"\n✕\n\"()(\"\n\"()()\"\n\"()())\"\n✕\n\"()()(\"\n\"()()()\"\n\"()((\"\n\"()(()\"\n\"()(())\"\n\"((\"\n\"(()\"\n\"(())\"\n\"(()))\"\n✕\n\"(())(\"\n\"(())()\"\n\"(()(\"\n\"(()()\"\n\"(()())\"\n\"(((\"\n\"((()\"\n\"((())\"\n\"((()))\"\nWe can also easily determine whether we have a potential full solution by looking at the number of parentheses in our current sequence. Since a\nvalid solution involves 𝑛pairs of parentheses, the length of a valid sequence must have length 2𝑛. Using this information, we can construct the\nfollowing backtracking solution for this problem:\n1\ngenerate_parentheses(int32_tstd::vector<std::string> n) {\n2\nstd::vector<std::string> solutions;\n3\nstd::string current_partial_solution;\n4\ncheck_parentheses(n, solutions, current_partial_solution, 0, 0);\n5\nreturn solutions;\n6\n} // generate_parentheses()\n7\n8\nvoid check_parentheses(int32_t n, std::vector<std::string>& solutions,\n9\nint32_t int32_tstd::string current_partial_solution, left, right) {\n10\nif (current_partial_solution.length() == 2 n) {*\n11\nsolutions.push_back(current_partial_solution);\n12\n} // if\n13\nelse {\n14\nif (left < n) {\n15\ncheck_parentheses(n, solutions, current_partial_solution + \"(\", left + 1, right);\n16\n} // if\n17\nif (right < left) {\n18\ncheck_parentheses(n, solutions, current_partial_solution + \")\", left, right + 1);\n19\n} // if\n20\n} // else\n21\n} // check_parentheses()", "word_count": 516, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0eadbd23-f34e-5386-942e-561cc748cdb2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 837, "real_page_number": null, "text": "22.1 Backtracking\n825\n¸ 22.1.4\nSolving Problems Using Backtracking: N-Queens\nExample 22.4 In the game of chess, the is a powerful piece that can move any number of squares vertically, horizontally, orqueen\ndiagonally. The movement capability of a queen is shown on the chessboard below:\n1\na\n2\nb\n3\nc\n4\nd\n5\ne\n6\nf\n7\ng\n8\nh\nq\nThe premise of the N-Queens problem is relatively straightforward: given a chessboard of size 𝑁×𝑁, is it possible to place 𝑁queens on\nthe chessboard without any of the queens directly threatening each other? In other words, is it possible to place 𝑁queens on a 𝑁×𝑁\nchessboard such that no queen is directly in the path of another queen? Implement a function that takes in a dimension 𝑛of a 𝑛×𝑛board\nand returns all possible placements of 𝑛queens such that no queen threatens any other queen on the board.\nSince this is a constraint satisfaction problem, we can solve it using backtracking. In this problem, we want to place queens on the chessboard\nunder the constraint that any queen we place cannot threaten any other queen already on the chessboard. Let’s look at the backtracking algorithm\nin action on a 4 4 board:×\n1\na\n2\nb\n3\nc\n4\nd\nA sensible first choice is to place a queen in the first position and start exploring partial solutions from there. This choice does not break any\nconstraints, since there are no other queens currently on the chessboard.\n1\na\n2\nb\n3\nc\n4\nd\nq\nWhat should our second choice be? We could consider every empty cell from left to right, then top to bottom. If we did this, we would put a\nqueen in the second cell of the chessboard, see that it is not promising, and undo the choice.\n1\na\n2\nb\n3\nc\n4\nd\nqq\n1\na\n2\nb\n3\nc\n4\nd\nqq\n1\na\n2\nb\n3\nc\n4\nd\nq\nHowever, this is not an efficient way to explore the search space, since we know that a queen placed in the same row as another queen will\nalways violate our constraint. To reduce our search space and make the backtracking process more efficient, we can instead make choices one\nat a time. That is, after we place a queen in one row, the next choice will place a queen in the row, which eliminates the need to checkrow next\nthe horizontal constraint on a regular basis. Note that queens can be placed one column at a time as well — the idea here is to not waste time\nmaking choices that you already know will not be promising without having to make the choice!", "word_count": 457, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8cc263cd-1fb9-53f5-85a9-3f4c43c01718", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 838, "real_page_number": null, "text": "826\nChapter 22. Backtracking and Branch and Bound\nIn our example using the chessboard, after we place a queen in the top row (row 4 using offical chessboard numbering), we will then4×4\nattempt to put our next queen in row 3. We consider columns 𝑎and 𝑏, but see that they are not promising. The first position in row 3 that does\nnot violate any constraints is column 𝑐.\n1\na\n2\nb\n3\nc\n4\nd\nq\n✕\n✕q\nNext, we will attempt to place a third queen in row 2. However, we have a problem: the presence of queens in squares and makes it𝑎4 𝑐3\nimpossible to place a queen in row 2 this is not threatened by another queen!\n1\na\n2\nb\n3\nc\n4\nd\nq\n✕\n✕\n✕\n✕\nq\nTherefore, our current partial solution cannot lead to a valid solution, so we undo the choice we most recently made (the queen in square 𝑐3).\nWe then consider the next open position in row 3 that does not violate any constraints, square 𝑑3.\n1\na\n2\nb\n3\nc\n4\nd\nq\n✕\n✕\nq\nWe will now attempt to place a third queen again. With queens in squares and 𝑑3, there is one square in row 2 that is not threatened by any𝑎4\nother queens: square 𝑏2. Thus, we can safely place a queen in this position.\n1\na\n2\nb\n3\nc\n4\nd\nq\n✕\n✕\n✕q\nq\nHowever, the placement of a queen in square prevents us from placing a queen in row 1. This partial solution cannot lead to a valid solution,𝑏2\nso we undo the choice of placing a queen in square 𝑏2.\n1\na\n2\nb\n3\nc\n4\nd\nq\n✕\n✕\n✕\n✕\nq\nq\n1\na\n2\nb\n3\nc\n4\nd\n✕\n✕\n✕\nq\nq\nWe have considered all the valid placements of queens in row 2, so we backtrack again by undoing the placement of the queen in square 𝑑3. We\nalso considered all valid placements of queens in row 3, so we backtrack by undoing the placement of our initial queen in square 𝑎4.\n1\na\n2\nb\n3\nc\n4\nd\nq\n✕\n✕\n1\na\n2\nb\n3\nc\n4\nd", "word_count": 386, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2d02f895-f398-56a8-ac7b-b412fef21478", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 839, "real_page_number": null, "text": "22.1 Backtracking\n827\nSince placing the queen at position did not lead us to a solution, we will try placing the queen at position 𝑏4, the next position in the first row.𝑎4\nThe next steps of the backtracking process are shown below:\n1\na\n2\nb\n3\nc\n4\nd\nq\n✕\n✕\n✕\n1\na\n2\nb\n3\nc\n4\nd\nq\nq\n✕\n✕\n✕\n1\na\n2\nb\n3\nc\n4\nd\nq\nq\nq\n✕\n✕\n✕\n1\na\n2\nb\n3\nc\n4\nd\nq\nq\nq\nq\nWe were able to place a queen in all four rows, so we have a solution to the problem that satisfies all constraints! To find additional solutions, we\nwould undo the queen placement in square and continue searching using the same approach as before.𝑐1\nTo summarize, the components of the N-Queens problem are as follows:\n• The we need to make is to place a queen in the next unconsidered square (from left to right) in the first row of the chessboard thatchoice\ndoes not yet have a queen.\n• To determine if a partial solution is a solution, we check if we have successfully placed a queen in the final row of thevalid complete\nchessboard.\n• To determine if a partial solution is promising, we check if the queen we place is threatened by any existing queens already on the\nchessboard.\nUsing this information, we can implement the following solution to the N-Queens problem:\n1\nusing std::vector<std::vector<bool>>;Board =\n2\n3\nn_queens(size_tstd::vector<Board> n) {\n4\nstd::vector<Board> solutions;\n5\nstd::vector<bool>(n, false));Board current_partial_solution(n,\n6\nplace_queen(solutions, current_partial_solution, 0, n);\n7\nreturn solutions;\n8\n} // n_queens()\n9\n10\nbool size_t size_t size_tpromising(Board& current_partial_solution, row, col, n) {\n11\n// check for queens in same column\n12\nfor (size_t r = 0; r < row; ++r) {\n13\nif true)(current_partial_solution[r][col] == {\n14\nreturn false;\n15\n} // if\n16\n} // for r\n17\n// check for queens in same 45 degree diagonal\n18\nfor (size_t r = row, c = col; r-- > 0 && c-- > 0; ) {\n19\nif true)(current_partial_solution[r][c] == {\n20\nreturn false;\n21\n} // if\n22\n} // for r, c\n23\n// check for queens in same 135 degree diagonal\n24\nfor (size_t r = row, c = col; r-- > 0 && c++ < n - 1; ) {\n25\nif true)(current_partial_solution[r][c] == {\n26\nreturn false;\n27\n} // if\n28\n} // for r, c\n29\nreturn true;\n30\n} // promising()\n31\n32\nvoid place_queen(std::vector<Board>& solutions, Board& current_partial_solution,\n33\nsize_t size_trow, n) {\n34\nif (row == n) {\n35\nsolutions.push_back(current_partial_solution);\n36\n} // if\n37\nelse {\n38\nfor (size_t col = 0; col < n; ++col) {\n39\nif (promising(current_partial_solution, row, col, n)) {\n40\ntrue;current_partial_solution[row][col] =\n41\nplace_queen(solutions, current_partial_solution, row + 1, n);\n42\nfalse;current_partial_solution[row][col] =\n43\n} // if\n44\n} // for col\n45\n} // else\n46\n} // place_queen()", "word_count": 513, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "11d1684c-30ff-5236-b9d2-13dd60315bc5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 840, "real_page_number": null, "text": "828\nChapter 22. Backtracking and Branch and Bound\nInstead of representing the board as a 2-D array, it is more efficient to implement the chessboard using several 1-D arrays that store row positions,\ncolumn availability, and diagonal availabilities. In this implementation, we will first define an array that stores the position of queens that have\nbeen placed so far for each row of the chessboard (e.g., as shown below):\nq\nq\nq\nq\n2\n0\n3\n1\nposition_in_row\nWe will also need an array of Booleans that stores the availability of queens in each column. If a queen already exists in a column, the array\nstores false; otherwise, it stores true.\nq\nq\nT\nF\nT\nF\ncolumn\nThe same is done for the left and right diagonals. There are a total of seven left and right diagonals, as shown below. The availability of these\ndiagonals are tracked using two arrays of size seven, starting from top corner diagonal as index zero (in the figures below, the number next to\neach diagonal represents its index of the array).\nq\nq\n0\n1\n2\n3\n4\n5\n6\nT\nF\nT\nT\nF\nT\nT\nleft_diagonal\nq\nq\n6\n5\n4\n3\n2\n1\n0\nT\nF\nF\nT\nT\nT\nT\nright_diagonal\nTo determine which diagonal a square of the board belongs to, we just need to do some simple math: given an 𝑛×𝑛board, the square at row 𝑟\nand column 𝑐belongs to left diagonal 𝑟+𝑐and right diagonal (𝑟−𝑐)+(𝑛−1). In other words, to check if a queen placed in row 𝑟, column\nleft_diagonal𝑐breaks any of the diagonal constraints, we can simply check the value at index 𝑟+𝑐of the array and the value at index\nright_diagonalof the array.(𝑟−𝑐)+(𝑛−1)", "word_count": 290, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "11dede8a-084b-5f32-8db6-1d7aa32c26bd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 841, "real_page_number": null, "text": "22.1 Backtracking\n829\nPutting this all together, we can implement the following solution to the N-Queens problem. Unlike before, we can now check if a column or\ndiagonal is available in time (by checking a Boolean stored in an array) without needing to conduct a linear traversal every time. Ourconstant\npromising() function still gets the job done, but much more efficiently!\n1\nusing std::vector<std::vector<bool>>;Board =\n2\n3\nclass NQueensSolver {\n4\nsize_t size;\n5\nstd::vector<size_t> position_in_row;\n6\nstd::vector<bool> column, left_diagonal, right_diagonal;\n7\nstd::vector<Board> solutions;\n8\nconst bool true;AVAILABLE =\n9\n10\npublic:\n11\nNQueensSolver(size_t n)\n12\ntrue),: size{n}, position_in_row(n, -1), column(n,\n13\ntrue), true)left_diagonal(2 n - 1, right_diagonal(2 n - 1, {}* *\n14\n15\nvoid place_queen(size_t row) {\n16\nif (row == size) {\n17\nstore_solution_board();\n18\nreturn;\n19\n} // place_queen()\n20\n21\nfor (size_t col = 0; col < size; ++col) {\n22\nif (promising(row, col)) {\n23\nposition_in_row[row] = col;\n24\ncolumn[col] = !AVAILABLE;\n25\nleft_diagonal[row + col] = !AVAILABLE;\n26\nright_diagonal[row - col + (size - 1)] = !AVAILABLE;\n27\nplace_queen(row + 1);\n28\n29\n// undo and backtrack\n30\ncolumn[col] = AVAILABLE;\n31\nleft_diagonal[row + col] = AVAILABLE;\n32\nright_diagonal[row - col + (size - 1)] = AVAILABLE;\n33\n} // if\n34\n} // for col\n35\n} // place_queen()\n36\n37\nbool promising(size_t size_trow, col) {\n38\nreturn column[col] == AVAILABLE &&\n39\nleft_diagonal[row + col] = AVAILABLE &&\n40\nright_diagonal[row - col + (size - 1)] = AVAILABLE;\n41\n} // promising()\n42\n43\nvoid store_solution_board() {\n44\nstd::vector<bool>(size, false));Board solution(size,\n45\nfor (size_t row; row < size; ++row) {\n46\nfor (size_t col; col < size; ++col) {\n47\nif (position_in_row[row] == col) {\n48\ntrue;solution[row][col] =\n49\n} // if\n50\n} // for col\n51\n} // for row\n52\nsolutions.push_back(solution);\n53\n} // store_solution_board()\n54\n55\nvoid solve() {\n56\nplace_queen(0);\n57\n} // solve()\n58\n59\nstd::vector<Board> get_solutions() {\n60\nreturn solutions;\n61\n} // get_solutions()\n62\n};\n63\n64\nn_queens(size_tstd::vector<Board> n) {\n65\nNQueensSolver nqs(n);\n66\nnqs.solve();\n67\nreturn nqs.get_solutions();\n68\n} // n_queens()", "word_count": 359, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1c817709-8226-5ed8-98fe-9e42cc124850", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 842, "real_page_number": null, "text": "830\nChapter 22. Backtracking and Branch and Bound\n¸ 22.1.5\n(✽)Solving Problems Using Backtracking: Sudoku Solver\nExample 22.5 Sudoku, originating from a Japanese term meaning \"single number\", is a combinatorial number puzzle involving a 9 9×\ngrid of digits that is further subdivided into nine 3 3 subgrids. An example of a Sudoku board is shown below:×\n2\n6\n5\n7\n3\n9\n8\n2\n4\n7\n9\n5\n4\n3\n5\n7\n9\n2\n3\n8\n8\n2\n6\n5\n7\n3\nThe goal of Sudoku is to fill out every cell of this grid with the digits 1 to 9 in a way such that every row, column, and 3 3 subgrid contains×\nall the digits from 1 to 9. For example, the following is a solution to the above Sudoku puzzle:\n9\n2\n4\n8\n6\n3\n7\n1\n5\n1\n7\n3\n4\n9\n5\n8\n2\n6\n8\n5\n6\n7\n1\n2\n3\n9\n4\n7\n9\n5\n6\n3\n8\n1\n4\n2\n4\n1\n2\n9\n5\n7\n6\n8\n3\n6\n3\n8\n1\n2\n4\n5\n7\n9\n2\n6\n7\n3\n4\n1\n9\n5\n8\n3\n4\n1\n5\n8\n9\n2\n6\n7\n5\n8\n9\n2\n7\n6\n4\n3\n1\nGiven a Sudoku board of characters representing the digits 1-9, implement a function that solves the Sudoku puzzle.\nThere are several ways we could approach this problem. We could use brute force to generate all possible boards (by considering 1-9 in every\nopen cell) and then validate every single one of those boards, but that would be inefficient. Instead, since we know that the board is governed by\na set of constraints, we can use the backtracking approach to reduce the number of possible board configurations we end up checking.\nTo devise a backtracking solution, we will first identify the choice we need to make to move toward a solution — in this case, the choice\nis to place a digit from 1 to 9 in an empty cell. Second, we want to devise a method to determine whether a partial solution is a completed\nsolution; this can be done by checking if the entire board is filled. Lastly, we want to devise a method to determine whether a partial solution is\npromising; this can be done by checking that the digit we want to place does not already exist in the same row, column, or subgrid of the Sudoku\nboard. Using these three components, we can write a straightforward backtracking to the Sudoku problem:\n• Traverse the cells of the given Sudoku grid and place a digit in the empty cells one by one (all digits from 1-9 should be considered for\neach empty cell).\n• Check if the grid is still valid by ensuring that the newly placed digit does not already exist in the same row, column, or subgrid. If the\ngrid is still valid after our choice, fix the newly placed digit in place and recurse on the remaining empty cells. Otherwise, prune the\nbranch by removing the digit that was placed.\n• Once all the cells have been filled with a valid digit, the algorithm completes and we have a solution.\nsolve_sudoku()A simple backtracking solution for the Sudoku solver problem is shown on the next page. The function takes in a 9×9\n'.'grid where empty cells are represented as the character and modifies the grid to store the solution to the Sudoku problem. For example,\nsolve_sudoku()calling on the grid on the left would turn it into the grid on the right:\n[ [\".\",\"2\",\".\",\".\",\"6\",\".\",\".\",\".\",\"5\"],\n[\".\",\"7\",\"3\",\".\",\"9\",\".\",\".\",\".\",\".\"],\n[\"8\",\".\",\".\",\".\",\".\",\"2\",\".\",\".\",\"4\"],\n[\"7\",\"9\",\"5\",\".\",\".\",\".\",\".\",\".\",\".\"],\n[\"4\",\".\",\".\",\".\",\".\",\".\",\".\",\".\",\"3\"],\n[\".\",\".\",\".\",\".\",\".\",\".\",\"5\",\"7\",\"9\"],\n[\"2\",\".\",\".\",\"3\",\".\",\".\",\".\",\".\",\"8\"],\n[\".\",\".\",\".\",\".\",\"8\",\".\",\"2\",\"6\",\".\"],\n[\"5\",\".\",\".\",\".\",\"7\",\".\",\".\",\"3\",\".\"] ]\n[ [\"9\",\"2\",\"4\",\"8\",\"6\",\"3\",\"7\",\"1\",\"5\"],\n[\"1\",\"7\",\"3\",\"4\",\"9\",\"5\",\"8\",\"2\",\"6\"],\n[\"8\",\"5\",\"6\",\"7\",\"1\",\"2\",\"3\",\"9\",\"4\"],\n[\"7\",\"9\",\"5\",\"6\",\"3\",\"8\",\"1\",\"4\",\"2\"],\n[\"4\",\"1\",\"2\",\"9\",\"5\",\"7\",\"6\",\"8\",\"3\"],\n[\"6\",\"3\",\"8\",\"1\",\"2\",\"4\",\"5\",\"7\",\"9\"],\n[\"2\",\"6\",\"7\",\"3\",\"4\",\"1\",\"9\",\"5\",\"8\"],\n[\"3\",\"4\",\"1\",\"5\",\"8\",\"9\",\"2\",\"6\",\"7\"],\n[\"5\",\"8\",\"9\",\"2\",\"7\",\"6\",\"4\",\"3\",\"1\"] ]", "word_count": 630, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "02007746-676e-5f55-9cd9-d1e17009e6a6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 843, "real_page_number": null, "text": "22.1 Backtracking\n831\nAn implementation of this solution is shown below:\n1\nvoid solve_sudoku(std::vector<std::vector<char>> &board) {\n2\nif (!place_digit(board, 0, 0)) {\n3\nstd::cout << \"No solution\" << std::endl;\n4\n} // if\n5\n} // solve_sudoku()\n6\n7\nbool promising(std::vector<std::vector<char>>& size_t size_t charboard, row, col, digit) {\n8\nfor (size_t i = 0; i < 9; ++i) {\n9\n// check same row\n10\nif (board[row][i] == digit) {\n11\nreturn false;\n12\n} // if\n13\n// check same column\n14\nif (board[i][col] == digit) {\n15\nreturn false;\n16\n} // if\n17\n// check same subgrid\n18\nif (board[row - row % 3 + i / 3][col - col % 3 + i % 3] == digit) {\n19\nreturn false;\n20\n} // if\n21\n} // for i\n22\nreturn true;\n23\n} // promising()\n24\n25\nbool place_digit(std::vector<std::vector<char>>& size_t size_tboard, row, col) {\n26\nif (row == 9) {\n27\nreturn true;\n28\n} // if\n29\nif (col == 9) {\n30\nreturn place_digit(board, row + 1, 0);\n// check first empty cell in next row\n31\n} // if\n32\nif (board[row][col] != '.') {\n33\nreturn place_digit(board, row, col + 1);\n// ignore if cell already filled\n34\n} // if\n35\nfor (char c = '1'; c <= '9'; ++c) {\n36\nif (promising(board, row, col, c)) {\n37\nboard[row][col] = c;\n38\nif (place_digit(board, row, col + 1)) {\n39\nreturn true;\n40\n} // if\n41\nboard[row][col] = '.';\n42\n} // if\n43\n} // for c\n44\nreturn false;\n45\n} // place_digit()\n¸ 22.1.6\nBacktracking Time Complexity\nWhat is the time complexity of a backtracking algorithm? Since backtracking is simply an extension of brute force that relies on pruning to\nreduce the solution space, the worst-case time complexity of backtracking is actually bounded by the time complexity of a brute force approach!\nWhy is this so? Consider the scenario where you are extremely unlucky and are unable to prune anything — if this happens, you are essentially\nchecking every possible solution. As a result, the worst-case time complexity of a backtracking algorithm is based on the total number of\nsolutions you would have to check if nothing is pruned. That being said, even though brute force serves as an upper bound on asymptotic\nruntime, the actual performance of backtracking is typically much faster (since the worst-case scenario rarely happens).\nExample 22.6 You are given a 𝑛×𝑛Sudoku board, where 𝑛is a perfect square number. Assuming that the number of possibilities for\neach cell is 𝑛(all numbers from 1 to 𝑛), and the number of empty cells at the beginning is 𝑚, what is the worst-case time complexity of a\nbacktracking solution for a Sudoku solver of this 𝑛×𝑛board?\n𝑛𝑚possibleThere are a total of 𝑚open cells, and 𝑛possibilities for each cell. Thus, by the fundamental counting principle, there are a total of\n𝑂(𝑛𝑚).boards that can be created. Thus, the worst-case time complexity of a backtracking solution to the Sudoku solver problem is bounded by", "word_count": 512, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f1fe2881-26d6-5be9-bb6f-a71b654ec75f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 844, "real_page_number": null, "text": "832\nChapter 22. Backtracking and Branch and Bound\n22.2\nBranch and Bound\n¸ 22.2.1\nBranch and Bound: Minimization Problems\nPreviously, we discussed the algorithm family of backtracking, which solves constraint satisfaction problems by performing a search over the\nsolution space and pruning partial solutions that cannot lead to a valid solution. This technique of pruning partial solutions can also be used to\nsolve problems. If we apply the concepts of backtracking to solve an optimization problem, we end up with a similar algorithmoptimization\npromising()family known as branch and bound. In branch and bound, the function now has two responsibilities:\n1. It must determine if a partial solution can ever lead to a complete solution that (similar to backtracking).satisfies all constraints\n2. It must also determine if the partial solution can ever lead to an solution.optimal\nThis second task can be completed by estimating a bound on the the best solution obtainable from continuing a partial solution. The bound\ngives you a reasonable estimate for the remaining work needed to complete a solution, without having to do the work itself. This allows the\nalgorithm to quickly identify whether it should continue looking down the branch it is currently exploring: if the bound of a partial solution\nis not better than the best complete solution known so far, then there is no reason to continue exploring that partial solution (this process of\nsystematically eliminating partial solutions from consideration is known as pruning).\nIt is important to note that bounds are simply estimates, and a bound does have to match the actual best solution of extending a partialnot\nsolution (which can often be expensive to compute). However, you still want your bounds to be as close to the actual solution as possible, since\na more accurate bound allows you to prune more efficiently.\nThere are also limits on what a valid bound can be: an estimated bound must be as the actual best solution attainableat least as optimistic\nfrom continuing a partial solution. Otherwise, your algorithm could potentially prune the optimal solution by mistakenly believing that its\nbranch is not promising (an issue known as overpruning)!\nThe precise calculation of bounds depends on whether you are trying to solve a minimization or a maximization problem. If you are working\nwith a problem, you would need to identify the following two bounds at every partial solution:minimization\n• A lower bound, which is an of the best solution you could get from extending the current partial solution. The loweroptimistic estimate\nbound is calculated by taking the cost of the partial solution so far and adding it to an of the remaining cost required tounderestimate\ncomplete the partial solution (we will discuss why an underestimate is needed for minimization problems later in the section).\nAn upper bound, which is the cost of the best complete solution encountered so far. The upper bound, as its name suggests, serves as an•\nupper limit for the lower bound estimate. If the calculated lower bound of a partial solution is greater than or equal to the current upper\nbound, then the partial solution can be pruned.\nThe lower and upper bounds are compared to determine if a branch is promising. If the lower bound is less than the current upper bound, then it\nmay be possible to discover a better solution along that branch, and you should continue extending the current partial solution. Otherwise, there\nis no way to discover a solution that is better by extending the partial solution, and the branch should be pruned.\nLet’s look at this process using an example. Suppose we are trying to solve a minimization problem, where the best complete solution we\nknow so far has a total cost of 281, and the current partial solution we are considering has a cost of 183. Our goal with branch and bound is to\ndiscover the optimal solution, so our algorithm needs to know whether the current partial solution can ever lead to a outcome that is better than\nour best-known solution of 281. If it cannot, then the partial solution cannot be optimal, and we can stop exploring it immediately.\n0\n183\n281\n(upper bound)\nworse than 281 (prune)\nbetter than 281 (promising)\n?\n?\nHowever, there isn’t much we can infer about the branch just by looking at the current cost of the partial solution (which is 183). For instance, it\nis entirely possible that every solution down this branch leads to a solution that is worse than 281 (which indicates the branch should be pruned).\n0\n183\n281\n(upper bound)\n326\n323\n337\n360\n349", "word_count": 773, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dbfbea92-31da-5186-a056-06f449a89686", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 845, "real_page_number": null, "text": "22.2 Branch and Bound\n833\nHowever, it is also possible that this branch eventually leads us down to an optimal solution.\n0\n183\n281\n(upper bound)\n326\n323\n337\n360\n(optimal)247\nThe cost of the current partial solution typically gives us no information on whether the current branch is promising. To determine if we should\ncontinue looking down a branch, we must measure the cost of choices that have not yet been included in the partial solution! This is where\nthe lower bound comes in: the lower bound estimates the best possible solution attainable from continuing the current partial solution. In the\nprevious example, the best solution possible from extending the current branch is 247, so 247 would be our ideal lower bound.\n0\n183\n281\n247\n(lower bound)\n326\n323\n337\n360\n247\nHowever, just because 247 is the ideal lower bound for this partial solution does not mean your lower bound calculation must be 247. In fact,\nfinding the actual best solution is not always recommended, especially if doing so is computationally expensive. For minimization problems,\nlower bounds are underestimates, so any lower bound value under 247 would work (although the closer to 247 you are, the better). For instance,\n240 would also be a valid lower bound estimation, as shown below. This indicates that it is possible to obtain a solution that is potentially\nif the current partial solution is extended. Since the best-known solution is 281, this indicates and that you should continueas good as 240\nexploring this branch, as it could lead you to a better solution.\n0\n183\n281\n240\n(lower bound)\n326\n323\n337\n360\n247\n240 281<\npromising\nOn the other hand, if the lower bound is larger than or equal to the upper bound, then the best solution from continuing a partial solution can\nnever be better than the best solution known so far. In the example below, the lower bound is 320, which indicates that it is impossible to obtain\na solution better than 320 by extending the current partial solution. Since 320 is worse than 281, it is safe to prune the branch.\n0\n183\n281\n320\n(lower bound)\n326\n323\n337\n360\n349\n≥281320\nnot promising", "word_count": 372, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6040e101-d6a5-51a5-8a74-bfc3f4925070", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 846, "real_page_number": null, "text": "834\nChapter 22. Backtracking and Branch and Bound\nIn general, you want the lower bound estimate to be to the actual best outcome of extending the branch. A lower bound thatas close as possible\nis too optimistic could cause you to waste time exploring solutions that aren’t promising. For example, a lower bound of 240 for the following\nbranch is valid, but you would mistakenly believe that there is a better solution when there actually isn’t. This would worsen the performance of\nyour branch and bound algorithm.\n0\n183\n281\n240\n(lower bound)\n326\n323\n337\n360\n349\n240 281<\npromising\n(but it isn’t)\nHowever, the lower bound estimate cannot ever be than the actual best outcome of extending a branch! If your lower bound is tooworse\npessimistic, you could mistakenly believe a branch is not promising even if it includes the actual optimal solution. For example, consider the\nfollowing partial solution, where the best possible outcome of extending the partial solution is 270. However, if your lower bound is too high\n(e.g., 290), your algorithm would conclude that it is impossible to obtain a solution better than 290 by continuing the branch, and the partial\nsolution would be incorrectly pruned.\n0\n183\n281 290\n(lower bound)\n326\n323\n337\n360\n270\n≥281290\nnot promising\n(but it is)\nTo summarize, the steps of a branch and bound algorithm for a minimization problem are as follows:\n1. Start with an initial upper bound of infinity.\n2. Find the first complete solution and use its cost as an upper bound to prune branches during the rest of the search.\nExplore the solution space (similar to backtracking). For each partial solution encountered, calculate a lower bound estimate for the total3.\ncost required to complete the solution.\n4. Prune partial solutions whose lower bound is greater than or equal to the current upper bound. This is because the lower bound is an\noptimisticestimate of the best solution you could get from continuingthe partialsolution, and the upper boundis the best-known solution;\nif the optimistic estimate of continuing a branch is worse than the best-known solution, there is no reason to continue down that branch.\n5. If a branch is fully explored without being pruned, and the solution of this completed branch is than the existing upper bound,better\nupdate the upper bound to this new value (since the upper bound represents the best solution encountered so far).\n6. After the entire solution space is explored, the current upper bound is the optimal solution.\ncurrent_bestThe pseudocode for exploring the search space is shown below. At the end of the search, the value of (i.e., the upper bound)\nstores the optimal solution to the minimization problem.\n1\nAlgorithm check_node(partial_solution, current_best):\n2\nif partial_solution is a valid full solution:\n3\nif partial_solution < current_best:\n4\ncurrent_best = partial_solution\n5\nelse:\n6\nif (promising(partial_solution, current_best)):\n7\nfor each choice that can be made to extend partial_solution:\n8\npartial_solution = partial_solution + choice\n9\ncheck_node(partial_solution, current_best)\n10\npartial_solution = partial_solution - choice\n11\n12\nbool promising(partial_solution, current_best):\n13\nlower_bound = calculate_lower_bound()\n14\nif lower_bound < current_best:\n15\nreturn true\n16\nreturn false", "word_count": 532, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fc440d04-2507-5581-80bc-7a429bc19187", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 847, "real_page_number": null, "text": "22.2 Branch and Bound\n835\n¸ 22.2.2\nBranch and Bound: Maximization Problems\nMaximization problems also work in a similar fashion, just with the bounds reversed: the upper bound now becomes the estimate for the best\nsolution attainable from extending a partial solution, and the lower bound now becomes the best complete solution encountered so far. The two\nbounds for maximization problems are summarized below:\n• The upper bound of a maximization problem is an for the best solution you could get from extending the currentoptimistic estimate\npartial solution. The upper bound is calculated by taking the value of the partial solution and adding it to an of the remainingoverestimate\nvalue obtainable from completing the partial solution.\n• The lower bound of a maximization problem is the value of the best complete solution encountered so far. The lower bound serves as\nthe lower limit for the upper bound estimation. If the calculated upper bound of a partial solution is not greater than the current lower\nbound, then the partial solution can be pruned.\nIf the upper bound is greater than the current lower bound, then it is possible to discover a solution that is better than the current best solution,\nand you should continue extending the current partial solution. Otherwise, there is no way to discover a solution that is better from extending\nthe partial solution, and the branch should be pruned.\nConsider the following example, where the current partial solution has a value of 183 and the best known complete solution has a value of\n281. Since this is a maximization problem, you want to find an upper bound on the best solution you could get from extending the current\nbranch. The upper bound must be an overestimate, so anything above 360 would work (since 360 is the best solution obtainable from continuing\nthe partial solution in this example). In this case, let’s assume our upper bound is 370. Since this upper bound is better than the lower bound of\n281, it is possible to obtain a better solution from extending the branch, so the partial solution is promising.\n0\n183\n281\n(lower bound)\n370\n(upper bound)\n326\n323\n337\n360\n349\n370 281>\npromising\nOn the other hand, if the estimated upper bound were less than or equal to the lower bound, then there would be no way to obtain a better\nsolution from extending the current branch. In such a case, the branch should be pruned. In the example below, the upper bound is 240, which\nindicatesthat240isthebestwecandofromcontinuingthispartialsolition. However, since281isthebest-knownsolutionforthismaximization\nproblem, this branch is not promising and is thereby pruned from our search.\n0\n183\n281\n240\n(upper bound)\n229\n232\n215\n225\n233\n≤281240\nnot promising\nSimilar to before, the upper bound you estimate for maximization problems should be as close as possible to the actual best solution from\nextending a partial solution (to allow you to prune efficiently). However, it must also be optimistic, or you could end up pruning themore\noptimal solution. These two error cases are shown below — the first (underpruning) would make the algorithm inefficient, while the second\n(overpruning) could cause you to prune the optimal solution and end up with the wrong answer:\n0\n183\n281\n320\n(upper bound)\n229\n232\n215\n225\n233\n320 281>\npromising\n(but it isn’t)\nupper bound too far from optimal", "word_count": 581, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ad0edb52-14f6-5ddd-9bde-7354086a4381", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 848, "real_page_number": null, "text": "836\nChapter 22. Backtracking and Branch and Bound\n0\n183\n281\n240\n(upper bound)\n229\n232\n290\n225\n233\n≤281240\nnot promising\n(but it is)\nupper bound too pessimistic\nTo summarize, the steps of a branch and bound algorithm for a maximization problem are as follows:\n1. Start with an initial lower bound of zero (or negative infinity, if negatives are allowed).\n2. Find the first complete solution and use its cost as a lower bound to prune branches during the rest of the search.\n3. Explore the solution space. For each partial solution encountered, calculate an upper bound estimate for the total value attainable from\ncompleting the solution.\n4. Prune partial solutions whose upper bound is less than or equal to the current lower bound. This is because the upper bound is an\noptimistic estimate of the best solution you could get from continuing the partial solution, and the lower bound is the best-known solution;\nif the optimistic estimate of continuing a branch is worse than the best-known solution, there is no reason to continue down that branch.\n5. If a branch is fully explored without being pruned, and the solution of this completed branch is than the existing lower bound,better\nupdate the lower bound to this new value (since the lower bound represents the best solution encountered so far).\n6. After the entire solution space is explored, the current lower bound is the optimal solution.\ncurrent_bestThe pseudocode for this approach is shown below. At the end of the search, the value of (i.e., the lower bound) stores the\noptimal solution to the maximization problem:\n1\nAlgorithm check_node(partial_solution, current_best):\n2\nif partial_solution is a valid full solution:\n3\nif partial_solution > current_best:\n4\ncurrent_best = partial_solution\n5\nelse:\n6\nif (promising(partial_solution, current_best)):\n7\nfor each choice that can be made to extend partial_solution:\n8\npartial_solution = partial_solution + choice\n9\ncheck_node(partial_solution, current_best)\n10\npartial_solution = partial_solution - choice\n11\n12\nbool promising(partial_solution, current_best):\n13\nupper_bound = calculate_upper_bound()\n14\nif upper_bound > current_best:\n15\nreturn true\n16\nreturn false\nBranch and bound algorithms are not always easy to implement, even if they follow a common pattern! The most challenging component of any\nbranch and bound algorithm is the process of estimating the bounds, since this step can make or break the algorithm. There are also performance\ntradeoffs you must consider when identifying your bounds — better bounds can save you time by allowing you to prune more efficiently, but\nthey can also require a non-trivial amount of time to compute! In some cases, a quick bound estimation can get the job done. In other cases,\nΘ(𝑛2)spending more time perfecting your bounds can be worth the runtime improvements you get from pruning. For instance, an algorithm to\nestimate bounds may seem inefficient, but it is often preferable if the alternative involves exploring additional branches.Θ(𝑛!)\nSimilar to backtracking, the worst-case time complexity of a branch and bound algorithm is bounded by the complexity of a brute force\napproach, which occurs in the case where nothing gets pruned. However, like before, this worst-case scenario rarely ever happens, so the\nperformance of branch and bound is typically superior to that of brute force.\nIn this class, we will focus on two important problems that can be solved using branch and bound. First, we will look at the traveling\nproblem, a minimization problem that can be solved using a branch and bound approach. Then, in chapter 24, we will look at thesalesperson\nproblem, a maximization problem that can also be solved using this algorithm family.knapsack\n22.3\nThe Traveling Salesperson Problem (TSP)\n¸ 22.3.1\nEstimating Bounds for TSP\nInthissection,wewilldiscussthetravelingsalespersonproblem(TSP),oneofthemostextensivelystudiedoptimizationproblemsincomputer\nscience. In the traveling salesperson problem, you are given a collection of cities and distances between each pair of cities, and you want to find\nthe shortest route that visits every city once and returns back to the starting city (such a cycle that traverses every node of a graph once is known\nas a Hamiltonian cycle).", "word_count": 687, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cddd5948-4b8c-57ba-9cd0-a48c0e9b3e6c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 849, "real_page_number": null, "text": "22.3 The Traveling Salesperson Problem (TSP)\n837\nSuppose you are given the following ten cities, and you want to find the shortest Hamiltonian cycle that visits every city exactly once:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCity\nCoordinate\n𝐴\n(2,3)\n𝐵\n(3,5)\n𝐶\n(4,3)\n𝐷\n(5,5)\n𝐸\n(5,2)\n𝐹\n(6,7)\n𝐺\n(7,5)\n𝐻\n(9,7)\n𝐼\n(9,5)\n𝐽\n(10,2)\nIf you were to use brute force to find a solution, you would have to check every Hamiltonian cycle possible to identify an optimal path. There are\n(𝑛−1)!a total of\n2\nsuch cycles, since from any starting vertex there are edges to choose from for the first vertex, edges to choose from for𝑛−1 𝑛−2\nthe second vertex, edges to choose from for the third vertex, and so on (we divide the final result by two since half of these permutations𝑛−3\nare simply mirror images of each other, i.e., 𝐴→𝐵→𝐶→𝐴and 𝐴→𝐶→𝐵→𝐴). Because there exist possible Hamiltonian cycles𝑂(𝑛!)\ngiven 𝑛cities, a brute force approach to TSP would also take time, which is computationally infeasible for large input sizes.𝑂(𝑛!)\nTo reduce the runtime of TSP, we can use a branch and bound approach to prune off paths we know cannot be optimal, therefore reducing\nthe size of our search space. To start off the branch and bound process, we will find the first complete solution and use its cost as an upper\nsearch.1bound to prune branches during the rest of the For our example, suppose the first complete solution we find is 𝐴→𝐵→𝐶→𝐷→\n33.095:2𝐸→𝐹→𝐺→𝐻→𝐼→𝐽→𝐴, which has a total cost of\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nConnection\nDistance\n𝐴→𝐵\n2.236\n𝐵→𝐶\n2.236\n𝐶→𝐷\n2.236\n𝐷→𝐸\n3.000\n𝐸→𝐹\n5.099\n𝐹→𝐺\n2.236\n𝐺→𝐻\n2.828\n𝐻→𝐼\n2.000\n𝐼→𝐽\n3.162\n𝐽→𝐴\n8.062\nTotal\n33.095\nNow we know that, whatever the optimal path is, it must have a weight less than or equal to 33.095. We will therefore set 33.095 as our initial\nupper bound. This allows us to prune future partial solutions that we know cannot lead us to a solution that is better than 33.095, the best\nsolution we have encountered so far.\nAfter identifying this upper bound, we will now need to implement a method to determine whether a partial solution is promising. Recall\nfrom the previous section that this step is done by estimating a for each partial solution, which serves as an optimistic estimate forlower bound\nthe best solution attainable from continuing the partial solution. For example, consider the following partial solution 𝐴→𝐵→𝐶→𝐸:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nConnection\nDistance\n𝐴→𝐵\n2.236\n𝐵→𝐶\n2.236\n𝐶→𝐸\n1.414\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\nTotal\n5.886\n1Thisisactuallynotthebestwaytosetaninitialupperbound,sincethefirstsolutionyoufindmaynotbeanidealbound. Instead,youshouldsettheinitialbound\ntotheresultofaTSPheuristic,whichwewilldiscussinalatersection.\n2Thedirectionofthearrowsherearearbitrarychosen,andcanbeflippedaroundtoproducetheexactsamepath. Thearrowsareincludedhere(andinfuture\nexamples)toeaseunderstandingofcertainTSPconcepts,sodon’tworrytoomuchaboutwhythedirectionofthepathisthewayitis. Twosolutionswiththe\nsameedgesareconsideredtobeidentical,evenifthedirectionofthearrowsareflipped.", "word_count": 578, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2e12509e-1006-5e10-ad7c-ed9ee2f308f6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 850, "real_page_number": null, "text": "838\nChapter 22. Backtracking and Branch and Bound\nTo determine whether or not we should extend this branch, we will need to estimate a lower bound on the the best solution we could get from\nextending this partial solution. If the lower bound is better than 33.095, we continue looking; otherwise, we should prune the branch. However,\nthis requires us to estimate the cost of connecting points currently not in our path!\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nIs it possible to connect\nthese remaining points to\nthe current path in a way\nso that the total weight\nis less than 33.095?\nConnection\nDistance\n𝐴→𝐵\n2.236\n𝐵→𝐶\n2.236\n𝐶→𝐸\n1.414\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\nTotal\n5.886\nThis step is rather tricky — we obviously cannot solve for the actual best solution from extending the branch, since that is the problem we are\ntrying to answer in the first place! Instead, we have to rely on an estimate of what the best possible solution could be. As noted previously, this\nestimate should be as close to the actual best solution as possible, but it can never be worse (which could cause you to prune off the optimal\nsolution). How can we devise an efficient mechanism that always gives us an that is to the actual best outcomeunderestimate as close as possible\nof extending the current branch?\nLet’s look at the previous figure again. We know the total cost of our current partial solution 𝐴→𝐵→𝐶→𝐸is 5.886. However, we do\nnot know the cost of connecting the remaining points that are currently not in our path. If we could quickly solve for this, we would be able\nto produce a good lower bound estimation for our branch and bound algorithm! What is the best way to estimate the lowest possible cost of\nconnecting these external points?\nIt turns out we can use a to estimate this cost. Since a minimum spanning tree gives us the lowest possible weight ofminimum spanning tree\nconnecting a set of points, an MST estimate is than the actual best cost of connecting the remaining points to ourguaranteed to be no worse\npath. In our example, the MST of the remaining six points has a total weight of 11.398:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of MST: 11.398\nWeight of Partial Solution: 5.886\nConnection\nDistance\n𝐴→𝐵\n2.236\n𝐵→𝐶\n2.236\n𝐶→𝐸\n1.414\n𝐷→𝐹\n2.236\n𝐷→𝐺\n2.000\n𝐺→𝐼\n2.000\n𝐻→𝐼\n2.000\n𝐼→𝐽\n3.162\n−\n−\n−\n−\nTotal\n17.284\nSince our current partial solution has a weight of 5.886, and the MST connecting the remaining points has a weight of 11.398, the best solution\nattainable from continuing the partial solution cannot be better than 5.886 + 11.398 = 17.284. This will always be an underestimate of the\nactual best solution, so we can use it as a valid lower bound for our branch and bound algorithm.", "word_count": 497, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a9f1ce04-e74f-5566-ac26-c23670be98af", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 851, "real_page_number": null, "text": "22.3 The Traveling Salesperson Problem (TSP)\n839\nHowever, we can do even better — even though a lower bound of 17.284 works, it is not the most efficient for pruning. We can get a closer\nestimate of the ideal best solution by also connecting the endpoints of our current partial solution with the MST of the remaining points. To\nensure that our lower bound remains optimistic, we will connect each endpoint of our current partial solution with the vertex that is notnearest\nin our path. In our example, the endpoints of our current partial solution are vertices A and E. Of the remaining vertices not in our path, vertex\nA is closest to vertex D, and vertex E is closest to vertex D. Thus, we would connect both A and E with D, as shown. (Since this is used to\nproduce a lower bound estimate and not a valid path, it is okay to connect the two endpoints to the same point in the MST.)\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of MST: 11.398\nWeight of Partial Solution: 5.886\nWeight AD: 3.606\nWeight DE: 3.000\nConnection\nDistance\n𝐴→𝐵\n2.236\n𝐵→𝐶\n2.236\n𝐶→𝐸\n1.414\n𝐷→𝐹\n2.236\n𝐷→𝐺\n2.000\n𝐺→𝐼\n2.000\n𝐻→𝐼\n2.000\n𝐼→𝐽\n3.162\n𝐴→𝐷\n3.606\n𝐸→𝐷\n3.000\nTotal\n23.890\nIf we sum these four components together, we get a total weight of 23.89, which we can use as our lower bound. This indicates that we cannot\nget a solution better than 23.89 if we were to continue exploring the partial solution 𝐴→𝐵→𝐶→𝐸. We can therefore compare this value with\nour current best solution to determine whether it is worthwhile to continue extending the partial solution. If 23.89 is better than the best-known\nsolution, we would continue looking; otherwise, we would prune the branch and consider another path.\nIn summary, to obtain a lower bound for the TSP problem, you can sum of the following four weights:\n1. The cost of the current partial solution.\n2. The cost of the MST connecting the remaining points not in the partial solution.\n3. The cost of connecting one end of the partial solution with its closest point in the MST.\n4. The cost of connecting the other end of the partial solution with its closest point in the MST.\nThis will always produce an optimistic estimate on the remaining cost, so it can be used as a valid lower bound. Notice that the estimate will\nlikely not be a valid solution, since an MST is not guaranteed to produce a Hamiltonian cycle on the remaining points! However, this is okay,\nsince this estimate is only used to determine whether a partial solution lead to an optimal solution, and not what the actual optimalcould\nsolution is. This method of estimating a lower bound is useful because it not only finds a lower bound close to the ideal, but it also can be\ndone rather efficiently: the cost of finding an MST is significantly cheaper than exploring all remaining permutations of a partial solution!\nHere is another example of this lower bound method in action, but with a partial solution that is not promising. Suppose we are currently\nexploring the partial solution 𝐸→𝐹→𝐽→𝐵→𝐷, as shown:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of Partial Solution: 21.118\nConnection\nDistance\n𝐸→𝐹\n5.099\n𝐹→𝐽\n6.403\n𝐽→𝐵\n7.616\n𝐵→𝐷\n2.000\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\nTotal\n21.118", "word_count": 576, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f66214ea-40a3-54f5-b1c3-9d07035d7ff0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 852, "real_page_number": null, "text": "840\nChapter 22. Backtracking and Branch and Bound\nTo estimate a lower bound for this partial solution, we first find the weight of the MST that connects the remaining points, which is 9.606:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of MST: 9.606\nWeight of Partial Solution: 21.118\nConnection\nDistance\n𝐸→𝐹\n5.099\n𝐹→𝐽\n6.403\n𝐽→𝐵\n7.616\n𝐵→𝐷\n2.000\n𝐴→𝐶\n2.000\n𝐶→𝐺\n3.606\n𝐺→𝐼\n2.000\n𝐻→𝐼\n2.000\n−\n−\n−\n−\nTotal\n30.724\nThen, we calculate the cost of connecting the endpoints of our current partial solution (𝐸and 𝐷) with their closest points in the MST. The\nclosest point to 𝐸is 𝐶(for a cost of 1.414), and the closest point to 𝐷is 𝐺(for a cost of 2).\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of MST: 9.606\nWeight of Partial Solution: 21.118\nWeight CE: 1.414\nWeight DG: 2\nConnection\nDistance\n𝐸→𝐹\n5.099\n𝐹→𝐽\n6.403\n𝐽→𝐵\n7.616\n𝐵→𝐷\n2.000\n𝐴→𝐶\n2.000\n𝐶→𝐺\n3.606\n𝐺→𝐼\n2.000\n𝐻→𝐼\n2.000\n𝐸→𝐶\n1.414\n𝐷→𝐺\n2.000\nTotal\n34.138\nAdding everything up, we get a lower bound of 21.118 + 9.606 + 2 + 1.414 = 34.138. This means that the best solution we could get from\ncontinuing the partial solution𝐸→𝐹→𝐽→𝐵→𝐷cannot be better than 34.138. Since we already know that the cost of the complete solution\n𝐴→𝐵→𝐶→𝐷→𝐸→𝐹→𝐺→𝐻→𝐼→𝐽→𝐴has a cost of 33.095, we know that our current partial solution is not promising. Thus,\nwe know that an optimal solution cannot possibly include 𝐸→𝐹→𝐽→𝐵→𝐷, allowing us to prune this branch from our search space.\n¸ 22.3.2\nGenerating Permutations\nWe now have a mechanism for determining whether a partial solution is promising. However, we still need a way to generate these partial\ngen_perms()solutions when exploring the solution space. In this class, we will use the following function to generate all partial solutions by\nperm_lengthpermuting over all possible paths where the first items of a path vector are fixed.\n1\ntemplate <typename T>\n2\nvoid size_tgen_perms(std::vector<T>& path, perm_length) {\n3\nif (perm_length == path.size()) {\n4\n// Base case, this gets hit if you fully explore a path without ever pruning it.\n5\n// This means the path currently stored in the vector may be an optimal solution,\n6\n// so compare it with the best solution known so far and update the upper bound if needed.\n7\nreturn;\n8\n} // if\n9\nif (!promising(path, perm_length)) {\n10\nreturn;\n11\n} // if\n12\nfor (size_t i = perm_length; i < path.size(); ++i) {\n13\nstd::swap(path[perm_length], path[i]);\n14\ngen_perms(path, perm_length + 1);\n15\nstd::swap(path[perm_length], path[i]);\n16\n} // for i\n17\n} // gen_perms()", "word_count": 435, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6697ff98-8a26-5c9a-ab5a-01efc3e84225", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 853, "real_page_number": null, "text": "22.3 The Traveling Salesperson Problem (TSP)\n841\ngen_perms()Let’s take a closer look at how works. Consider the traveling salesperson problem for 10 vertices, numbered from 0 to 9. To\ngen_perms(),use we will store all of these vertices in a vector that stores the current path:\npath\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\ngen_perms() path perm_lengthusesthis vectortoidentifythecurrentpartialsolutionat pointalongoursearch. Thisisdoneusingtheany\ngen_perms() perm_lengthvariable, which represents the current length of our partial solution. To start the search, is called with a of 1:\ngen_perms(1);\npath\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nperm_length = 1\nThis indicates that the current partial solution has a length of 1 and only includes the vertex at the first position of the vector. We then check if\nthis partial solution is promising using the lower bound strategy explained previously — if it is, we extend the partial solution by adding another\nvertex to the back of our path. This is done by moving the vertex we want to add to the back of the partial solution, and then recursing on the\nperm_lengthvector with a value that is larger by one. This process is implemented using the loop on line 10 of the provided code.\npath iLet’s look at this loop in action using the vector above. In the first iteration of the loop, is equal to 1, so index 1 is swapped with\nperm_lengthitself. We then make a recursive call with a of 2, which adds vertex 1 to our partial solution.\npath\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nperm_length = 1\npath\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nperm_length = 2\nperm_lengthWe then check if the partial solution is promising. If it is, we make a recursive call with a of 3, which adds vertex 2 to→10\nthe partial solution.\npath\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nperm_length = 3\npromising()However, let’s suppose that our function determines that the partial solution isn’t promising. In this case, we would→10\nbe able to prune off this branch. The pruning step happens when the function returns on line 8, and we return to the stack frame where\nperm_length is 1. Notice that this tells our algorithm to ignore any permutation that starts with →1, since we will no longer make any0\nadditional recursive calls that begin with these two items!\npath\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nperm_length = 2\npath\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nperm_length = 1\nAfter the recursive call unrolls, we undo the swap we completed on line 11 (to ensure our loop explores the remaining branches correctly) and\nplace the next element we want to consider at the end of the partial solution. A new element is considered with each iteration of the loop — here,\ni perm_lengthis incremented to 2 on the next iteration, so we swap the vertex at index 2 to the back of the partial solution and recurse with a\nof 2. This allows us to explore all branches that start with →2.0\npath\n0\n2\n1\n3\n4\n5\n6\n7\n8\n9\nperm_length = 1\npath\n0\n2\n1\n3\n4\n5\n6\n7\n8\n9\nperm_length = 2\nThis process continues for the remainder of the search (we explore all paths that start with →3, then →4, and so on). At every step, we add0 0\na vertex to our path by swapping it into the next position of our partial solution and fixing it in place, and then recursively generate permutations\non the remaining vertices not in our path. We then check if each partial solution is promising by comparing its lower bound with the best\ncomplete solution known so far (i.e., the upper bound). If it is promising, we continue extending the branch. Otherwise, we swap out the element\nwe most recently added to our partial solution and try something else in its place.\nperm_lengthEventually, if every partial solution along a branch is promising, you will end up in a position where is equal to the size of\npaththe vector. When this happens, the entire path is fixed, and you have a complete solution that could potentially be better than the best\nknown solution so far. In this case, you should check to see if your current path is indeed better, and update your current best solution if it is.\npath\n0\n2\n1\n3\n4\n5\n6\n7\n8\n9\nperm_length = 9\nThere are two important details to note here. First, a path is always promising does guarantee an optimal solution. This is because we usenot\nan to determine if a branch is promising, and it is entirely possible for that estimate to be too optimstic. Because of this, you cannotestimate\nblindly update your best solution if you reach the base case — instead, you will need to compare the current path with the best solution you’ve\ngen_perms()encountered to determine if it is actually better. Second, you cannot ensure that a solution is optimal until the initial call runs\nto completion. This is because you need to consider all possible solutions before you can safely conclude that one is better than the rest!", "word_count": 915, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d4fd287e-e848-59f1-9cd5-8da93ca96211", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 854, "real_page_number": null, "text": "842\nChapter 22. Backtracking and Branch and Bound\npromising()Remark: The function estimates the cost of continuing a partial solution, which allows you to prune branches that cannot\nlead to an optimal solution. However, there comes a point where the work required to estimate a lower bound is actually than the workgreater\nrequired to explore all the remaining branches. Suppose there are 𝑘vertices that remain unvisited in our current partial solution, where 𝑘=\npath.size() - perm_length. The cost of checking all remaining branches along this partial solution is 𝑘!, since we have to permute\n𝑘2 𝑘2over the 𝑘vertices that have not yet been added to the path. On the other hand, the cost of estimating a lower bound is +2𝑘, where the\nwork comes from building the MST, and the 2𝑘work comes from connecting the two endpoints of the current tour with their closest points\nin the MST. If we look at the total work required for different values for 𝑘, we would get the following:\nk\nTSP (k!)\n(k2Estimate 2k)+\n1\n1\n3\n2\n2\n8\n3\n6\n15\n4\n24\n24\n5\n120\n35\n6\n720\n48\n7\n5040\n63\n8\n40320\n80\n9\n362880\n99\n10\n3628800\n120\nIt’s pretty clear that you would prefer to estimate if 𝑘is large. However, if there are fewer than five points remaining, then you aren’t\ngetting much benefit from estimating a lower bound over simply brute forcing the remaining solutions. Thus, you can speed up your\npromising() truebranch-and-bound algorithm by having always return if the number of remaining points is less than 5.𝑘=\n1\nbool size_tpromising(std::vector<T>& path, perm_length) {\n2\nif (path.size() - perm_length < 5) {\n3\nreturn true;\n4\n} // if\n5\n... // estimate lower bound and compare with upper bound\n6\n} // promising()\n22.4\nTSP Heuristics\nIn the previous section, we devised a reasonably efficient branch and bound algorithm for solving the traveling salesperson problem. However, if\nwe were to run our branch and bound algorithm on an input file containing several hundred points, we would see that it would take an incredibly\nlong time to run. Even though branch and bound is an improvement over brute force, there are still a lot of branches that we have to explore, and\nthe cost of estimating a lower bound is not negligible, especially as the number of vertices grows.\nAt the end of the day, TSP is still a computationally expensive problem, and an efficient branch and bound implementation can still struggle\non large input sizes. Because of this, we often rely on heuristics to approximate solutions for TSP problems where the input size is prohibitively\nlarge. Unlike brute force or branch and bound, heuristics do guarantee an optimal solution, but they have better time complexities and oftennot\nproduce approximations that are close to optimal. Heuristics can be classified into separate categories based on their behavior: constructive\napproximate an optimal solution by extending a partial solution step-by-step using a predefined set of rules, whileheuristics local search\nstart with a complete solution and attempt to improve it by applying a series of small changes. In this section, we will discuss a fewheuristics\nheuristics that can be used to efficiently approximate the optional solution of a traveling salesperson problem.\n¸ 22.4.1\nNearest Neighbor and 2-Opt\nNearest neighbor and 2-opt are actually two different heuristics that can be combined to produce a close-to-optimal TSP solution in polynomial\ntime. Nearest neighbor is a relatively straightforward heuristic that relies on the greedy approach to identify an optimal path. The steps of the\nnearest neighbor heuristic are as follows:\n1. Start with an initial partial solution containing a single arbitrarily selected city.\n2. Identify the unvisited neighbor closest to the vertex most recently added to the partial solution, and add it to the path.\n3. After all points are added to the partial solution, connect the first and last vertices in the path to complete the solution.\nFor example, let’s run the nearest neighbor heuristic on the following points, with 𝐴as our initial starting vertex.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ", "word_count": 694, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6ba25c11-0f32-5a7f-86f9-8a72817e3dfb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 855, "real_page_number": null, "text": "22.4 TSP Heuristics\n843\nThe closest unvisited vertex to 𝐴is vertex 𝐶, so we add 𝐶to our partial solution next:\n➊\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nThe closest unvisited vertex to 𝐶is 𝐸, so we add 𝐸to our partial solution next:\n➋\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nThe closest unvisited vertex to 𝐸is 𝐷, so we add 𝐷to our partial solution next:\n➌\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nThe closest unvisited vertex to 𝐷is 𝐺, so we add 𝐺to our partial solution next:\n➍\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ", "word_count": 109, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "146a62d0-16ec-5bcb-914b-f369041fbbfc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 856, "real_page_number": null, "text": "844\nChapter 22. Backtracking and Branch and Bound\nContinuing this process, we would get the following (where ties are broken arbitrarily):\n➎\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n➏\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n➐\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n➑\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n➒\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ", "word_count": 76, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3cc284dd-33f7-544b-be5f-78d62ac172ea", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 857, "real_page_number": null, "text": "22.4 TSP Heuristics\n845\nAt the end, we connect the final point with our initial vertex. This is the result we get from the nearest neighbor heuristic:\n➓\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nConnection\nDistance\n𝐴→𝐶\n2.000\n𝐶→𝐸\n1.414\n𝐸→𝐷\n3.000\n𝐷→𝐺\n2.000\n𝐺→𝐼\n2.000\n𝐼→𝐻\n2.000\n𝐻→𝐹\n3.000\n𝐹→𝐵\n3.606\n𝐵→𝐽\n7.616\n𝐽→𝐴\n8.062\nTotal\n34.698\nΘ(𝑛2)The nearest neighbor heuristic can be implemented very efficiently for TSP, with a worst-case time complexity of for 𝑛points. However,\nas you can see, it does not always produce the best result. To improve this greedy solution, we can use a strategy known as 2-opt, which can be\nused to improve an existing, non-optimal tour. It turns out that, if you are given any two edges in a tour, you can safely connect thenon-adjacent\nfirst endpoints of the edges together and the second endpoints of the edges together without disconnecting the overall tour. That is, given two\nnon-adjacent edges 𝐴𝐵and 𝐶𝐷, you can replace them with 𝐴𝐶and 𝐵𝐷without invalidating your current path. This idea forms the basis of\n2-opt, which cost.iterates over all pairs of non-adjacent edges in an existing tour and performs a swap if doing so reduces the total\nFor example, consider the following tour, where there exists a crossing between 𝐵𝐷and 𝐸𝐶. If we perform 2-opt on this path, we would\nswap the endpoints of these two edges since it reduces the total weight of our path.\nA\nB\nC\nD\nE\nF\nWeight of Path: 6.828\nA\nB\nC\nD\nE\nF\nWeight of Path: 6\nTo swap two edges and (𝑗,𝑗+1), where 𝑖and 𝑗are the indices of two vertices in our path, we would replace them with the new edges(𝑖,𝑖+1)\nand (𝑖+1,𝑗+1). For instance, swapping vertices 𝐵and 𝐶on the path on the left turns it into the path on the right, where 𝐵𝐷and 𝐶𝐸are(𝑖,𝑗)\nstd::reverse()replaced with 𝐵𝐶and 𝐶𝐷. To do this efficiently, we can simply the order of all vertices from to 𝑗(the STL’s𝑖+1reverse\ncan be helpful here). For example, consider the path above:\nA →B →D →C →E →F →A\nIf we wanted to swap the edges 𝐵𝐷and 𝐶𝐸(𝑖=1, 3), we can reverse the order of all vertices between 𝐷and 𝐶.𝑗=\n→C →DA →B →E →F →A\nSimilarly, if we wanted to swap 𝐴𝐵and 𝐸𝐹(𝑖=0, 4), we would reverse the order of all vertices between 𝐵and 𝐸.𝑗=\n→E →D →C →BA →F →A\nHowever, a swap is only completed if it improves the solution. To determine whether a swap is a good idea, we will measure the cost differential\nof replacing the old edges with the new edges:\n=cost differential new cost - old cost\nIn the example above, swapping 𝐵𝐷and 𝐶𝐸is a beneficial decision, since it reduces the cost of the overall tour:\n= 𝑑𝑖𝑠𝑡(𝐵𝐶)+𝑑𝑖𝑠𝑡(𝐷𝐸)cost differential\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nnewedges\n- 𝑑𝑖𝑠𝑡(𝐵𝐷)−𝑑𝑖𝑠𝑡(𝐶𝐸)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\noldedges\n= -0.828\nOn the other hand, swapping 𝐴𝐵and 𝐸𝐹would make things worse, so a swap is not made:\n= 𝑑𝑖𝑠𝑡(𝐴𝐸)+𝑑𝑖𝑠𝑡(𝐵𝐹)cost differential\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nnewedges\n- 𝑑𝑖𝑠𝑡(𝐴𝐵)−𝑑𝑖𝑠𝑡(𝐸𝐹)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\noldedges\n= +0.828\nOne last detail to note is that after a swap is made, more beneficial swaps could arise that did not exist before. Because of this, we will need to\nthe 2-opt algorithm whenever a swap is made, so that we can discover these new beneficial changes. Failure to do so could cause us torestart\ntour.3overlook beneficial swaps that were unearthed by previous changes we made to the\n3Eventhough2-optmayneedtoberunmultipletimes,thereisatradeoffbetweenperformanceandaccuracyhere. Themoretimesyourun2-optonyourpath,\nthebetteryourpathdistancewillbe,butthelongeryouralgorithmwilltake. Wewilldiscussthisabitmoreaboutthisoverthenextfewpages.", "word_count": 646, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b99fe666-baea-5250-ac79-3c17e96971c5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 858, "real_page_number": null, "text": "846\nChapter 22. Backtracking and Branch and Bound\nThe pseudocode for the 2-opt heuristic is shown below. We iterate over all pairs of non-adjacent edges in our tour and determine if swapping the\ni + 1 jtwo edges improves the solution. If so, we reverse the order of all vertices from index to in our path vector. As long as an iteration of\n0.4for i =the nested loop discovers a path improvement and performs a swap, we restart another iteration of the outer loop from This process\ncontinues until no improvements can be made by swapping any two non-adjacent edges, giving us our final path.\n1\nbest_distance = calc_distance(current_path)\n// path after running nearest neighbor\n2\nhas_improvement = true\n// did you find a beneficial swap during the most recent iteration?\n3\nwhile has_improvement == true:\n4\nhas_improvement = false\n5\nfor i in range [0, num_cities - 2):\n6\nfor j in range [i + 2, num_cities):\n7\nchange = dist(i, j) + dist(i + 1, j + 1) - dist(i, i + 1) - dist(j, j + 1)\n8\nif change < 0:\n9\nreverse all vertices from index i + 1 to index j\n10\nbest_distance = best_distance + change\n11\nhas_improvement = true\nRemark: We will look at several different ways to implement the 2-opt heuristic in this section, so this is not the only way to do things! The\nimplementation we are currently discussing is the complete, standard implementation of 2-opt, but this may not meet the time constraints\nrequired for course assignments. Later in this section, we will look at ways to reduce the complexity of the standard 2-opt implementation to\nmeet these requirements (although at the cost of the heuristic’s accuracy).\nLet’s run this 2-opt heuristic on the path we obtained from using nearest neighbor.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nConnection\nDistance\n𝐴→𝐶\n2.000\n𝐶→𝐸\n1.414\n𝐸→𝐷\n3.000\n𝐷→𝐺\n2.000\n𝐺→𝐼\n2.000\n𝐼→𝐻\n2.000\n𝐻→𝐹\n3.000\n𝐹→𝐵\n3.606\n𝐵→𝐽\n7.616\n𝐽→𝐴\n8.062\nTotal\n34.698\nThe first pair of edges we consider are 𝐴𝐶and 𝐸𝐷. However, swapping these two edges does not improve our solution, so a swap is not made.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(A)𝑖=0\n(E)𝑗=2\nchange = dist(𝐴𝐸) + dist(𝐶𝐷) - dist(𝐴𝐶) - dist(𝐸𝐷)\n= 3.162 + 2.236 - 2 - 3 = +0.398 (↑cost, no swap)\nNext, we consider the edges 𝐴𝐶and 𝐷𝐺. Swapping these two edges also fails to improve the solution.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(A)𝑖=0\n(D)𝑗=3\nchange = dist(𝐴𝐷) + dist(𝐶𝐺) - dist(𝐴𝐶) - dist(𝐷𝐺)\n= 3.606 + 3.606 - 2 - 2 = +3.212 (↑cost, no swap)\n4Thepathvectorshouldhavethestartingvertexatboththebeginningandtheend,e.g.,𝐴→𝐵→𝐶→𝐴.", "word_count": 475, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b7aa03c4-4e36-5a6d-afb3-8a21e9e8f30f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 859, "real_page_number": null, "text": "22.4 TSP Heuristics\n847\nThe first edges we encounter that are beneficial to swap are edges 𝐴𝐶and 𝐵𝐽. Swapping these two edges gives us an improvement of 1.297,\nso we perform the swap as shown (the dotted edges represent the edges that are swapped). Notice that this reverses the direction of all edges\nsituated between the two new connections that were added.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(A)𝑖=0\n(B)𝑗=8\nchange = dist(𝐴𝐵) + dist(𝐶𝐽) - dist(𝐴𝐶) - dist(𝐵𝐽)\n= 2.236 + 6.083 - 2 - 7.616 = -1.297 (↓cost, swap 𝐴𝐶/𝐵𝐽with 𝐴𝐵/𝐶𝐽)\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of New Solution: 34.698 - 1.297 = 33.401\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nThe next beneficial swap we encounter involve the edges 𝐵𝐹and 𝐶𝐽.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(B)𝑖=1\n(C)𝑗=8\nchange = dist(𝐵𝐶) + dist(𝐹𝐽) - dist(𝐵𝐹) - dist(𝐶𝐽)\n= 2.236 + 6.403 - 3.606 - 6.083 = -1.049 (↓cost, swap 𝐵𝐹/𝐶𝐽with 𝐵𝐶/𝐹𝐽)\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of New Solution: 33.401 - 1.049 = 32.352\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ", "word_count": 209, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3936d7e5-4625-5956-8489-f0f11fe98309", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 860, "real_page_number": null, "text": "848\nChapter 22. Backtracking and Branch and Bound\nThe remaining swaps performed during the 2-opt process are shown below.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(B)𝑖=1\n(J)𝑗=9\nchange = dist(𝐵𝐽) + dist(𝐶𝐴) - dist(𝐵𝐶) - dist(𝐽𝐴)\n= 7.616 + 2 - 2.236 - 8.062 = -0.683 (↓cost, swap 𝐵𝐶/𝐽𝐴with 𝐵𝐽/𝐶𝐴)\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of New Solution: 32.352 - 0.683 = 31.669\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(... a few iterations omitted ...)\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(J)𝑖=2\n(I)𝑗=5\nchange = dist(𝐽𝐼) + dist(𝐹𝐺) - dist(𝐽𝐹) - dist(𝐼𝐺)\n= 3.162 + 2.236 - 6.403 - 2 = -3.005 (↓cost, swap 𝐽𝐹/𝐼𝐺with 𝐽𝐼/𝐹𝐺)\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of New Solution: 31.669 - 3.005 = 28.664\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(... a few iterations omitted ...)", "word_count": 167, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7a86e762-90ee-520c-9152-9eeecece9604", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 861, "real_page_number": null, "text": "22.4 TSP Heuristics\n849\nforThe first iteration of the nested loop completes here, since there are no more beneficial swaps among adjacent pairs for (i.e.,𝑖> 2\npairs situated after vertex 𝐽, which is at index 2 of our current path). Since we were able to improve the path in this first iteration (i.e.,\nhas_improvement true), i = 0.is we begin a second iteration again from This allows us to discover new swaps that can be made after\nprevious changes we made to the tour. During this second iteration, the first beneficial swap we encounter is between 𝐵𝐽and 𝐹𝐺:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(B)𝑖=1\n(F)𝑗=5\nchange = dist(𝐵𝐹) + dist(𝐽𝐺) - dist(𝐵𝐽) - dist(𝐹𝐺)\n= 3.606 + 4.243 - 7.616 - 2.236 = -2.003 (↓cost, swap 𝐵𝐽/𝐹𝐺with 𝐵𝐹/𝐽𝐺)\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nWeight of New Solution: 28.664 - 2.003 = 26.661\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nNo other beneficial swaps are found during this second iteration. Since we were still able to improve the path during our previous iteration, we\nforwill begin a third iteration of the nested loop using this updated path. However, this third iteration fails to find any beneficial swaps, so\nhas_improvement false. whileremains This causes the loop to terminate, completing our algorithm. The final tour is shown below:\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 26.661\nNote that 2-opt is a heuristic, so this solution is to be the actual optimal solution! In fact, you can see that this path is actually notnot guaranteed\noptimal: during the second iteration, we could have gotten a better solution had we swapped 𝐵𝐽and 𝐷𝐸(𝑖=1, 7) instead of 𝐵𝐽and𝑗= 𝐹𝐺\n(𝑖=1, 5). However, our algorithm does not know this, so we perform a swap as soon as we encounter one that is beneficial — this causes𝑗=\nthe less optimal swap to be performed (since is considered before 7). Since swapping one of these pairs precludes us from swapping𝑗= 𝑗=5\nthe other, the algorithm as currently implemented does not find the optimal solution. However, this is okay, since the goal of a heuristic is to\nfind a solution that is close to optimal in a reasonable amount of time (which 2-opt successfully achieves).", "word_count": 401, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f1ac08ba-3eb6-519b-a5da-9faf8a0a3fd2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 862, "real_page_number": null, "text": "850\nChapter 22. Backtracking and Branch and Bound\nforWhat is the time complexity of this 2-opt implementation (reproduced below)? Let’s first look at the nested loop spanning from lines 5-11.\nforHere, the body of the inner loop can take up to time, since we need to perform a reversal whenever a swap is made (and reversingΘ(𝑛)\nΘ(𝑛2)takes linear time). Since this work is the most expensive step and may be run up to times when looping over all pairs of non-adjacent\nΘ(𝑛2)×Θ(𝑛) Θ(𝑛3).foredges (lines 5 and 6), the overall worst-case time complexity of the nested loop is =\n1\nbest_distance = calc_distance(current_path)\n// path after running nearest neighbor\n2\nhas_improvement = true\n// did you find a beneficial swap during the most recent iteration?\n3\nwhile has_improvement == true:\n4\nhas_improvement = false\n5\nfor i in range [0, num_cities - 2):\n6\nfor j in range [i + 2, num_cities):\n7\nchange = dist(i, j) + dist(i + 1, j + 1) - dist(i, i + 1) - dist(j, j + 1)\n8\nif change < 0:\n9\nreverse all vertices from index i + 1 to index j\n10\nbest_distance = best_distance + change\n11\nhas_improvement = true\nwhile forHowever, what about the outer loop that repeats as long as an improvement has been made? The nested loop on lines 5 and 6 may\nwhiletake polynomial time, but the overall performance of 2-opt also depends on how many times the outer loop on line 3 executes.\nwhileThis outer loop runs as long as there are pairs in our path that can be swapped to further improve the path’s total distance. Just\nhow many times can this happen? The answer to this problem is not trivial. van Leeuwen and Schoone (1980) proved that an algorithm which\n𝑂(𝑛3)removes intersections from a Hamiltonian path of 𝑛vertices on an Euclidean plane will ultimately produce an intersection-free tour after\nremoved.5 𝑂(𝑛3)removals, regardless of the order in which these intersections are However, this bound assumes the restriction that 2-opt can\nonly perform swaps to remove edges that intersect in the path. If we are allowed to perform a swap between any pair of non-adjacent edges that\nreduces total cost (regardless of whether the pair forms an intersection in the path), then the total number of swaps performed by 2-opt can\n(2014).6become exponential in the worst case, as proven by Englert, Röglin, and Vöcking\nwhile forTo avoid this, one optimization is to disregard the outer loop entirely. Instead, the nested loop is run a constant number of\ntimes, regardless of the number of improvements that are made. By doing so, we sacrifice the accuracy of the algorithm for improved efficiency.\nforThe following pseudocode is similar to the one above, except that it only runs the nested loop once.\n1\nbest_distance = calc_distance(current_path)\n// path after running nearest neighbor\n2\nfor i in range [0, num_cities - 2):\n3\nfor j in range [i + 2, num_cities):\n4\nchange = dist(i, j) + dist(i + 1, j + 1) - dist(i, i + 1) - dist(j, j + 1)\n5\nif change < 0:\n6\nreverse all vertices from index i + 1 to index j\n7\nbest_distance = best_distance + change\nforA slightly more accurate (but less performant) implementation would still run the nested loop a constant number of times, but would\nforinstead the inner loop whenever an improvement is made (note that the loop is restarted and not the outer loop). This ensuresrestart inner\nthat you can still perform beneficial swaps that are unearthed by previous swaps within the same iteration of the inner loop without having to\nrestart the entire algorithm. This is shown below (again, this only runs the nested loops once, but you can tweak this based on your needs for\nperformance vs. accurary — if you run the algorithm more times, you will get a better approximation, but your performance will also be slower).\n1\nbest_distance = calc_distance(current_path)\n// path after running nearest neighbor\n2\nfor i in range [0, num_cities - 2):\n3\nfor j in range [i + 2, num_cities):\n4\nchange = dist(i, j) + dist(i + 1, j + 1) - dist(i, i + 1) - dist(j, j + 1)\n5\nif change < 0:\n6\nreverse all vertices from index i + 1 to index j\n7\nbest_distance = best_distance + change\n8\nrestart inner for loop from j = i + 2\nRemark: The 2-opt heuristic improves an existing solution by reconnecting of non-adjacent edges until no more pairs can be swappedpairs\nto further improve the solution. However, 2-opt is not the only heuristic that uses this strategy to optimize a tour. Another heuristic, 3-opt,\ndoes the same thing, but it instead reconnects edges in groups of three rather than two.\nHeuristics such as 2-opt and 3-opt are often defined as heuristics, which improve existing solutions by repeatedly reconnecting 𝑘edgesk-opt\nof a tour until no more changes can be made to improve the solution. A 3-opt approach often yields a better approximation of the optimal\nsolution than 2-opt, but it is also more computationally expensive (and therefore slower to run). For this class, you will not need to worry\nabout 3-opt (or any value of 𝑘above that) — 2-opt should be more than enough for any TSP problem you are required to solve.\n5\"UntanglingaTravelingSalesmanTourinthePlane\"byvanLeeuwenandSchoone(1980).\n6\"WorstCaseandProbabilisticAnalysisofthe2-OptAlgorithmfortheTSP\"byEnglert,Röglin,andVöcking(2014).", "word_count": 940, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "511e5f7f-02db-51c2-979c-5df9f78d7d7d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 863, "real_page_number": null, "text": "22.4 TSP Heuristics\n851\n¸ 22.4.2\nNearest Insertion\nAnother group of heuristics that can be used to approximate a solution for TSP are heuristics. Insertion heuristics work by judiciallyinsertion\nadding vertices to a partial path in a way that minimizes the additional cost incurred at each step. In thissection, we will introduce three insertion\nΘ(𝑛2)heuristics that can produce a TSP estimate in time: nearest, farthest, and arbitrary insertion.\nNearest Insertion\n1. Initialize a partial tour with a vertex 𝑖, chosen arbitrarily (you can just start with the first vertex available).\n2. Identify the vertex 𝑗such that the distance between 𝑖and 𝑗is minimum, and set the partial tour to 𝑖→𝑗→𝑖.\n3. Identify the vertex in the partial tour that is to any node that is already in the partial tour.𝑘not closest\n4. Find the best place to insert vertex 𝑘into the partial tour to minimize cost. To do this, first identify the edge in the partial path(𝑖,𝑗)\nsuch that 𝑐𝑘𝑗−𝑐𝑖𝑗is minimal (where 𝑐𝑖𝑗denotes the distance between 𝑖and 𝑗), and then insert 𝑘between 𝑖and 𝑗. Notice that𝑐𝑖𝑘+\n𝑐𝑖𝑘+𝑐𝑘𝑗−𝑐𝑖𝑗represents the change in overall cost after vertex 𝑘is added in between vertices 𝑖and 𝑗, since you are removing the cost\nbetween 𝑖and 𝑗and adding the costs incurred by connecting 𝑘with 𝑖and 𝑗.\n5. Once all the vertices have been added to the path, the algorithm completes. Otherwise, repeat steps 3-5 until all vertices have been added.\nFor example, let’s run the nearest insertion heuristic on our previous points, using vertex 𝐴as our starting vertex:\n➊\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 0.000\nFirst, we find the closest vertex to 𝐴, which in this case is vertex 𝐶. Thus, we will start our initial path as 𝐴→𝐶→𝐴:\n➋\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 4.000\nNext, we want to find the vertex in our current path that is closest to any of the existing vertices in our path. Here, the closest vertex is vertexnot\n𝐸, as it is closest to vertex 𝐶. We then iterate over the edges of our current path to find the position to insert vertex 𝐸. In this case, there are\nonly two vertices connected, so the only option is to place 𝐸in between 𝐴and 𝐶.\n➌\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 6.576", "word_count": 404, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "28763338-8bb1-5689-8a85-516b18325358", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 864, "real_page_number": null, "text": "852\nChapter 22. Backtracking and Branch and Bound\nThe next vertex to add is vertex 𝐵, since it is closest to the vertices in our current partial path. The optimal position to insert 𝐵is between 𝐴and\n𝐶, since it minimizes the cost of adding 𝐵to the path.\n➍\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 9.048\nContinuing this process, we would build our tour in the following manner:\n➎\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 11.048\n➏\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 14.418\n➐\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 18.197\n➑\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 21.025\n➒\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 23.433\n➓\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 27.293\nThis the final result of the nearest insertion heuristic, with total weight 27.293.", "word_count": 184, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "480fcd92-ded5-554b-a706-1aa3c25035e7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 865, "real_page_number": null, "text": "22.4 TSP Heuristics\n853\n¸ 22.4.3\nFarthest Insertion\nFarthest insertion is a similar heuristic, but as its name implies, it inserts vertices that are from the current partial path rather thanfarthest\nnearest. The steps for this heuristic are as shown:\nFarthest Insertion\n1. Initialize a partial tour with a vertex 𝑖, chosen arbitrarily (you can just start with the first vertex available).\n2. Identify the vertex 𝑗such that the distance between 𝑖and 𝑗is maximum, and set the partial tour to 𝑖→𝑗→𝑖.\n3. Identify the vertex in the partial tour that is from any node that is already in the partial tour.𝑘not farthest\n4. Find the best place to insert vertex 𝑘into the partial tour to minimize cost. To do this, first identify the edge in the partial path(𝑖,𝑗)\nsuch that 𝑐𝑘𝑗−𝑐𝑖𝑗is minimal (where 𝑐𝑖𝑗denotes the distance between 𝑖and 𝑗), and then insert 𝑘between 𝑖and 𝑗. Notice that𝑐𝑖𝑘+\n𝑐𝑖𝑘+𝑐𝑘𝑗−𝑐𝑖𝑗represents the change in overall cost after vertex 𝑘is added in between vertices 𝑖and 𝑗, since you are removing the cost\nbetween 𝑖and 𝑗and adding the costs incurred by connecting 𝑘with 𝑖and 𝑗.\n5. Once all the vertices have been added to the path, the algorithm completes. Otherwise, repeat steps 3-5 until all vertices have been added.\nLet’s run the farthest insertion heuristic on our previous points, again with vertex 𝐴as our starting vertex:\n➊\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 0.000\nFirst, we find the farthest vertex from 𝐴, which in this case is vertex 𝐻. Thus, we will start our initial path as 𝐴→𝐻→𝐴:\n➋\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 16.124\nNext, we want to find the vertex in our current path that is farthest from any of the existing vertices in our path, which in this case is vertexnot\n𝐽. We then iterate over all the edges of our existing path to determine the best position to insert 𝐽. Here, there are only two vertices connected,\nso the only option is to place 𝐽in between 𝐴and 𝐻.\n➌\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 21.223", "word_count": 365, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "19e62402-81c3-5e61-b337-0edd834c56f0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 866, "real_page_number": null, "text": "854\nChapter 22. Backtracking and Branch and Bound\nThe next vertex to add is vertex 𝐷, since it is farthest from any of the vertices in our current partial path. The optimal position to insert 𝐷is\nbetween 𝐴and 𝐻, since it minimizes the cost of adding 𝐷to the path.\n➍\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 21.239\nContinuing this process, we would build our tour in the following manner:\n➎\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 21.339\n➏\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 22.103\n➐\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 22.733\n➑\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 24.733\n➒\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 24.796\n➓\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 25.049\nThis is the final result of the farthest insertion heuristic, with total weight 25.049.", "word_count": 187, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "85271b8e-167c-5988-b0d5-1735cef2cba9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 867, "real_page_number": null, "text": "22.4 TSP Heuristics\n855\n¸ 22.4.4\nArbitrary Insertion\nLastly, we have arbitrary insertion, which arbitrarily selects vertices to add to the partial path (instead of finding the vertex that is closest or\nfarthest away). The steps of arbitrary insertion are essentially identical to those of nearest insertion, without the step of identifying the vertex\nthat is closest to any node already in the partial tour.\nArbitrary Insertion\n1. Initialize a partial tour with a vertex 𝑖, chosen arbitrarily (you can just start with the first vertex available).\n2. Choose another arbitrary vertex 𝑗and set the initial partial tour to 𝑖→𝑗→𝑖.\n3. Arbitrarily select a vertex 𝑘that is currently not in the partial tour.\n4. Find the best place to insert vertex 𝑘into the partial tour to minimize cost. To do this, first identify the edge in the partial path(𝑖,𝑗)\nsuch that 𝑐𝑘𝑗−𝑐𝑖𝑗is minimal (where 𝑐𝑖𝑗denotes the distance between 𝑖and 𝑗), and then insert 𝑘between 𝑖and 𝑗. Notice that𝑐𝑖𝑘+\n𝑐𝑖𝑘+𝑐𝑘𝑗−𝑐𝑖𝑗represents the change in overall cost after vertex 𝑘is added in between vertices 𝑖and 𝑗, since you are removing the cost\nbetween 𝑖and 𝑗and adding the costs incurred by connecting 𝑘with 𝑖and 𝑗.\n5. Once all the vertices have been added to the path, the algorithm completes. Otherwise, repeat steps 3-5 until all vertices have been added.\nFor example, let’s run arbitrary insertion on the previous set of points. Since vertices can be chosen arbitrarily, we will add vertices to the path\nin alphabetical order. We begin with 𝐴as our starting vertex:\n➊\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 0.000\nNext, we choose an arbitrary vertex and add it to our path. Since we are adding vertices in alphabetical order, we will add vertex B:\n➋\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 4.472\nThen, we add the unvisited vertex that is next alphabetically within the collection of unvisited vertices, or vertex 𝐶:\n➌\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 6.472\nRemark: Since you are selecting points arbitrarily when deciding how to build your path, it does not matter what your starting points are\n(unlike nearest or farthest insertion, which require you to start with the unvisited vertex that is nearest or farthest from the starting vertex).\nAs a result, with arbitrary insertion, you can skip the first two iterations above and just start your partial tour as 𝐴→𝐵→𝐶→𝐴.", "word_count": 412, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8ad0bdb9-fc86-5933-b166-786750b70601", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 868, "real_page_number": null, "text": "856\nChapter 22. Backtracking and Branch and Bound\nThe next vertex to add is vertex 𝐷, which is inserted into the path as shown:\n➍\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 8.472\nContinuing this process, we would build our tour in the following manner:\n➎\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 10.650\n➏\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 14.492\n➐\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 16.492\n➑\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 20.084\n➒\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 21.256\n➓\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nCurrent Weight of Path: 26.661", "word_count": 148, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "47479f4e-f885-558a-a3b7-d815d7c5ea26", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 869, "real_page_number": null, "text": "22.4 TSP Heuristics\n857\nThis is the final result of the arbitrary insertion heuristic, with total weight 26.661. As you can see, all three of these insertion heurstics were\ntime.7Θ(𝑛2)able to produce solutions that were close to optimal (25.049) in If you aren’t concerned with finding the exact optimal solution for\na problem, a heuristic can provide a significant improvement over a branch-and-bound approach!\nRemark: The heuristics we have covered so far are not exhaustive, and many other heuristics exist for the traveling salesperson problem.\nFor instance, is another insertion heuristic that behaves similarly to nearest insertion, but instead of choosing to insertcheapest insertion\nthe vertex that is closest to any vertex the current partial tour, it chooses the vertex that, when added, results in the shortest possible tour.\nis a heuristic that uses advanced graph concepts to approximate a solution that is always within a factor of 3/2 ofChristofides algorithm\nthe optimal solution. The is an adaptive local-search heuristic that dynamically adjusts between different 𝑘-optLin-Kernighan heuristic\nheuristics to produce an estimate that is closer to optimal.\nMany of these more advanced heuristics may deliver better approximations, but similar the 3-opt approach discussed earlier, they are also a\nΘ(𝑛2)lot more expensive to compute. For the class, a heuristic such as nearest, farthest, or arbitrary insertion is enough for generating a\nreasonably close approximation for TSP in a short amount of time, which is why these additional heuristics are not covered in more detail.\nSo far, we have talked about how heuristics can be useful for approximating TSP in polynomial time. However, what if you absolutely needed to\nknow the exact optimal solution? Would heuristics be useless in such a scenario?\nAlthoughheuristicscannotguaranteeoptimality,theycanstillbeusefulinhelpingyoufindtheoptimalsolutionduringthebranch-and-bound\nprocess. Recall that the efficiency of branch-and-bound for TSP relies on an accurate bound, which is used to prune branches we canupper\nguaranteearesub-optimal. Currently, weinitiallysettheupperboundtothecostofthefirstcompletesolutionweencounter(whichwearbitrarily\ndecided as the path 𝐴→𝐵→𝐶→𝐷→𝐸→𝐹→𝐺→𝐻→𝐼→𝐽→𝐴). Every branch that we explore later would then be compared to this\ninitial bound until a better solution is found.\nSince the upper bound is used as a threshold to decide whether a partial branch is promising, we want our upper bound to be as close as\npossible to the actual optimal solution to support efficient pruning. However, there is no guarantee that the first complete solution we find is\nclose to optimal! If our first solution is too costly, we would waste time exploring sub-optimal branches at the beginning of our algorithm.\nTo address this issue, we can instead set our initial upper bound to rather than the first solution we find during ourthe solution of a heuristic\nsearch. This ensures that our initial upper bound is close-to-optimal, allowing us to prune branches efficiently from the get-go. Even though a\nheuristic may take time to run, the cost savings of setting a tighter upper bound is often well worth this extra effort!\n7Don’t judge the performance ofeach heuristicbased onthis example alone! Even thoughfarthestinsertionended up performingthe best,this dependson the\ninput. Givenanothersetofcities,itisentirelypossibleforadifferentheuristictoperformbetter.", "word_count": 563, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e2fd6df6-6500-5696-9128-bf609df9a5d9", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 870, "real_page_number": null, "text": "858\nChapter 22. Backtracking and Branch and Bound\nChapter 22 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Which of the following is a constraint satisfaction problem and not an optimization problem?\nA) Finding the route with the minimum distance to New York City\nB) Finding the minimum spanning tree (MST) of a graph\nC) Finding the shortest path out of a maze\nD) Finding a route between two classrooms without going outside\nE) None of the above\n2. Which of the following statements is/are TRUE?\nI. Branch and bound is an algorithmic strategy that can be used to find the optimal solution to a constraint satisfaction problem.\nII. Branch and bound algorithms immediately stop exploring the search space once the first solution satisfying all constraints is\nfound.\nIII. The efficiency and correctness of a branch and bound algorithm may depend on the effectiveness of the pruning function.\nA) I only\nB) III only\nC) I and II only\nD) I and III only\nE) I, II, and III\n3. You are currently running the traveling salesperson problem (TSP) to try to find the minimum distance required to visit 50 locations in the\nstate of Michigan. The upper bound you have using the branch and bound algorithm is 331 miles. Currently, you are considering a path\nwhere the distance required to visit 25 locations is 165 miles, the MST connecting the remaining 25 locations has a total distance of 141\nmiles, and the connections that link the current path to the MST have a total distance of 10 miles. Which of the following is TRUE?\nA) Because the distance of the current path, 316 miles, is less than the current upper bound of 331 miles, the upper bound should be\nchanged to 316 miles\nBecause the distance of the current path, 316 miles, is less than the current upper bound of 331 miles, the current path beingB)\nconsidered should be dropped as a potential solution\nC) The current path being considered is guaranteed to be the lowest-weighted Hamiltonian cycle\nD) The current path being considered cannot be the lowest-weighted Hamiltonian cycle\nE) Because the weight of the path being considered has a minimum possible distance of 316 miles, it would be okay to consider this as\na lower bound for extending this potential solution\n4. You are currently using branch and bound to calculate the TSP path for 20 different points. A snapshot of the branch and bound process is\nshown below:\nAt the moment of the above snapshot, the value of the upper bound is 129. Let 𝑑represent the total weight of the MST that connects all 12\npoints within the dotted region, including the point on the border. Which of the following statements MUST be true?\nA) If 𝑑is less than 100, the upper bound should be changed\nB) If 𝑑is equal to 100, the upper bound should be changed\nC) If 𝑑is greater than 100, the upper bound should be changed\nD) Both A and C\nE) None of the above\n5. Which of the following statements is TRUE?\nA) Heuristics are algorithms that can be used to calculate an optimal solution very quickly\nB) Heuristics often have time complexities that are much more optimal than that of brute force\nC) Heuristics are extremely useful if the standard method for solving a problem takes too long\nD) Heuristics can be used to estimate an upper bound to a TSP problem\nE) None of the above", "word_count": 620, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "3e13ce15-112e-5b28-820a-ac1322b74cc1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 871, "real_page_number": null, "text": "22.4 TSP Heuristics\n859\n6. Suppose you want to visit the computer science buildings of every major university in the United States, and you want to find the shortest\nroute that will allow you to visit each campus exactly once before returning to your starting location. What is the worst-case time complexity\nof using brute force to calculate this shortest path, assuming that you have 𝑛campuses that you want to visit?\nΘ(2𝑛)A)\nΘ(𝑛2𝑛)B)\nΘ(𝑛2)C)\nΘ(𝑛𝑛)D)\nE) Θ(𝑛!)\n7. Which of the following statements is/are TRUE?\nI. Running the branch and bound algorithm on the traveling salesperson problem is guaranteed to make the program run faster,\ncompared to a brute force solution.\nII. The worst-case time complexity of running the traveling salesperson problem using a branch and bound algorithm is better than\nthe worst-case time complexity of solving the traveling salesperson problem using brute force.\nIII. When solving the traveling salesperson problem using branch and bound, overestimating the cost of extending a partial solution\ncould cause you to fail to find the optimal solution.\nA) I only\nB) II only\nC) III only\nD) II and III only\nE) I, II, and III\n8. Which of the following statements is/are TRUE?\nI. When solving a minimization problem using branch and bound, a branch should be pruned if the upper bound is less than the\nlower bound.\nWhen solving a maximization problem using branch and bound, a branch should be pruned if the upper bound is greater than theII.\nlower bound.\nIII. When solving a maximization problem using branch and bound, the upper bound is calculated by taking the current partial\nsolution and adding it to an underestimate of the value obtainable from completing the partial solution.\nA) I only\nB) II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n9. You are given two fully connected graphs, one with four vertices and the other with five vertices. How many more distinct Hamiltonian\ncycles exist in the graph with five vertices, compared to the one with four vertices? That is, if the graph with four vertices has distinctℎ4\nHamiltonian cycles and the graph with five vertices has distinct Hamiltonian cycles, what is the value of −ℎ4?ℎ5 ℎ5\nA) 24\nB) 48\nC) 72\nD) 96\nE) 120\n10. Which of the following problems can be solved using a backtracking algorithm?\nI. Finding the minimum spanning tree of a graph\nII. Determining if a graph can be colored with 𝑛different colors without any adjacent vertices sharing the same color\nIII. Implementing a Sudoku solver\nA) I only\nB) II only\nC) III only\nD) II and III only\nE) I, II, and III\n11. Which of the following statements is true about branch and bound, but not necessarily true for backtracking?\nA) The algorithm can be used to solve problems involving constraints\nB) The algorithm will return an optimal solution\nC) The algorithm typically improves on the runtime performance of brute force\nD) The algorithm prunes partial solutions that cannot yield a valid solution\nE) None of the above", "word_count": 521, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "627e8bbc-a0fe-5dda-8ad6-8c9375e1d5ca", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 872, "real_page_number": null, "text": "860\nChapter 22. Backtracking and Branch and Bound\n12. You are using a branch and bound algorithm to determine the shortest route you can take to visit five buildings on campus. The following\ntable stores the distances between each pair of buildings that you want to visit:\nBBB\nDOW\nEECS\nFXB\nGGBL\nBBB\n0\n8\n19\n30\n17\nDOW\n8\n0\n20\n28\n13\nEECS\n19\n20\n0\n16\n12\nFXB\n30\n28\n16\n0\n15\nGGBL\n17\n13\n12\n15\n0\nThe shortest complete solution you have found so far has a total distance of 71. You begin looking down a new branch: BBB →FXB →\nDOW →GGBL. Should you prune this branch?\nA) Yes, the partial branch has the same length as the shortest complete path seen so far\nB) Yes, the partial branch is longer than the shortest complete path seen so far\nC) Yes, the partial branch is shorter than the shortest complete path seen so far\nD) No, the branch has the same length as the shortest complete path seen so far\nE) No, the branch is shorter than the shortest complete path seen so far\n13. Given two integers 𝑛and 𝑘, return all possible combinations of 𝑘numbers out of 1, …, 𝑛. You may return the answer in any order.\nstd::vector<std::vector<int32_t>> k_combinations(int32_t int32_tn, k);\nExample: Given 4, 2, one possible solution is:𝑛= 𝑘=\n[ [2, 4], [3, 4], [2, 3], [1, 2], [1, 3], [1, 4] ]\n14. A valid IP address consists of exactly four integers separated by single dots. Each integer must have a value between 0 and 255 (inclusive)\nandcannothaveanyleadingzeros(otherthan0itself). Forexample,\"183.70.2.81\"isavalidIPaddress,but\"18.3.70.281\"and\"18.37.028.1\"\ndigits digits.are not. Given a string containing only digits, return all possible IP addresses that can be formed by inserting dots into\nYou are not allowed to reorder or remove any digits, and you may return the valid IP addresses in any order.\nget_valid_ips(conststd::vector<std::string> std::string& digits);\ndigits = \"183203281\",Example: Given you would return the following IP addresses (in any order):\n[\"183.20.32.81\", \"183.203.2.81\", \"183.203.28.1\"]\nstd::stoi()Hint: You can use to parse a string into an integer.\n15. In the game of chess, the is a piece that can only move diagonally. The movement capability of a bishop is shown on the board below:bishop\n1\na\n2\nb\n3\nc\n4\nd\n5\ne\n6\nf\n7\ng\n8\nh\nb\nYou want to implement a backtracking solution to the N-Bishops problem, where you want to return all possible placements of 𝑁bishops\npromising()on an 𝑁×𝑁chessboard such that no bishop threatens any other bishop on the board. Implement the following function,\nrow col, std::vector<std::vector<bool>>whichdetermineswhetheritispossibletoplaceabishopatrow andcolumn givena\nboard[i][j] true i, j).that stores the current contents of the board (where being indicates there is a bishop at row column You may\nrowassume that the board is square (i.e., same number of rows and columns), and that only bishops up to row have already been placed.\nbool promising(const std::vector<std::vector<bool>>& size_t size_tboard, row, col);", "word_count": 531, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "493b400b-5b56-5411-af28-a107990bf7c1", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 873, "real_page_number": null, "text": "22.4 TSP Heuristics\n861\nChapter 22 Exercise Solutions\n1. The correct answer is (D). For constraint satisfaction, you just want to know if a solution exists, not what the best solution is. This only\napplies to answer choice (D). In choice (A), you want to find the distance to New York City; in choice (B), you want to find theminimum\nspanning tree; and in choice (C), you want to find the path of the maze.minimum shortest\n2. The correct answer is (D). Only statement II is false, since branch and bound continues to explore the remaining search space in case\na better solution can be found. Statement I is true, since branch and bound can be used to find the optimal solution of a problem, and\nstatement III is true, since a pruning function that prunes too much can result in an incorrect solution, and a pruning function that prunes\ntoo little may degrade performance since time is spent exploring paths that could have been discarded.\n3. The correct answer is (E). The total weight of the MST of the non-visited points is a lower bound, but it is not guaranteed to be the optimal\nsolution. This is because the MST of the remaining points may not actually be a valid path, as shown below:\nHere, the image on the left represents the MST, while the image on the right represents the optimal path that crosses these points. Note that\nthe figure on the left is not a valid path that visits each point once! In this problem, you are told that the MST has a distance of 141 miles,\nbut you cannot conclude that the optimal path also has a distance of 141 miles. Thus, choice (A) is not correct: the upper bound should not\nbe changed to 316 miles, as the path length can only be 316 miles if the minimum spanning tree is also the optimal path (which, as shown\nabove, is not always true). However, we do know that the optimal weight through the points than the weight of the MST,cannot be smaller\nas the MST must have the smallest possible weight. As a result, we know that the distance of the path we are considering cannot be less than\n316 miles, making (E) the correct answer. Note that (B) is not true because the path is still promising (the best we can get from continuing\nthe path is 316 miles, which is less than the best so far of 331 miles, so we should continue exploring the branch). Options (C) and (D) are\nfalse because you cannot conclude from this information that the current partial path leads to the optimal solution, or that it doesn’t.\n4. The correct answer is (E). The upper bound is the best complete solution encountered so far. Similar to the explanation for the previous\nproblem, knowing the weight of the MST only allows you to identify whether to continue exploring the branch or not, but you should not\nupdate the upper bound until you compute the weight of a complete solution that is better than the best solution you have encountered so far.\n5. The correct answer is (A). Heuristics do not always calculate the optimal solution. The precision of the calculation is sacrificed for speed,\nbut these algorithms to provide answers that are close enough. This is useful for solving problems like TSP where the actual process of\nfinding the best solution takes up a lot of time and may be too expensive to complete in practice.\n6. The correct answer is (E). Brute force involves iterating over all possibilities and returning the minimum distance, which would require\ntime since there are possible tours. In other words, brute force tries every possible permutation of these 𝑛cities, and there areΘ(𝑛!) 𝑛!\npermutations in total (𝑛locations to choose for location 1, ways to move from location 1 to location 2, ways to move from𝑛! 𝑛−1 𝑛−2\nlocation 2 to location 3, …, 1 way to move from location to location 𝑛).𝑛−1\n7. The correct answer is (C). Statement I is false because there is no guarantee that branch and bound will make the program run faster; it is\npossible that we generate permutations in an order such that the first permutation is better than our initial upper bound, the next permutation\nis better than the first permutation, and so on — in this case, nothing gets pruned, and the program does not run faster. Statement II is false\nfor similar reasons, we could end up exploring branches in a way such that we are not able to prune anything, which would yield a time\ncomplexity that is the same as brute force. Statement III is true: if you overestimate the cost of extending a partial solution, you may end up\ntreating the path is not promising even though it actually may lead to the optimal solution.\n8. The correct answer is (A). Statement I is true: when solving a minimization problem using branch and bound, the upper bound is the\nbest solution encountered so far, so if the lower bound of extending the current branch is already worse than this upper bound, the branch\nshould be pruned. Statement II is false: when solving a maximization problem using branch and bound, the lower bound is the best solution\nencountered so far, so if the upper bound is greater than the lower bound, we could still find a better solution and the branch should not be\npruned (note that we are maximizing, so a higher upper bound is desired since that means the current branch could yield a larger solution\nthat the best we have encountered). Statement III is false: when solving a maximization problem, we want to overestimate the upper bound\nsince underestimating would cause us to believe that a branch may not yield an optimal solution even when it could.\n9. The correct answer is (D). There are 4! = 24 distinct Hamiltonian cycles in the graph with four vertices, and 5! = 120 distinct Hamiltonian\ncycles in the graph with five vertices. Thus, there are 120 - 24 = 96 more Hamiltonian cycles in the graph with five vertices than the one\nwith four vertices.\n10. The correct answer is (D). Backtracking can be used to solve constraint satisfaction problems, but it is not guaranteed to find an optimal\nsolution. Thus, it cannot be used to solve I, which is an optimization problem, but it can be used to solve II and III, which care only about\nthe existence of a solution rather than a best solution.\n11. The correct answer is (B). Branch and bound can be used to find an optimal solution, while backtracking is designed to find the existence\nof a solution (and not necessarily the best). All the other choices apply to both backtracking and branch and bound.\n12. The correct answer is (A). The current partial branch has a weight of 30 + 28 + 13 = 71, which is the same as the best complete solution\nfound so far. Since this partial branch already has a weight equal to a complete solution, there is no reason to continue extending this branch\n(since at best the outcome you get cannot go below 71), and the branch can be pruned.", "word_count": 1220, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "d689e555-4c34-5582-a83f-45104e9e4051", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 874, "real_page_number": null, "text": "862\nChapter 22. Backtracking and Branch and Bound\n13. This is a constraint satisfaction problem that can be solved using a backtracking approach. Similar to examples 22.1 and 22.2, we will add\nvalues to a running collection, storing each output in the solution once it reaches a size of 𝑘. An implementation of this is shown below:\n1\nstd::vector<std::vector<int32_t>> k_combinations(int32_t int32_tn, k) {\n2\nstd::vector<std::vector<int32_t>> solution;\n3\nstd::vector<int32_t>());backtrack(n, k, 1, solution,\n4\nreturn solution;\n5\n} // k_combinations()\n6\n7\nvoid backtrack(int32_t int32_t int32_tn, k, start,\n8\nstd::vector<std::vector<int32_t>>& std::vector<int32_t>solution, current) {\n9\nif (k == 0) {\n10\nsolution.push_back(current);\n11\nreturn;\n12\n} // if\n13\nfor (int32_t i = start; i <= n; ++i) {\n14\ncurrent.push_back(i);\n15\nbacktrack(n, k - 1, i + 1, solution, current);\n16\ncurrent.pop_back();\n17\n} // for i\n18\n} // backtrack()\nWe can use a similar backtracking concept to solve this problem. We will iterate over all the characters of the input string and decide if we14.\ncan append each potential number to generate a valid IP. If we successfully added four numbers to our IP from the provided characters\nwithout pruning the solution, then we know that we have a valid IP. An implementation of this is shown below:\n1\nget_valid_ips(conststd::vector<std::string> std::string& digits) {\n2\nstd::vector<std::string> solution;\n3\nbacktrack(digits, \"\", 0, 0, solution);\n4\nreturn solution;\n5\n} // get_valid_ips()\n6\n7\nvoid backtrack(const size_t int32_tstd::string& digits, std::string curr_path, idx, num_parts,\n8\nstd::vector<std::string>& solution) {\n9\n// Can't have more than 4 numbers in an IP\n10\nif (num_parts > 4) {\n11\nreturn;\n12\n} // if\n13\n// This gets hit if we added exactly four numbers after processing all digits without pruning\n14\nif (num_parts == 4 && idx == digits.length()) {\n15\n// Remove trailing period\n16\ncurr_path.pop_back();\n17\nsolution.push_back(curr_path);\n18\nreturn;\n19\n} // if\n20\nfor (int32_t i = 1; i <= 3 && idx + i <= digits.length(); ++i) {\n21\nstd::string num = digits.substr(idx, i);\n22\n// Prune invalid solutions\n23\nif (num[0] == '0' && i != 1) {\n24\nbreak;\n25\n} // if\n26\nelse if (std::stoi(num) <= 255) {\n27\nbacktrack(digits, curr_path + digits.substr(idx, i) + \".\", idx + i, num_parts + 1, solution);\n28\n} // else if\n29\n} // for i\n30\n} // backtrack()\n15. To determine whether the placement of a bishop is promising, we just need to check if there are any other bishops in the diagonal directions\nrowfrom the provided position. If there are, then the solution is not promising. Since we know that only bishops up to row have been\nrowplaced, we only need to check the upper left and upper right diagonals (as there are no bishops below in the given board). One\nimplementation of this is shown below (note that this is similar to the N-Queens problem, but only checking the diagonals).\n1\nbool promising(const std::vector<std::vector<bool>>& size_t size_tboard, row, col) {\n2\nfor (size_t r = row, c = col; r-- > 0 && c-- > 0; ) {\n3\nif true)(board[r][c] == {\n4\nreturn false;\n5\n} // if\n6\n} // for r, c\n7\nfor (size_t r = row, c = col; r-- > 0 && c++ < board.size(); ) {\n8\nif true)(board[r][c] == {\n9\nreturn false;\n10\n} // if\n11\n} // for r, c\n12\nreturn true;\n13\n} // promising()", "word_count": 583, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fb126886-68ed-5c18-a58c-7615a6dea76c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 875, "real_page_number": null, "text": "Chapter 23\nDynamic Programming\n23.1\nFoundations of Dynamic Programming\n¸ 23.1.1\nIntroduction to Dynamic Programming\nDynamic programming is an algorithmic strategy that breaks a problem down into smaller subproblems and stores the results of repeated\nso that they are not recomputed if they are ever needed again. To understand the motivation behind dynamic programming, let’ssubproblems\nreturn to the example we used to introduce recursion back in chapter 5: suppose you are waiting in a very long line, and you want to know\nwhat position you are in. Since you cannot leave the line without losing your position, you make the assumption that the person in front of you\nalready knows their position, so you ask them for their position and add one to get your own position. The person in front then makes the same\nassumption and asks the same question to the person in front of them. This process continues until the first person is reached, who knows that\nthey are first in line. This person then tells the person behind them that they are second in line, who then tells the person behind them that they\nare third in line, and so on. Eventually, the person directly in front of you learns their position in line, and they turn around and tell you your\nown position. This real-life example demonstrates the recursive process, and is summarized using the following code:\n1\nint32_t get_line_position(int32_t person) {\n2\nif return(person == 1) 1;\n3\nelse return 1 + get_line_position(person - 1);\n4\n} // get_line_position()\nNow, let’s suppose that the person behind you asks you for their position in line a few minutes later. If this were real life, you would immediately\ntell that person their position, since you already know your own position in line from asking before! However, notice that the recursive function\nabove doesn’t do this. Unlike you, the function doesn’t remember that it has already made a recursive call to get its position in line, so it ends up\nmaking the same recursive call all over again! This is equivalent to you asking the person in front of you for your position again, even though\nyou already did so a few minutes earlier. Assuming that the line hasn’t changed, you end up going through the line again for no reason, wasting\ncall.1time to obtain a position number that you already knew from a previous recursive\n1Inreallifeyourpositioninline doeseventually change,sothisisn’tthe bestexampletouse —inthiscase,just assumethatthelinedoesn’tmoveforwardto\nbetterreflectactualrecursiveproblems,wheremakingarecursivecallonthesameinputwillalwaysproducethesamesolution.", "word_count": 453, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fffe252b-c2b4-52fd-8894-3121d4d79ff8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 876, "real_page_number": null, "text": "864\nChapter 23. Dynamic Programming\nThere are many different problems that exhibit this type of behavior, where a recursive call may be called multiple times on the exact same input\nthroughout the lifetime of solving the problem. This is inefficient, since you are essentially computing the same thing multiple times (and each\nrecursive call is often quite expensive). This is where dynamic programming comes into play! The idea behind dynamic programming is to store\nline. In the context ofreusedthe solutions of each recursive call so that they can be whenever the same recursive call is needed later down the\nthe previous example, this is equivalent to \"remembering\" your position in line so that you don’t have to re-ask for it if you ever need it again.\n¸ 23.1.2\nFibonacci Numbers\nTo begin our exploration of dynamic programming, let’s first take a look at one of the most common problems involving recursion: computing\nFibonacci numbers.\n𝑛thExample 23.1 n, fib(n)Given a positive integer write a function that returns the number in the Fibonacci sequence: 0, 1, 1, 2, 3, 5,\nfib(0) = 0, fib(1) = 1,8, 13, 21, ..., where and every subsequent number is the sum of the previous two numbers in the sequence.\n𝑛thn,A recurrence relation for this problem can be derived in a straightforward manner: given any integer we can obtain the Fibonacci number\nfib(n-1) fib(n-2).by simply summing up and\n𝑓𝑖𝑏(𝑛)=\n⎧\n⎪\n⎨\n⎪⎩\n0,\nif 𝑛=0\n1,\nif 𝑛=1\n𝑓𝑖𝑏(𝑛−1)+𝑓𝑖𝑏(𝑛−2),\nif 𝑛>1\nConverting this recurrence relation to code, we get:\n1\nuint64_t fib(int32_t n) {\n2\nif return(n == 0) 0;\n3\nif return(n == 1) 1;\n4\nreturn fib(n - 1) + fib(n - 2);\n5\n} // fib()\nn,If we run this function using small values of we are able to get the correct solution in a reasonable amount of time. However, if we were to try\nn = 50),it with a larger number (such as the function would take forever to run. To understand why this happens, let’s take a look at exactly\nfib() fib() n = 6:what is happening when is called. The following tree depicts every recursive call that is made when is invoked with\nfib(6)\nfib(4)\nfib(2)\nfib(0)\nfib(1)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nfib(5)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nfib(4)\nfib(2)\nfib(0)\nfib(1)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nAt each node of the tree, we make two recursive calls and add up their return values, where each addition requires a constant amount of work.\n𝑂(2𝑛) 𝑂(2𝑛).fib(n)Since there are a total of nodes in the tree, the total work required to solve for can thus be represented as This is why the\ninputs!2 fib()function takes forever to run on larger Additionally, since is not tail recursive and has a recursion depth of 𝑛, the auxiliary\nspace required by this recursive implementation is Θ(𝑛).\nToreturntheFibonaccinumberforalargevalueof𝑛inareasonableamountoftime, wewillneedtoreduceouralgorithm’stimecomplexity.\nTo do so, we can either reduce the number of recursive calls we make, or we can reduce the amount of work that is done at each recursive\ncall. Since each recursive call only performs a single addition operation, we cannot further reduce the work that is done at each recursive call.\nTherefore, our only option is to reduce the number of recursive calls that we make.\n2This fib(n)actuallyisn’tthetightestpossibleboundforthethisrecursive function,astheFibonaccisequencehasaclosedformsolutionof\n𝑓𝑖𝑏(𝑛)=\n1\n√\n5\n(\n1+\n√\n5\n2\n)𝑛\n−1\n√\n5\n(\n1−\n√\n5\n2\n)𝑛\nwhichisΘ((1+\n√\n5)∕2)𝑛)≈Θ(1.618𝑛). 𝑂(2𝑛)Visually,youcanseetheactualtimecomplexityisslightlybetterthan sinceonesideofthetreeisalwayslarger\nthantheotherbecausethetworecursivecallsarenotthesamesize. Alsoyes,youcouldtechnicallywritea solutionusingthisclosed-formsolution,butΘ(1)\nthatisbesidethepointsincethisisn’tsomethingwewouldreasonablyexpectyoutodointhisclass.", "word_count": 701, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "31c71f0c-9463-5347-b67f-ab15388702fa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 877, "real_page_number": null, "text": "23.1 Foundations of Dynamic Programming\n865\nUpon closer inspection of the recursion tree, there is a major issue with our naïve recursive approach: several identical recursive calls are\nfib(6) fib(4)completed once. For example, a single call to ends up calling twice before it returns.more than\nfib(6)\nfib(4)\nfib(2)\nfib(0)\nfib(1)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nfib(5)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nfib(4)\nfib(2)\nfib(0)\nfib(1)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nfib(3) fib(2)Similarly, is called three times, and five times. However, these additional calls are pointless, since you already know the\nsolutions of these recursive calls. For larger values of 𝑛, this replicated work can end up being quite wasteful! It would be nice if our program\ncould remember which recursive calls it has already made so that it never duplicates any work.\nThis is where the strategy of dynamic programming comes into play. When solving a problem using dynamic programming, you want to\nstore the solutions to all the subproblems you encounter in a fast-access data structure such as a vector or a hash table. This container is known\nas a memo, which maps the of each subproblem to the solution of that subproblem. That way, if you ever encounter the sameinput arguments\nsubproblem more than once, you can quickly retrieve its solution by using its input arguments to reference your memo. Forin constant time\nfib(4) memo[4] = 3).example, the solution of is 3, so your dynamic programming memo should map the input to the output 3 (i.e.,𝑛=4\nTypically, the dimensions of a memo should correspond to the number of arguments that are required to uniquely identify each subproblem\nor recursive call. In the Fibonacci example, each recursive call only accepts a single argument 𝑛, so our memo will be one-dimensional based on\nthe value of 𝑛. However, there will be problems where a recurrence relation may involve multiple inputs within a single recursive call — for\nexample, given the recurrence 𝐹(𝑚−1,𝑛)+𝐹(𝑚,𝑛−1), your memo would need to be two-dimensional, one dimension for 𝑚and the𝐹(𝑚,𝑛)=\nother for 𝑛. That way, your memo will be large enough to support all possible input combinations up to 𝑚and 𝑛. We will look at some examples\nthat require multidimensional memos later in this chapter.\nFirst, let’s analyze the dynamic programming solution for the Fibonacci problem, with initial input size 6. To start, we want to initialize𝑛=\nfib(0) fib(n).a memo that can be used to store the solutions to all subproblems we encounter, from to This can be done using a vector of\ni fib(i). fib(0) fib(1)size 7, where each index stores the value of We know that and are equal to 0 and 1, so we can fill out these\nvalues in the memo immediately (or directly return them in our recursive function).\nmemo\n0\n1\n0\n1\n2\n3\n4\n5\n6\nfib(6). memo[6] fib(6)We start by making a recursive call to is currently uninitialized, which indicates that we have never called before,\nfib(6). fib(5).so we make a recursive call to This recursive call then makes a call to\nfib(6)\nfib(5)\nmemo\n0\n1\n0\n1\n2\n3\n4\n5\n6\nmemo[5] fib(5) fib(5), fib(4).is uninitialized, so we haven’t called before. We make a recursive call to which calls\nfib(6)\nfib(5)\nfib(4)\nmemo\n0\n1\n0\n1\n2\n3\n4\n5\n6", "word_count": 564, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "98758eee-0afa-573b-9b39-2672c908c6d7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 878, "real_page_number": null, "text": "866\nChapter 23. Dynamic Programming\nfib(1) fib(0).This process continues until we hit the base cases of and These subproblems can be solved trivially, so we simply return 1\nand 0, respectively.\nfib(6)\nfib(5)\nfib(4)\nfib(3)\nfib(2)\nfib(0)\nfib(1)\nmemo\n0\n1\n0\n1\n2\n3\n4\n5\n6\nfib(2) fib(1) fib(0)The recursion unrolls back to the stack frame, which sums up the results of and to get a value of 1 + 0 = 1. Thus,\nfib(2) has a return value of 1, which we store at position 2 of the memo.\nfib(6)\nfib(5)\nfib(4)\nfib(3)\nfib(2)\nfib(0)\nfib(1)\nmemo\n0\n1\n1\n0\n1\n2\n3\n4\n5\n6\nfib(3), fib(1).The recursion unrolls back to which then makes a recursive call to This is a base case, so we return 1.\nfib(6)\nfib(5)\nfib(4)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nmemo\n0\n1\n1\n0\n1\n2\n3\n4\n5\n6", "word_count": 154, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "99838a3d-2581-5567-85e7-47212c13d888", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 879, "real_page_number": null, "text": "23.1 Foundations of Dynamic Programming\n867\nfib(3) fib(3)then sums up its two values to get 1 + 1 = 2. Before returns 2, it stores its solution at position 3 of the memo.\nfib(6)\nfib(5)\nfib(4)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nmemo\n0\n1\n1\n2\n0\n1\n2\n3\n4\n5\n6\nfib(4), fib(2). fib(2)The recursion unrolls back to which then makes a recursive call to However, has already been computed before,\nso we can just take its value from the memo.\nfib(6)\nfib(5)\nfib(4)\nfib(2)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nmemo\n0\n1\n1\n2\n0\n1\n2\n3\n4\n5\n6\nfib(6),This process repeats until we reach our initial stack frame of which returns 8.\nfib(6)\nfib(4)\nfib(5)\nfib(3)\nfib(4)\nfib(2)\nfib(3)\nfib(1)\nfib(2)\nfib(0)\nfib(1)\nmemo\n0\n1\n1\n2\n3\n5\n8\n0\n1\n2\n3\n4\n5\n6", "word_count": 147, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ee849fd1-2b1a-5f98-affa-d459349dbabd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 880, "real_page_number": null, "text": "868\nChapter 23. Dynamic Programming\nThe shaded nodes of the previous tree represent recursive calls we were able to trivially solve, either from returning a base case or from pulling\na solution directly from the memo. By getting rid of duplicate recursive calls, we were able to bring the time complexity down from exponential\nfib(0) fib(1)to linear. The code for this dynamic programming implementation is shown below. Since we already know that and return 0\nand 1, we can initialize them in the memo immediately. We then call a helper function that performs the work of solving each subproblem and\nstoring its solution in the memo.\n1\nuint64_t fib(int32_t n) {\n2\nstd::vector<uint64_t> memo(n + 1);\n3\nreturn fib_helper(n, memo);\n4\n} // fib()\n5\n6\nuint64_t fib_helper(int32_t std::vector<uint64_t>&n, memo) {\n7\nif (n == 0 || n == 1) {\n8\nreturn n;\n9\n} // if\n10\nif (memo[n] != 0) { // already solved before, fetch from memo\n11\nreturn memo[n];\n12\n} // else if\n13\n// solve and store in memo\n14\nreturn memo[n] = fib_helper(n - 1, memo) + fib_helper(n - 2, memo);\n15\n} // fib_helper()\n¸ 23.1.3\nTop-Down and Bottom-Up Dynamic Programming\nThis process of storing the results of recursive calls so that they can be used later is formally known as memoization. Memoization can often be\nimplemented without significant changes to an existing recursive solution. To memoize a recursive function:\n• On function entry:\n1. Query the memo using the function input(s) to determine if a recursive call has been made on that input before.\n2. If the input has been called before, retrieve the result from the memo instead of recomputing with a recursive call.\n• On function exit:\n1. Save the result of the recursive call in the memo, in the position corresponding to the function input(s).\nThus, memoization solutions often exhibit a structure similar to a naïve recursive approach, as shown below in pseudocode:\n1\nfunction dp_helper(dp_state, memo):\n2\nif dp_state is a base case:\n3\nreturn base case solution\n4\nif dp_state in memo:\n5\nreturn memo[dp_state]\n6\ncalculate solution for dp_state using recursive calls\n7\nsave the solution for dp_state in memo\n8\nreturn solution for dp_state\n9\n10\nfunction solve_original_problem(input):\n11\ninitialize memo\n12\nreturn dp(starting_state, memo)\nSofar,wehavediscussedtherecursivememoizationstrategyforsolvingdynamicprogrammingproblems. However,thisisnotthesoleapproach:\nfib(6),you can avoid recursion by precomputing the entire memo at the beginning of the algorithm! For example, if you are asked to compute\nyou could simply precompute all Fibonacci numbers up to the sixth Fibonacci number and then return its value. This method achieves the same\nlinear time complexity as the previous approach, and it is implemented rather than recursively. The code is shown below:iteratively\n1\nuint64_t fib(int32_t n) {\n2\nstd::vector<uint64_t> memo(n + 1);\n3\nmemo[0] = 0;\n4\nmemo[1] = 1;\n5\nfor (size_t i = 2; i <= n; ++i) {\n6\nmemo[i] = memo[i - 1] + memo[i - 2];\n7\n} // for i\n8\nreturn memo[n];\n9\n} // fib()\nThis process of starting at the smallest possible subproblem and building upward toward a solution is known as tabulation. Both memoization\nand tabulation are valid approaches for solving a dynamic programming problem! The main difference between the two approaches is in how\nthe subproblems are computed along the way. With memoization, you start with the inputs of the original problem and make your way down to\nthe smallest subproblems using recursive calls. With tabulation, you start from the smallest subproblems and use their solutions to build upward\nsolve.3toward the original problem you are trying to In both cases, the solutions of intermediate subproblems are stored in a memo so that they\ncan be referenced later.\n3Note: ifyouusetabulation,youmustmakesureyouaresolvingthesubproblemsinthecorrectorder,sothateachsubproblem’sdependenciesareallsolved\nbeforethesubproblemitself. Thismayrequireyoutoiterateoverthememoinaspecificway,whichmaynotalwaysbeobvious!", "word_count": 688, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ac2aafc6-17f4-5049-85c2-4153d5de2ead", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 881, "real_page_number": null, "text": "23.1 Foundations of Dynamic Programming\n869\nFormally, dynamic programming that involves recursion and memoization falls into the category of top-down dynamic programming, while\ndynamic programming that involves iteration and tabulation falls into the category of bottom-up dynamic programming. The top-down\napproach starts with the original problem and divides the input into smaller subproblems that are solved recursively, while the bottom-up\napproach starts by solving the smallest subproblems and uses these solutions to solve for larger subproblems, eventually reaching the solution\nof the original problem. If a problem can be solved using top-down dynamic programming, it can also be solved using bottom-up dynamic\nprogramming (and vice versa), and choosing one over the other is often a matter of preference.\nThere are a few minor differences between the two approaches. Top-down dynamic programming only computes subproblems that are\nneeded, since a result is only stored in the memo if a recursive call has already been made on its input. On the other hand, bottom-up dynamic\nprogramming precomputes possible subproblems, so it may solve for subproblems that are never used. That being said, this does not meanall\nthat top-down is always better. For instance, a top-down approach is more likely to stack overflow for large inputs compared to a bottom-up\napproach, since it relies on recursion rather than iteration. A top-down approach may also prevent you from reusing positions of the memo for\nmultiple subproblems to reduce memory usage (which we will discuss in section 23.8). Furthermore, one implementation may be easier or\ncleaner to write than the other depending on the type of problem you are trying to solve.\nThe performance of these two approaches also depends on the problem at hand. Bottom-up implementations are more cache-friendly\nand perform slightly better than top-down in many cases, since querying a precomputed table is often faster than making a recursive call and\nencountered.4conditionally checking a memo for every subproblem However, if the number of relevant subproblems only comprise a small\nportion of the overall output space, top-down may be faster than bottom-up if the cost of precomputing all possible subproblems exceeds the\ncost of making recursive calls only on the subproblems that matter.\n¸ 23.1.4\nBinomial Coeﬃcients\nAs another example, let’s look at the problem, which depends on two inputs in its recurrence relation rather than one. Inbinomial coefficient\n(𝑛mathematics, we define the binomial coefficient\n𝑘\n) as the number of ways to choose 𝑘unordered outcomes out of 𝑛possibilities, defined as\nfollows for non-negative integers 𝑛and 𝑘:\n(𝑛\n𝑘\n)=\n𝑛!\n𝑘!(𝑛−𝑘)!\n𝑛≥𝑘,Example 23.2 bi_coeff(n, k)Given two non-negative integers 𝑛and 𝑘, where implement a function that returns the binomial\n(𝑛coefficient\n𝑘\n).\n(𝑛One way to approach this problem is to calculate 𝑛!, 𝑘!, and (𝑛−𝑘)!, and then use these values to solve for\n𝑘\n). However, this doesn’t always\nwork, since factorials can get quite large: integer overflow becomes an issue when for 32-bit integers, and for 64-bit integers.𝑛> 𝑛>12 20\nInstead, a better way would be to use the definition of the binomial coefficient formula, shown below (this should have been covered inrecursive\nEECS 203, but we won’t expect you to derive anything like this on your own):\n(𝑛\n𝑘\n) (𝑛−1=\n𝑘−1\n)+(𝑛−1\n𝑘\n)\n(𝑛\n0\n)=1\n(𝑛\n𝑛\n)=1\nConverting this to a recursive function, we would get the following:\n1\nuint64_t bi_coeff(int32_t int32_tn, k) {\n2\nif (k == 0 || k == n) {\n3\nreturn 1;\n4\n} // if\n5\nreturn bi_coeff(n - 1, k - 1) + bi_coeff(n - 1, k);\n6\n} // bi_coeff()\n(7However, this implementation is not fully efficient since there are duplicate subproblems. This is shown below for\n4\n):\n(7\n4\n)\n(6\n4\n)\n(5\n4\n)\n(4\n4\n)\n(4\n3\n)\n(3\n3\n)\n(3\n2\n)\n(2\n2\n)\n(2\n1\n)\n(5\n3\n)\n(4\n3\n)\n(3\n3\n)\n(3\n2\n)\n(2\n2\n)\n(2\n1\n)\n(4\n2\n)\n(3\n2\n)\n(2\n2\n)\n(2\n1\n)\n(3\n1\n)\n(6\n3\n)\n(5\n3\n)\n(4\n3\n)\n(3\n3\n)\n(3\n2\n)\n(2\n2\n)\n(2\n1\n)\n(4\n2\n)\n(3\n2\n)\n(2\n2\n)\n(2\n1\n)\n(3\n1\n)\n(5\n2\n)\n(4\n2\n)\n(3\n2\n)\n(2\n2\n)\n(2\n1\n)\n(3\n1\n)\n(4\n1\n)\n4Note: wearetalkingaboutactualruntimehere,and timecomplexity.not", "word_count": 757, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5cf88a87-edbe-5098-af86-b7e06098d358", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 882, "real_page_number": null, "text": "870\nChapter 23. Dynamic Programming\nTo avoid repeating recursive calls that have already been made, we can use dynamic programming and store the solutions of these subproblems\nin a memo. In this case, each subproblem is identified using two input values, 𝑛and 𝑘, so our memo will need to be two-dimensional with size\n(𝑛to support all possible subproblems. An example of a suitable memo is shown below: here, the solution toΘ(𝑛𝑘)\n𝑘\n) for any valid 𝑛and 𝑘is\nstored in row 𝑛, column 𝑘.\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\n6\n7\n𝑘\n𝑛\n(𝑛We know that\n0\n) (𝑛and\n𝑛\n) are both equal to 1 from the base case, so we can fill out their corresponding cells immediately (or return 1 in our\nrecursive helper function if any of these base cases are reached):\n0\n1\n2\n3\n4\n0\n1\n1\n1\n1\n2\n1\n1\n3\n1\n1\n4\n1\n1\n5\n1\n6\n1\n7\n1\n𝑘\n𝑛\nIf we use a top-down dynamic programming approach, we would take our original recursive function and add a memo to keep track of\nsubproblems we have encountered before. A top-down solution is shown below:\n1\nuint64_t bi_coeff(int32_t int32_tn, k) {\n2\nstd::vector<std::vector<uint64_t>> std::vector<uint64_t>(kmemo(n + 1, + 1));\n3\nreturn bi_coeff_helper(n, k, memo);\n4\n} // bi_coeff()\n5\n6\nuint64_t bi_coeff_helper(int32_t int32_t std::vector<std::vector<uint64_t>>&n, k, memo) {\n7\nif (k == 0 || n == k) {\n8\nreturn 1;\n9\n} // if\n10\nif (memo[n][k] != 0) { // recursive call made on (n, k) already\n11\nreturn memo[n][k];\n12\n} // if\n13\n// solve and store in memo\n14\nreturn memo[n][k] = bi_coeff_helper(n - 1, k - 1, memo) + bi_coeff_helper(n - 1, k, memo);\n15\n} // bi_coeff_helper()\nIf we use a bottom-up dynamic programming approach, we would loop over our memo and precompute all subproblems up to our original input.\nTo implement this, we begin by looking at the smaller subproblems (e.g., the base cases), and then solving for the larger subproblems using\n(1the solutions we compute from the smaller subproblems. For example, we know that\n0\n) (1and\n1\n) are both equal to 1. Thus, we can solve for\n(2\n1\n) (1=\n0\n)+(1\n1\n) (22. Then, we can use the solution of=1+1=\n1\n)\n(3to solve for\n1\n)\n(3and\n2\n)\n(4, which we can use to solve for\n1\n)\n(4,\n2\n)\n(4,\n3\n)\n, and so on,\n(𝑛until we eventually obtain the solution for\n𝑘\n). The code for a bottom-up solution is shown below.\n1\nuint64_t bi_coeff(int32_t int32_tn, k) {\n2\nstd::vector<std::vector<uint64_t>>\n3\nstd::vector<uint64_t>(kmemo(n + 1, + 1));\n4\nfor (int32_t curr_n = 0; curr_n <= n; ++curr_n) {\n5\nfor (int32_t curr_k = 0; curr_k <= std::min(curr_n, k); ++curr_k) {\n6\nif (curr_n == curr_k || curr_k == 0) {\n7\nmemo[curr_n][curr_k] = 1;\n8\n} // if\n9\nelse {\n10\nmemo[curr_n][curr_k] = memo[curr_n - 1][curr_k - 1] + memo[curr_n - 1][curr_k];\n11\n} // else\n12\n} // for curr_k\n13\n} // for curr_n\n14\nreturn memo[n][k];\n15\n} // bi_coeff()", "word_count": 550, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cb9cbc5b-5966-50eb-b0db-5938ca8afd78", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 883, "real_page_number": null, "text": "23.2 Dynamic Programming Implementation Strategies\n871\nBy storing the solutions of repeated subproblems in a memo, we only need to solve each subproblem once, regardless of how many times it is\nneeded. Since there are only subproblems we may encounter given values 𝑛and 𝑘, and each subproblem takes a constant amount of timeΘ(𝑛𝑘)\nto solve, the worst-case time complexity of a dynamic programming approach is Θ(𝑛𝑘). Furthermore, since we used a memo with dimensions\n(𝑛+1)×(𝑘+1), the auxiliary space used by our solution is also Θ(𝑛𝑘).\n23.2\nDynamic Programming Implementation Strategies\nIn general, dynamic programming is useful for problems that exhibit the following characteristics:\n1. Optimal substructure. A problem with an optimal substructure is one whose solution can be constructed using the optimal solutions of\nits smaller subproblems. If a problem has optimal substructure, the correct solution for some input size 𝑛can be computed by simply\nreferencing the solutions of subproblems with input size less than 𝑛.\n2. Overlapping subproblems. A problem exhibits the property of overlapping subproblems if multiple instances of the same subproblem\nare encountered while recursively decomposing the original problem down to its base cases.\nFor example, the Fibonacci problem exhibits both optimal substructure and overlapping subproblems. If we know the values of and𝑓𝑖𝑏(𝑛−2)\nfor any 𝑛, we can immediately use their solutions to compute the value of 𝑓𝑖𝑏(𝑛). In addition, if we recursively break any input down𝑓𝑖𝑏(𝑛−1)\nto its base case, we can see that multiple identical subproblems appear more than once.\nfib(n)\nfib(n-2)\nfib(n-4)\nfib(n-6)\n...\n...\nfib(n-5)\n...\n...\nfib(n-3)\nfib(n-5)\n...\n...\nfib(n-4)\n...\n...\nfib(n-1)\nfib(n-3)\nfib(n-5)\n...\n...\nfib(n-4)\n...\n...\nfib(n-2)\nfib(n-4)\n...\n...\nfib(n-3)\n...\n...\nThe existence of overlapping subproblems is important to have! A common misconception is that dynamic programming can be used to\nimprove the performance of any recusive algorithm. This is not true: for dynamic programming to be beneficial, there must exist overlapping\nsubproblems that recur multiple times over the course of solving a problem. If the subproblems can be solved independently without any overlap,\nthere is no need to store their solutions in a memo since you will never need them more than once (for these problems, an approach such as\ndivide-and-conquer may be more applicable). Only when there exist does a memo become useful in significantlydependent subproblems\nalgorithm.5improving the performance of an\nIdentifying and solving a dynamic programming problem can be challenging at times, but this is a topic that can be mastered with practice.\nAt the end of the day, the goal of dynamic programming is the same regardless of what type of problem you are working with: you want to (1)\nbreak a larger problem into a collection of smaller subproblems, (2) solve each of these smaller subproblems and store their solutions in aonce\nmemo, and (3) use these solutions to compute the solution of the original problem, either in a top-down (recursive) or bottom-up (iterative)\nmanner. The procedure for solving a dynamic programming problem can be roughly summarized using the following steps:\n1. Verify that the problem you are solving can be broken up into smaller subproblems.\nTo determine if dynamic programming can be efficiently used to solve a problem, first ask yourself if the problem you are trying to solve can\nbe problem. That is, if you had the solutions of the problem on smaller input,expressed using the solutions of smaller instances of the same\ncan you use those solutions to help you calculate the solution on a larger input? If you can, then the problem has an optimal substructure, and\ndynamic programming may be viable approach.\n2. Identify the problem variables that each subproblem depends on.\nNext, determine the input variables that uniquely identify each subproblem; this will allow you to identify the number of subproblems you may\nneed to solve, and therefore the size of your memo. For example, the subproblems of the Fibonacci problem depend on the value of 𝑛, while the\nsubproblems of the binomial coefficient problem depend on the values of 𝑛and 𝑘. Therefore, there are potentially subproblems that youΘ(𝑛)\nneed to solve for the Fibonacci problem, and subproblems that you need to solve for the binomial coefficient problem. You only want toΘ(𝑛𝑘)\nfocus on the variables that among the subproblems you may encounter — if a variable never changes across all subproblems, then itdiffer\ndoesn’t affect the number of subproblems you may need to solve.\n5Theterm\"dependent\"hereindicatesthattheprocessofsolvingonesubproblemmaydependonasolutionthatwasalreadysolvedbyanothersubproblem,and\nthatthetwosubproblemsshareresourcesorneedtobesolvedtogether.not", "word_count": 779, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "478c0c11-f317-5fe6-a4ef-4d00b7449adc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 884, "real_page_number": null, "text": "872\nChapter 23. Dynamic Programming\n3. Express the problem in the form of a recurrence relation.\nUse the information from the previous two steps to express the problem in the form of a recurrence relation, including all the base cases. This is\nOncean important step that you should complete before you begin writing any code, since it will make implementation significantly easier!\nyou have identified how the subproblems relate to each other, the recurrence relation should come naturally: simply write an expression that\ndescribes how several smaller subproblems can be combined to compute the solution of the larger problem you are trying to solve.\nFor example, we know that 0, 1, and that and can be summed to obtain the value of 𝑓𝑖𝑏(𝑛). Therefore,𝑓𝑖𝑏(0) 𝑓𝑖𝑏(1) 𝑓𝑖𝑏(𝑛−2) 𝑓𝑖𝑏(𝑛−1)= =\nwe would express the Fibonacci problem using the following recurrence relation:\n𝑓𝑖𝑏(𝑛)=\n⎧\n⎪\n⎨\n⎪⎩\n0,\nif 𝑛=0\n1,\nif 𝑛=1\n𝑓𝑖𝑏(𝑛−1)+𝑓𝑖𝑏(𝑛−2),\nif 𝑛>1\nForthebinomialcoefficientproblem,weknowthat and arebothequalto1,andthat isthesumof𝐶(𝑛−1,𝑘−1)+𝐶(𝑛−1,𝑘).𝐶(𝑛,0) 𝐶(𝑛,𝑛) 𝐶(𝑛,𝑘)\nThus, we would express the binomial coefficient problem using the following recurrence relation:\n𝐶(𝑛,𝑘)=\n⎧\n⎪\n⎨\n⎪⎩\n1,\nif 𝑘=0\n1,\nif 𝑛=𝑘\n𝐶(𝑛−1,𝑘−1)+𝐶(𝑛−1,𝑘),\nif 𝑛>𝑘\n4. Identify the overlapping subproblems.\nFor dynamic programming to be useful, we want there to be overlapping subproblems. Therefore, it is always good to check your recurrence\nrelation to see if duplicate subproblems actually exist (a good way to do this is to draw out a recurrence tree, like the one on the previous page,\nand look for subproblems that appear multiple times). If they don’t exist, then dynamic programming may not be the best approach to use, and\nan alternative approach such as divide-and-conquer should be considered instead.\n5. Convert the recurrence relation into code, using either a top-down (recursive) or bottom-up (iterative) approach.\nOnce you have identified the recurrence relation, convert it into code using either a top-down or bottom-up approach. If you decide to use\na top-down approach, start by writing a straightforward recursive solution that solves for relevant subproblems using recursive calls. Then,\nadd memoization to prevent the same subproblem from being computed twice — this is done by storing the solutions of recursive calls in a\nmemo so that they can be easily retrieved later when they are needed again. If you decide to use a bottom-up approach, start with the base cases\nof the problem and use the recursive relationship of the problem to build upward toward the original input size you are trying to solve. Each\nintermediary subproblem you encounter is stored in a memo so that it can be quickly referenced later while solving for larger subproblems.\nRegardless of how you approach a dynamic programming problem, the core component of any solution revolves around the memo, which stores\nthe solutions of overlapping subproblems so that they don’t have to be solved more than once. By using a bit of additional memory to store these\nsubproblems, dynamic programming can greatly reduce the runtime of a recursive function! Over the next few sections, we will look at a few\nexamples of problems that can be solved using dynamic programming.\n23.3\nCommon Dynamic Programming Patterns\n¸ 23.3.1\nCounting Distinct Ways\nThere are several common categories of problems that can be efficiently solved using dynamic programming. One common problem you might\nsee involves to get to some target value or state, given a set of rules that you must follow to reach thatcounting the number of distinct ways\ntarget. These problem types often exhibit both the optimal substructure and overlapping subproblems that make dynamic programming useful.\nTo see why, consider the following example. Suppose you want to get to some state 𝐶, which is accessible from states 𝐴and 𝐵. There are 5\ndistinct ways to get to state 𝐴, and 10 distinct ways to get to state 𝐵. Knowing this, how many distinct ways are there to get to state 𝐶?\nA\nB\nC\n5\n10\nThe answer is pretty straightforward: since there are 5 ways to get to state 𝐴and 10 ways to get to state 𝐵, the total number of ways to get to\nstate 𝐶must be 5 + 10 = 15, since 𝐶can only be reached from 𝐴or 𝐵. This simple example can actually be generalized to any state, allowing\nus to conceptualize a recurrence relation for any problem that fits this pattern. To solve problems that ask you to count the distinct number of\nit. That is, if you wanted to countways to reach some target state, you should sum up all the possible ways to reach states that directly precede\nthe number of ways to reach some target state 𝐶, and you can get to 𝐶from either 𝐴or 𝐵, then the number of ways to get to 𝐶is equal to the\nsum of the number of ways to get to 𝐴and 𝐵. We will look at a few examples of this pattern in this section.", "word_count": 841, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ec5be321-758b-5c18-a9df-99493ba773bc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 885, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n873\nExample 23.3 You are located at the top-left corner of a 𝑚×𝑛grid, and you want to reach the bottom-right corner. However, you are only\nallowed to move or at any point in time. Write a function that takes in 𝑚and 𝑛and returns the total number of paths youdown right unique\ncan take to reach the bottom-right corner of the grid.\nExample: Given the following grid, there are three unique paths that you can take to reach the bottom-right corner:2×3\n1. down, right, right\n2. right, down, right\n3. right, right, down\n★\nThe key detail to notice here is that there are only two ways for you to reach the target square: either by moving down from the square directly\nabove, or by moving right from the square directly to the left. Thus, if we know there are 𝑥ways to reach the square directly above, and 𝑦ways\nto reach the square directly to the left, then there must be 𝑥+𝑦ways to reach the target square.\n𝑥\n𝑦\n𝑥+𝑦\nThus, given any square at (row 𝑟, column 𝑐) of the grid, we can calculate the number of ways to reach this square by recursively computing the\nnumber of ways to get to (row 𝑟−1, column 𝑐) and (row 𝑟, column 𝑐−1), and then summing up these two values. The base case is our starting\nposition at (row 1, column 1), which we can trivially reach in exactly one way. There are also zero ways to reach any out-of-bounds square. This\ngives us the following recurrence relation 𝐹(𝑟,𝑐), which represents the total number of ways to reach the square in (row 𝑟, column 𝑐):\n𝐹(𝑟,𝑐)=\n⎧\n⎪\n⎨\n⎪⎩\n1,\nif and𝑟= 𝑐=1 1\n0,\nif or (out-of-bounds)𝑟< 𝑐<1 1\n𝐹(𝑟−1,𝑐)+𝐹(𝑟,𝑐−1),\notherwise\nIf we draw out the recursion tree, we would see that there are overlapping subproblems:\n𝐹(𝑟,𝑐)\n𝐹(𝑟,𝑐−1)\n𝐹(𝑟,𝑐−2)\n𝐹(𝑟,𝑐−3)\n...\n...\n𝐹(𝑟−1,𝑐−2)\n...\n...\n𝐹(𝑟−1,𝑐−1)\n𝐹(𝑟−1,𝑐−2)\n...\n...\n𝐹(𝑟−2,𝑐−1)\n...\n...\n𝐹(𝑟−1,𝑐)\n𝐹(𝑟−1,𝑐−1)\n𝐹(𝑟−1,𝑐−2)\n...\n...\n𝐹(𝑟−2,𝑐−1)\n...\n...\n𝐹(𝑟−2,𝑐)\n𝐹(𝑟−2,𝑐−1)\n...\n...\n𝐹(𝑟−3,𝑐)\n...\n...\nThus, dynamic programming can be used to solve this problem. Each subproblem is identified using two variables, the row and column, so we\nwill initialize a 𝑚×𝑛memo of integers to store the solutions of subproblems we encounter (where the solution to for any (𝑟, 𝑐) is stored𝐹(𝑟,𝑐)\nmemo[r][c]).at position If we use a top-down approach to solve this problem, we would make recursive calls on and per(𝑟−1,𝑐) (𝑟,𝑐−1)\nthe recurrence relation above, but we will check the memo before every recursive call to determine if the call has been made before. If the call\nhas been made before, we just fetch its solution from the memo; otherwise, we make the recursive call and write the solution to the memo.", "word_count": 471, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ca5cb9b3-e102-51eb-a924-5055295c0f65", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 886, "real_page_number": null, "text": "874\nChapter 23. Dynamic Programming\nbelow:6The code for a top-down solution is shown\n1\nint32_t count_paths(int32_t int32_tm, n) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(nmemo(m + 1, + 1));\n3\nreturn count_paths_helper(m, n, memo);\n4\n} // count_paths()\n5\n6\nint32_t count_paths_helper(int32_t int32_t std::vector<std::vector<int32_t>>&r, c, memo) {\n7\nif (r == 1 && c == 1) { // base case\n8\nreturn 1;\n9\n} // if\n10\nif (r < 1 || c < 1) { // out-of-bounds\n11\nreturn 0;\n12\n} // if\n13\nif (memo[r][c] != 0) { // recursive call made before, fetch from memo\n14\nreturn memo[r][c];\n15\n} // if\n16\n// solve and store in memo\n17\nreturn memo[r][c] = count_paths_helper(r - 1, c, memo) + count_paths_helper(r, c - 1, memo);\n18\n} // count_paths_helper()\nIf we use a bottom-up approach, we would start with the base cases and build upward toward the solution using the relationships between\nsubproblems. Here, we know that 1, so we can use that to compute that and 𝐹(2,1), which can be used to compute 𝐹(2,2),𝐹(1,1) 𝐹(1,2)=\nand so on, until we reach 𝐹(𝑚,𝑛). The code for a bottom-up solution is shown below.\n1\nint32_t count_paths(int32_t int32_tm, n) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(nmemo(m + 1, + 1, 1));\n3\nfor (int32_t r = 2; r <= m; ++r) {\n4\nfor (int32_t c = 2; c <= n; ++c) {\n5\nmemo[r][c] = memo[r - 1][c] + memo[r][c - 1];\n6\n} // for c\n7\n} // for r\n8\nreturn memo[m][n];\n9\n} // count_paths()\nSince there are a total of subproblems that we may encounter, and each subproblem is solved only once in constant time with the help ofΘ(𝑚𝑛)\nthe memo (i.e., add up two values), the time complexity of the above dynamic programming solution is Θ(𝑚𝑛). Similarly, since a memo of size\nis used, the auxiliary space used by the implementation is also Θ(𝑚𝑛).Θ(𝑚𝑛)\nExample 23.4 You are climbing a staircase requiring 𝑛steps to reach the top. Write a function that takes in the number of steps 𝑛and\nreturns the number of distinct ways you climb the stairs, if you can only climb either 1 or 2 steps at any point in time.\nSimilar to the previous problem, the number of ways to reach the top of the staircase is dependent on the number of ways to reach the steps\n𝑛thdirectly below it. In this case, we can reach the step from either the step directly below it (by climbing one step) or two steps below it (by\nclimbing two steps). Thus, if there are 𝑥ways to reach the step at position and 𝑦ways to reach the step at position 𝑛−1, then there must be𝑛−2\n𝑥+𝑦ways to reach the step at position 𝑛. The base cases occur when and 1, which both have a solution of 1 (there is only one way to𝑛= 𝑛=0\nreach the top of the stairs if the staircase only consists of one step, or if there are no stairs at all).\n𝑥+𝑦\n𝑦\n𝑥\nSince this problem can be defined in terms of solutions to smaller subproblems, we can express the problem using the following recurrence\n𝑖threlation, where represents the number of ways to reach the step:𝐹(𝑖)\n𝐹(𝑖)=\n{\n1,\nif or𝑖= 𝑖=0 1\n𝐹(𝑖−1)+𝐹(𝑖−2),\nif 𝑖>1\n6Inthisimplementation,weareinitializingamemowithdimensions sothatwecanuse1-indexing,wherethesolutionfor isstoredin(𝑚+1)×(𝑛+1) 𝐹(𝑚,𝑛)\nmemo[𝑚][𝑛]insteadofmemo[𝑚−1][𝑛−1]. However,thisisnotnecessary,anditis perfectlyfinetouse0-indexinganddeclareamemowithdimensions𝑚×𝑛;\nyouwilljustneedtoreturnmemo[𝑚−1][𝑛−1]attheendofthealgorithminsteadofmemo[𝑚][𝑛]andtreatthebasecaseas insteadof 1).(𝑟=0,𝑐= (𝑟=1,𝑐=0)", "word_count": 644, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0f5846c3-3fde-52c2-8330-7d066ca92485", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 887, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n875\nIf we draw out the recursion tree for this recurrence relation, we would see that there are overlapping subproblems:\nF(n)\nF(n-2)\nF(n-4)\nF(n-6)\n...\n...\nF(n-5)\n...\n...\nF(n-3)\nF(n-5)\n...\n...\nF(n-4)\n...\n...\nF(n-1)\nF(n-3)\nF(n-5)\n...\n...\nF(n-4)\n...\n...\nF(n-2)\nF(n-4)\n...\n...\nF(n-3)\n...\n...\nWe can therefore implement a dynamic programming solution for this problem. Each subproblem is identified using a single variable (a \"step\nposition\" that goes up to 𝑛), so we will initialize a one-dimensional memo of length 𝑛(where the solution to for any position 𝑖is stored𝐹(𝑖)\nmemo[i]).at position If we use a top-down approach to solve this problem, we would make the recursive calls as shown by the recurrence\nrelation, but with a check before each call to see if its solution is already in the memo. The code for a top-down solution is shown below:\n1\nint32_t climb_stairs(int32_t n) {\n2\nstd::vector<int32_t> memo(n + 1);\n3\nreturn climb_stairs_helper(n, memo);\n4\n} // climb_stairs()\n5\n6\nint32_t climb_stairs_helper(int32_t std::vector<int32_t>&i, memo) {\n7\nif (i == 0 || i == 1) { // base cases\n8\nreturn 1;\n9\n} // if\n10\nif (memo[i] != 0) { // recursive call made before, fetch from memo\n11\nreturn memo[i];\n12\n} // if\n13\n// solve and store in memo\n14\nreturn memo[i] = climb_stairs_helper(i - 1, memo) + climb_stairs_helper(i - 2, memo);\n15\n} // climb_stairs_helper()\nIf we use a bottom-up approach, we start with the base cases and build upward to our desired solution. To calculate 𝐹(𝑛), we would use 𝐹(1)\nand to calculate 𝐹(3), and then use the solution of to calculate 𝐹(4), and so on, until we reach 𝐹(𝑛). The code for a bottom-up𝐹(2) 𝐹(3)\nsolution is shown below:\n1\nint32_t climb_stairs(int32_t n) {\n2\nif (n == 0 || n == 1) {\n3\nreturn 1;\n4\n} // if\n5\nstd::vector<int32_t> memo(n + 1);\n6\nmemo[0] = 1;\n7\nmemo[1] = 1;\n8\nfor (int32_t i = 2; i <= n; ++i) {\n9\nmemo[i] = memo[i - 1] + memo[i - 2];\n10\n} // for i\n11\nreturn memo[n];\n12\n} // climb_stairs()\nThe worst-case time complexity and auxiliary space of this dynamic programming approach are both Θ(𝑛), since there are 𝑛subproblems that\nare solved at most once, each taking time to compute with the help of a memo of size Θ(𝑛).Θ(1)", "word_count": 409, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "14a9e1b6-f43a-5d71-9979-6d0bc5e5d04e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 888, "real_page_number": null, "text": "876\nChapter 23. Dynamic Programming\nExample 23.5 In the game of chess, the knight travels in an \"L\" shape, moving two spaces in any direction, then turning 90 degrees left or\nright and moving forward another space. For example, the following chessboard depicts the squares that the knight in the middle can access\nin one move.\n1\na\n2\nb\n3\nc\n4\nd\n5\ne\n6\nf\n7\ng\n8\nh\nn\nn\nn\nn\nn\nn\nn\nn\nn\nWrite a function that, when given positive integers 𝑚and 𝑛, returns the total number of distinct ways a knight at the top-left corner of a 𝑛×𝑛\nchessboard can move to the bottom-right corner in exactly 𝑚moves.\nNotice that this problem is very similar to the grid problem we covered earlier. There are two positions that allow a knight to directly reach the\nbottom-right corner of the grid, as shown:\n1\na\n2\nb\n3\nc\n4\nd\n5\ne\n6\nf\n7\ng\n8\nh\nn\nn\nTherefore, ifwesolveforthenumberofwaystogettothesetwopositions, wecanuseittosolveforthenumberofwaystogettothebottom-right\ncorner. However, there is an additional variable that we have to worry about in this problem: the number of moves! Simply knowing the number\nof ways to get to a square is no longer enough, since you have to make sure the knight lands in the bottom-right corner after the correct number\nof moves. To address this, we must consider this additional dimension in our analysis of the recurrence relation.\nUsing the dynamic programming pattern covered in this section, we would conclude that the number of ways to reach the bottom-right\ncorner is equal to the total number of ways to reach squares that allow a knight direct access to the bottom-right corner. However, since we are\nconsidering move count as well, we will add this additional dimension to the subproblems we want to solve. Here, the number of ways to reach\nthe bottom-right corner in exactly 𝑚moves is equal to the number of ways to reach the two preceding squares in moves:𝑚−1\nn\nn\nAfter - 1 Movesm\nThe total number of ways to get to\nthese positions in moves...𝑚−1\nn\nAfter Movesm\n...is equal to the number of ways to get\nto the bottom-right corner in 𝑚moves!", "word_count": 402, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2b3823a5-b8ca-5ca9-8fa7-912301d6ef4c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 889, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n877\nSimilarly, the number of ways to get to these positions in moves can be expressed in terms of the number of ways to get to preceding𝑚−1\npositions in moves.𝑚−2\nn\nn\nn\nn\nn\nn\nAfter - 2 Movesm\nThe total number of ways to get to any\nof these positions in moves...𝑚−2\nn\nAfter - 1 Movesm\n...is equal to the number of ways to\nget to this square in moves!𝑚−1\nWe can therefore express this problem using the following recurrence relation, where represents the total number of ways for a knight𝐹(𝑖,𝑟,𝑐)\nto reach the square at (row 𝑟, column 𝑐) in exactly 𝑖moves:\n𝐹(𝑖,𝑟,𝑐)=\n⎧\n⎪\n⎪\n⎪\n⎪\n⎨\n⎪\n⎪\n⎪\n⎪⎩\n1,\nif 𝑖=0,𝑟=0,𝑐=0\n0,\n0,𝑟≠0,𝑐≠0if 𝑖=\n0,\n𝑟≥𝑛or 𝑐≥𝑛(out-of-bounds)if or or𝑟< 𝑐<0 0\n𝐹(𝑖−1,𝑟−2,𝑐−1)+𝐹(𝑖−1,𝑟−2,𝑐+1)\n+𝐹(𝑖−1,𝑟+2,𝑐−1)+𝐹(𝑖−1,𝑟+2,𝑐+1)\n+𝐹(𝑖−1,𝑟−1,𝑐−2)+𝐹(𝑖−1,𝑟−1,𝑐+2)\n+𝐹(𝑖−1,𝑟+1,𝑐−2)+𝐹(𝑖−1,𝑟+1,𝑐+2),\nif and 𝑟and 𝑐not out-of-bounds𝑖>0,\nThis recurrence relation exhibits the property of overlapping subproblems, so we can use dynamic programming to implement a solution.\nEach subproblem is identified using three changing variables — the move number, row, and column — so we will initialize a memo with\ndimensions 𝑚×𝑛×𝑛(since row and column go up to 𝑛and the number of moves goes up to 𝑚), where the solution to is stored in𝐹(𝑖,𝑟,𝑐)\nmemo[i][r][c]. memo[i]You can think of this as a memo of chessboards, where stores the number of ways to reach each chessboard\nmoves.7iposition in exactly\n1\nmemo[0]\n1\n1\nmemo[1]\n2\n1\n1\n1\n1\n2\n1\n1\n1\n1\nmemo[2]\n2\n1\n2\n2\n8\n3\n2\n8\n4\n4\n1\n4\n2\n1\n3\n2\n3\n2\n4\n3\n2\n1\nmemo[3]\n⋮\n7Theblankcellsintheillustrationrepresentunreachablesquaresandhaveavalueof0(sincethereare0waystoreachthem).", "word_count": 318, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3d87e171-7229-559f-b059-bd20588f7b09", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 890, "real_page_number": null, "text": "878\nChapter 23. Dynamic Programming\nA top-down solution for this problem is shown below:\n1\nint32_t knight_moves(int32_t int32_tm, n) {\n2\nstd::vector<std::vector<std::vector<int32_t>>> memo(m + 1,\n3\nstd::vector<std::vector<int32_t>>(n, std::vector<int32_t>(n, -1)));\n4\nreturn knight_moves_helper(m, n - 1, n - 1, memo);\n5\n} // knight_moves()\n6\n7\nint32_t knight_moves_helper(int32_t int32_t int32_tmoves_left, row, col,\n8\nstd::vector<std::vector<std::vector<int32_t>>>& memo) {\n9\nif (moves_left == 0 && row == 0 && col == 0) {\n10\nreturn 1;\n11\n} // if\n12\nif (row < 0 || col < 0 || row >= memo[0].size() || col >= memo[0].size() || moves_left == 0) {\n13\nreturn 0;\n14\n} // if\n15\nif (memo[moves_left][row][col] != -1) {\n// recursive call made before\n16\nreturn memo[moves_left][row][col];\n17\n} // if\n18\n// solve and store in memo\n19\nreturn memo[moves_left][row][col] =\n20\nknight_moves_helper(moves_left - 1, row - 2, col - 1, memo) +\n21\nknight_moves_helper(moves_left - 1, row - 2, col + 1, memo) +\n22\nknight_moves_helper(moves_left - 1, row + 2, col - 1, memo) +\n23\nknight_moves_helper(moves_left - 1, row + 2, col + 1, memo) +\n24\nknight_moves_helper(moves_left - 1, row - 1, col - 2, memo) +\n25\nknight_moves_helper(moves_left - 1, row - 1, col + 2, memo) +\n26\nknight_moves_helper(moves_left - 1, row + 1, col - 2, memo) +\n27\nknight_moves_helper(moves_left - 1, row + 1, col + 2, memo);\n28\n} // knight_moves_helper()\nA bottom-up solution is shown below:\n1\nint32_t knight_moves(int32_t int32_tm, n) {\n2\nstd::vector<std::vector<std::vector<int32_t>>> memo(m + 1,\n3\nstd::vector<std::vector<int32_t>>(n, std::vector<int32_t>(n)));\n4\nmemo[0][0][0] = 1;\n5\nfor (int32_t num_move = 1; num_move <= m; ++num_move) {\n6\nfor (int32_t row = 0; row < n; ++row) {\n7\nfor (int32_t col = 0; col < n; ++col) {\n8\nmemo[num_move][row][col] =\n9\nmemo_val_bounds_check(num_move - 1, row - 2, col - 1, memo) +\n10\nmemo_val_bounds_check(num_move - 1, row - 2, col + 1, memo) +\n11\nmemo_val_bounds_check(num_move - 1, row + 2, col - 1, memo) +\n12\nmemo_val_bounds_check(num_move - 1, row + 2, col + 1, memo) +\n13\nmemo_val_bounds_check(num_move - 1, row - 1, col - 2, memo) +\n14\nmemo_val_bounds_check(num_move - 1, row - 1, col + 2, memo) +\n15\nmemo_val_bounds_check(num_move - 1, row + 1, col - 2, memo) +\n16\nmemo_val_bounds_check(num_move - 1, row + 1, col + 2, memo);\n17\n} // for col\n18\n} // for row\n19\n} // for num_move\n20\nreturn memo[m][n - 1][n - 1];\n21\n} // knight_moves()\n22\n23\nint32_t memo_val_bounds_check(int32_t int32_t int32_tmove, row, col,\n24\nstd::vector<std::vector<std::vector<int32_t>>>& memo) {\n25\nif (row < 0 || col < 0 || row >= memo[0].size() || col >= memo[0].size()) {\n26\nreturn 0;\n27\n} // if\n28\nreturn memo[move][row][col];\n29\n} // memo_val_bounds()\nΘ(𝑚𝑛2), Θ(𝑚𝑛2)The worst-case time and space complexity of this implementation is since each of the subproblems can be solved in time,Θ(1)\nand each subproblem is only solved once with the help of the memo.", "word_count": 498, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b051c959-54b9-5344-ad7c-2e31e66cde46", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 891, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n879\nExample 23.6 You are given 𝑛dice, each with 𝑚faces numbered 1, 2, …, 𝑚. Given 𝑛, 𝑚, and a target value 𝑥, write a function that returns\nthe total number of distinct ways to roll the dice so that the face-up values sum to 𝑥. For example, given 2, 6, and 7, you would𝑛= 𝑚= 𝑥=\nreturn 6, since there are six distinct ways to roll a 7: (1, 6), (2, 5), (3, 4), (4, 3), (5, 2), and (6, 1).\nSince we are asked to count the number of distinct ways to attain a goal, we will start by identifying a recurrence relation that sums up the\nnumber of ways to reach all preceding states. To help with this process, consider the following example: suppose you are given three six-sided\ndice, with a target value of 15 (𝑛=3, 6, and 15). Here, there are six different preceding states that allow us to reach exactly 15:𝑚= 𝑥=\n• If the sum of the first two dice is 9.\n• If the sum of the first two dice is 10.\n• If the sum of the first two dice is 11.\n• If the sum of the first two dice is 12.\n• If the sum of the first two dice is 13.\n• If the sum of the first two dice is 14.\nIf the sum of the first two dice is anything but a value between 9 and 14, then it would be impossible to roll the third die in a way such that the\ntotal sum of all three dice is exactly 15. Therefore, the total number of ways to roll a sum of 15 using three dice is equal to the total number of\nways to roll a sum between 9 and 14 using two dice!\nWe can generalize this to any 𝑛, 𝑚, and 𝑥. The base case happens when 1, which trivially implies there is only one way to sum to any𝑛=\ntarget between 1 and 𝑚(since there is only one die). For any other 𝑛, the number of ways to roll a sum of 𝑥using 𝑛𝑚-sided dice is equal to the\nnumber of ways to roll a value between min(𝑥−𝑚, 1) and using dice. This can be converted into the following recurrence relation,𝑥−1 𝑛−1\nwhere represents the number of ways to roll a sum of 𝑥using 𝑛𝑚-sided dice (note that 𝑚is not a changing variable in this case, so each𝐹(𝑛,𝑥)\nsubproblem can be uniquely identified using just 𝑛and 𝑥):\n𝐹(𝑛,𝑥)=\n⎧\n⎪\n⎨\n⎪⎩\n1,\n≤𝑥≤𝑚if 𝑛=1,1\n0,\nif otherwise𝑛=1,\n𝐹(𝑛−1,𝑥−1)+𝐹(𝑛−1,𝑥−2)+…+min(𝐹(𝑛−1,𝑥−𝑚),𝐹(𝑛−1,1)),\nif 𝑛>1\nIf you were to draw out the recurrence tree, you would see that there are overlapping subproblems (e.g., and both𝐹(𝑛−1,𝑥−1) 𝐹(𝑛−1,𝑥−2)\nrely on the value of 𝐹(𝑛−2,𝑥−3)). Therefore, dynamic programming can be used to solve this problem. To store our subproblems, we will\nmemo[i][j]use a two-dimensional memo with size based on 𝑛and 𝑥, where stores the number of ways to roll a sum of 𝑗using 𝑖dice. A\ntop-down solution is shown below:\n1\nint32_t dice_roll(int32_t int32_t int32_tn, m, x) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int>(xmemo(n + 1, + 1, -1));\n3\nstd::vector<int32_t>(xmemo[1] = + 1, 0);\n4\nfor (int32_t j = 1; j <= m && j <= x; ++j) {\n5\nmemo[1][j] = 1;\n// only one way to reach [1, m] if only one dice\n6\n} // for j\n7\nreturn dice_roll_helper(n, m, x, memo);\n8\n} // dice_roll()\n9\n10\nint32_t dice_roll_helper(int32_t int32_t int32_tnum_dice, faces, target,\n11\nstd::vector<std::vector<int32_t>>& memo) {\n12\nif (memo[num_dice][target] != -1) {\n// recursive call made before\n13\nreturn memo[num_dice][target];\n14\n} // if\n15\n// solve and store in memo\n16\nint32_t num_ways = 0;\n17\nfor (int32_t diff = 1; diff <= faces && diff < target; ++diff) {\n18\nnum_ways += dice_roll_helper(num_dice - 1, faces, target - diff, memo);\n19\n} // for diff\n20\nreturn memo[num_dice][target] = num_ways;\n21\n} // dice_roll_helper()\nA bottom-up solution is shown below:\n1\nint32_t dice_roll(int32_t int32_t int32_tn, m, x) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(xmemo(n + 1, + 1));\n3\nfor (int32_t j = 1; j <= m && j <= x; ++j) {\n4\nmemo[1][j] = 1;\n// only one way to reach [1, m] if only one dice\n5\n} // for j\n6\nfor (int32_t i = 2; i <= n; ++i) {\n7\nfor (int32_t j = i; j <= x; ++j) {\n8\nfor (int32_t k = 1; k <= m && k < j; ++k) {\n9\nmemo[i][j] += memo[i - 1][j - k];\n10\n} // for k\n11\n} // for j\n12\n} // for i\n13\nreturn memo[n][x];\n14\n} // dice_roll()\nThe auxiliary space used by this implementation is for the size of the memo. The time complexity is Θ(𝑚𝑛𝑥), since each of theΘ(𝑛𝑥) 𝑛𝑥\nsubproblems takes time to compute (as you have to loop over the number of faces to identify all preceding states).Θ(𝑚)", "word_count": 847, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5d4ef0fc-7b3d-5b94-9172-ff25ae0fa080", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 892, "real_page_number": null, "text": "880\nChapter 23. Dynamic Programming\nExample 23.7 Your friend is texting you a message — however, they only have a flip phone and therefore have to rely on the following\nmapping of digits to send messages:\n1\n2\nabc\n3\ndef\n4\nghi\n5\njkl\n6\nmno\n7\npqrs\n8\ntuv\n9\nwxyz\n*\n0\n␣\n#\nTo add a letter, your friend must press the key of the corresponding digit 𝑛times, where 𝑛is the position of the letter in the key. For example,\nto add the letter ‘s’ to the message, the number 7 has to be pressed four times in succession. For this problem, you may assume that the 0, 1,\n*, and # keys are never used.\nHowever, due to a data error, you ended up receiving a string of pressed keys instead of the message itself. For instance, if your friend\nsent the message \"eecs\", you would get the message \"33332227777\". Implement a function that, when given a string representing the\nsequence of keys pressed, returns the total number of possible messages that could have been sent.\n\"22228\",Example: Given the string you would return 7, since there are seven possible messages that could have been sent with the\nsequence of keys: \"aaaat\", \"aabt\", \"act\", \"baaat\", \"babt\", \"bbat\", and \"cat\".\nThis is another problem where we are asked to count ways, so we will consider a dynamic programming approach. Like before, our recurrence\nwill rely on the idea of summing up the number of ways to reach the preceding states of each potential sequence. However, what are these\npreceding states? To conceptualize this, consider the sequence of presses \"2222\". What are the possible ways that could allow us to end up with\nthis sequence? Notice there are three possibilities that allow us to reach this combination:\n• Our previous message consisted of the sequence \"222\", and \"2\" was pressed to add the letter \"a\".\n• Our previous message consisted of the sequence \"22\", and \"22\" was pressed to add the letter \"b\".\n• Our previous message consisted of the sequence \"2\", and \"222\" was pressed to add the letter \"c\".\nTherefore, the total number of possible messages from the sequence \"2222\" is the sum of the number of possible messages from the sequences\n\"222\", \"22\", and \"2\", which are the states that could have preceded \"2222\".\nNote that you only have to consider multiple states if the last numbers in the sequence are the same. For a sequence like \"2223\", the only\npreceding state is \"222\", since the final \"3\" is different from the previous digit and cannot be merged with the digits before it to form a new letter\n(i.e., you cannot get to \"2223\" directly from \"22\" or \"2\"). Therefore, the number of ways to obtain the combination \"2223\" is the same as the\nnumber of ways to obtain the combination \"222\", since \"222\" is the only state that allows you to directly reach \"2223\".\n𝑛thWe can use this information to devise a recurrence relation. For a sequence of presses up to the digit (0-indexing), the total number of\npossible messages can be represented as:𝐹(𝑛)\n𝐹(𝑛)=\n⎧\n⎪\n⎪\n⎪\n⎨\n⎪\n⎪\n⎪⎩\n1,\n𝑛≤0if\n𝐹(𝑛−1)\n+𝐹(𝑛−2)\nif the last two digits match\n+𝐹(𝑛−3)\nif the last three digits match\n+𝐹(𝑛−4)\nif the last four digits match (for ‘7’ and ‘9’)\nThis recurrence involves repeated subproblems (as shown by the recurrence tree below), so dynamic programming would be a viable approach.\n\"22233\"\n\"2223\" + ‘d’\n\"222\" + ‘e’\n\"222\" + ‘d’\n\"22\" + ‘a’\n\"2\" + ‘b’\n\"22\" + ‘a’\n\"2\" + ‘b’\n\"2\" + ‘a’\n‘b’\n‘a’\n\"2\" + ‘a’\n‘b’\n‘a’\n‘a’\n‘a’", "word_count": 616, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6653201c-3971-572b-b765-faae79dbb9e3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 893, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n881\nA top-down solution is shown below:\n1\nint32_t get_number_of_messages(const std::string& sequence) {\n2\nstd::vector<int32_t> memo(sequence.size(), -1);\n3\nmemo[0] = 1;\n4\nreturn get_number_of_messages_helper(sequence, sequence.size() - 1, memo);\n5\n} // get_number_of_messages()\n6\n7\nint32_t get_number_of_messages_helper(const int32_tstd::string& sequence, position,\n8\nstd::vector<int32_t>& memo) {\n9\n// position == 0 indicates the number of possible messages in the subsequence up to index 0\n10\n// if position goes below 0, you should return the number of possible messages given an\n11\n// empty sequence, which is 1 since there is only 1 possible message from an empty string\n12\nif (position == -1) {\n13\nreturn 1;\n14\n} // if\n15\nif (memo[position] != -1) {\n// recursive call made before\n16\nreturn memo[position];\n17\n} // if\n18\n// solve and store in memo\n19\nint32_t max_key_press = (sequence[position] == '7' || sequence[position] == '9') ? 4 : 3;\n20\nint32_t press_num = 0, curr_pos = position, total_messages = 0;\n21\n// iterate over the previous characters (up to the maximum possible repeats) and\n22\n// add subproblem if the characters match\n23\nwhile (press_num++ < max_key_press && curr_pos >= 0 && sequence[curr_pos] == sequence[position]) {\n24\n--curr_pos;\n25\ntotal_messages += get_number_of_messages_helper(sequence, curr_pos, memo);\n26\n} // while\n27\nreturn memo[position] = total_messages;\n28\n} // get_number_of_messages_helper()\nA bottom-up solution is shown below:\n1\nint32_t get_number_of_messages(const std::string& sequence) {\n2\nstd::vector<int32_t> memo(sequence.size() + 1, -1);\n3\nfor (size_t i = 1; i <= sequence.size(); ++i) {\n4\nmemo[i] = memo[i - 1];\n5\nif (i >= 2 && sequence[i - 1] == sequence[i - 2]) {\n6\nmemo[i] += memo[i - 2];\n7\nif (i >= 3 && sequence[i - 1] == sequence[i - 3]) {\n8\nmemo[i] += memo[i - 3];\n9\nif ((sequence[i - 1] == '7' || sequence[i - 1] == '9') &&\n10\ni >= 4 && sequence[i - 1] == sequence[i - 4]) {\n11\nmemo[i] += memo[i - 4];\n12\n} // if\n13\n} // if\n14\n} // if\n15\n} // for i\n16\nreturn memo.back();\n17\n} // get_number_of_messages()\nThe time complexity of this solution is given a sequence of length 𝑛, since each of the 𝑛subproblems takes time to compute. TheΘ(𝑛) Θ(1)\nauxiliary space used by this implementation is for the size of the memo (however, as we will see in section 23.4, it is possible to reduce theΘ(𝑛)\nauxiliary space of the bottom-up solution to if we only keep track of the subproblems we may actually need in the future).Θ(1)\n¸ 23.3.2\nPath and Decision Optimization\nAnother common pattern of dynamic programming problems involves finding the best path or sequence of decisions to reach a certain goal,\ngiven a cost or value associated with each state. To identify the best path to reach any given state for these types of problems, we can use a\nstrategy similar to the one we used to count distinct ways: compute the best possible path to reach any state state,directly preceding the current\nand add it to the cost or value of the current state.\nExample 23.8 You are given a 𝑚×𝑛grid filled with non-negative integers. Write a function that finds a path from the top-left corner of\nthe grid to the bottom-right with and returns this sum, assuming that you can only move down or right at any point in time.the minimum sum\nFor example, given the following 3 4 grid, you would return 14, since this is the cost of the minimum path that starts from the top-left and×\nends at the bottom-right with only down and right movements:\n2\n1\n2\n1\n6\n4\n3\n9\n3\n7\n1\n5\n2\n1\n2\n1\n6\n4\n3\n9\n3\n7\n1\n5", "word_count": 635, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "736d52ac-6288-5798-9f3e-097c191fe109", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 894, "real_page_number": null, "text": "882\nChapter 23. Dynamic Programming\nThis problem is very similar to the one in which we had to count the number of ways to get to the bottom-right corner of a grid. Notice here that\nthere are two ways to reach the bottom-right square: either from the cell above or the cell to the left.\n↓\n→\nTherefore, if we knew the best path to either of these two squares, we would be able to compute the overall best path by simply adding it to the\nvalue of the final square! This gives us our recursive relationship between subproblems, where represents the minimum cost path to𝐹(𝑟,𝑐)\nreach the cell at (row 𝑟, column 𝑐):\nmin\n2\n1\n2\n1\n6\n4\n3\n9\n3\n7\n1\n5\n𝐹(2,2)=9\n2\n1\n2\n1\n6\n4\n3\n9\n3\n7\n1\n5\n𝐹(1,3)=15\n+\n2\n1\n2\n1\n6\n4\n3\n9\n3\n7\n1\n5\n𝐺[2][3] = 5\n=\n2\n1\n2\n1\n6\n4\n3\n9\n3\n7\n1\n5\n𝐹(2,3)=14\nThe recurrence relation for this problem is as follows. Given a grid of values 𝐺, we can compute the minimum path to reach (row 𝑟, column 𝑐)\nby solving the subproblems and 𝐹(𝑟,𝑐−1), and then combining the smaller of these two values with the value of 𝐺[𝑟][𝑐]. The base𝐹(𝑟−1,𝑐)\ncase occurs when there is only a single square in the grid (𝑟= 0), which trivially implies that the minimum path cost is simply the value of𝑐=\nthat square, 𝐺[0][0]. Since this recurrence relation exhibits overlapping subproblems, a dynamic programming approach can be used.\n𝐹(𝑟,𝑐)=\n⎧\n⎪\n⎨\n⎪⎩\n𝐺[0][0],\nif 𝑟=0,𝑐=0\n∞(i.e., should be ignored),\nif or (out-of-bounds)𝑟< 𝑐<0 0\nmin(𝐹(𝑟−1,𝑐),𝐹(𝑟,𝑐−1))+𝐺[𝑟][𝑐],\nif 𝑟>0,𝑐>0\nRemark: Since this is an optimization problem with an optimal substructure, it is good practice to check if a greedy solution exists before\nwe start planning a dynamic programming solution. If we can find an algorithm that guarantees an optimal solution by simply making a\nlocally optimal choice at each step, we would not need to potentially solve and store every possible subproblem as required by a dynamic\nprogramming approach.\nAscoveredinchapter21,agreedyapproachworksiftheproblemexhibitstwoproperties: anoptimal\n(where the optimal solution of a problem can be computed using the optimal solutionssubstructure\nof its subproblems) and the (at least one optimal solution contains the firstgreedy-choice property\ngreedy choice). Although the problem exhibits an optimal substructure, it does exhibit thenot\ngreedy-choice property. In the following grid, the first greedy choice would be to move to the right,\nsince the cost of going right (1) is better than the cost of going down (2). However, the optimal\nsolution goes down first, with a total path cost of 1 + 2 + 1 + 1 + 1 = 6. Since the greedy-choice\nproperty does not hold, we can conclude that the problem cannot be solved using a greedy approach.\n1\n1\n1\n2\n9\n9\n1\n1\n1\ngreedy\noptimal\n1\n1\n1\n2\n9\n9\n1\n1\n1\n1\n1\n1\n2\n9\n9\n1\n1\n1\nA top-down solution for this problem is shown below:\n1\nint32_t min_path(const std::vector<std::vector<int32_t>>& grid) {\n2\nsize_t m = grid.size(), n = grid[0].size();\n3\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(n,memo(m, -1));\n4\nreturn min_path_helper(m - 1, n - 1, grid, memo);\n5\n} // min_path()\n6\n7\nint32_t min_path_helper(int32_t int32_t const std::vector<std::vector<int32_t>>&row, col, grid,\n8\nstd::vector<std::vector<int32_t>>& memo) {\n9\nif (row == 0 && c == 0) {\n10\nreturn grid[0][0];\n11\n} // if\n12\nif (row < 0 || col < 0) {\n13\n// prevents algorithm from choosing a path out-of-bounds\n14\nreturn std::numeric_limits<int>::max();\n15\n} // if\n16\nif (memo[row][col] != -1) {\n// recursive call made before\n17\nreturn memo[row][col];\n18\n} // if\n19\nelse {\n// solve and store in memo\n20\nreturn memo[row][col] = std::min(min_path_helper(row - 1, col, grid, memo),\n21\nmin_path_helper(row, col - 1, grid, memo)) + grid[row][col];\n22\n} // else\n23\n} // min_path_helper()", "word_count": 690, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5be849eb-8b01-5d4a-a79e-e603fe3b6acf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 895, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n883\nA bottom-up solution for this problem is shown below:\n1\nint32_t min_path(const std::vector<std::vector<int32_t>>& grid) {\n2\nsize_t m = grid.size(), n = grid[0].size();\n3\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(n));memo(m,\n4\nmemo[0][0] = grid[0][0];\n5\nfor (size_t i = 1; i < m; ++i) {\n6\nmemo[i][0] = memo[i - 1][0] + grid[i][0];\n7\n} // for i\n8\nfor (size_t j = 1; j < n; ++j) {\n9\nmemo[0][j] = memo[0][j - 1] + grid[0][j];\n10\n} // for j\n11\nfor (size_t i = 1; i < m; ++i) {\n12\nfor (size_t j = 1; j < n; ++j) {\n13\nmemo[i][j] = std::min(memo[i - 1][j], memo[i][j - 1]) + grid[i][j];\n14\n} // for j\n15\n} // for i\n16\nreturn memo[m - 1][n - 1];\n17\n} // min_path()\nThetimeandspacecomplexityofthisproblemisΘ(𝑚𝑛), sinceupto subproblemsneedtobesolvedifamemoisusedtostoreoverlappingΘ(𝑚𝑛)\nsolutions, and each subproblem takes time to compute.Θ(1)\nExample 23.9 You are given an integer array of coin denominations and a target value. Write a function that returns the fewest number of\n0coins that you need to make change for the given target, or if no such way to make change exists.\nThis is a problem we have seen before when discussing greedy algorithms. Although the greedy approach works for some coin denominations, it\ndoes not find an optimal solution in all cases (see example 21.3). On the contrary, a dynamic programming approach guarantee an optimaldoes\nsolution, regardless of which coin denominations are used! This is because dynamic programming considers all relevant subproblems when\nbuilding up a solution, and not just the locally optimal ones.\nSincethisisadecisionoptimizationproblem,wherewehavetooptimizethecoinweselectateachstep,wecanbeginadynamicprogramming\nsolution by considering the optimal solutions of all preceding states that allow us to reach 𝑥cents with the addition of an available coin. Given\nany set of 𝑛coin denomiations 𝑑1,𝑑2,…,𝑑𝑛and a target amount 𝑥, there are up to 𝑛preceding states that allow us to directly reach our goal:\n• If we have already made optimal change for cents, we can simply add a cent coin.𝑥−𝑑1 𝑑1\n• If we have already made optimal change for cents, we can simply add a cent coin.𝑥−𝑑2 𝑑2\n• If we have already made optimal change for 𝑥−𝑑𝑛cents, we can simply add a 𝑑𝑛cent coin.\nTherefore, to determine the minimum number of coins needed to make change for 𝑥cents, we can first compute the minimum number of coins\nto make change for cents, cents, …, and 𝑥−𝑑𝑛cents, find the solution out of these subproblems that requires the fewest number of𝑥−𝑑1 𝑥−𝑑2\ncoins, and then add one to that solution (for the additional coin we need to reach 𝑥cents). The recurrence relation is shown below, where 𝐹(𝑥)\nrepresents the minimum number of coins needed to make change for 𝑥cents.\n𝐹(𝑥)=\n{\n0,\nif 𝑥=0\n1+min(𝐹(𝑥−𝑑1),𝐹(𝑥−𝑑2),…,𝐹(min(𝑥−𝑑𝑛,0)),\nif 𝑥>0\nThis recurrence exhibits overlapping subproblems, so dynamic programming can be used. A top-down solution is shown below:\n1\nint32_t coin_change(const std::vector<int32_t>& int32_tcoins, target) {\n2\nstd::vector<int32_t> memo(target + 1, -1);\n3\nreturn coin_change_helper(coins, target, memo);\n4\n} // coin_change()\n5\n6\nint32_t coin_change_helper(std::vector<int32_t>& int32_tcoins, rem_change,\n7\nstd::vector<int32_t>& memo) {\n8\nif (rem_change == 0) {\n9\nreturn 0;\n10\n} // if\n11\nif (rem_change < 0) {\n12\nreturn -1;\n// invalid\n13\n} // if\n14\n// if recursive call made before, fetch solution from memo\n15\nif (memo[rem_change] != -1) { // recursive call made before\n16\nreturn memo[rem_change];\n17\n} // if\n18\nelse { // solve and store in memo\n19\nint32_t std::numeric_limits<int32_t>::max();curr_min =\n20\nfor (int32_t coin_value : coins) {\n21\nint32_t result = coin_change_helper(coins, rem_change - coin_value, memo);\n22\nif (result >= 0 && result < curr_min) {\n23\ncurr_min = result + 1;\n24\n} // if\n25\n} // for coin_value\n26\nreturn memo[rem_change] =\n27\nstd::numeric_limits<int32_t>::max())(curr_min == ? -1 : curr_min;\n28\n} // else\n29\n} // coin_change_helper()", "word_count": 700, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5bfc3c44-dac5-5c3a-86fd-3b95820fe55a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 896, "real_page_number": null, "text": "884\nChapter 23. Dynamic Programming\nA bottom-up solution is shown below:\n1\nint32_t coin_change(const std::vector<int32_t>& int32_tcoins, target) {\n2\nstd::vector<int32_t> memo(target + 1);\n3\nfor (int32_t i = 1; i <= target; ++i) {\n4\nstd::numeric_limits<int32_t>::max();memo[i] =\n5\nfor (int32_t coin_value : coins) {\n6\nif (coin_value <= i &&\n7\nstd::numeric_limits<int32_t>::max())memo[i - coin_value] != {\n8\nmemo[i] = std::min(memo[i], memo[i - coin_value] + 1);\n9\n} // if\n10\n} // for coin_value\n11\n} // for i\n12\nreturn std::numeric_limits<int32_t>::max()memo[target] == ? -1 : memo[target];\n13\n} // coin_change()\nGiven a target of 𝑥cents and 𝑛coins, the time complexity of this solution is Θ(𝑛𝑥), and the auxiliary space is Θ(𝑥).\nExample 23.10 You currently have a job that requires you to travel around to different cities. Your travel days are planned in advance, and\ndays.they are stored in a sorted integer array Luckily, you are able to buy train passes that allow you travel freely within a certain range of\ntime after the date of purchase. There are three types of passes you can buy:\n• a 1-day pass\n• a 7-day pass\n• a 30-day pass\nFor instance, if you buy a 7-day pass on day 25, you can travel for 7 days starting on day 25: days 25, 26, 27, 28, 29, 30, and 31. The price\ncosts, costs[0] costs[1]of each pass is stored in an array where stores the price of a 1-day pass, stores the price of a 7-day pass,\ncosts[2] days costs,and stores the price of a 30-day pass. Given and implement a function that returns the minimum cost required\ndaysto travel on all the specified days in the array.\ndays = [1, 5, 11, 13, 17, 20, 37] costs = [3, 7, 15], 18,Example: Given and you would return since this is the\nminimum cost required to travel on all the given days (buy a 30-day pass on day 1 for $15, and a 1-day pass on day 37 for $3).\nWe want to have a valid pass on all of our specified travel days up to some final travel day 𝑥. Notice that there exist three options that allow us to\ntravel on day 𝑥while also potentially minimizing total cost:\n• We buy a 1-day pass on day 𝑥\n• We buy a 7-day pass on day 𝑥−6\n• We buy a 30-day pass on day 𝑥−29\nIf we buy a 1-day pass on day 𝑥, the minimum cost across all of our travel days would be the price of the 1-day pass on day 𝑥plus the minimum\ncost of satisfying all of our travel days up to day 𝑥−1. If we let denote the minimum cost of traveling up to day 𝑥, then the minimum cost𝐹(𝑥)\nof traveling up to day 𝑥, assuming we buy a 1-day pass on day x, is equal to:\n(cost of 1-day pass)𝐹(𝑥−1) +\nIf we buy a 7-day pass on day 𝑥−6, the minimum cost across all of our travel days would be the price of the 7-day pass on day plus the𝑥−6\nminimum cost of satisfying all of our travel days up to day 𝑥−7. In other words, the minimum cost of traveling up to day 𝑥, assuming we buy a\n7-day pass on day – 6, is equal to:x\n(cost of 7-day pass)𝐹(𝑥−7) +\nSimilarly, if we buy a 30-day pass on day 𝑥−29, the minimum cost across all of our travel days would be the price of the 30-day pass on day\nplus the minimum cost of satisfying all of our travel days up to day 𝑥−30. The minimum cost of traveling up to day 𝑥, assuming we buy𝑥−29\na 30-day pass on day – 29, is equal to:x\n(cost of 30-day pass)𝐹(𝑥−30) +\nThe actual minimum cost to travel on day 𝑥would therefore be the best of these three options:\nmin(𝐹(𝑥−1)+cost[0], 𝐹(𝑥−7)+cost[1], 𝐹(𝑥−30)+cost[2])𝐹(𝑥)=\nHowever, there is an additional detail we must consider when solving this problem: if the current day 𝑥we are considering is not a travel day,\nthen we do not have to consider buying a pass for that day. Thus, we can forward propogate the best cost from the previous travel day 𝑥−1\nand use that value as the best cost up to day 𝑥. The recurrence relation for this problem is shown below. The base case occurs when 𝑥< 1\n(outside the travel window), which has a cost of zero since you do not have to buy any tickets before the earliest possible travel date. There exist\noverlapping subproblems, so dynamic programming can be used.\n𝐹(𝑥)=\n⎧\n⎪\n⎨\n⎪⎩\n0,\nif 𝑥<1\n𝐹(𝑥−1),\nif 𝑥not in travel days\nmin(𝐹(𝑥−1)+cost[0],𝐹(𝑥−7)+cost[1],𝐹(𝑥−30)+cost[2]),\n𝑥≤finalif travel day, otherwise", "word_count": 799, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "16ef769b-b68f-58d0-8b11-9f3dabe22679", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 897, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n885\ntime.8A top-down solution is shown below. Here, we use an unordered set to identify which days are travel days in constant\n1\nint32_t min_cost_travel(const std::vector<int32_t>& const std::vector<int32_t>&days, costs) {\n2\nstd::vector<int32_t> memo(days.back() + 1, -1);\n3\nstd::unordered_set<int32_t> days_set(days.begin(), days.end());\n4\nreturn min_cost_helper(days_set, costs, days.back(), memo);\n5\n} // min_cost_travel()\n6\n7\nint32_t min_cost_helper(const std::unordered_set<int32_t>& days_set,\n8\nconst std::vector<int32_t>& costs,\n9\nint32_t std::vector<int32_t>&day, memo) {\n10\nif (day < 1) { // base case\n11\nreturn 0;\n12\n} // if\n13\nif (memo[day] != -1) { // recursive call made before\n14\nreturn memo[day];\n15\n} // if\n16\nelse { // solve and store in memo\n17\n// if not a travel day, forward propogate cost from previous day\n18\nif (!days_set.count(day)) {\n19\nreturn memo[day] = min_cost_helper(days_set, costs, day - 1, memo);\n20\n} // if\n21\nelse {\n22\nreturn memo[day] = std::min({\n23\nmin_cost_helper(days_set, costs, day - 1, memo) + costs[0],\n24\nmin_cost_helper(days_set, costs, day - 7, memo) + costs[1],\n25\nmin_cost_helper(days_set, costs, day - 30, memo) + costs[2]\n26\n});\n27\n} // else\n28\n} // else\n29\n} // min_cost_helper()\nA bottom-up solution is shown below:\n1\nint32_t min_cost_travel(const std::vector<int32_t>& const std::vector<int32_t>&days, costs) {\n2\nstd::vector<int32_t> memo(days.back() + 1);\n3\nmemo[0] = 0; // not needed since default init, but included for clarity\n4\nstd::unordered_set<int> days_set(days.begin(), days.end());\n5\nfor (int32_t i = 1; i < memo.size(); ++i) {\n6\nif (days_set.count(i)) {\n7\nmemo[i] = std::min({\n8\nmemo[i - 1] + costs[0],\n9\nmemo[std::max(0, i - 7)] + costs[1],\n10\nmemo[std::max(0, i - 30)] + costs[2]\n11\n});\n12\n} // if\n13\nelse {\n14\nmemo[i] = memo[i - 1];\n15\n} // else\n16\n} // for i\n17\nreturn memo.back();\n18\n} // min_cost_travel()\nThe time and auxiliary space complexity of this dynamic programming solution is Θ(𝑛), where 𝑛is the final travel day.\nExample 23.11 prices, prices[j]You are given an integer 𝑘and an integer array where represents the price of a given stock on the\njth day. Implement a function that returns the maximum profit you can attain, if you are only allowed to complete at most 𝑘transactions.\nA buy + sell operation is considered together as part of a single transaction. You are not allowed to engage in multiple transactions\nsimultaneously; that is, you must sell the stock before you can buy again. For this problem, you are also not allowed to sell a stock before\nbuying it (sorry, no short selling allowed!).\nSimilar to the previous problem, we can solve this question by first identifying the choices that can be made on each day, and then identifying a\nrecursive relationship that allows us to build up an optimal solution using these choices. Since each \"buy\" must always be paired with a \"sell\"\nlater down the line, we can simplify our recurrence by considering buying and selling in unison (rather than as separate actions); this allows us\nto define the problem in terms of individual transactions instead of separate buy and sell operations. In this case, there are two choices we can\nmake on each day 𝑗:\n𝑗th• We can do nothing on the day.\n𝑗th• We can complete a transaction on the day.\n𝑗thLet us define as the maximum profit we can earn from having performed a total of 𝑖transactions on the day. If we do on the𝐹(𝑖,𝑗) nothing\n𝑗th day, then the maximum profit we can earn given 𝑖transactions would be\n𝐹(𝑖,𝑗−1)\nThis is because the maximum profit we can earn from performing 𝑖transactions up to day 𝑗is the same as the maximum profit we can earn from\nperforming 𝑖transactions up to day if we do not complete any transaction on day 𝑗.𝑗−1\n8Note std::min()that (lines22-26)musttakeinaninitializerlistifyouwanttofindtheminimumofmorethantwovalues.", "word_count": 655, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e144b2ee-3609-50e0-887e-9b405d251e64", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 898, "real_page_number": null, "text": "886\nChapter 23. Dynamic Programming\n𝑗thOn the other hand, if we want to complete a transaction on the day, we must the stock on day 𝑗(this is because we are consideringsell\nbuy/sell together as a single transaction). Since the problem prohibits us from engaging in multiple transactions at the same time, this means\nthat we must have purchased the stock on some day 𝑑, where 𝑗, for us to be able to sell it on day 𝑗. Assuming 𝑖transactions are allowed, the𝑑<\nmaximum profit we can earn if we buy on day 𝑑and sell on day 𝑗is\n(prices[𝑗] - prices[𝑑]) + 𝐹(𝑖−1,𝑑)\n𝑖thNote that this equation includes the profit from performing the transaction (prices[𝑗] - prices[𝑑]), plus the optimal profit from performing\ntransactions up to day 𝑑(this is because all previous transactions must be finished before day 𝑑since simultaneous transactions are𝑖−1 𝑖−1\n𝑗thnot allowed). Therefore, to identify the best profit we can earn from completing a transaction on the day, we just have to find the optimal day\n𝑑from 0 to that maximizes our profit:𝑗−1\n((prices[𝑗] - prices[𝑑]) + 𝐹(𝑖−1,𝑑))max0≤𝑑≤𝑗−1\n𝑗thThis forms the basis of our recurrence relation. To determine the maximum profit attainable from 𝑖transactions by the day, we simply take\n𝑗th 𝑗ththe better outcome between doing nothing on the day and completing a transaction on the day. The base cases occur when the number of\ntransactions 𝑖is 0 (which results in a profit of 0) and when 𝑗is 0 (which also results in a profit of 0 since you don’t have any days to trade).\n𝐹(𝑖,𝑗)=\n{\n0,\nif or𝑖= 𝑗=0 0\nmax(𝐹(𝑖,𝑗−1),max0≤𝑑≤𝑗−1(prices[𝑗]−prices[𝑑])+𝐹(𝑖−1,𝑑)),\nif or𝑖> 𝑗>0 0\nThis recurrence relation exhibits the property of overlapping subproblems, so dynamic programming can be used. Using this recurrence relation,\nwe can write a top-down solution as shown below. Since each subproblem depends on two changing variables 𝑖and 𝑗— where 𝑖can take\non values from to the number of transactions 𝑘, and 𝑗can take on values from to the number of trading days 𝑛— there are a total of0 0\nsubproblems we many encounter. Thus, we can initialize a memo of size to store our subproblems, where(𝑖+ (𝑗+ (𝑘+ (𝑛+1)× 1) 1)× 1)\nmemo[𝑖][𝑗] stores the solution to the subproblem 𝐹(𝑖,𝑗).\n1\nint32_t max_profit(int32_t const std::vector<int32_t>&k, prices) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(prices.size()memo(k + 1, + 1, -1));\n3\nreturn max_profit_helper(k, prices, k, prices.size(), memo);\n4\n} // max_profit()\n5\n6\nint32_t max_profit_helper(int32_t const std::vector<int32_t>& int32_t int32_tk, prices, i, j,\n7\nstd::vector<std::vector<int32_t>>& memo) {\n8\nif (i == 0 || j == 0) {\n9\nreturn 0;\n10\n} // if\n11\nif (memo[i][j] != -1) {\n12\nreturn memo[i][j];\n13\n} // if\n14\nint32_t best = max_profit_helper(k, prices, i, j - 1, memo);\n15\nfor (int32_t d = 1; d <= j; ++d) {\n16\nbest = std::max(best, prices[j - 1] - prices[d - 1] +\n17\nmax_profit_helper(k, prices, i - 1, d, memo));\n18\n} // for d\n19\nreturn memo[i][j] = best;\n20\n} // max_profit_helper()\nIf we use a bottom-up approach, we would start with our base cases and build our solution upwards using the recurrence. We start by initializing\nall cells in row and column 0 of our memo with our base case value of 0. Then, we would iterate over our memo row by row, applying the rules\nof our recurrence relation to solve each subproblem and fill out its corresponding cell in the memo. For example, consider this partially filled\nprices = [3, 5, 2, 7, 4, 1, 8, 6, 9].memo given and𝑘=3\n3\n5\n2\n7\n4\n1\n8\n6\n9\nprices\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n𝑖=0\n𝑖=1\n𝑖=2\n𝑖=3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n5\n5\n5\n7\n8\n8\n0\n0\n2\n2\n7\n7\n7\n0", "word_count": 670, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1a0c9245-92e8-5ddd-b272-338f095de14e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 899, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n887\nSuppose we want to calculate what goes in memo[2][7], or the best profit we can attain from 2 transactions up the 7th day. If we do nothing on\nthe 7th day, then the profit we earn would be equal to the best profit we can attain from 2 transactions up to the 6th day, or the value in the cell\ndirectly to the left:\n3\n5\n2\n7\n4\n1\n8\n6\n9\nprices\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n𝑖=0\n𝑖=1\n𝑖=2\n𝑖=3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n5\n5\n5\n7\n8\n8\n0\n0\n2\n2\n7\n7\n7\n0\nIf we decide to complete a transaction on the 7th day, then the profit we earn would be equal to the best profit we can attain from 1 transaction\ncompleted by some optimal day 𝑑< 7, plus the profit from completing the transaction on day 7. This requires us to iterate over all columns in\nthe row up to day 6 and identify the optimal day to complete the first transaction:𝑖=1\n3\n5\n2\n7\n4\n1\n8\n6\n9\nprices\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n𝑖=0\n𝑖=1\n𝑖=2\n𝑖=3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n5\n5\n5\n7\n8\n8\n0\n0\n2\n2\n7\n7\n7\n0\nIn this case, the best profit from completing the final transaction on day 7 can be obtained by completing the first transaction on day 4 for a\nprofit of 5 (this result has already been computed in memo[1][4]), and then completing the second transaction on day 7 for a profit of 7, for a\ncombined profit of 5 + 7 = 12. Therefore, we would earn 7 (i.e., memo[2][6]) if we did nothing on the 7th day, and 12 (i.e., prices[7] - prices[6]\n+ memo[1][6]) if we completed our final transaction on the 7th day. The latter outcome is better, so memo[2][7] would store a value of 12.\n3\n5\n2\n7\n4\n1\n8\n6\n9\nprices\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n𝑖=0\n𝑖=1\n𝑖=2\n𝑖=3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n5\n5\n5\n7\n8\n8\n0\n0\n2\n2\n7\n7\n7\n12\n0\nThe process repeats for the remaining cells. For instance, when determining the value of memo[2][8] (the best profit attainable from completing\nthe final transaction on day 8), we can either do nothing on the 8th day to earn a profit of memo[2][7] = 12, or we can complete the second\ntransaction on the 8th day. However, if we loop over all the columns in the row up to day 7 and compute the optimal profit from completing𝑖=1\nthe second transaction on day 8, there is no solution that ends up better than 12. Therefore, the best decision would be to do nothing on day 8, so\nmemo[2][8] also stores a value of 12.\n3\n5\n2\n7\n4\n1\n8\n6\n9\nprices\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n𝑖=0\n𝑖=1\n𝑖=2\n𝑖=3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n5\n5\n5\n7\n8\n8\n0\n0\n2\n2\n7\n7\n7\n12 12\n0\nAfter filling out the entire memo, the solution to the original problem can be found in the bottom right cell. As shown, the optimal profit in our\nexample is 15 (buy on day 3, sell on day 4, buy on day 6, sell on day 7, buy on day 8, and sell on day 9).\n3\n5\n2\n7\n4\n1\n8\n6\n9\nprices\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n𝑖=0\n𝑖=1\n𝑖=2\n𝑖=3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n5\n5\n5\n7\n8\n8\n0\n0\n2\n2\n7\n7\n7\n1312 12\n0\n0\n2\n2\n7\n7\n7\n14 14 15", "word_count": 721, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "062daedc-2e41-54bd-bcee-f6425d4c0f94", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 900, "real_page_number": null, "text": "888\nChapter 23. Dynamic Programming\nThe code for a bottom-up solution is shown below.\n1\nint32_t max_profit(int32_t const std::vector<int32_t>&k, prices) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(prices.size()memo(k + 1, + 1));\n3\nfor (int32_t i = 1; i <= k; ++i) {\n4\nfor (int32_t j = 1; j <= prices.size(); ++j) {\n5\nint32_t best = memo[i][j - 1];\n6\nfor (int32_t d = 1; d <= j; ++d) {\n7\nbest = std::max(best, prices[j - 1] - prices[d - 1] + memo[i - 1][d]);\n8\n} // for d\n9\nmemo[i][j] = best;\n10\n} // for j\n11\n} // for i\n12\nreturn memo[k][prices.size()];\n13\n} // max_profit()\npricesSincetherearetotalof subproblemsyoumayencounter(where𝑘isthenumberoftransactionsand𝑛isthenumberofdaysintheΘ(𝑘𝑛)\nvector), and the time to solve each subproblem takes up to (as you have to loop over the previous row with each subproblem), the overallΘ(𝑛)\nΘ(𝑘𝑛2).time complexity of this implementation is Since we use a (𝑘+1) (𝑛+1) memo, the auxiliary space of this implementation is Θ(𝑘𝑛).×\nRemark: The above solution is actually not the most efficient way to solve the problem, and a better solution exists. This improvedΘ(𝑘𝑛)\nsolution relies on an observation that allows you to compute the best profit from completing a transaction without needing toin constant time\nloop over the previous row!\nTo illustrate this, suppose we want to find the best profit of completing 2 transactions by day 3. We would need to loop over values of 𝑑from\n0 to 2 and compute the maximum of the following:\nprices[3] - prices[0] + 𝐹(1,0)\nprices[3] - prices[1] + 𝐹(1,1)\nprices[3] - prices[2] + 𝐹(1,2)\nSince prices[3] is constant across all three, the value of 𝑑with the largest value of (-prices[𝑑] + 𝐹(1,𝑑)) would yield the maximum profit.\nNow, suppose we want to solve for the best profit of completing 2 transactions by day 4. Using our current implementation, we would loop\nover values of 𝑑from 0 to 3 and compute the maximum of the following:\nprices[4] - prices[0] + 𝐹(1,0)\nprices[4] - prices[1] + 𝐹(1,1)\nprices[4] - prices[2] + 𝐹(1,2)\nprices[4] - prices[3] + 𝐹(1,3)\nHowever, we already computed (-prices[𝑑] + 𝐹(1,𝑑)) when solving the previous subproblem! Thus, we do not need to loop overmax0≤𝑑≤2\nthe previous row again — we can instead compare the value of (-prices[3] + 𝐹(1,3)) with the value of (-prices[𝑑] + 𝐹(1,𝑑)) wemax0≤𝑑≤2\npreviously computed and add the larger of the two to prices[4] to get the maximum profit from completing a transaction on day 4. In general,\nif we keep track of the largest value of (-prices[𝑑] + 𝐹(𝑖−1,𝑑)) we’ve seen for each value of 𝑖, we can use it to compute the best profit of\nlater subproblems without needing an additional loop! The updated solution is shown below:\n1\nint32_t max_profit(int32_t const std::vector<int32_t>&k, prices) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(prices.size()memo(k + 1, + 1));\n3\nfor (int32_t i = 1; i <= k; ++i) {\n4\nint32_t temp_max = !prices.empty() ? -prices[0] : 0;\n5\nfor (int32_t j = 1; j <= prices.size(); ++j) {\n6\nmemo[i][j] = std::max(memo[i][j - 1], prices[j - 1] + temp_max);\n7\ntemp_max = std::max(temp_max, memo[i - 1][j - 1] - prices[j - 1]);\n8\n} // for j\n9\n} // for i\n10\nreturn memo[k][prices.size()];\n11\n} // max_profit()\nWith this change, each subproblem only takes time to solve, which brings our overall time complexity down to Θ(𝑘𝑛).Θ(1)\nRemark: Although we won’t discuss it in this chapter, the traveling salesperson problem can also be solved using dynamic programming.\nΘ(2𝑛𝑛2) Θ(𝑛2𝑛)The isanalgorithmthatusesdynamicprogrammingtosolveTSP,anditrunsin timeanduses auxiliaryHeld-Karpalgorithm\nspace. In theory, this is asymptotically faster than the worse-case time complexity of branch-and-bound. However, this \"improved\"Θ(𝑛!)\nperformance isn’t really discernible in practice, and the massive space complexity typically makes Held-Karp impractical for larger input\nsizes, compared to a branch-and-bound solution.", "word_count": 674, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1a90909c-bcfc-534b-b22a-5fd6c18e0909", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 901, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n889\n¸ 23.3.3\nDecision Making: Take It or Leave It\nSome dynamic programming problems require you to choose a subset of items to include in a solution given a set of constraints, where you\nhave to decide if each input value should be included or excluded to attain a desired result. To solve these problems, iterate over each item and\ncompare previous states where the item is and isn’t used, and use the solutions to these subproblems to determine if you should include the item\nor exclude it. A few examples are covered below.\nExample 23.12 You are a robber planning to rob houses along a street. Each house has a certain amount of money stashed, stored in\nhouses.an integer array However, you are not allowed to rob houses, since adjacent houses have security systems that willadjacent\nhousesautomatically alert the police if both houses are broken into on the same night. Write a function that takes in the array and returns\nthe maximum amount of money you can rob in one night without alerting the police.\nAt each house, you have two choices you can make: you can either rob the house, or you can choose not to rob it. As a result, this is a decision\nproblem, and the decision you make is dependent on the previous houses you decide to rob. If the house you are considering to rob is located at\nindex 𝑖, then the choice to rob that house means that you cannot rob the house located at index (but you can rob the house at index and𝑖−1 𝑖−2\nthe optimal houses up to that house). On the other hand, if you choose not to rob the house at index 𝑖, you will be able to rob the house at index\nand all optimal houses up to that point. Therefore, the decision to rob the house at index 𝑖boils depends on which is more profitable:𝑖−1\n• Rob the house at index 𝑖and combine the money with the optimal amount from robbing up to house 𝑖−2\n• Do not rob the house at index 𝑖and keep the optimal amount from robbing up to house 𝑖−1\nThis gives us the following recurrence relation, where represents the optimal profit attainable from robbing houses up to index 𝑖. The base𝐹(𝑖)\ncase occurs when 0, since you earn 0 profit if you cannot rob any house at all.𝑖<\n𝐹(𝑖)=\n{\n0,\nif 𝑖<0\nmax(𝐹(𝑖−2)+houses[𝑖],𝐹(𝑖−1)),\n𝑖≥0if\nA top-down solution is shown below:\n1\nint32_t rob_houses(const std::vector<int32_t>& houses) {\n2\nstd::vector<int32_t> memo(houses.size(), -1);\n3\nreturn rob_helper(houses, houses.size() - 1, memo);\n4\n} // rob_houses()\n5\n6\nint32_t rob_helper(const std::vector<int32_t>& int32_t std::vector<int32_t>&houses, idx, memo) {\n7\nif (idx < 0) {\n8\nreturn 0;\n9\n} // if\n10\nif (memo[idx] != -1) {\n11\nreturn memo[idx];\n12\n} // if\n13\nelse {\n14\nreturn memo[idx] = std::max(rob_helper(houses, idx - 2, memo) + houses[idx],\n15\nrob_helper(houses, idx - 1, memo));\n16\n} // else\n17\n} // rob_helper()\nA bottom-up solution is shown below:\n1\nint32_t rob_houses(const std::vector<int32_t>& houses) {\n2\nif (houses.size() == 0) {\n3\nreturn 0;\n4\n} // if\n5\nif (houses.size() == 1) {\n6\nreturn houses[0];\n7\n} // if\n8\nstd::vector<int32_t> memo(houses.size());\n9\nmemo[0] = houses[0];\n10\nmemo[1] = std::max(houses[0], houses[1]);\n11\nfor (int32_t i = 2; i < houses.size(); ++i) {\n12\nmemo[i] = std::max(memo[i - 2] + houses[i], memo[i - 1]);\n13\n} // for i\n14\nreturn memo.back();\n15\n} // rob_houses()\nGiven 𝑛houses, there are 𝑛potential subproblems that you will need to solve, each of which takes time. Since each subproblem is solvedΘ(1)\nat most once using dynamic programming, the time complexity of the above solution is Θ(𝑛). Similarly, since a memo of size is declared,Θ(𝑛)\nthe auxiliary space usage is Θ(𝑛).", "word_count": 648, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "58a23130-5db8-5f25-820d-44662b697655", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 902, "real_page_number": null, "text": "890\nChapter 23. Dynamic Programming\nExample 23.13 Given an array of non-negative integers and a target sum 𝑥, write a function that determines if there exists a subset of the\ngiven set with a sum equal to 𝑥.\n[6, 9, 10, 15, 36, 44, 68] true,Example: Given the array and a target of 104, you would return since 9 + 15 + 36 + 44 = 104.\nSince we choosing which numbers to include in our subset, this is a decision making problem. Therefore, we can consider each value in the array\none-by-one and use the solutions of smaller subproblems to determine whether each value should be included or excluded from our solution.\nHow do we know whether a value should be included or excluded? To answer this, consider the value 68 in the example above. Notice that the\ninclusion of 68 in the subset sum depends on the following two subproblems:\n[6, 9, 10, 15, 36, 44]• Can the previous values sum up to 104? (If so, exclude 68 in solution.)\n[6, 9, 10, 15, 36, 44]• Can the previous values sum up to 36? (If so, include 68 in solution.)104−68=\nIf the previous numbers we can choose from can already sum up to our target value of 104, then we know that the solution to the entire problem\ntrue.must be However, if the previous numbers cannot sum to 104, then including 68 can help sum to 104 only if the previous numbers can\n36. If the previous numbers cannot sum to 36, then there is no way to include 68 to attain oursum up to the difference between 104 and 68, or\ntarget sum of 104.\nThis idea can be applied to every value in our input array. If we define as the value at index of the array (using 0-indexing)𝑣𝑛−1 𝑛−1\nand as whether the first 𝑛values in our input array can sum up to a target value of 𝑥, we can express the problem using the following𝐹(𝑛,𝑥)\nrecurrence relation.\n𝐹(𝑛,𝑥)=\n⎧\n⎪\n⎨\n⎪⎩\ntrue,\nif 𝑛=0,𝑥=0\nfalse,\n0,𝑥≠0if 𝑛=\nor𝐹(𝑛−1,𝑥) 𝐹(𝑛−1,𝑥−𝑣𝑛−1),\notherwise\nHere, represents whether the previous values can already sum to 𝑥, and represents whether the previous𝐹(𝑛−1,𝑥) 𝑛−1 𝐹(𝑛−1,𝑥−𝑣𝑛−1)\nvalues can sum to 𝑥−𝑣𝑛−1. If any of these two are true, then is also true; otherwise it is false.𝑛−1 𝐹(𝑛,𝑥)\nstd::unordered_map<>A top-down solution is shown below. This solution uses a to keep track of subproblems because Booleans\ntrue false, falseonly take on a value of or and we want to differentiate between subproblems that return and subproblems that have not\nbeen encountered before.\n1\nbool subset_sum(const std::vector<int32_t>& int32_tnums, target) {\n2\nstd::vector<std::unordered_map<int32_t, bool>> memo(nums.size() + 1);\n3\nreturn subset_sum_helper(nums, nums.size(), target, memo);\n4\n} // subset_sum()\n5\n6\nbool subset_sum_helper(const std::vector<int32_t>& int32_t int32_tnums, idx, target,\n7\nstd::vector<std::unordered_map<int32_t, bool>>& memo) {\n8\nif (idx == 0) {\n9\nreturn target == 0;\n10\n} // if\n11\nif (target < 0) {\n12\nreturn false;\n13\n} // if\n14\nif (memo[idx].count(target)) {\n15\nreturn memo[idx][target];\n16\n} // if\n17\nelse {\n18\nreturn memo[idx][target] = subset_sum_helper(nums, idx - 1, target, memo) ||\n19\nsubset_sum_helper(nums, idx - 1, target - nums[idx - 1], memo);\n20\n} // else\n21\n} // subset_sum_helper()\nA bottom-up solution is shown below:\n1\nbool subset_sum(std::vector<int32_t>& int32_tnums, target) {\n2\nstd::vector<std::vector<bool>> std::vector<bool>(targetmemo(nums.size() + 1, + 1));\n3\nfor (int32_t i = 0; i <= nums.size(); ++i){\n4\ntrue;memo[i][0] =\n5\n} // for i\n6\nfor (int32_t i = 1; i <= target; ++i) {\n7\nfalse;memo[0][i] =\n8\n} // for i\n9\nfor (int32_t i = 1; i <= nums.size(); ++i) {\n10\nfor (int32_t j = 1; j <= target; ++j) {\n11\nmemo[i][j] = memo[i - 1][j];\n12\nif (j >= nums[i - 1]) {\n13\nmemo[i][j] = memo[i - 1][j] || memo[i - 1][j - nums[i - 1]];\n14\n} // if\n15\n} // for j\n16\n} // for i\n17\nreturn memo[nums.size()][target];\n18\n} // subset_sum()\nThere are a total of subproblems we may need to solve, each taking time. Since the memo allows us to solve each subproblem atΘ(𝑛𝑥) Θ(1)\nmost once, the overall time complexity of our dynamic programming solution is therefore Θ(𝑛𝑥). Similarly, since we are using a (𝑛+1)×(𝑥+1)\nmemo, the auxiliary space used by this implementation is also Θ(𝑛𝑥).", "word_count": 747, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "28e0873b-a663-53d0-903c-8b6825645425", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 903, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n891\nRemark: One of the best examples of a decision making problem that can be solved using dynamic programming is the 0-1 knapsack\nproblem, which involves finding the best subset of items to fit in a limited capacity knapsack to maximize overall value. However, this\nproblem will not be discussed here since it will get its own chapter… so if you are interested in seeing another example of this dynamic\nprogramming pattern, you can read about it in the next chapter!\n¸ 23.3.4\nInterval Merging\nInterval merging is another pattern you may see when working through dynamic programming problems. In these problems, you are given a\ncollection of items that you have to merge together with optimal cost. To solve interval merging questions, you should identify the optimal\nmerging strategy for all possible subintervals and combine these solutions to determine the best way to merge the entire input. As an example,\none common approach for solving these problems is to iterate over each position in the given interval, recursively solve the subproblems\ncorresponding to the intervals to the left and right of this position, and then combine these solutions in a manner that will allow you to obtain\nthe optimal cost. We will look at a few examples below.\nExample 23.14 You are trying to weld together a series of 𝑛pipes with lengths 𝑤0, 𝑤1, …, 𝑤𝑛−1. The cost to weld together two pipes of\nlengths 𝑤𝑖and 𝑤𝑗is max(𝑤𝑖, 𝑤𝑗), and welding these pipes creates a new pipe of length 𝑤𝑖+𝑤𝑗. Implement a function that returns the\nminimum cost required to weld together all the pipes, given the constraint that only adjacent pipes can be welded together. That is, pipes 1\nand 2 can be welded together, but pipes 1 and 3 cannot.\n[3, 5, 2, 6]:Example: Given pipes of lengths\n3\n5\n2\n6\nYou would first merge together the pipes with lengths 3 and 5, for a running cost of max(3, 5) = 5.\n8\n2\n6\nThen, you would merge the pipes with lengths 2 and 6 together, for a running cost of 5 + max(2, 6) = 11.\n8\n8\nLastly, you would merge the remaining two pipes together, for a running cost of 11 + max(8, 8) = 19. This is the minimum cost required to\nmerge all the pipes together, so your function would return 19.\n16\nA dynamic programming solution to this problem, if there even is one, is not inherently obvious. Since this is an optimization problem, you\ncould check if a greedy approach works first. However, you would quickly see that there is no viable greedy algorithm that always produces an\noptimal solution, as only adjacent pipes can be welded together (had this restriction been removed, then the greedy approach of welding the\nsmallest pipes together would work).", "word_count": 473, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7a24ca64-cb93-5191-b830-6b031e60c250", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 904, "real_page_number": null, "text": "892\nChapter 23. Dynamic Programming\nSince greedy does not work, let’s try to come up with a dynamic programming solution instead. A good starting point is to think about ways we\ncan break the problem up into smaller instances of the same problem, and use these solutions to build up the solution for a larger problem. To\ncome up with our subproblems, we can make the following observation: regardless of how the pipes are welded together, there must always\nlast. By choosing where we want to complete our final weld, we can split the remaining pipe into two independentexist a position that is welded\nsubproblems. For instance, there are three positions we can choose for the last weld using the example above:\n3\n13\n8\n8\n10\n6\nIf we define as the optimal cost of welding all pipes from pipe 𝑥up to pipe 𝑦(inclusive, using 0-indexing), we can recursively assign a𝐹(𝑥,𝑦)\ncost for each of these final welds. The optimal cost to merge all the pipes if our final weld occurs directly after pipe 0 is\nmax(3, 13) + +𝐹(0,0) 𝐹(1,3)\n3\n13\nThe optimal cost to merge all the pipes if our final weld occurs directly after pipe 1 is\nmax(8, 8) + +𝐹(0,1) 𝐹(2,3)\n8\n8\nThe optimal cost to merge all the pipes if our final weld occurs directly after pipe 2 is\nmax(10, 6) + +𝐹(0,2) 𝐹(3,3)\n10\n6\nIn general, if you are given 𝑛pipes, the optimal merge cost if the final weld occurs directly after pipe 𝑘can be expressed as\nmax(\n𝑘\n∑\n𝑖=0\n𝑤𝑖,\n𝑛−1\n∑\n𝑗=𝑘+1\n𝑤𝑗)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\ncosttoweld\npipes andk+1k\n+\n𝐹(0,𝑘)\n⏟⏟⏟\ncosttoweldall\npipesfrom0tok\n+ 𝐹(𝑘+1,𝑛−1)\n⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟\ncosttoweldall\npipesfromk+1ton-1\nTherefore, to determine the optimal cost to merge all pipes from pipe 𝑥up to pipe 𝑦, we would solve the above equation for all possible values of\n𝑘and take the minimum cost. This results in the recurrence relation shown below. The base case for occurs when 𝑦, since it takes 0𝐹(𝑥,𝑦) 𝑥=\ncost to merge a pipe with itself.\n𝐹(𝑥,𝑦)=\n{\n0,\nif 𝑥=𝑦\nmin𝑥≤𝑘<𝑦(max(∑𝑘\n𝑖=𝑥𝑤𝑖,∑𝑦\n𝑤𝑗)+𝐹(𝑥,𝑘)+𝐹(𝑘+1,𝑦)),𝑗=𝑘+1\notherwise", "word_count": 375, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "37e0561c-75e3-5b3c-b7a2-57dd9614ae2f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 905, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n893\nSince the problem requires us to find the optimal cost of merging all pipes from pipe 0 to pipe 𝑛−1, we want to solve for 𝐹(0,𝑛−1). If we were\nto write a naïve recursive solution for this problem, we would get something like this:\n1\nint32_t weld_pipes(const std::vector<int32_t>& pipes) {\n2\nreturn weld_helper(pipes, 0, pipes.size() - 1);\n3\n} // weld_pipes()\n4\n5\nint32_t weld_helper(const std::vector<int>& int32_t int32_tpipes, first, last) {\n6\nif (first == last) {\n7\nreturn 0; // no work to merge pipe with itself\n8\n} // if\n9\nint32_t std::numeric_limits<int32_t>::max();best =\n10\nfor (int32_t k = first; k < last; ++k) {\n11\nint32_t cost_left = std::accumulate(pipes.begin() + first, pipes.begin() + k + 1, 0);\n12\nint32_t cost_right = std::accumulate(pipes.begin() + k + 1, pipes.begin() + last + 1, 0);\n13\nbest = std::min(best, std::max(cost_left, cost_right) +\n14\nweld_helper(pipes, first, k) + weld_helper(pipes, k + 1, last));\n15\n} // for k\n16\nreturn best;\n17\n} // weld_helper()\nHowever, this implementation is inefficient, since there are overlapping subproblems, and we are solving several subproblems more than once\n(shown by the partial recurrence tree below):\n𝐹(1,𝑛−1)\n…\n𝐹(3,𝑛−1)\n𝐹(1,2)\n𝐹(2,2)\n𝐹(1,1)\n𝐹(2,𝑛−1)\n…\n𝐹(4,𝑛−1)\n𝐹(2,3)\n𝐹(3,𝑛−1)\n𝐹(2,2)\n𝐹(1,1)\nTo address this problem, we can use dynamic programming to store the solutions of these repeating subproblems. Since computing each\nsubproblem requires knowledge of both the weight and optimal cost of the pipes to the left and right of the final weld, we want to store both\nand in our memo (or use two separate memos, one for weight and one for cost). This allows us to fetch the weight of any pipe inweight cost\nconstant time, without having to complete a separate linear-time summation every time. An example of this memo structure is shown below:\n3\n5\n2\n6\nweights\n0\n1\n2\n3\n0\n1\n2\n3\ncosts\n0\n1\n2\n3\n0\n1\n2\n3\nweights[i][j] i j costs[i][j]Here, stores the weight of the fully welded pipe from pipe to pipe (i.e., +…+𝑤𝑗), and𝑤𝑖+𝑤𝑖+1\ni jstores the optimal cost of welding together all pipes from pipe up to pipe (i.e., 𝐹(𝑖,𝑗)). The shaded cells do not need to be filled out since\nweights[i][j] == weights[j][i].their values are mirrored across the diagonal:\nUsing the example provided, the memos would be filled out as follows. The solution of the problem would eventually be stored in the top-right\ncosts costs[0][n-1].cell of the table, since our goal is to solve for the optimal cost of welding all pipes from pipe 0 to pipe 𝑛−1, or\nweights\n0\n1\n2\n3\n0\n1\n2\n3\n3\n8\n10\n16\n5\n7\n13\n2\n8\n6\ncosts\n0\n1\n2\n3\n0\n1\n2\n3\n0\n5\n12\n19\n0\n5\n12\n0\n6\n0", "word_count": 479, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5210321d-ac59-54d8-b1e4-7be2a5ab4a71", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 906, "real_page_number": null, "text": "894\nChapter 23. Dynamic Programming\nA top-down solution is shown below:\n1\nint32_t weld_pipes(const std::vector<int32_t>& pipes){\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(pipes.size()));weights(pipes.size(),\n3\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(pipes.size(),costs(pipes.size(), -1));\n4\nreturn weld_helper(pipes, costs, weights, 0, pipes.size() - 1);\n5\n} // weld_pipes()\n6\n7\nint32_t weld_helper(const std::vector<int32_t>& std::vector<std::vector<int32_t>>&pipes, costs,\n8\nstd::vector<std::vector<int32_t>>& int32_t int32_tweights, begin, end){\n9\nif (costs[begin][end] != -1) {\n10\nreturn costs[begin][end];\n// no need to redo work\n11\n} // if\n12\nif (begin == end){\n// base cases\n13\ncosts[begin][end] = 0;\n14\nweights[begin][end] = pipes[begin];\n15\nreturn 0;\n16\n} // if.\n17\nint32_t std::numeric_limits<int32_t>::max();min_cost =\n18\nfor (int32_t k = begin; k < end; ++k) {\n19\nint32_t cost_k = weld_helper(pipes, costs, weights, begin, k) +\n20\nweld_helper(pipes, costs, weights, k + 1, end);\n21\ncost_k += std::max(weights[begin][k], weights[k + 1][end]);\n22\nmin_cost = std::min(min_cost, cost_k);\n23\n} // for k\n24\ncosts[begin][end] = min_cost;\n25\nweights[begin][end] = weights[begin][begin] + weights[begin + 1][end];\n26\nreturn costs[begin][end];\n27\n} // weld_helper()\nA bottom-up solution is shown below:\n1\nint32_t weld_pipes(const std::vector<int32_t>& pipes) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(pipes.size()));weights(pipes.size(),\n3\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(pipes.size()));costs(pipes.size(),\n4\nfor (int32_t col = 0; col < pipes.size(); ++col) {\n5\nfor (int32_t row = col; row >= 0; --row) {\n6\nif (row == col) { // base cases\n7\ncosts[row][col] = 0;\n8\nweights[row][col] = pipes[row];\n9\ncontinue;\n10\n} // if\n11\nint32_t std::numeric_limits<int32_t>::max();min_cost =\n12\nfor (int32_t k = row; k < col; ++i){\n13\nint32_t cost_k = costs[row][k] + costs[k + 1][col] +\n14\nstd::max(weights[row][k], weights[k + 1][col]);\n15\nmin_cost = std::min(min_cost, cost_k);\n16\n} // for k\n17\nweights[row][col] = weights[row][row] + weights[row + 1][col];\n18\ncosts[row][col] = min_cost;\n19\n} // for row\n20\n} // for col\n21\nreturn costs[0][pipes.size() - 1];\n22\n} // weld_pipes()\nΘ(𝑛3) Θ(𝑛2)The time complexity of this dynamic programming solution is with auxiliary space, where 𝑛is the number of pipes. This is because\nΘ(𝑛2)there are subproblems we may need to solve, each taking time.Θ(𝑛)\nRemark: Unlike the top-down solution, which directly converts the recurrence relation into code, the bottom-up solution may be a bit more\ndifficult to follow. If you are having trouble understanding what is going on in the bottom-up solution, the following provides a step-by-step\nillustration of what is happening (feel free to skip this if you are comfortable with the code).\nFirst, notice that the cell at (row 𝑖, column 𝑗) of the memo corresponds to the optimal cost of merging together all pipes between 𝑖and 𝑗,\ninclusive. Since the base case occurs when you try to merge a pipe with itself, each base case is stored along the diagonal of our memo,\nshaded below. When using bottom-up dynamic programming, we want to solve the base cases before we move on to larger subproblems: this\nfor row colis why the loop on line 5 starts from the center diagonal of the table and iterates upward by row (i.e., = →0).\n0\n1\n2\n3\n0\n1\n2\n3", "word_count": 505, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a162d0cc-da9a-5743-9035-4d72e0f285a6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 907, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n895\nforAfter declaring our weight and cost memos on lines 2 and 3, our bottom-up solution considers each pipe one by one using the loop\n(row colon line 4. The very first subproblem we consider is = 0, = 0), which involves welding pipe 0 with itself. This is a base case,\nas welding a pipe with itself does not change its weight nor has any cost. Thus, we enter our base case condition on line 6 and fill out our\nmemos using the logic on lines 7 and 8:\nweights\n0\n1\n2\n3\n0\n1\n2\n3\n3\ncosts\n0\n1\n2\n3\n0\n1\n2\n3\n0\ncol (row colWe then increment and consider all subproblems that weld up to pipe 1. First, we solve for = 1, = 1), which is the cost of\n(row colwelding pipe 1 with itself. Similar to = 0, = 0), we fill out the corresponding cells as base cases.\nweights\n0\n1\n2\n3\n0\n1\n2\n3\n3\n5\ncosts\n0\n1\n2\n3\n0\n1\n2\n3\n0\n0\nrow (row colThen, we decrement and consider the subproblem = 0, = 1), which represents the optimal weight and cost of welding pipes 0\nand 1. This optimal cost is equal to the optimal cost of welding pipes 0 and 0 + the optimal cost of welding pipes 1 and 1 + the cost of\nwelding pipes 0 and 1. This result is computed on lines 13-15 and saved in the memo.\nweights\n0\n1\n2\n3\n0\n1\n2\n3\n3\n8\n5\ncosts\n0\n1\n2\n3\n0\n1\n2\n3\n0\n5\n0\ncolNow that we have filled out the entirety of column 1, we increment and consider all subproblems that weld up to pipe 2. First, we solve\n(row colfor = 2, = 2), which is our base case:\nweights\n0\n1\n2\n3\n0\n1\n2\n3\n3\n8\n5\n2\ncosts\n0\n1\n2\n3\n0\n1\n2\n3\n0\n5\n0\n0\nrow (row colThen, we decrement and consider the subproblem = 1, = 2), which is the optimal cost of welding pipes 1 and 2. This optimal\ncost is equal to the optimal cost of welding pipes 1 and 1 + the optimal cost of welding pipes 2 and 2 + the cost of welding pipes 1 and 2.\nweights\n0\n1\n2\n3\n0\n1\n2\n3\n3\n8\n5\n7\n2\ncosts\n0\n1\n2\n3\n0\n1\n2\n3\n0\n5\n0\n5\n0", "word_count": 439, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "88151819-0fbd-51f8-8a47-f5605c552a1f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 908, "real_page_number": null, "text": "896\nChapter 23. Dynamic Programming\nrow (row colWe decrement again and consider the subproblem = 0, = 2), which is the optimal cost of welding pipes 0 2. This isthrough\na bit more complex to solve, since there are two ways to weld all the pipes from 0 to 2: we can either\n• weld pipe 0 with pipe 1, then weld this combined pipe with pipe 2\n• weld pipe 1 with pipe 2, then weld this combined pipe with pipe 0\nforThis is what the loop on line 12 handles — it loops through all the possible locations of the final weld (for this subproblem, 0 and 1),\ncalculates the cost to weld, and stores the minimum cost in the memo. In this example, the optimal cost of welding 0 and 1 first is equal to 5\n+ 8 = 13 (5 from merging pipes 0 and 1, and 8 from merging the combined pipe with pipe 2 — both values can be retrieved from (row 0,\ncolumn 1) of the cost and weight memos). On the other hand, the optimal cost of welding 1 and 2 first is equal to 5 + 7 = 12 (5 from merging\npipes 1 and 2, and 7 from merging the combined pipe with pipe 0). Since 12 is the better cost, it is used as the solution for this subproblem.\nweights\n0\n1\n2\n3\n0\n1\n2\n3\n3\n8\n10\n5\n7\n2\ncosts\n0\n1\n2\n3\n0\n1\n2\n3\n0\n5\n12\n0\n5\n0\n(row col (rowWe repeat this procedure for the final pipe, filling out column 3 from the base case = 3, = 3) up to the final solution = 0,\ncol (row col= 3). The value at = 0, = 3) of the cost memo is then returned.\nExample 23.15 You are given a rod of length 𝑛inches and the prices of all rod prices with length less than or equal to 𝑛. Write a function\nthat calculates the maximum value obtainable from cutting up the rod and selling the pieces. (Assume you can only cut the rod to form\ninteger lengths.)\nExample: Given a rod of length and the following prices:𝑛=8\nlength\n0\n1\n2\n3\n4\n5\n6\n7\n8\nprice\n0\n1\n4\n7\n9\n13\n14\n15\n19\nyou would return 20, since this is the maximum value you can earn from selling the rod (by cutting the rod into subrods with lengths 3 and 5,\nand selling them for 7 and 13, respectively).\nOne method for solving this problem would be to consider all possible ways to cut the rod and choose the option that is best. However, given a\n2𝑛−1rod of length 𝑛, there are a total of ways to cut the rod: this is because there are places we can make a cut, and we have two decisions𝑛−1\nwe can make at each of these positions (cut or no cut). Therefore, this brute force solution would take exponential time, which isn’t viable𝑛−1\nif we want to solve the problem efficiently.\nTo improve our solution, we can use a dynamic programming approach instead. First, we will need to find a way to break our problem up\ninto smaller, overlapping subproblems. To illustrate how this is done, let’s consider the following rod of length 𝑛.\n𝑛\nLet’s assume we make the first cut at some length 𝑖on the rod. If we let 𝑝𝑖represent the price of a rod with length 𝑖, and let represent the𝐹(𝑛)\nmaximum profit we can earn from a rod of length 𝑛, the total earnings from this specific cut can be defined as\nmaximum profit if first cut at length 𝑖= 𝑝𝑖+𝐹(𝑛−𝑖)\n𝑖\n𝑛−𝑖\nFrom this, we can see that the maximum profit attainable from a rod of length 𝑛, or 𝐹(𝑛), can be computed by finding the maximum 𝑝𝑖+𝐹(𝑛−𝑖)\nacross all possible values of 𝑖from 1 to 𝑛. Therefore, the recurrence relation we end up with is\n𝐹(𝑛)=\n{\n0,\nif 𝑛=0\nmax1≤𝑖≤𝑛(𝑝𝑖+𝐹(𝑛−𝑖)),\nif 𝑛>0", "word_count": 688, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "598a9ba2-f5e0-5bd5-8e0f-0b386d84c386", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 909, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n897\nConverting this naïve implementation to code, we would get the following:\n1\nint32_t cut_rod(const std::vector<int32_t>& int32_tprices, n) {\n2\nif (n == 0) {\n3\nreturn 0;\n4\n} // if\n5\nint32_t std::numeric_limits<int32_t>::min();best =\n6\nfor (int32_t i = 1; i <= n; ++i) {\n7\nbest = std::max(best, prices[i] + cut_rod(prices, n - i));\n8\n} // for i\n9\nreturn best;\n10\n} // cut_rod()\nHowever, we end up with a bunch of overlapping subproblems, since every call to makes additional recursive calls to𝐹(𝑖) 𝐹(𝑖−1),𝐹(𝑖−2),…,\nall the way down to 𝐹(0). To prevent our algorithm from performing duplicate work, we can store the results of our subproblems in a memo. A\ntop-down approach is shown below:\n1\nint32_t cut_rod(const std::vector<int32_t>& int32_tprices, n) {\n2\nstd::vector<int32_t> memo(n + 1, -1);\n3\nreturn cut_rod_helper(prices, n, memo);\n4\n} // cut_rod()\n5\n6\nint32_t cut_rod_helper(const std::vector<int32_t>& int32_t std::vector<int32_t>&prices, n, memo) {\n7\nif (n == 0) {\n8\nreturn 0;\n9\n} // if\n10\nif (memo[n] != -1) {\n11\nreturn memo[n];\n12\n} // if\n13\nelse {\n14\nint32_t std::numeric_limits<int32_t>::min();best =\n15\nfor (int32_t i = 1; i <= n; ++i) {\n16\nbest = std::max(best, prices[i] + cut_rod_helper(prices, n - i, memo));\n17\n} // for i\n18\nreturn memo[n] = best;\n19\n} // else\n20\n} // cut_rod_helper()\nA bottom-up solution is shown below:\n1\nint32_t cut_rod(const std::vector<int32_t>& int32_tprices, n) {\n2\nstd::vector<int32_t> memo(n + 1, -1);\n3\nmemo[0] = 0;\n4\nfor (int32_t j = 1; j <= n; ++j) {\n5\nint32_t std::numeric_limits<int32_t>::min();best =\n6\nfor (int32_t i = 1; i <= j; ++i) {\n7\nbest = std::max(best, prices[i] + memo[j - i]);\n8\n} // for i\n9\nmemo[j] = best;\n10\n} // for j\n11\nreturn memo[n];\n12\n} // cut_rod()\nΘ(𝑛2), forThe time complexity of this dynamic programming implementation is since each subproblem runs through 𝑖iterations of a𝐹(𝑖)\n𝑛2loop (from 𝑖to 0), producing iterations in total for an initial input size of 𝑛. Additionally, the auxiliary space used by this solution is Θ(𝑛),\nsince an array of size 𝑛is declared to store our subproblems.", "word_count": 374, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2b102113-c118-582c-a482-7b4f161d9057", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 910, "real_page_number": null, "text": "898\nChapter 23. Dynamic Programming\nExample 23.16 value.You are given 𝑛balloons, indexed from 0 to 𝑛−1. Each balloon is painted with a number stored in an array If\nithyou burst the balloon, you will earn points based on the product of the popped ballon and its adjacent balloons. Write a function that\nreturns the maximum number of points you can earn from bursting all the balloons.\nFor instance, suppose there are four balloons, with values 4, 2, 7, and 9.\nIt is optimal to first pop the balloon with a value of 2, to earn 4 2 7 = 56 points.× ×\nThen, it would be optimal to pop the balloon with a value of 7, to earn 4 7 9 = 252 points.× ×\nThen, it would be optimal to pop the balloon with a value of 4, to earn 4 9 = 36 points.×\nThen, the final balloon is popped to earn 9 points.\nAll of the balloons have been popped, so your final score is 56 + 252 + 36 + 9 = 353. This is the highest score you can attain from popping\nthe balloons, so you should return 353.\nIf you wanted to solve this problem using brute force, you would need to iterate over all the popping orders and calculate the number of points\nyou can earn for each order, taking the best at the very end. Given 𝑛balloons, however, there are unique popping orders. In addition, it takes𝑛!\ntime to sum up the number of points earned for each order, resulting in an overall time complexity of Θ(𝑛×𝑛!). Is there a way to do better?Θ(𝑛)\nSince this is an optimization problem, we could try to see if a greedy solution exists first. However, if you were to try some approaches out,\nyou would see that there is no greedy strategy that satisfies all possible examples. (One common incorrect approach is to pop from smallest to\nlargest among the balloons not at the ends, and then pop the final two balloons in ascending order, but this does not work in the case of 2, 8, 2).\nInstead, we will have to see if there exists a way to recursively break the problem into smaller subproblems, so that an approach such as\ndivide-and-conquer or dynamic programming can be used. A reasonable starting point would be to consider what would happen if we pop any\nballoon in the sequence:\nNotice here that we end up with two remaining intervals after the balloon at index 𝑘is popped: one subsequence consisting of all balloons to the\nleft of 𝑘and one consisting of all balloons to the right. We can therefore think of the problem in terms of intervals, where the optimal score to\npop all the balloons in a smaller interval can be used to construct the optimal score of a larger interval. In this case, we want to find a recurrence\nrelation that expresses our optimal solution in terms of the two subsequences that remain after the balloon at index 𝑘is popped.\nHowever, there is a flaw with this approach. If we think about the recurrence in terms of the balloon we pop, we run into the issuefirst\nwhere the solution of one interval cannot be solved without knowledge of another. For instance, if we popped balloon 𝑘and then wanted to find\nthe optimal cost of popping balloon 𝑘−1, our solution would depend on whether balloon has been popped already, as would be𝑘+1 𝑘+1\nadjacent to 𝑘−1. This prevents us from being able to solve the subproblems independently!\nTo fix this issue, we need to reverse our thinking. The reason why the subproblems cannot be solved independently is that balloon 𝑘has\nalready been popped, which creates a dependency between balloons and 𝑘+1. Thus, to be able to solve the subproblems properly, we𝑘−1\nmust keep balloon 𝑘unpopped to separate balloons and so that their solutions do not depend on each other. This is the key insight for𝑘−1 𝑘+1\nsolving this problem: 𝑘beforebecause we cannot pop balloon solving the left and right subproblems without introducing a dependency between\npop. Notice that this is the same idea we used to solvelastsubproblems, we will instead think about our recurrence in terms of the balloon we\nthe pipe welding problem covered earlier, where we defined our subproblems using the position of the last weld rather than the first weld.", "word_count": 741, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3a0f9741-46ee-5372-8314-70a9368740f7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 911, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n899\nTo illustrate this, suppose there is one balloon remaining to pop, located at index 𝑘:\nWhat is the maximum possible score attainable after popping this last balloon? If we define as the maximum score attainable from𝐹(𝑥,𝑦)\npopping all balloons from index 𝑥up to index 𝑦, we can express the maximum score from popping all balloons from 𝑥to 𝑦if the balloon at\n𝑥≤𝑘≤𝑦) equation:9(assuming using the followingindex k is popped last\n𝐹(0,𝑘−1)\n⏟⏞⏞⏞⏟⏞⏞⏞⏟\nmaximumscorefrom\npoppingleftballoons\n𝐹(𝑘+1,𝑛−1)+\n⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟\nmaximumscorefrom\npoppingrightballoons\nvalue[𝑥−1]×value[𝑘]×value[𝑦+1]+\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\npointsfrompopping\nfinalballoon\nTo compute the maximum score attainable after popping balloons, we can simply solve the previous equation for all possible final positionsall\n𝑘and take the largest solution. This gives us our recurrence relation, as shown below. If 𝑥and 𝑦are out-of-bounds, we can simply substitute in\na value of 1, since anything multiplied with 1 does not change in value.\n𝐹(𝑥,𝑦)=\n{\n1,\n𝑥,𝑦≥𝑛(out-of-bounds)if or𝑥,𝑦<0\nmax𝑥≤𝑘≤𝑦(𝐹(𝑥,𝑘−1)+𝐹(𝑘+1,𝑦)+value[𝑥−1]×value[𝑘]×value[𝑦+1]),\notherwise\nSince this recurrence relation involves overlapping subproblems that may be needed more than once, a dynamic programming approach can be\nused. We begin our implementation by declaring a memo where the solution to is stored at position memo[𝑥][𝑦]. The completed memo𝐹(𝑥,𝑦)\nfor the provided example is shown below, where the final solution for 𝑛balloons is stored at memo[0][𝑛−1].𝐹(0,𝑛−1)\n0\n1\n2\n3\n0\n1\n2\n3\n8\n84\n344 353\n56\n308 344\n126 144\n63\nA top-down solution is shown below:\n1\nint32_t get_balloon_value(const std::vector<int32_t>& int32_tnums, index) {\n2\nif (index < 0 || index >= nums.size()) {\n3\nreturn 1;\n4\n} // if\n5\nreturn nums[index];\n6\n} // get_balloon_value()\n7\n8\nint32_t max_points(const std::vector<int32_t>& nums) {\n9\nstd::vector<std::vector<int32_t>> std::vector<int>(nums.size(),memo(nums.size(), -1));\n10\nreturn max_points_helper(nums, 0, nums.size() - 1, memo);\n11\n} // max_points()\n12\n13\nint32_t max_points_helper(const std::vector<int32_t>& int32_t int32_tnums, left, right,\n14\nstd::vector<std::vector<int32_t>>& memo) {\n15\nif (right < left) {\n16\nreturn 0;\n17\n} // if\n18\nif (memo[left][right] != -1) {\n19\nreturn memo[left][right];\n20\n} // if\n21\nint32_t best = 0;\n22\nfor (int32_t i = left; i <= right; ++i) {\n23\nint32_t points =\n24\nmax_points_helper(nums, left, i - 1, memo) + max_points_helper(nums, i + 1, right, memo) +\n25\nget_balloon_value(nums, left - 1) get_balloon_value(nums, i)* *\n26\nget_balloon_value(nums, right + 1);\n27\nbest = std::max(best, points);\n28\n} // for i\n29\nreturn memo[left][right] = best;\n30\n} // max_points_helper()\n9Note that we still needto consider any adjacent balloons outsidethe range that we popped, which iswhywemultiply the value of the last balloonwith[𝑥,𝑦]\nvalue[𝑥−1]andvalue[𝑦+1]intheaboveequation. Inaddition,wewillalsoneedtoconsidertheedgecasewherevalue[𝑥−1]orvalue[𝑦+1]goesout-of-bounds;\nwhenthishappens,weshouldsubstituteinavalueof1toensurewedon’tindexofftheendofthearray.", "word_count": 497, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f42bef3e-d2d4-57b7-ab69-5e857d3837ad", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 912, "real_page_number": null, "text": "900\nChapter 23. Dynamic Programming\nA bottom-up solution is shown below. The loop on line 7 considers all subproblems of each interval length, the loop on line 8 considers all\nstarting positions of the interval to pop, and the loop on line 10 considers all positions of the final pop within each interval.\n1\nint32_t get_balloon_value(const std::vector<int32_t>& int32_tnums, index) {\n2\nreturn (index < 0 || index >= nums.size()) ? 1 : nums[index];\n3\n} // get_balloon_value()\n4\n5\nint32_t max_points(const std::vector<int32_t>& nums) {\n6\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(nums.size()));memo(nums.size(),\n7\nfor (int32_t len = 0; len < nums.size(); ++len) {\n8\nfor (int32_t left = 0; left < nums.size() - len; ++left) {\n9\nint32_t right = left + len;\n10\nfor (int32_t k = left; k <= right; ++k) {\n11\nint32_t best_left = (k == left) ? 0 : memo[left][k - 1];\n12\nint32_t best_right = (k == right) ? 0 : memo[k + 1][right];\n13\nmemo[left][right] = std::max(memo[left][right],\n14\nbest_left + best_right + get_balloon_value(nums, left - 1) *\n15\nget_balloon_value(nums, k) get_balloon_value(nums, right + 1));*\n16\n} // for k\n17\n} // for left\n18\n} // for len\n19\nreturn memo[0][nums.size() - 1];\n20\n} // max_points()\nget_balloon_value()Remark: Instead of using a separate function to keep track of whether an index is out of bounds, an alternative\nwould be to insert a dummy value of 1 at both the beginning and end of the original input vector — this dummy value gets indexed if you go\nout of bounds (and multiplying any value by 1 does not change the value).\nΘ(𝑛2)In this dynamic programming approach, there are a total of subproblems to solve, each taking up to time (since you have to loopΘ(𝑛)\nacross all indices between left and right). Using the memo, each subproblem is solved at most once, so the overall time complexity is worst-case\nΘ(𝑛3). Θ(𝑛2).The dimensions of the memo is 𝑛×𝑛, so the auxiliary space used by this solution is\nExample 23.17 valYou are given a convex 𝑛-sided polygon where each vertex has an integer value. You are given a vector of these\nithval[i]values, where is the value associated with the vertex (in counterclockwise order). Your goal is to triangulate the polygon into\ntriangles in a way that the sum of the products of each of the triangles’ vertices.𝑛−2 minimizes\n[1, 3, 5, 7, 9],Example: Given the vertex values you would return the sum 113. This is because there are five ways to triangulate a\npolygon with these values, and this is the minimum sum attainable:\n1\n3\n5\n7\n9\nValue of triangulation:\n(1×3×5)+(1×5×7)+(1×7×9)\n=15+35+63=113\n1\n3\n5\n7\n9\nValue of triangulation:\n(3×5×7)+(3×7×9)+(1×3×9)\n=105+189+27=321\n1\n3\n5\n7\n9\nValue of triangulation:\n(1×3×5)+(1×5×9)+(5×7×9)\n=15+45+315=375\n1\n3\n5\n7\n9\nValue of triangulation:\n(3×5×7)+(1×3×7)+(1×7×9)\n=105+21+63=189\n1\n3\n5\n7\n9\nValue of triangulation:\n(5×7×9)+(3×5×9)+(1×3×9)\n=315+135+27=477\nWhile this may not seem like an interval merging dynamic programming problem at first, it in fact is quite similar to the balloon problem\ndiscussed previously. To visualize how the intervals can be defined, we must first make the observation that every edge of the polygon must be\npart of after any possible triangulation. Why is this the case? If any of a polygon’s edges were not part of a triangle, thenexactly one triangle\nthat polygon would not be fully triangulated. On the other hand, if any edge were part of more than one triangle, then the triangulation would\ninclude triangles that overlap. Both of these cases are not valid, so each edge of the polygon can only be part of one triangle after any potential\ntriangulation. You can prove that this is the case by drawing some triangulations out.", "word_count": 641, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c51fed56-4f33-56f7-9dc3-149b2b281933", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 913, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n901\nBecause each edge must be part of exactly one triangle in the final triangulation, we can reduce our problem into smaller subproblems by fixing\na single edge and then considering all the possible triangles we can get that includes that edge. To visualize how this works, consider the\nfollowing polygon, where we fix the edge from vertex 𝑖to vertex 𝑗.\n𝑖\n𝑗\nFor any polygon with 𝑛vertices, there are possible triangles that can be formed using each edge. This is shown for the polygon above:𝑛−2\n𝑖\n𝑗\n𝑖\n𝑗\n𝑖\n𝑗\n𝑖\n𝑗\nWe can use this idea to break our problem down into smaller subproblems. Consider the following triangle that is formed using the edge 𝑖𝑗,\nwhere the other vertex is denoted as 𝑘. Using the rules of the problem, we know that the score of △𝑖𝑗𝑘is equal to the value of 𝑖×𝑗×𝑘.\n𝑖\n𝑗\n𝑘\nHowever, this leaves us with two additional polygons to the left and right of △𝑖𝑗𝑘. To identify the best possible triangulation that involves\n△𝑖𝑗𝑘, we will also have to know the best way to triangulate these additional polygons.\n𝑖\n𝑗\n𝑘\nThis where the recursive subproblems come into play! Let us define as the minimum triangulation possible for the polygon spanning the𝐹(𝑖,𝑘)\nrange from vertex 𝑖to vertex 𝑘, and as the minimum triangulation possible for the polygon spanning the range from vertex 𝑘to vertex 𝑗.𝐹(𝑘,𝑗)\n𝑖\n𝑘\n𝐹(𝑖,𝑘)\n𝑗\n𝑘\n𝐹(𝑘,𝑗)\nBydoingso,wecandenotetheminimumtriangulationthatincludes△𝑖𝑗𝑘asequalto𝐹(𝑖,𝑘)+𝐹(𝑘,𝑗)+value[𝑖]×value[𝑗]×value[𝑘]. However,\nthis is just one triangle that can be formed using edge 𝑖𝑗. To determine the best possible triangulation for the polygon, we would need toentire\nidentify the minimum triangulation that includes △𝑖𝑗𝑘for possible values of 𝑘between 𝑖and 𝑗.𝑛−2all\n𝑖\n𝑗\n𝑘1\n𝑖\n𝑗\n𝑘2\n𝑖\n𝑗\n𝑘3\n𝑖\n𝑗\n𝑘4\nmin(𝐹(𝑖,𝑘1)+𝐹(𝑘1,𝑗)+value[𝑖]×value[𝑗]×value[𝑘1]𝐹(𝑖,𝑗)=\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nminimumtriangulationinvolving△𝑖𝑗𝑘1\n𝐹(𝑖,𝑘2)+𝐹(𝑘2,𝑗)+value[𝑖]×value[𝑗]×value[𝑘2]+\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nminimumtriangulationinvolving△𝑖𝑗𝑘2\n𝐹(𝑖,𝑘3)+𝐹(𝑘3,𝑗)+value[𝑖]×value[𝑗]×value[𝑘3]+\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nminimumtriangulationinvolving△𝑖𝑗𝑘3\n𝐹(𝑖,𝑘4)+𝐹(𝑘4,𝑗)+value[𝑖]×value[𝑗]×value[𝑘4]+\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\nminimumtriangulationinvolving△𝑖𝑗𝑘4\n)", "word_count": 342, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "48b8cf5e-e9c7-5351-912b-d656532390ef", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 914, "real_page_number": null, "text": "902\nChapter 23. Dynamic Programming\nMore generally, we can write this recurrence relation for any 𝑛-sided polygon as such, where represents the minimum value attainable𝐹(𝑖,𝑗)\nfrom triangulating the polygon formed by the vertices from 𝑖to 𝑗(where 𝑗):𝑖<\n𝐹(𝑖,𝑗)=\n{\n0,\n𝑗−𝑖≤1if (adjacent vertices, so no triangle can be formed)\nmin𝑖<𝑘<𝑗(𝐹(𝑖,𝑘)+𝐹(𝑘,𝑗)+value[𝑖]×value[𝑘]×value[𝑗]),\notherwise\nAsthisrecurrencerelationinvolvesoverlappingsubproblems,wecanuseadynamicprogrammingapproachtosolveit. Sinceeachsubproblemis\ndefinedintermsof𝑖and𝑗,wewilldeclareatwo-dimensionalmemosuchthatthesolutionto isstoredatpositionmemo[𝑖][𝑗]. Then,tosolve𝐹(𝑖,𝑗)\neach subproblem, we iterate over all vertices 𝑘between 𝑖and 𝑗and find the minimum value of 𝐹(𝑖,𝑘)+𝐹(𝑘,𝑗)+value[𝑖]×value[𝑘]×value[𝑗],\nstoring each result in our memo. A top-down solution is shown below:\n1\nint32_t min_triangulation(const std::vector<int32_t>& values) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(values.size(),memo(values.size(), -1));\n3\nreturn min_triangulation_helper(values, 0, values.size() - 1, memo);\n4\n} // min_triangulation()\n5\n6\nint32_t min_triangulation_helper(const std::vector<int32_t>& int32_t int32_tvalues, i, j,\n7\nstd::vector<std::vector<int32_t>>& memo) {\n8\nif (j - i <= 1) {\n9\nreturn 0;\n10\n} // if\n11\nif (memo[i][j] != -1) {\n12\nreturn memo[i][j];\n13\n} // if\n14\nint32_t std::numeric_limits<int32_t>::max();best =\n15\nfor (int32_t k = i + 1; k < j; ++k) {\n16\nint32_t curr = min_triangulation_helper(values, i, k, memo) +\n17\nmin_triangulation_helper(values, k, j, memo) +\n18\nvalues[i] values[k] values[j];* *\n19\nbest = std::min(best, curr);\n20\n} // for k\n21\nreturn memo[i][j] = best;\n22\n} // min_triangulation_helper()\ni for j forA bottom-up solution is shown below. Notice that is in the outer loop, while is in the inner loop. Thisdecremented incremented\nwas done intentionally, as it ensures that all dependent subproblems (in this case, and 𝐹(𝑘,𝑗)) are already solved we try to solve𝐹(𝑖,𝑘) before\nfor the larger subproblem of 𝐹(𝑖,𝑗). Remember that the subproblems must be solved in the correct order if you use a bottom-up approach!\n1\nint32_t min_triangulation(const std::vector<int32_t>& values) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(values.size(),memo(values.size(), 0));\n3\nfor (int32_t i = values.size() - 1; i >= 0; --i) {\n4\nfor (int32_t j = i + 1; j < values.size(); ++j) {\n5\nint32_t& best = memo[i][j];\n6\nfor (int32_t k = i + 1; k < j; ++k) {\n7\nint32_t std::numeric_limits<int32_t>::max()curr = best == 0 ? : best;\n8\nbest = std::min(curr, memo[i][k] + memo[k][j] + values[i] values[k] values[j]);* *\n9\n} // for k\n10\n} // for j\n11\n} // for i\n12\nreturn memo[0][values.size() - 1];\n13\n} // min_triangulation()\n𝑛2What is the time complexity of this solution? Given a polygon with 𝑛vertices, there are a total of subproblems that we may need to compute,\neach of which may take up to time to solve (since we have to do a linear pass to find the minimum sum). The overall time complexity ofΘ(𝑛)\nΘ(𝑛×𝑛2) Θ(𝑛3). Θ(𝑛2)this problem is therefore worst-case The auxiliary space used by this problem is for the size of the memo.=\n¸ 23.3.5\nDynamic Programming on Strings and Sequences\nDynamic programming questions on strings and sequences tend to have more variation, so there is not always a fast-and-hard rule that can be\napplied to all types of problems. However, for most of these problems, you will want to iterate over the characters of the given sequence(s)\nand solve a subproblem based on a condition (e.g., such as when characters match). We will look at a couple examples of string and sequence\ndynamic programming problems in this section.\nExample23.18 s1 s2,Giventwostrings and returnthelengthofthelongestcommonsubsequence(LCS),or0ifnocommonsubsequence\nexists. A subsequence is a sequence derived from an original sequence with certain elements deleted, without changing the relative order of\nthe remaining elements (for example, {A, C, D, F} is a subsequence of {A, B, C, D, E, F, G}).\ns1 = \"parrot\" s2 = \"almost\", \"aot\"Example: Given and you would return 3, since the LCS of has a length of 3.\nIf we were to brute force this problem, we would have to check all of the subsequences in one string and see if they are subsequences of the\nother string. Assuming the shorter string has a length of 𝑚and the longer string has a length of 𝑛, the time complexity of such an approach\nΘ(𝑛2𝑚), 2𝑚subsequenceswould be since there are that can be built using the smaller string, and comparing each subsequence with the other\nstring each requires at most work. As a result, this is not a viable approach if you want to solve this problem efficiently.Θ(𝑛)", "word_count": 780, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5ac45370-c2c7-54e8-aae2-a6b5eef59482", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 915, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n903\nInstead, we can try to improve the solution by finding a recurrence relation that relates the length of a string to smaller subproblems — this\nallows us to explore better algorithm families such as dynamic programming. It turns out that such a recurrence relation does exist, and it\nexhibits the property of overlapping subproblems that makes dynamic programming useful. Notice that, when given two strings and with𝑠1 𝑠2\nrespective lengths 𝑚and 𝑛, there are two possibilities that arise:\n1. The last characters of and match. If this happens, then this last character be the last character of the longest common𝑠1 𝑠2 must\nsubsequence (LCS). This is shown using a proof by contradiction: if this last character is not the final character of the LCS, we can add it\nto make the LCS even longer, as it is shared by both strings after all the other characters.\n2. The last characters of and do not match. If this happens, we know that at least one of these two characters is not in the LCS. We𝑠1 𝑠2\ndo not know which character is not, but we can recursively solve for it by comparing two values:\n• the LCS between and the first characters of (i.e., all but the last character of 𝑠2) — this would tell us the longest LCS𝑠1 𝑛−1 𝑠2\nwe could obtain if the last character of were not in our final LCS.𝑠2\n• the LCS between and the first characters of (i.e., all but the last character of 𝑠1) — this would tell us the longest LCS𝑠2 𝑚−1 𝑠1\nwe could obtain if the last character of were not in our final LCS.𝑠1\nWe would then take the larger of these two values to identify which character should belong in our LCS.\nTo illustrate this, let’s look at the example provided.\np\na\nr\nr\no\nt\na\nl\nm\no\ns\nt\nSince these two strings share the same final letter, we know for certain that the longest common subsequence ends with the letter ’t’ (otherwise,\nwe can simply add this letter at the end to extend the length of any common subsequence). Therefore, the longest common subsequence of the\nentire string must be the longest common subsequence of \"parro\" or \"almos\", plus the letter ’t’.\nLCS(\n\"parro\"\n,\n\"almos\"\n)\n+\n1\np\na\nr\nr\no\na\nl\nm\no\ns\nt\nHowever, what if the last two characters were not the same? For example, suppose we wanted to find the longest common subsequence between\n\"parro\" and \"almos\". Since the last characters do not match, one of these letters must not be in the final longest common subsequence.\np\na\nr\nr\no\na\nl\nm\no\ns\nThus, we are left with two possiblities. If the \"o\" in \"parro\" is omitted, then the longest common subsequence is simply the longest common\nsubsequence between \"parr\" and \"almos\":\nLCS(\n\"parr\"\n,\n\"almos\"\n)\np\na\nr\nr\na\nl\nm\no\ns\nOn the other hand, if the \"s\" in \"almos\" is omitted, then the longest common subsequence is the longest common subsequence between \"parro\"\nand \"almo\":\nLCS(\n\"parro\"\n,\n\"almo\"\n)\np\na\nr\nr\no\na\nl\nm\no\nTo identify which is correct, we will have to solve both recurrences and take the better result.\nLastly, note that the longest common subsequence of any empty string is trivially 0, which provides a base case for our problem. Putting\neverything together, we end up with the following recurrence relation, where 𝑀and 𝑁represent the lengths of the two strings and 𝑆2, and𝑆1\nrepresents all characters in from position 1 to position 𝑀(using one-indexing):𝑆1[1…𝑀] 𝑆1\nLCS(𝑆1[1…𝑀],𝑆2[1…𝑁])=\n⎧\n⎪\n⎨\n⎪⎩\n0,\nif or𝑀= 𝑁=0 0\nLCS(𝑆1[1…𝑀−1],𝑆2[1…𝑁−1])+1,\nif 𝑆1[𝑀] 𝑆2[𝑁]=\nmax(LCS(𝑆1[1…𝑀−1],𝑆2[1…𝑁]),LCS(𝑆1[1…𝑀],𝑆2[1…𝑁−1])),\n≠𝑆2[𝑁]if 𝑆1[𝑀]", "word_count": 646, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cd96f9c2-b6c4-5d07-8474-ce93c0cb7117", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 916, "real_page_number": null, "text": "904\nChapter 23. Dynamic Programming\nSince overlapping subproblems are involved, a dynamic programming approach is useful for solving this problem. There are a total of 𝑀+1\npossibilities for the length of the first string (0, 1, …, 𝑀) and possibilities for the length of the second string (0, 1, …, 𝑁), so we will𝑁+1\ncreate a memo with dimensions (𝑀+1)×(𝑁+1). If we use a top-down approach to solve this problem, we will make recursive calls using\nthe conditions of the recurrence relation above, retrieving any solutions from the memo if they have been encountered before. The code for a\ntop-down solution is shown below:\n1\nint32_t lcs(const conststd::string& s1, std::string& s2) {\n2\nsize_t m = s1.length(), n = s2.length();\n3\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(nmemo(m + 1, + 1, -1));\n4\nreturn lcs_helper(s1, s2, s1.length(), s2.length(), memo);\n5\n} // lcs_helper()\n6\n7\nint32_t lcs_helper(const const int32_t int32_tstd::string& s1, std::string& s2, m, n,\n8\nstd::vector<std::vector<int32_t>>& memo) {\n9\nif (m == 0 || n == 0) {\n10\nreturn 0;\n11\n} // if\n12\nif (memo[m][n] != -1) {\n13\nreturn memo[m][n];\n14\n} // if\n15\n// final characters match (minus 1 since strings use 0-indexing)\n16\nif (s1[m - 1] == s2[n - 1]) {\n17\nreturn memo[m][n] = lcs_helper(s1, s2, m - 1, n - 1, memo) + 1;\n18\n} // if\n19\nelse {\n20\nreturn memo[m][n] = std::max(\n21\nlcs_helper(s1, s2, m - 1, n, memo),\n22\nlcs_helper(s1, s2, m, n - 1, memo)\n23\n);\n24\n} // else\n25\n} // lcs_helper()\nIf we use a bottom-up approach to solve the problem, we would build up the subproblems starting from the base case. Using the rules of\nthe recurrence relation, if the two characters at and match, then we would set the value of memo[𝑖][𝑗] to memo[𝑖−1][𝑗−1] + 1𝑆1[𝑖] 𝑆2[𝑗]\n(examples of this case are shown below).\na\nl\nm\no\ns\nt\n0\n1\n2\n3\n4\n5\n6\np\na\nr\nr\no\nt\n0\n1\n2\n3\n4\n5\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n2\n2\n2\n0\n1\n1\n1\n2\n2\n3\nIf == 𝑆1[𝑗], then we would set the value of memo[𝑖][𝑗] to max(memo[𝑖−1][𝑗], memo[𝑖][𝑗−1]).𝑆1[𝑖]\na\nl\nm\no\ns\nt\n0\n1\n2\n3\n4\n5\n6\np\na\nr\nr\no\nt\n0\n1\n2\n3\n4\n5\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n2\n2\n2\n0\n1\n1\n1\n2\n2\n3", "word_count": 484, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "05db147f-1f35-560a-9fb1-eababafed438", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 917, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n905\nThe code for a bottom-up approach is shown below:\n1\nint32_t lcs(const conststd::string& s1, std::string& s2) {\n2\nsize_t m = s1.length(), n = s2.length();\n3\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(nmemo(m + 1, + 1));\n4\nfor (int32_t i = 1; i <= m; ++i) {\n5\nfor (int32_t j = 1; j <= n; ++j) {\n6\nif (s1[i - 1] == s2[j - 1]) {\n7\nmemo[i][j] = memo[i - 1][j - 1] + 1;\n8\n} // if\n9\nelse {\n10\nmemo[i][j] = std::max(memo[i - 1][j], memo[i][j - 1]);\n11\n} // else\n12\n} // for j\n13\n} // for i\n14\nreturn memo[m][n];\n15\n} // lcs()\nSince there are subproblems that we may encounter, each of which can be solved in constant time, the overall time complexity of thisΘ(𝑀𝑁)\nsolution is Θ(𝑀𝑁). Similarly, a memo of size is used, so the auxiliary space used by this solution is also Θ(𝑀𝑁).Θ(𝑀𝑁)\nExample 23.19 Solve the longest common subsequence problem, but return the actual LCS instead of its length. If there are multiple LCS,\nyou may return any of them.\nIt may seem this problem requires us to store more information, but our memo from the previous example is enough to find the actual LCS! We\nsimply have to backtrack from the solution cell (memo[𝑀][𝑁]) back to a base case, using the final character at each position to determine where\nwe came from. If the characters and match, then that character must be in the LCS, and we must have come from memo[𝑖−1][𝑗−1]𝑆1[𝑖] 𝑆2[𝑗]\nwhen solving the subproblem. On the other hand, if the characters and do not match, then we must have come from the larger of𝑆1[𝑖] 𝑆2[𝑗]\nmemo[𝑖−1][𝑗] and memo[𝑖][𝑗−1] (or either, if both memo values are equal). If we use these rules to walk through the memo, we can identify\nthe characters in our LCS by keeping track of the characters that match along the way.\n\"aot\":Two backtracking paths are shown below, both of which lead to the answer of\na\nl\nm\no\ns\nt\n0\n1\n2\n3\n4\n5\n6\np\na\nr\nr\no\nt\n0\n1\n2\n3\n4\n5\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n2\n2\n2\n0\n1\n1\n1\n2\n2\n3\na\nl\nm\no\ns\nt\n0\n1\n2\n3\n4\n5\n6\np\na\nr\nr\no\nt\n0\n1\n2\n3\n4\n5\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n2\n2\n2\n0\n1\n1\n1\n2\n2\n3\nThe code for the backtracking process is shown below:\n1\nlcs(const conststd::string std::string& s1, std::string& s2) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(nmemo(m + 1, + 1));\n3\n// ...solve subproblems and fill out memo...\n4\nint32_t i = m, j = n;\n5\nwhile (i > 0 && j > 0) {\n6\nif (s1[i - 1] == s2[j - 1]) {\n// minus 1 since strings use 0-indexing\n7\nlcs = s1[i - 1] + lcs;\n8\n--i;\n9\n--j;\n10\n} // if\n11\nelse if (memo[i][j - 1] > memo[i - 1][j]) {\n12\n--j;\n13\n} // else if\n14\nelse {\n15\n--i;\n16\n} // else\n17\n} // while\n18\n} // lcs()\nSince the length of the discovered path has a length of at most 𝑀+𝑁+1, the time complexity of finding the actual LCS string using the memo\nis 𝑁). This is asymptotically smaller than the time complexity required to solve the original problem, so the overall timeΘ(𝑀+ Θ(𝑀𝑁)\ncomplexity of the problem remains Θ(𝑀𝑁), regardless of whether we want to find the length of the LCS or the actual LCS itself.", "word_count": 690, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0c23f8ce-e690-59e1-b83f-c69186c2ff1b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 918, "real_page_number": null, "text": "906\nChapter 23. Dynamic Programming\nExample 23.20 s1 s2,You are given two strings, and with respective lengths of 𝑚and 𝑛. You are allowed to perform the following three\noperations on a word:\n1. Insert a character.\n2. Delete a character.\n3. Replace a character.\ns1 s2Write a function that returns the minimum number of operations needed to turn into (known as the distance).edit\ns1 = \"sunday\" s2 = \"saturday\",Example: Given and you would return 3, since a minimum of 3 operations are needed to turn\n\"sunday\" \"saturday\":into\n(sunday →surday)'n' 'r'• Operation 1: replace with\n→sturday)'t' 'u' (surday• Operation 2: insert before\n→saturday)'a' 't' (sturday• Operation 3: insert before\nAlthough this problem may seem new, it actually follows a dynamic programming pattern that we have already discussed. Similar to the longest\ncommon subsequence problem, we can compare the last characters of the two strings to devise a recurrence relation that relates the edit distance\nof larger strings to the edit distances of smaller substrings. Notice that there are only two outcomes that can happen if we compare the last\ncharacters of the source and target string: either they match, or they don’t.\n1. If the last characters match, then no operation is needed on the last character to convert from the source string to the target string. Thus,\nthe edit distance between the two strings is equal to the characters. For example,edit distance of the two strings up to their second-to-last\nsuppose we want to convert the string \"sunday\" to the string \"saturday\". Since the last two characters of these strings are the same, the\nedit distance between \"sunday\" and \"saturday\" must be the same as the edit distance between \"sunda\" and \"saturda\".\ns\nu\nn\nd\na\ny\ns\na\nt\nu\nr\nd\na\ny\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n𝑑𝑖𝑠𝑡(\"sunda\",\"saturda\")\n+ 0\n2. If the last characters do match, then we have to perform one operation on the last character of the source string to match it with thenot\nlast character of the target string. To determine which operation to perform (i.e., insert, delete, or replace), we will identify the one that\ninvolves the fewest number of edit operations. For example, suppose we are trying to convert the starting string \"cat\" to the ending string\n\"dog\". The last characters between these two strings do not match, so there are three choices we can make:\n• We can perform an to match these last two characters. If we perform an insertion, the total edit distance is equal to theinsertion\nedit distance between \"cat\" and \"do\" + 1 (to insert the \"g\" in \"dog\").\nWe can perform a to match these last two characters. If we perform a deletion, the total edit distance is equal to the edit• deletion\ndistance between \"ca\" and \"dog\" + 1 (to delete the \"t\" in \"cat\").\n• We can perform a to match these last two characters. If we perform a replacement, the total edit distance is equal toreplacement\nthe edit distance between \"ca\" and \"do\" + 1 (to replace the \"t\" in \"cat\" with the \"g\" in \"dog\").\nThe edit distance of the original two strings would therefore be the minimum of these three options.\nInsertion\nc\na\nt\nd\no\ng\n⏟⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏟\n𝑑𝑖𝑠𝑡(\"cat\",\"do\")\n+ 1\nDeletion\nc\na\nt\nd\no\ng\n⏟⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏟\n𝑑𝑖𝑠𝑡(\"ca\",\"dog\")\n+ 1\nReplacement\nc\na\nt\nd\no\ng\n⏟⏞⏞⏟⏞⏞⏟\n+ 1𝑑𝑖𝑠𝑡(\"ca\",\"do\")\nThe base cases occur if the length of the starting or ending string is 0. If the starting string has length 0, the minimum edit distance would just be\nthe length of the ending string (since all of its characters will have to be inserted). Similarly, if the ending string has length 0, the minimum edit\ndistance would be the length of the starting string (since all of its characters will have to be deleted). Putting this all together, if we define 𝐷(𝑖,𝑗)\nas the edit distance between and (where represents all characters in up to index 𝑖), we can devise the following𝑆1[0...𝑖] 𝑆2[0...𝑗] 𝑆1[0...𝑖] 𝑆1\nrecurrence relation for this problem:\nD(𝑖,𝑗)=\n⎧\n⎪\n⎪\n⎨\n⎪\n⎪⎩\n𝑖,\n𝑖≠0,𝑗=if 0\n𝑗,\n0,𝑗≠0if 𝑖=\n𝐷(𝑖−1,𝑗−1),\nif 𝑆1[𝑖] 𝑆2[𝑗]==\nmin(𝐷(𝑖−1,𝑗),𝐷(𝑖,𝑗−1),𝐷(𝑖−1,𝑗−1))+1,\n≠𝑆2[𝑗]if 𝑆1[𝑖]", "word_count": 718, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "78569f90-425f-513f-a514-0b52fcc218d5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 919, "real_page_number": null, "text": "23.3 Common Dynamic Programming Patterns\n907\nThere are overlapping subproblems involved in this recurrence relation, so dynamic programming can be applied. Given strings of lengths 𝑚\nand 𝑛, there are a total of subproblems that may be encountered (one subproblem for every possible substring pair), so we will declare aΘ(𝑚𝑛)\nmemo of size Θ(𝑚𝑛). If we use a top-down approach, we would make the corresponding recursive calls and store the solutions we encounter in\nour memo along the way. A top-down solution is shown below:\n1\nint32_t edit_distance(const conststd::string& s1, std::string& s2) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(s2.length()memo(s1.length() + 1, + 1, -1));\n3\nreturn edit_distance_helper(s1, s2, s1.length(), s2.length(), memo);\n4\n} // edit_distance()\n5\n6\nint32_t edit_distance_helper(const const int32_t int32_tstd::string& s1, std::string& s2, i, j,\n7\nstd::vector<std::vector<int32_t>>& memo) {\n8\nif (i == 0) {\n9\nreturn j;\n10\n} // if\n11\nif (j == 0) {\n12\nreturn i;\n13\n} // if\n14\nif (memo[i][j] != -1) {\n15\nreturn memo[i][j];\n16\n} // if\n17\nif (s1[i - 1] == s2[j - 1]) {\n18\nreturn memo[i][j] = edit_distance_helper(s1, s2, i - 1, j - 1, memo);\n19\n} // if\n20\nelse {\n21\nint32_t insertion = edit_distance_helper(s1, s2, i, j - 1, memo);\n22\nint32_t deletion = edit_distance_helper(s1, s2, i - 1, j, memo);\n23\nint32_t replacement = edit_distance_helper(s1, s2, i - 1, j - 1, memo);\n24\nreturn memo[i][j] = std::min({insertion, deletion, replacement}) + 1;\n25\n} // else\n26\n} // edit_distance_helper()\nIf we use a bottom-up approach, we would build up the edit distances of all pairs of substrings, starting from the base cases. This is done by\ncomparing the characters corresponding to each cell of the memo and identifying the relevant subproblems we need the solutions to. If the\ncharacters match, we simply copy over the value in the cell on the top left.\ns\na\nt\nu\nr\nd\na\ny\n0\n1\n2\n3\n4\n5\n6\n7\n8\ns\nu\nn\nd\na\ny\n0\n1\n2\n3\n4\n5\n6\n𝑥\n𝑥+1\nIf the characters do not match, we add one to the minimum of the cell directly above, the cell directly on the left, and the cell directly on the top\nleft (this picks the best outcome out of insertion, deletion, and replacement).\ns\na\nt\nu\nr\nd\na\ny\n0\n1\n2\n3\n4\n5\n6\n7\n8\ns\nu\nn\nd\na\ny\n0\n1\n2\n3\n4\n5\n6\n𝑦\n𝑧\n𝑥\nmin(𝑥, 𝑦, 𝑧) + 1\nAfter filling out the entire memo, the solution would be stored at the cell in the bottom right corner.", "word_count": 448, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c79d41d6-7681-53ca-bc7d-39cc39da8590", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 920, "real_page_number": null, "text": "908\nChapter 23. Dynamic Programming\nThe code for a bottom-up solution is shown below:\n1\nint32_t edit_distance(const conststd::string& s1, std::string& s2) {\n2\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(s2.length()memo(s1.length() + 1, + 1));\n3\n// base cases\n4\nfor (int32_t i = 0; i <= s1.length(); ++i) {\n5\nmemo[i][0] = i;\n6\n} // for i\n7\nfor (int32_t j = 0; j <= s2.length(); ++j) {\n8\nmemo[0][j] = j;\n9\n} // for j\n10\nfor (int32_t i = 1; i <= s1.length(); ++i) {\n11\nfor (int32_t j = 1; j <= s2.length(); ++j) {\n12\nif (s1[i - 1] == s2[j - 1]) {\n13\nmemo[i][j] = memo[i - 1][j - 1];\n14\n} // if\n15\nelse {\n16\nmemo[i][j] = 1 + std::min({memo[i][j - 1], memo[i - 1][j], memo[i - 1][j - 1]});\n17\n} // else\n18\n} // for j\n19\n} // for i\n20\nreturn memo[s1.length()][s2.length()];\n21\n} // edit_distance()\nThere are a total of subproblems, where 𝑚and 𝑛are the lengths of the two strings, and each subproblem can be solved in constantΘ(𝑚𝑛)\ntime. With the help of the memo, the time complexity of the edit distance problem is therefore Θ(𝑚𝑛). Similarly, this solution initializes a\nmemo, so the auxiliary space used is also Θ(𝑚𝑛).(𝑚+1)×(𝑛+1)\nExample 23.21 You are given a sequence of numbers 𝑆. Write a function that returns the length of the longest increasing subsequence\n(LIS) of 𝑆(i.e., a subsequence of 𝑆with all its elements in strictly increasing order).\n[6, 4, 1, 3, 8, 5, 7, 9, 2], [1, 3, 5, 7, 9].Example: Given the sequence you would return 5, since the LIS is\nTo solve this problem, we could brute force a solution and identify every subsequence, check if it is increasing, and keep track of the longest\nsubsequence we’ve encountered. However, this is not optimal. How can we improve the performance of our solution?\nThe key insight to notice is that we can define the problem recursively by keeping track of the LIS of previous states and using these values\nto compute the LIS of states we encounter later on. For instance, consider the sequence in the example:\n6\n4\n1\n3\n8\n5\n7\n9\n2\nLet’s select any arbitrary value from this sequence, such as 5. For 5 to be in the longest increasing subsequence, it must directly follow either 4,\n1, or 3 (otherwise, the sequence wouldn’t be strictly increasing):\n6\n4\n1\n3\n8\n5\n7\n9\n2\nTherefore, if we know the longest increasing subsequences ending at either 4, 1, or 3, we can simply add one to the longest of these three\nvalues to get the longest increasing subsequence ending at 5. That is, if the longest increasing subsequence ending at 4 has length 𝑥, the\nlongest increasing subsequence ending at 1 has length 𝑦, and the longest increasing subsequence ending at 3 has length 𝑧, the longest increasing\nsubsequence ending at 5 has length max(𝑥,𝑦,𝑧) + 1.\nIn general, given any index 𝑖of the sequence, the longest increasing subsequence ending at index 𝑖can be computed by finding the longest\nincreasing subsequence ending at any index 𝑗such that 𝑖and 𝑆[𝑖], and adding 1 to that value:𝑗< 𝑆[𝑗]<\nLIS(𝑖) 1+max(LIS(𝑗) such that 𝑖and𝑗< 𝑆[𝑗]<𝑆[𝑖])=\nOne base case occurs when index 𝑖is 0, since the length of the LIS ending at index 𝑖must be 1 (the first element itself). Another base case\noccurs when the value at index 𝑖is smaller than all the elements before it; this also implies that the LIS ending with the value at index 𝑖is 1, as\nno value before it can be added to form an increasing subsequence. Putting this all together, we get the following recurrence relation:\nLIS(𝑖)=\n{\n1,\n≥𝑆[𝑖] ≤𝑗<if or for all 0𝑖= 𝑆[𝑗] 𝑖0\n1+max(LIS(𝑗) such that 𝑖and𝑗< 𝑆[𝑗]<𝑆[𝑖]),\notherwise\nSince each subproblem requires us to compute the LIS of earlier values in the array, the same subproblem may be needed more than once. As a\nresult, there exist overlapping subproblems, and dynamic programming can be used. Here, each subproblem can be distinguished using an index\nvalue from 0 to 𝑛−1, where 𝑛is the number of elements in the original array, so our memo will also be one-dimensional with size 𝑛.", "word_count": 724, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "31b4d7dd-df38-54cd-8c39-1011e96f99f4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 921, "real_page_number": null, "text": "23.4 Memo Space Optimization for Bottom-Up Dynamic Programming\n909\nIf we use a top-down approach, we would make the recursive calls indicated by the recurrence relation, storing any solutions we encounter in\nour memo. A top-down solution to this problem is shown below:\n1\nint32_t lis(const std::vector<int32_t>& nums) {\n2\nstd::vector<int32_t> memo(nums.size(), -1);\n3\nfor (int32_t i = nums.size() - 1; i >= 0; --i) {\n4\nlis_helper(nums, i, memo);\n5\n} // for i\n6\nreturn *std::max_element(memo.begin(), memo.end());\n7\n} // lis()\n8\n9\nint32_t lis_helper(const std::vector<int32_t>& int32_t std::vector<int32_t>&nums, idx, memo) {\n10\nif (idx == 0) {\n11\nreturn 1;\n12\n} // if\n13\nif (memo[idx] != -1) {\n14\nreturn memo[idx];\n15\n} // if\n16\nint32_t best = 1;\n17\nfor (int32_t i = idx - 1; i >= 0; --i) {\n18\nif (nums[i] < nums[idx]) {\n19\nbest = std::max(best, 1 + lis_helper(nums, i, memo));\n20\n} // if\n21\n} // for i\n22\nreturn memo[idx] = best;\n23\n} // lis_helper()\nIf we use a bottom-up approach, we would build up the subproblems starting from the base case. To determine what goes at memo[𝑖], we would\niterate over the sequence up to index 𝑖and find the largest value of memo[𝑗] such that 𝑆[𝑖], and then add one to this value. A bottom-up𝑆[𝑗]<\nsolution is shown below:\n1\nint32_t lis(const std::vector<int32_t>& nums) {\n2\nstd::vector<int32_t> memo(nums.size(), 1);\n3\nfor (int32_t i = 1; i < nums.size(); ++i) {\n4\nfor (int32_t j = 0; j < i; ++j) {\n5\nif (nums[j] < nums[i]) {\n6\nif (memo[j] + 1 > memo[i]) {\n7\nmemo[i] = memo[j] + 1;\n8\n} // if\n9\n}\n// if\n10\n}\n// for j\n11\n}\n// for i\n12\nreturn *std::max_element(memo.begin(), memo.end());\n13\n}\n// lis()\nnumsFor any index 𝑖, we have to iterate over all 𝑖previous values in the array to determine if it contributes to the subsequence (and potentially\nΘ(𝑛2) numsneeding to solve a subproblem at each index). With the help of the memo, this can completed in time, where 𝑛is the size of the\narray. The auxiliary space used by this algorithm is Θ(𝑛), for the size of the memo.\n23.4\nMemo Space Optimization for Bottom-Up Dynamic Programming\nWhen using bottom-up dynamic programming, there are certain situations where you can optimize memory usage by reusing previous memo\ncells for multiple subproblems. This optimization technique can be used if you know that a subproblem will never be needed again when\nbuilding up your solution.\n𝑖thFor instance, consider the Fibonacci problem introduced at the beginning of this chapter. To find the Fibonacci number, you only need to\n(𝑖−1)th (𝑖−2)thquery the and Fibonacci numbers.\nmemo\n0\n1\n1\n2\n3\n5\n8\n0\n1\n2\n3\n4\n5\n6\nmemo[4] memo[5] memo[6].In the example above, you only need to know and to compute The remaining elements in the memo are not\nmemo[0]needed at all — in fact, you will never need them again when you solve for larger subproblems! Thus, there is no point in storing up\nmemo[3],to since they will never be referenced later.", "word_count": 532, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f5a2fc84-7893-5b00-b5a6-240c40f3ca41", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 922, "real_page_number": null, "text": "910\nChapter 23. Dynamic Programming\nFor problems like these, you can save on memory by recycling the memo space used for subproblems that will never be needed later. This often\ninvolves collapsing a dimension of the memo (for example, or based on how far back you need to query to solve aΘ(𝑛) Θ(𝑚𝑛)→Θ(𝑛))→Θ(1)\nsubproblem. In the example above, we only need to keep track of two values at any point in time, so we can reduce the size of our memo from\nto Θ(1). A solution using this strategy is shown below:Θ(𝑛)\n1\nuint64_t fib(int32_t n) {\n2\nif (n < 2) {\n3\nreturn n;\n4\n} // if n\n5\nuint64_t prev2 = 0, prev1 = 1, curr = 0;\n6\nfor (int32_t i = 2; i <= n; ++i) {\n7\ncurr = prev2 + prev1;\n8\nprev2 = prev1;\n9\nprev1 = curr;\n10\n} // for i\n11\nreturn curr;\n12\n} // fib()\nThis strategy can be used to optimize multidimensional dynamic programming problems as well. Consider the knight moves problem covered\nΘ(𝑚𝑛2) 𝑛2earlier. Initially, we used a memo of size to store all the ways to reach each of the cells on the chessboard using up to 𝑚moves.\n1\nmemo[0]\n1\n1\nmemo[1]\n2\n1\n1\n1\n1\n2\n1\n1\n1\n1\nmemo[2]\n2\n1\n2\n2\n8\n3\n2\n8\n4\n4\n1\n4\n2\n1\n3\n2\n3\n2\n4\n3\n2\n1\nmemo[3]\n⋮\nHowever, some of this information will never be used again once the subproblems become large enough! For any 𝑖, the solutions for 𝑖moves\nonly depend on the solutions for moves, so there is no reason to store the subproblems for and fewer moves if you’ve already built up𝑖−1 𝑖−2\n𝑖ththe solution to the move. This insight allows us to collapse a dimension of our memo, as we only need to store two boards in our memo at\nany single point in time. We can implement this by using one board for odd-numbered moves and another board for even-numbered moves,\nalternating between the two as we build up the solution.\n1\nmemo[0]\n1\n1\nmemo[1]\n2\n1\n1\n1\n1\n2\n1\n1\n1\n1\nmemo[0]\n1\n1\nmemo[1]\n2\n1\n1\n1\n1\n2\n1\n1\n1\n1\nmemo[0]\n2\n1\n2\n2\n8\n3\n2\n8\n4\n4\n1\n4\n2\n1\n3\n2\n3\n2\n4\n3\n2\n1\nmemo[1]", "word_count": 414, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "25bcc38b-daf0-5495-8a73-27fad06c324b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 923, "real_page_number": null, "text": "23.4 Memo Space Optimization for Bottom-Up Dynamic Programming\n911\nThe code for this optimized solution is shown below: note that the solution is nearly identical to the bottom-up solution introduced earlier, but\nwith a memo of outer dimension 2 instead of 𝑚+1, and with modulo 2 applied when indexing a move (on lines 8, 20, and 29) to determine\nwhether a subproblem should be stored in the even or odd board of our memo.\n1\nint32_t knight_moves(int32_t int32_tm, n) {\n2\nstd::vector<std::vector<std::vector<int32_t>>> memo(2,\n3\nstd::vector<std::vector<int32_t>>(n, std::vector<int32_t>(n)));\n4\nmemo[0][0][0] = 1;\n5\nfor (int32_t num_move = 1; num_move <= m; ++num_move) {\n6\nfor (int32_t row = 0; row < n; ++row) {\n7\nfor (int32_t col = 0; col < n; ++col) {\n8\nmemo[num_move % 2][row][col] =\n9\nmemo_val_bounds_check(num_move - 1, row - 2, col - 1, memo) +\n10\nmemo_val_bounds_check(num_move - 1, row - 2, col + 1, memo) +\n11\nmemo_val_bounds_check(num_move - 1, row + 2, col - 1, memo) +\n12\nmemo_val_bounds_check(num_move - 1, row + 2, col + 1, memo) +\n13\nmemo_val_bounds_check(num_move - 1, row - 1, col - 2, memo) +\n14\nmemo_val_bounds_check(num_move - 1, row - 1, col + 2, memo) +\n15\nmemo_val_bounds_check(num_move - 1, row + 1, col - 2, memo) +\n16\nmemo_val_bounds_check(num_move - 1, row + 1, col + 2, memo);\n17\n} // for col\n18\n} // for row\n19\n} // for num_move\n20\nreturn memo[m % 2][n - 1][n - 1];\n21\n} // knight_moves()\n22\n23\nint32_t memo_val_bounds_check(int32_t int32_t int32_tmove, row, col,\n24\nstd::vector<std::vector<std::vector<int32_t>>>& memo) {\n25\nif (row < 0 || col < 0 || row >= memo[0].size() || col >= memo[0].size()) {\n26\nreturn 0;\n27\n} // if\n28\nelse {\n29\nreturn memo[move % 2][row][col];\n30\n} // else\n31\n} // memo_val_bounds()\nΘ(𝑚𝑛2) Θ(𝑛2).This optimization brings the auxiliary space usage from down to From a complexity perspective, this is the best we can do —\nhowever, it is actually possible to further reduce the number of boards we need to store from two down to one! A single board is sufficient\n𝑖thbecause odd and even-numbered moves use different squares of the table, and the subproblem positions of the move will never conflict with\n(𝑖−1)ththe subproblem positions of the move. In the figure below, the white squares are only used on even-numbered moves (0, 2, 4, …), while\nthe black squares are only used on odd-numbered moves (1, 3, 5, …).\nSince odd and even subproblems do not overlap, there is a way to judiciously build up the subproblems using a single board, by computing the\nblack squares in terms of white and the white squares in terms of black (zeroing out the old value at each cell before solving each subproblem).\nfactor.10Θ(𝑛2)The auxiliary space remains since our memory is only reduced by a constant\nIt is important to note that this strategy of recycling memo positions only works for bottom-up dynamic programming, and top-down.not\nThis is because the bottom-up approach gives you control over the order in which the subproblems are encountered as you build up your solution,\nmaking it easy to identify which subproblems will never be needed again (and whose memo positions can therefore be recycled). On the other\nhand, top-down dynamic programming relies on recursion to solve the subproblems, and there is no practical way to dictate the order in which\nthe subproblems are encountered throughout the recursive process. Since there is no way of predetermining which subproblems will never be\nneeded again during future recursive calls, we cannot reuse memo positions in a similar fashion if using a top-down approach.\n10Althoughthesingle-boardsolutiontechnicallyuseslessmemory,itmightnotbeworthittogothisfar,sincetheamountofmemoryyouaresavingoverthe\ntwo-boardsolutionisoftennegligiblecomparedtothetimeitwouldtaketowritesuchasolution,especiallysincethespacecomplexitywouldstillbethesame.\nThisexampleisprovidedtoshowthatitis tosolvetheproblemusingasingleboard,butactuallymicro-optimizinguptothispointisnotnecessary.possible", "word_count": 697, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f1c820ba-19d6-5d61-aee4-a967a6118c88", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 924, "real_page_number": null, "text": "912\nChapter 23. Dynamic Programming\n23.5\nSummary of Dynamic Programming Patterns\nA summary of the previous dynamic programming patterns is provided in the table below. Note that these are merely common strategies that\ncan be used to solve dynamic programming problems, and that not all problems fall perfectly into any one of these categories. To fully master\ndynamic programming, the best method is to practice and expose yourself to as many types of dynamic programming problems as possible.\nType\nDefinition\nApproach\nCount Ways\nCount the number of ways to achieve a goal\nSum all possible ways to reach the current state\nPath Optimization\nFind the best path to a goal, given a cost for each step\nChoose the best (min or max) cost of all possible states\nfrom which the current state is reachable, and add the\ncost of the current state\nDecision Making\nChoose a subset of items to include given constraints\nDecide whether to include the current item by looking at\nprevious states where that item was and was not used,\nand choosing the better option\nInterval Merging\nCombine all elements with the optimal cost\nIdentify the cost of merging each possible subinterval\nand use the optimal merging strategy at each step to\ncombine the entire input\nString/List\nTypically involves modifying, adding, or removing\ncharacters from strings or lists\nDepends on the type of problem, but try looking at each\nposition of the string/list and identify a recurrence\nrelationship from the subproblems that involves previous\nitems in the string/list, and then use this relationship to\nmake a choice that leads you toward the solution", "word_count": 266, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7a703bd9-4683-5d81-9138-86b56060887c", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 925, "real_page_number": null, "text": "23.5 Summary of Dynamic Programming Patterns\n913\nChapter 23 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Trueorfalse? Dynamicprogrammingoftenreducesboththetimeandmemoryusedtosolveaproblemwithmultipleoverlappingsubproblems.\nA) True\nB) False\n2. Which of the following statements about dynamic programming is FALSE?\nA) When applying dynamic programming, the result of a repeated subproblem does not get recomputed even if it is needed multiple\ntimes\nB) Top-down dynamic programming only computes the answers to subproblems that are needed\nC) Dynamic programming can be used to reduce the runtime of a recursive function to be less than the time required to evaluate the\nfunction\nD) If a problem can be solved using dynamic programming, then it must also be solvable using divide-and-conquer\nE) None of the above\n3. Consider a grid, where you are allowed to move one cell to the right, one cell upwards, or one cell diagonally (up and right) on a single turn.\nYou start at the bottom-left corner of the grid, which has coordinates (0, 0). What is the time complexity of counting the number of distinct\npaths from (0, 0) to some other coordinate (𝑥, 𝑦), if you use dynamic programming?\nA) Θ(𝑥+𝑦)\nB) Θ(𝑥𝑦)\nΘ(𝑥2𝑦2)C)\nΘ(2𝑥+𝑦)D)\nΘ(3𝑥+𝑦)E)\nbi_coeff()4. Consider the following function, which computes the binomial coefficient 𝑛choose 𝑘:\n1\nuint64_t bi_coeff(int32_t int32_tn, k) {\n2\nif (k == 0 || k == n) {\n3\nreturn 1;\n4\n} // if\n5\nreturn bi_coeff(n - 1, k - 1) + bi_coeff(n - 1, k);\n6\n} // bi_coeff()\nbi_coeff()If we modify to use dynamic programming with a top down approach, what would be the worst-case time complexity of the\nnew method?\nA) Θ(𝑛)\nB) Θ(𝑘)\nC) Θ(𝑛+𝑘)\nD) Θ(𝑛𝑘)\nΘ(2𝑛)E)\n5. A group of 𝑛college students are attending a speed dating event. At this event, each student can remain single or partner up with another\nstudent, as long as they partner up only once. You want to find the total number of ways in which these students can remain single or be\npaired up. For example, for a group of 3 students, there are 4 possible outcomes:\n{1}, {2}, {3}\n{1, 2}, {3}\n{1}, {2, 3}\n{1, 3}, {2}\nWhat is the recurrence relation for this problem, and what is the time complexity of solving this problem if you use dynamic programming?\nA) Recurrence Relation: 𝑓(𝑛) 𝑓(𝑛−1)+𝑓(𝑛−2)=\nTime Complexity: Θ(𝑛)\nB) Recurrence Relation: 𝑓(𝑛) 𝑓(𝑛−1)+(𝑛−1)×𝑓(𝑛−2)=\nTime Complexity: Θ(𝑛)\nC) Recurrence Relation: 𝑓(𝑛) 𝑓(𝑛−1)+𝑓(𝑛−2)=\nΘ(𝑛2)Time Complexity:\nD) Recurrence Relation: 𝑓(𝑛) 𝑓(𝑛−1)+(𝑛−1)×𝑓(𝑛−2)=\nΘ(𝑛2)Time Complexity:\nE) None of the above\n6. Given an integer array of 𝑛distinct coin denominations, what is the worst-case time complexity of counting the total number of coin\ncombinations that sum to a given value 𝑘, if you use dynamic programming?\nA) Θ(𝑛)\nB) Θ(𝑛+𝑘)\nC) Θ(𝑛𝑘)\nΘ(𝑛2𝑘)D)\nΘ(2𝑘)E)", "word_count": 532, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "f0e4b4a3-295c-5137-9719-9e8212b55734", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 926, "real_page_number": null, "text": "914\nChapter 23. Dynamic Programming\n7. Consider two solutions to the Fibonacci problem, one that uses a naïve approach that does not use memoization, and one that memoizes the\nencountered subproblems using top-down dynamic programming. Which of the following statements is TRUE regarding the auxiliary\nspace complexity of the two approaches?\nA) The naïve approach uses auxiliary space, while the top-down dynamic programming approach uses auxiliary spaceΘ(1) Θ(1)\nB) The naïve approach uses auxiliary space, while the top-down dynamic programming approach uses auxiliary spaceΘ(𝑛) Θ(1)\nC) The naïve approach uses auxiliary space, while the top-down dynamic programming approach uses auxiliary spaceΘ(𝑛)Θ(1)\nD) The naïve approach uses auxiliary space, while the top-down dynamic programming approach uses auxiliary spaceΘ(𝑛) Θ(𝑛)\nΘ(2𝑛)E) The naïve approach uses auxiliary space, while the top-down dynamic programming approach uses auxiliary spaceΘ(𝑛)\n8. Which of the following is/are advantages in using a bottom-up dynamic programming approach to solve a problem instead of a top-down\ndynamic programming approach?\nI. The bottom-up approach may end up doing less work since it only computes the answers to subproblems that are needed.\nII. The bottom-up approach makes it easier to reuse positions in the underlying memo if you know which subproblems will never\nbe referenced again.\nIII. The bottom-up approach is less likely to stack overflow if the top-down solution cannot be made tail recursive.\nA) III only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n9. Consider the coin change making problem, where you want to compute the fewest number of coins needed to make change for a given target\namount 𝑇. In this problem, the provided coin denominations are 1¢, 4¢, and 5¢. Which of the following could potentially be a valid partial\nsnapshot of the underlying memo if a bottom-up dynamic programming approach is used? Note that $X.X1 represents some unknown\namount of money that has a 1 in its hundredths position, and that the provided change amounts are consecutive.\nA)\nChange Amount\n…\n$X.X1\n$X.X2\n$X.X3\n$X.X4\n$X.X5\n$X.X6\n…\nT\nMinimum Number of Coins Needed\n…\n45\n46\n47\n48\n49\n50\n…\n???\nB)\nChange Amount\n…\n$X.X1\n$X.X2\n$X.X3\n$X.X4\n$X.X5\n$X.X6\n…\nT\nMinimum Number of Coins Needed\n…\n45\n45\n45\n45\n45\n45\n…\n???\nC)\nChange Amount\n…\n$X.X1\n$X.X2\n$X.X3\n$X.X4\n$X.X5\n$X.X6\n…\nT\nMinimum Number of Coins Needed\n…\n45\n46\n40\n41\n42\n47\n…\n???\nD)\nChange Amount\n…\n$X.X1\n$X.X2\n$X.X3\n$X.X4\n$X.X5\n$X.X6\n…\nT\nMinimum Number of Coins Needed\n…\n45\n43\n44\n45\n43\n44\n…\n???\nE)\nChange Amount\n…\n$X.X1\n$X.X2\n$X.X3\n$X.X4\n$X.X5\n$X.X6\n…\nT\nMinimum Number of Coins Needed\n…\n45\n45\n45\n45\n45\n46\n…\n???\n10. Consider a problem whose solution can be expressed using the following recurrence relation:\n𝐹(𝑛)=\n{\n0,\n𝑛≤8if\n2𝐹(𝑛−8)+𝐹(𝑛−1),\n𝑛>8\nIf you solve this problem using an optimal implementation of dynamic programming, what is the worst-case time complexity for computing\nwith additional space?𝐹(𝑛) 𝑂(𝑛)\nA) Θ(1)\nB) Θ(log(𝑛))\nC) Θ(𝑛)\nΘ(𝑛2)D)\nΘ(2𝑛)E)\n11. You are given a set of 𝑁integers and a target integer 𝐾. Suppose you want to find the subset of these 𝑁integers with the largest size, such\nthat the total sum of all elements in the subset is less than or equal to 𝐾. Which algorithm family should you use to solve this problem, if\nyou want the the best time complexity?\nA) If 𝐾, choose greedy, otherwise choose dynamic programminglog(𝑁)<\nB) If 𝐾, choose greedy, otherwise choose dynamic programminglog(𝑁)>\nC) If 𝐾, choose greedy, otherwise choose brute force𝑁<\nD) If 𝐾, choose greedy, otherwise choose brute force𝑁>\nE) The greedy algorithm should be chosen for all values of 𝑁and 𝐾", "word_count": 646, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2dd6c96c-3794-526f-8896-9c4f14a1197c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 927, "real_page_number": null, "text": "23.5 Summary of Dynamic Programming Patterns\n915\n12. The Tribonacci sequence is a sequence of numbers where each term is the sum of the previous three numbers in the sequence. If we let 𝑇𝑛\n𝑛threpresent the value of the sequence, the value of 𝑇𝑛can be computed as:\n𝑇𝑛=\n⎧\n⎪\n⎪\n⎨\n⎪\n⎪⎩\n0,\nif 𝑛=0\n1,\nif 𝑛=1\n1,\nif 𝑛=2\n𝑇𝑛−1+𝑇𝑛−2+𝑇𝑛−3,\n𝑛≥3\n𝑛thImplement a function that returns the Tribonacci number.\nuint64_t tribonacci(int32_t n);\nYou are given an AVL tree of height ℎ. Implement a function that returns the minimum number of nodes the tree can have. A tree with13.\nheight 1 only has one node. Hint: since AVL trees must be balanced, the height of each node’s left and right children cannot have a\ndifference that exceeds 1.\nint32_t min_avl_nodes(int32_t h);\n4,Example: Given 3, you would return since the smallest possible AVL tree with a height of 3 has 4 nodes.ℎ=\n14. You are located at the top-left corner of a 𝑚×𝑛grid, and you want to reach the bottom-right corner. However, you are only allowed to move\nor at any point in time. Furthermore, there are obstacles in your path that you cannot bypass. Given the values of 𝑚and 𝑛and adown right\ngrid grid[i][j]𝑚×𝑛vector of bools that identifies whether the cell at can be accessed (true for accessible, false for obstacle), return\nthe total number of unique paths you can take to reach the bottom-right corner of the grid.\nint32_t count_paths(int32_t int32_t const std::vector<std::vector<bool>>&m, n, grid);\nExample: Given the following 2 x 3 grid with an obstacle in the bottom-left square, there are two unique paths that you can take to reach\nthe bottom-right corner:\ngrid\n★\n✖\ngrid\n1\n1\n1\n1\n0\n1\nstr, str.15. Given a string implement a function that returns the length of the longest palindromic subsequence in A palindrome is a string\nthat is spelled the same forward and backward, and a subsequence is a sequence that can be derived from another sequence by deleting\nsome or no elements without changing the order of the remaining elements.\nint32_t longest_palindromic_subsequence(const std::string& str);\n\"abcabcabcabc\",Example: Given the string you would return 7, since the longest palindromic subsequence has a length of 7 (one\n\"abcacba\").such solution is\n16. There are 𝑛pieces of gold, each of varying weights, that are laid in front of you in the shape of a circle. You are allowed to take as much\ngold as you want, under the condition that you are not allowed to take two pieces of gold that are adjacent to each other. Given an integer\nweightsarray that represents the weight of each piece of gold in the circle, return the greatest total weight of gold you can take without\nbreaking this constraint. Note that the circular arrangement means that the gold at index is adjacent to the gold at index 0.𝑛−1\nint32_t max_gold_weight(const std::vector<int32_t>& weights);\n[2, 3, 2],Example: Given the array you would return 3, since the greatest total weight is attained by taking the gold at index 1. You\ncannot take the gold at indices 0 and 2 since they are adjacent in the circle.", "word_count": 539, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5aa7750d-fef6-5b50-88ab-fc207af5ceaf", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 928, "real_page_number": null, "text": "916\nChapter 23. Dynamic Programming\n17. You are given a secret message that is encoded as a string of numbers, where each letter corresponds to a number from 1 to 26:\n• \"A\" →\"1\"\n• \"B\" →\"2\"\n• …\n• \"Y\" →\"25\"\n• \"Z\" →\"26\"\nFor example, the string \"BEAM\" would be encoded as \"25113\" (since B corresponds to 2, E corresponds to 5, A corresponds to 1, and M\ncorresponds to 13). However, there are multiple ways to decode a string of numbers into letters (for example, the sequence \"25113\" can\nalso be decoded as \"YAM\", since Y corresponds to 25, A corresponds to 1, and M corresponds to 13). Implement a function that takes in a\nstring of digits and returns the number of unique strings that can be formed by decoding it. For instance, the sequence \"25113\" can be\ndecoded into six possible strings: BEAAC, BEAM, BEKC, YAAC, YAM, and YKC, so you would return 6 if given this sequence.\nNote that it is invalid for a letter to be decoded from a leading zero (e.g., 101 can only be decoded as JA and not AJ, since you can only\nbreak this sequence into 10 + 1, and NOT 1 + 01). If a string is impossible to decode (e.g., \"000\"), the function should return 0.\nint32_t num_ways_to_decode(const std::string& seq);\n18. You are preparing to drive a shift for the new ride-sharing platform Tuber. There are 𝑛points on the road you are driving your vehicle\non, labeled from 1 to 𝑛in the direction of travel. You want to drive from point 1 to point 𝑛and make as much money as possible picking\nRideRequestup passengers, without changing the direction of travel. Given an array of objects that denote the origin and destination\npoints of each passenger and the tip they will give, return the most amount of money you can earn by picking up passengers. You can only\ndest - origin + tipdrive one at most one passenger at a time, and each passenger will pay you for the ride. You may assume that\nthere is at least one ride request and that 𝑛is at least 1.\nstruct int32_t int32_t int32_tRideRequest { origin; dest; tip; };\nint32_t max_earnings(const int32_tstd::vector<RideRequest>& rides, n);\nExample: Given the following passengers with 8:𝑛=\n0: {Origin = 1, Dest = 6, Tip = 2}\n1: {Origin = 4, Dest = 5, Tip = 4}\n2: {Origin = 7, Dest = 8, Tip = 1}\n3: {Origin = 2, Dest = 4, Tip = 1}\nyou would pick up passengers 1, 2, and 3 to earn a maximum amount of $9, so you would return 9.\n𝑖𝑡ℎpilestones stones[i]There are 𝑛piles of arranged in a row, where the has stones. A move consists of merging exactly19. 𝑘\nconsecutive piles into one pile, and the cost of a move is equal to the total number of stones in these 𝑘piles. Implement a function that,\nstones -1.when given and 𝑘, returns the minimum cost to merge all piles of stones into one pile. If impossible, return\nint32_t merge_stones(const std::vector<int32_t>& int32_tstones, k);\nstones = [3, 2, 4, 1]Example 1: Given and 2, you would return 20, since this is the minimum cost involved with merging all𝑘=\nthe stones together, 2 piles at a time:\n• We merge [3, 2] for a cost of 5, leaving us with [5, 4, 1]\n• We merge [4, 1] for a cost of 5, leaving us with [5, 5]\n• We merge [5, 5] for a cost of 10, leaving us with [10]\nstones = [3, 2, 4, 1]Example 2: Given and 3, you would return -1, since there is no way to merge all the stones into one pile if𝑘=\nyou are only allowed to merge three piles together at a time.", "word_count": 644, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a52c84e2-34f6-5d15-8ac6-4fbfb1b5c2ef", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 929, "real_page_number": null, "text": "23.5 Summary of Dynamic Programming Patterns\n917\n20. You are a school administrator tasked with assigning bus stops for a school bus that goes down a neighborhood street. Given a sorted\nhousesarray that identifies the positions of the houses on the street that belong to children attending the school, as well as an integer 𝑘\nrepresenting the number of stops that the school bus should make on that street, return the minimum possible combined distance between\neach student house and the closest school bus stop (measured in terms of the number of houses a student would have to pass to reach\nthe closest stop). You may assume that the house at position 𝑖is the same distance apart from the house at position for the entire𝑖+1\nhousesneighborhood (i.e., all houses are equidistant), and that 𝑘is at least 1 and no greater than the size of the provided array.\nint32_t min_stop_distance(const std::vector<int32_t>& int32_thouses, k);\nhouses = [2, 6, 9, 11, 17]Example 1: Given (i.e., houses 2, 6, 9, 11, and 17 have children that attend the school) and 2, you𝑘=\nwould return 12, since this is the minimum combined distance possible from assigning two bus stops (e.g., stops at houses 4 and 11):\n• The student at house 2 travels 2 houses to reach stop 4.\n• The student at house 6 travels 2 houses to reach stop 4.\n• The student at house 9 travels 2 houses to reach stop 11.\n• The student at house 11 travels 0 houses to reach stop 11.\n• The student at house 17 travels 6 houses to reach stop 11.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nÑ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ\nSTOP\nSTOP\nExample 2: Using the same houses but with 3, you would return 6, since this is the minimum combined distance possible from𝑘=\nassigning three bus stops (e.g., stops at houses 4, 10, and 17):\n• The student at house 2 travels 2 houses to reach stop 4.\n• The student at house 6 travels 2 houses to reach stop 4.\n• The student at house 9 travels 1 houses to reach stop 10.\n• The student at house 11 travels 1 houses to reach stop 10.\n• The student at house 17 travels 0 houses to reach stop 17.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nÑ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ Ñ\nSTOP\nSTOP\nSTOP\nHint: If you are only able to assign one stop given multiple houses, the optimal solution would be to place the stop at the position of the\nmedian house among the given houses.", "word_count": 482, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cc7b63d9-ced7-578b-99d4-a381167be88e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 930, "real_page_number": null, "text": "918\nChapter 23. Dynamic Programming\n21. In the game of chess, the bishop is a piece that can only move diagonally. For example, a bishop in the following position can only move to\nany square along any of the arrows shown:\n1\na\n2\nb\n3\nc\n4\nd\n5\ne\n6\nf\n7\ng\n8\nh\nb\nImplement a function that computes the number of unique ways 𝑘bishops can be placed on an 𝑛×𝑛chessboard such that no two bishops\nthreaten each other, given the values of 𝑛and 𝑘.\nint32_t num_bishop_placements(int32_t int32_tn, k);\nExample: Given and 2, you would return 4, since there are 4 ways to place 2 bishops on a 2 2 chessboard without any bishop𝑛= 𝑘=2 ×\nthreatening another bishop:\nbb\nbb\nb\nb\nb\nb\nHint 1: Bishops that are placed on differently colored squares can never attack each other, so they can be placed independently. For\nexample, if you wanted to know the number of ways to place five bishops on a chessboard, such that three bishops are on white squares\nand two are on black squares, you can compute the number of ways to place three white-squared bishops and two black-squared bishops\nindependently and multiply the results together to get the number of ways in total.\nHint 2: Since a bishop can move to any square on its diagonal, each diagonal can only hold at most one bishop. It may be useful to think of\nthis problem in terms of the diagonals on the chessboard, which may allow you to turn this into a \"counting ways\" problem.\nHint 3: You are given a pre-implemented function that can use in your solution\nint32_t num_squares_in_diag(int32_t n, int32_t i);\n𝑖(1-indexed) 𝑛×𝑛.that gives you the number of squares that exist on diagonal for a chessboard of dimensions For example, if you call this\n𝑛= 𝑖=function with and 9, the function would return 7, since there are 7 squares along diagonal 9 of a 8 8 chessboard:8 ×\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15", "word_count": 350, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d92d8e14-5f7a-5d87-b49b-302fd6b8f31e", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 931, "real_page_number": null, "text": "23.5 Summary of Dynamic Programming Patterns\n919\nChapter 23 Exercise Solutions\n1. Thecorrectansweris(B).Dynamicprogrammingoftenreducesthetimerequiredtosolveaproblemwithmultipleoverlappingsubproblems,\nbut this typically comes at the tradeoff of increased memory (to store the solutions of encountered subproblems in a memo).\n2. The correct answer is (D). Statement (D) is false because divide-and-conquer is used to solve problems with independent subproblems,\nwhile dynamic programming is best used to for problems with overlapping (dependent) subproblems. Statement (A) is true because the\nsolution of a repeated subproblem is stored in a memo the first time it is computed, so that it does not need to be recomputed if it is needed\nagain. Statement (B) is true because top-down dynamic programming only computes subproblems that are encountered during the recursive\ncalls down to the base case. Statement (C) is true because dynamic programming can bring down the runtime of a recursive function by\nstoring the solutions of repeated subproblems so that they don’t need to be recomputed.\n3. The correct answer is (B). If we use dynamic programming, the worst-case time complexity becomes Θ(𝑥𝑦), because there are 𝑥𝑦\nsubproblems we may have to solve (the number of distinct paths to each cell of the grid), and solving each subproblem takes constant time.\n4. The correct answer is (D). If we use dynamic programming, the worst-case time complexity becomes Θ(𝑛𝑘), because there are 𝑛𝑘\nsubproblems we may have to solve (for each possible combination of 𝑛and 𝑘), and solving for each subproblem takes constant time. See\nsection 23.1.4 for more detail.\n𝑛th5. The correct answer is (B). In this example, for every additional person that goes to the event, there are two options that are possible:\n𝑛th 𝑛th• This person remains single. How many ways can this occur? Because this person does not pair up with any of the other 𝑛−1\n𝑛thpeople, the total number of ways this can happen is exactly the same as the number of ways without this person present, or the\nnumber of ways people can remain single or paired up. If denotes the number of ways 𝑛people can pair up, there are a𝑛−1 𝑓(𝑛)\n𝑛thtotal of pairing combinations possible if the person remains single.𝑓(𝑛−1)\n𝑛th 𝑛th• This person pairs up with someone. How many ways can this occur? There are ways the person can pair up, one for𝑛−1\n𝑛theach of the other people present (i.e., the person has people to choose from). Now what about the people that𝑛−1 𝑛−1 𝑛−2\n𝑛tharen’t paired up with the person? The number of ways that these people can pair up is equal to the total number of ways that 𝑛−2\npeople can remain single or pair up. Thus, if represents the number of ways that people can pair up, and for each of𝑓(𝑛−2) 𝑛−2\n𝑛ththese ways there are ways an additional person can pair up, the total number of pairing combinations possible if the person𝑛−1\npairs up is the product of these two values, or (𝑛−1)×𝑓(𝑛−2).\nThis problem involves overlapping subproblems (the number of ways 𝑛people can pair up is dependent on the number of ways people𝑛−1\ncan pair up, and so on), so we can use dynamic programming. Here, our recurrence relation is\n𝑓(𝑛) 𝑓(𝑛−1)+(𝑛−1)×𝑓(𝑛−2)=\n𝑛thwhere the number of ways to organize 𝑛people is equal to the number of ways to organize people (if the person remains single)𝑛−1\n𝑛thadded to the number of ways to organize people after pairing up the person with someone else. We would store the number of𝑛−2\npairing combinations possible for all values from 1 to 𝑛, which can be done with a single array. As a result, since we are storing and\ntraversing through a single one-dimensional array with 𝑛values, this algorithm has a time complexity of Θ(𝑛).\n6. Thecorrectansweris(C).Thetotalnumberofcoincombinationsthatsumtoagivenvalue𝑘whengivencoindenominations[𝑖1,𝑖2,…,𝑖𝑛]\ncan be computed by adding the number of coin combinations that sum to 𝑘−𝑖1, with the number of combinations that sum to 𝑘−𝑖2, …,\nwith the number of combinations that sum to 𝑘−𝑖𝑛. Thus, the total number of subproblems we need to solve is Θ(𝑛𝑘), since we would have\nto add together values for all possible sums from 0 to 𝑘. One possible bottom-up solution for this problem is shown below:Θ(𝑛)\n1\nint32_t num_combinations(int32_t const std::vector<int32_t>&target, denominations) {\n2\nstd::vector<int32_t> memo(target + 1);\n3\nmemo[0] = 1;\n// one way to make amount zero (with no coins)\n4\nfor (int32_t denom : denominations) {\n5\nfor (size_t i = denom; i < memo.size(); ++i) {\n6\nmemo[i] += memo[i - denom];\n// add ways to make amount i - denom\n7\n} // for i\n8\n} // for denom\n9\nreturn memo[target];\n10\n} // num_combinations()\n7. The correct answer is (D). In the naïve recursive approach, the auxiliary space required is equal to the recursion depth, which is 𝑛(the full\nrecursion depth is needed because computing Fibonacci numbers recursively is not tail recursive since you would need to return the sum of\ntwo recursive calls). With dynamic programming, the auxiliary space becomes the size of the memo, which when optimally implemented\n𝑛thwould have a size of 𝑛(to store every Fibonacci number up to the number).\n8. The correct answer is (D). Only statements II and III are true. Statement I is a property of top-down but not bottom-up dynamic\nprogramming. Statement II is true because you can reuse memory for positions of the memo if you know that future subproblems will not\nneed to rely on previous values to be computed (see section 23.4). Statement III is true because bottom-up does not rely on recursive calls\nto build the memo, but rather computes the subproblems iteratively starting from the base case, which is less prone to stack overflows.", "word_count": 998, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "2b27320a-5aa2-5ef4-8e50-a324ea18fb21", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 932, "real_page_number": null, "text": "920\nChapter 23. Dynamic Programming\n9. The correct answer is (E). To determine which snapshots cannot be valid, we will look at how the subproblems are constructed using the\nrecurrence relation for this problem:\nmin(𝑚𝑒𝑚𝑜[𝑥−1],𝑚𝑒𝑚𝑜[𝑥−4],𝑚𝑒𝑚𝑜[𝑥−5])+1𝑚𝑒𝑚𝑜[𝑥]=\nOption (A) is not possible because if the optimal solution to make change for $X.X1 is 45 coins, then there is a way to make change for\n$X.X5 and $X.X6 using 46 coins, since you could just add a 4¢ or 5¢ coin, respectively. Option (B) is not possible because if the optimal\nsolution to make change for $X.X6 is 45 coins, then one of subproblems that can reach a value of $X.X6 with the addition of a single coin\n($X.X1, $X.X2, or $X.X5) must have an optimal solution less than 45 coins. Option (C) is not possible because if the optimal solution to\nmake change for $X.X5 uses 42 coins, then there is a way to make change for $X.X6 using 43 coins with the addition of a 1¢ coin. Option\n(D) is not possible because if the optimal solution to make change for $X.X5 uses 43 coins, then one of $X.X4, $X.X1, or $X.X0 (not\nshown) must have an optimal solution using 42 coins. Since both $X.X4 and $X.X1 require 45 coins, we know that the optimal solution for\n$X.X0 must be 42 if this snapshot were possible. However, if that were the case, then the optimal solution for $X.X1 would end up being\n43 coins since we could just add a 1¢ coin to the optimal solution for $X.X0, which contradicts with the optimal solution of 45 we have to\n$X.X1. Option (E) is the only one that can be constructed in using the recursive definition for this problem without any contradictions.\n10. The correct answer is (C). Using dynamic programming, our worst-case time complexity is bounded by the number of unique subproblems\nwe may need to solve. Since the number of subproblems here is on the order of 𝑛(i.e., down to the base case),𝐹(𝑛),𝐹(𝑛−1),𝐹(𝑛−2),…,\nand each subproblem takes constant time to solve, the overall time complexity of a dynamic programming solution is worst-case Θ(𝑛).\n11. The correct answer is (A). If you use the greedy approach, you would need to first sort the integers and then iterate over and take every\nnumber you can without going over 𝐾: this would take time in the worst case due to the sorting bottleneck. If you use dynamicΘ(𝑁log(𝑁)\nprogramming, you would not need to pre-sort the integers, and you could use the decision making \"take it or leave it\" implementation\napproach to produce a solution that takes time. Thus, if is less than 𝐾, you should use greedy (since would beΘ(𝑁𝐾) log(𝑁) 𝑁log(𝑁)\nbetter than 𝑁𝐾); otherwise, you should use dynamic programming.\n12. This is similar to the Fibonacci problem discussed in section 23.1.3, with the difference in that we are summing the previous three terms\nrather than the previous two. A bottom-up solution is provided below:\n1\nuint64_t tribonacci(int32_t n); {\n2\nstd::vector<uint64_t> memo(n + 1);\n3\nmemo[0] = 0;\n4\nmemo[1] = 1;\n5\nmemo[2] = 1;\n6\nfor (int32_t i = 3; i <= n; ++i) {\n7\nmemo[i] = memo[i - 1] + memo[i - 2] + memo[i - 3];\n8\n} // for i\n9\nreturn memo[n];\n10\n} // tribonacci()\n13. One possible solution is as follows. Since the formula for calculating the minimum number of nodes in an AVL tree of height ℎ\n𝑁(ℎ) 1+𝑁(ℎ−1)+𝑁(ℎ−2)=\ninvolves overlapping subproblems, we can store the minimum number of nodes for trees smaller than ℎto help us solve for the height of a\nminimum AVL tree of height ℎ. A bottom-up solution is provided below:\n1\nint32_t min_avl_nodes(int32_t h) {\n2\nstd::vector<int32_t> memo(h + 1);\n3\nmemo[0] = 0;\n4\nmemo[1] = 1;\n5\nfor (int32_t i = 2; i < h + 1; ++i) {\n6\nmemo[i] = 1 + memo[i - 1] + memo[i - 2];\n7\n} // for i\n8\nreturn memo[h];\n9\n} // min_avl_nodes()", "word_count": 675, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "eec32945-25d6-5c06-9d4f-cad8f434c95d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 933, "real_page_number": null, "text": "23.5 Summary of Dynamic Programming Patterns\n921\n14. This is a variation of the standard counting paths problem presented in example 23.3, with the added twist that certain cells have obstacles\nthat cannot be crossed. To solve this problem, we can use the same approach there, but with an extra step of initializing obstacle cells with\n0 in our memo (since there are 0 unique paths that can go through those cells). A bottom-up solution is provided below:\n1\nint32_t count_paths(int32_t int32_t const std::vector<std::vector<bool>>&m, n, grid) {\n2\nif (!grid[0][0]) {\n3\nreturn 0;\n4\n} // if\n5\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(n,memo(m, 0));\n6\nmemo[0][0] = 1;\n7\nfor (int32_t col = 1; col < n; ++col) {\n8\nif (grid[0][col]) {\n9\nmemo[0][col] = memo[0][col - 1];\n10\n} // if\n11\n} // for col\n12\nfor (int32_t row = 1; row < m; ++row) {\n13\nif (grid[row][0]) {\n14\nmemo[row][0] = memo[row - 1][0];\n15\n} // if\n16\n} // for row\n17\nfor (int32_t row = 1; row < m; ++row) {\n18\nfor (int32_t col = 1; col < n; ++ col) {\n19\nif (grid[row][col]) {\n20\nmemo[row][col] = memo[row - 1][col] + memo[row][col - 1];\n21\n} // if\n22\nelse {\n23\nmemo[row][col] = 0;\n24\n} // else\n25\n} // for col\n26\n} // for row\n27\nreturn memo[m - 1][n - 1];\n28\n} // count_paths()\nTo solve this problem, we can use the same approach we used to solve the longest common subsequence (LCS) problem (example 23.18),15.\nwith the special condition that we will consider the given string and its reverse (e.g., to find the longest palindromic subsequence in the\nstring \"abcabc\", we will run the same algorithm used in the longest common subsequence problem using the strings \"abcabc\" and \"cbacba\").\nThis works because running the LCS algorithm on a string and its reverse will ensure that the resulting subsequence exists in both directions\nof the string, which means that it is palindromic. One possible solution is shown below (essentially the same as the LCS solution in example\n23.18, except using the original string and its reverse instead of two separate input strings):\n1\nint32_t longest_palindromic_subsequence(const std::string& str) {\n2\nstd::string reversed = str;\n3\nstd::reverse(reversed.begin(), reversed.end());\n4\nsize_t len = str.length();\n5\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(lenmemo(len + 1, + 1));\n6\nfor (int32_t i = 1; i <= len; ++i) {\n7\nfor (int32_t j = 1; j <= len; ++j) {\n8\nif (str[i - 1] == reversed[j - 1]) {\n9\nmemo[i][j] = memo[i - 1][j - 1] + 1;\n10\n} // if\n11\nelse {\n12\nmemo[i][j] = std::max(memo[i - 1][j], memo[i][j - 1]);\n13\n} // else\n14\n} // for j\n15\n} // for i\n16\nreturn memo[len][len];\n17\n} // longest_palindromic_subsequence()", "word_count": 476, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "d37b4ef8-523a-533e-851a-a06e8a1f2232", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 934, "real_page_number": null, "text": "922\nChapter 23. Dynamic Programming\n16. This problem is a variation of the \"house robber\" problem discussed in example 23.12, with the additional constraint that the \"houses\" (in\nthis case, gold) is arranged in a circle rather than a line. This adds an additional complication to the problem, since we are no longer able to\nstart at the beginning of the array and decide whether we should take each gold or leave it, as that could potentially omit an optimal solution\nthat involves wrapping around the end. The key observation to be made here is that we can resolve the issue of the circular arrangement by\nsplitting this problem into two separate problems that we run the original \"house robber\" algorithm on — one that skips the first gold bar,\nand one that skips the last gold bar — and then taking the better solution of the two. This works because it is impossible for us to take both\nthe first and last gold bar in the input array due to the adjacency constraint, so the optimal solution must be included in one of these two\nsubproblems. One possible solution is shown below:\n1\nint32_t max_gold_weight(const std::vector<int32_t>& weights) {\n2\nsize_t num_weights = weights.size();\n3\nif (num_weights == 1) {\n4\nreturn weights[0];\n5\n} // if\n6\nreturn std::max(helper(weights, 0, num_weights - 2), helper(weights, 1, num_weights - 1));\n7\n} // max_gold_weight()\n8\n9\nint32_t helper(const std::vector<int32_t>& size_t size_tweights, left_idx, right_idx) {\n10\nint32_t best_with_prev = 0;\n// max value obtainable up to the previous gold\n11\nint32_t best_with_curr = 0;\n// max value obtainable up to the current gold\n12\nfor (size_t i = left_idx; i <= right_idx; ++i) {\n13\n// take the best between taking the current gold, or ignoring the\n14\n// current gold and instead use the best value from the previous adjacent gold\n15\nint32_t best_with_next = std::max(best_with_curr, best_with_prev + weights[i]);\n16\n// update values for next iteration\n17\nbest_with_prev = best_with_curr;\n18\nbest_with_curr = best_with_next;\n19\n} // for i\n20\nreturn best_with_curr;\n21\n} // helper()\nThis problem can be solved using the \"counting ways\" approach. The key insight to notice here is that, at any point in the string, you can17.\neither decode a single character on its own, or decode it with an adjacent character if the resulting value is between 10 and 26. In other\n𝑖thwords, the total number of ways to decode the string up to the character (1-indexed) is equal to\n𝑖−1th• the number of ways to decode up to the character (if we decode digit 𝑖on its own), plus\n𝑖−2nd• the number of ways to decode up to the character it is possible to decode digits and 𝑖together (i.e., between 10 and 26)𝑖−1if\nThe base case happens when 0, as there is exactly one way to decode nothing. To demonstrate, consider the input string \"1823\". There𝑖=\nare two ways to decode the final digit of \"3\". You can either decode the \"3\" on its own into \"C\", for which the number of ways to do so is\nthe total number of ways to decode \"182\"; or, you can decode the \"3\" alongside the previous digit of \"2\" to get \"23\" →\"W\", for which the\nnumber of ways to do so is the total number of ways to decode \"18\" on its own. A bottom-up solution is provided below:\n1\nint32_t num_ways_to_decode(const std::string& seq) {\n2\nstd::vector<int32_t> memo(seq.size() + 1);\n3\nmemo[0] = 1; // one way to decode empty string\n4\nfor (size_t i = 1; i <= seq.size(); ++i) {\n5\n// count number of ways to decode digit on its own\n6\nif (seq[i - 1] != '0') {\n7\nmemo[i] += memo[i - 1];\n8\n} // if\n9\n// count number of to decode two digits together\n10\nif (i > 1 && (seq[i - 2] == '1' || (seq[i - 2] == '2' && seq[i - 1] <= '6'))) {\n11\nmemo[i] += memo[i - 2];\n12\n} // if\n13\n} // for i\n14\nreturn memo[seq.size()];\n15\n} // num_ways_to_decode()", "word_count": 690, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8a030cee-b2ce-5762-a0a8-4fc04f8c1535", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 935, "real_page_number": null, "text": "23.5 Summary of Dynamic Programming Patterns\n923\n18. This is a decision making problem, where you have to decide whether to accept or skip a potential passenger. Given a passenger at position\n𝑖, you have two choices:\n• Skip the passenger - the most amount of money you can earn from this point on will be the maximum earnings you can earn from the\nremaining rides.\n• Take the passenger - the most amount of money you can earn from this point on will be the earnings from this ride, plus the maximum\nearnings you can earn from the remaining rides that are compatible (i.e., no overlap with the current ride).\nThis naturally yields a recursive structure. If the passenger at position 𝑖yields a net earning of 𝑘, and 𝑗is the position of the first ride that\nstarts after the current ride’s endpoint, we can define the maximum earnings from position 𝑖onwards as:\n𝑓(𝑖)=\n{\n0,\n𝑖≥𝑛if\nmax(𝑓(𝑖+1),𝑘+𝑓(𝑗)),\notherwise\nOne such solution (bottom-up) is shown below. Here, we first preprocess the given input vector by sorting the inputs by start time. Then,\nwe loop through the sorted values from back to front and solve the recurrence relation above.\n1\nint32_t max_earnings(const int32_tstd::vector<RideRequest>& rides, n) {\n2\nstd::vector<std::vector<RideRequest>> rides_by_start_time(n);\n3\nfor (const auto& request : rides) {\n4\nrides_by_start_time[request.origin].push_back(request);\n5\n} // for request\n6\nstd::vector<int32_t> memo(n + 1);\n7\nfor (size_t i = n - 1; i >= 1; --i) {\n8\nfor (const auto& request : rides_by_start_time[i]) {\n9\nauto earnings = request.dest - request.origin + request.tip;\n10\nmemo[i] = std::max(memo[i], memo[request.dest] + earnings);\n11\n} // for request\n12\nmemo[i] = std::max(memo[i], memo[i + 1]);\n13\n} // for i\n14\nreturn memo[1];\n15\n} // num_odd_subsequence()\nThis is an interval merging problem similar to the pipe merging problem discussed in example 23.15, so we can apply the logic for that19.\npattern to approach this question. Here, if we define as the minimum cost to merge all stones in the position range into𝑓(𝑖,𝑗,𝑝) [𝑖,𝑗) 𝑝\ndifferent piles, we can devise the following recurrence relation for this problem:\n𝑓(𝑖,𝑗,𝑘)=\n⎧\n⎪\n⎨\n⎪⎩\n0,\nif 𝑗and𝑖== 𝑝=1\n𝑓(𝑖,𝑗,𝑘)+sum(𝑖…𝑗),\n𝑖≠𝑗andif 𝑝=1\nmin𝑖≤𝑥<𝑗(𝑓(𝑖,𝑥,1)+𝑓(𝑥+1,𝑗,𝑘−1)),\notherwise\n𝑖≠𝑗andThe base case occurs if 𝑗and 𝑝= 1, since there is no merge necessary here to combine the same pile of stones into one pile. If𝑖=\n𝑝= 1, then the minimum cost to merge 𝑖to 𝑗into 1 pile is equal to the minimum cost to merge 𝑖to 𝑗into 𝑘piles (since we have to move 𝑘\n𝑝≠1,piles at once per move), plus the cost of merging these 𝑘piles into 1 (which is the sum of all the piles from 𝑖to 𝑗). Otherwise, if the\nminimum cost would be the minimum possible cost of merging piles 𝑖to 𝑥into 1 pile and piles to 𝑗into piles for all values of𝑥+1 𝑘−1 𝑥\nfrom 𝑖to 𝑗. A top-down implementation of this solution is provided below:\n1\nusing std::vector<std::vector<std::vector<int32_t>>>;MemoVector =\n2\n3\nint32_t merge_stones(const std::vector<int32_t>& int32_tstones, k) {\n4\nsize_t n = stones.size();\n5\nif ((n - 1) % (k - 1) != 0)\n6\nreturn -1;\n// impossible to merge into one pile, so return -1\n7\nstd::vector<int32_t> prefix_sums(n + 1);\n// pre-process the sum of stones up to ith position\n8\nfor (int32_t i = 0; i < n; ++i) {\n9\nprefix_sums[i + 1] = prefix_sums[i] + stones[i];\n10\n} // for i\n11\nstd::vector<std::vector<int32_t>>(n, std::vector<int32_t>(kMemoVector memo(n, + 1, -1)));\n12\nreturn helper(0, n - 1, 1, prefix_sums, k, memo);\n13\n} // merge_stones()\n14\n15\nint32_t helper(int32_t int32_t int32_t std::vector<int32_t>&i, j, p, prefix_sums,\n16\nint32_t k, MemoVector& memo) {\n17\nif (i == j)\n18\nreturn std::numeric_limits<int32_t>::max()p == 1 ? 0 : / 4;\n// smaller to prevent overflow\n19\nif (memo[i][j][p] != -1)\n20\nreturn memo[i][j][p];\n21\nif (p == 1)\n22\nreturn memo[i][j][p] = helper(i, j, k, prefix_sums, k, memo) +\n23\n(prefix_sums[j + 1] - prefix_sums[i]);\n24\nint32_t std::numeric_limits<int32_t>::max()cost = / 4;\n25\nfor (int32_t x = i; x < j; ++x) {\n26\ncost = std::min(cost, helper(i, x, 1, prefix_sums, k, memo) +\n27\nhelper(x + 1, j, p - 1, prefix_sums, k, memo));\n28\n} // for x\n29\nreturn memo[i][j][p] = cost;\n30\n} // helper()", "word_count": 724, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6464d1a8-d4a3-5f9c-93ce-8490e25a97be", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 936, "real_page_number": null, "text": "924\nChapter 23. Dynamic Programming\n20. To solve this problem using dynamic programming, you can think of the provided houses in terms of groups that should be assigned the\nsame bus stop (this is similar to the interval merging strategy). If you only have one bus stop that can be assigned (i.e., 1), the solution𝑘=\nwould be to place the stop at the position of the median house (e.g., if given one bus stop to serve houses 3, 5, and 9, you would place the\nbus stop at house 5 to minimize total distance). Similarly, if the number of bus stops is equal to the number of houses that need to be served,\nthe solution would be to place a stop at each of these houses for a distance of 0. These are our base cases.\nWhat if you instead had bus stops to assign to multiple houses? You can think of this problem recursively by splitting the houses into𝑘>1\nsubintervals and then computing the optimal assignment out of all the possible outcomes. As an example, if you had four houses (denoted\nA-D), and two bus stops that can be assigned, the optimal solution is the best of these options (where the square brackets denote the range\nof houses each bus stop serves):\n• [A], [B, C, D] →one bus stop serving house A, one bus stop serving houses B-D\n• [A, B], [C, D] →one bus stop serving houses A-B, one bus stop serving houses C-D\n• [A, B, C], [D] →one bus stop serving houses A-C, one bus stop serving house D\nUsing an interval merging approach, we can solve this problem recursively by iterating over all the positions where we can split the houses\ninto two groups, assigning one bus stop to the first group, and recursively assigning bus stops for the second group — taking the best𝑘−1\nsolution we encounter in the end. For instance, if we define as the solution for assigning 𝑘bus stops between houses 𝑥and 𝑦, our𝐹(𝑥,𝑦,𝑘)\noptimal solution for the four-house example above would be the smallest of:\n• 𝐹(𝐴,𝐴,1)+𝐹(𝐵,𝐷,𝑘−1)\n• 𝐹(𝐴,𝐵,1)+𝐹(𝐶,𝐷,𝑘−1)\n• 𝐹(𝐴,𝐶,1)+𝐹(𝐷,𝐷,𝑘−1)\nThe overall recurrence relation can be expressed as follows (where is the median house within the range from house 𝑥to 𝑦):𝑚𝑒𝑑𝑖𝑎𝑛(𝑥,𝑦)\n𝐹(𝑥,𝑦,𝑘)=\n⎧\n⎪\n⎨\n⎪⎩\n∑𝑦\n𝑖=𝑥|ℎ𝑜𝑢𝑠𝑒𝑠[𝑖]−ℎ𝑜𝑢𝑠𝑒𝑠[𝑚𝑒𝑑𝑖𝑎𝑛(𝑥,𝑦)]|,\nif 𝑘=1\n0,\nif (i.e., number of houses in range)𝑘=𝑦−𝑥+1\nmax𝑥≤𝑖≤𝑦(𝐹(𝑥,𝑖,1)+𝐹(𝑖+1,𝑦,𝑘−1)),\notherwise\nOne possible implementation of a top-down solution is shown below:\n1\nusing std::vector<std::vector<std::vector<int32_t>>>;MemoVector =\n2\n3\nint32_t min_stop_distance(const std::vector<int32_t>& int32_thouses, k) {\n4\nstd::vector<std::vector<int32_t>>(houses.size(),MemoVector memo(houses.size(),\n5\nstd::vector<int32_t>(k + 1, -1)));\n6\nreturn helper(0, houses.size() - 1, k, houses, memo);\n7\n} // min_stop_distance()\n8\n9\nint32_t helper(int32_t int32_t int32_tleft, right, num_stops,\n10\nconst std::vector<int>& houses, MemoVector& memo) {\n11\nint32_t num_houses = right - left + 1;\n12\nif (right < left || num_houses < num_stops) {\n13\nreturn std::numeric_limits<int32_t>::max() / 4;\n// smaller to prevent overflow\n14\n} // if\n15\nif (memo[left][right][num_stops] != -1) {\n16\nreturn memo[left][right][num_stops];\n17\n} // if\n18\nif (num_stops == 1) {\n19\nint32_t mid = (left + right) / 2;\n20\nint32_t sum_distances = 0;\n21\nfor (int32_t curr = left; curr <= right; ++curr) {\n22\nsum_distances += std::abs(houses[curr] - houses[mid]);\n23\n} // for curr\n24\nmemo[left][right][num_stops] = sum_distances;\n25\nreturn sum_distances;\n26\n} // if\n27\nint32_t std::numeric_limits<int32_t>::max()min_distance = / 4;\n28\nfor (int32_t curr = left; curr <= right; ++curr) {\n29\nint32_t curr_best = helper(left, curr, 1, houses, memo) +\n30\nhelper(curr + 1, right, num_stops - 1, houses, memo);\n31\nmin_distance = std::min(min_distance, curr_best);\n32\n} // for curr\n33\nmemo[left][right][num_stops] = min_distance;\n34\nreturn min_distance;\n35\n} // helper()", "word_count": 622, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9dd09463-b54f-51f2-98d2-459b99f95811", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 937, "real_page_number": null, "text": "23.5 Summary of Dynamic Programming Patterns\n925\n21. To solve this problem using dynamic programming, we will first think of this problem in terms of the diagonals that you can place each\nbishop on. To demonstrate, we will first number each diagonal of the chessboard:\n3 5 6 871 2 4\n9\n10\n11\n12\n13\n14\n15\nLet represent the total number of ways to place 𝑗bishops on any diagonal up to diagonal 𝑖, when considering only the squares that𝐹(𝑖,𝑗)\nshare the same color as diagonal 𝑖(since bishops on different squares cannot attack each other, they can be considered independently).\n𝑗thThere are two pathways to get here: we can either place the bishop on diagonal 𝑖and bishops on the previous diagonals of the same𝑗−1\ncolor; or, we can place no bishops on diagonal 𝑖and place all 𝑗bishops on the previous diagonals of the same color. The total number of\nways to place 𝑗bishops up to diagonal 𝑖is equal to the sum of the number of ways to place bishops under these two scenarios.\n𝑗thIf we decide to place the bishop on diagonal 𝑖, the total number of ways to place this bishop is equal to the number of squares on diagonal\n𝑖, minus for the opposite direction diagonals that are blocked by the previous bishop placements. For instance, if we wanted to𝑗−1 𝑗−1\nplace a second bishop in diagonal 5, we would only have four ways to do so even though the diagonal has five squares, since a bishop on\nsquare b7 would block a bishop from being placed on square c6.\n1\na\n2\nb\n3\nc\n4\nd\n5\ne\n6\nf\n7\ng\n8\nh\nb\n5\nThis information is enough for us to devise a recurrence relation for this problem. Our base cases occur when there are no bishops to be\nplaced, or when a singular bishop needs to be placed in the first diagonal:\n• (one way to place no bishops (i.e., empty board), no matter what 𝑖is)𝐹(𝑖,0)=1\n• (one way to place a single bishop in the first diagonal)𝐹(1,1)=1\nFor the remaining squares, the total number of ways to place 𝑗bishops up to diagonal 𝑖is equal to the sum of\n• the number of ways to place 𝑗bishops up to diagonal (subtracting two since we only consider squares of the same color)𝑖−2\n𝑗th• the number of ways to place bishops up to diagonal 𝑖−2, multiplied by the number of ways to place the bishop on diagonal𝑗−1 𝑖\n(which is equal to the number of squares in the diagonal minus (𝑗−1) for the squares threatened by the other bishops)\n𝐹(𝑖−2,𝑗)+[𝐹(𝑖−2,𝑗−1)×(𝑛𝑢𝑚_𝑠𝑞𝑢𝑎𝑟𝑒𝑠(𝑖)−(𝑗−1))]𝐹(𝑖,𝑗)=\nThe solution for the entire problem can then be obtained by summing all possible combinations of placing 𝑥bishops on the black diagonals\n≤𝑥≤𝑘.and 𝑘−𝑥bishops on the white diagonals, for Since the last black diagonal has index and the last white diagonal has index2𝑛−10\n2𝑛−2, and the placements on black and white squares can be done independently, the total number of unique ways to place 𝑘bishops is:\n∑𝑘\n𝐹(2𝑛−1,𝑥)×𝐹(2𝑛−2,𝑘−𝑥)𝑥=0\nOne implementation of a bottom-up solution is provided below:\n1\nint32_t num_bishop_placements(int32_t int32_tn, k) {\n2\nint32_t num_diagonals = 2 n - 1;*\n3\nif (k > num_diagonals) {\n4\nreturn 0;\n// not possible since more bishops than diagonals\n5\n} // if\n6\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(kmemo(num_diagonals + 1, + 1));\n7\nfor (int32_t i = 0; i <= num_diagonals; ++i) {\n8\nmemo[i][0] = 1;\n9\n} // for i\n10\nmemo[1][1] = 1;\n11\nfor (int32_t i = 2; i <= num_diagonals; ++i) {\n12\nfor (int32_t j = 1; j <= k; ++j) {\n13\nmemo[i][j] = memo[i - 2][j] + memo[i - 2][j - 1] (num_squares_in_diag(n, i) - (j - 1));*\n14\n} // for j\n15\n} // for i\n16\nint32_t total = 0;\n17\nfor (int32_t i = 0; i <= k; ++i) {\n18\ntotal += (memo[2 n - 1][i] memo[2 n - 2][k - i]);* * *\n19\n} // for i\n20\nreturn total;\n21\n} // num_bishop_placements()", "word_count": 699, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fd90d339-6734-5579-855d-31dd90f32ce7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 938, "real_page_number": null, "text": "926\nChapter 23. Dynamic Programming\nThis page has been intentionally left blank.", "word_count": 12, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "385ffd11-fef6-5a44-aa0f-9af3fbdf2d3a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 939, "real_page_number": null, "text": "Chapter 24\nThe Knapsack Problem\n24.1\nTypes of Knapsack Problems\nThe knapsack problem is an important computational problem that revolves around the following task: given a knapsack of capacity 𝐶and a\ncollection of 𝑛items, where each item 𝑖has weight 𝑤𝑖and value 𝑣𝑖, find the maximum value you can store in the knapsack without exceeding its\ncapacity. The following is an example of a knapsack problem:\nYou are given a knapsack with weight capacity 11, as well as the following 5 items. Return the maximum value you can store in your\nknapsack without going over its capacity.\nItem\n1\n2\n3\n4\n5\nWeight\n1\n2\n5\n6\n7\nValue\n1\n6\n18\n22\n28\nThere are several variations of the knapsack problem that you may encounter, each with slightly different rules on how items can be selected.\nThe following list summarizes the most common of these variations:\n• 0-1 Knapsack: Each of the 𝑛items are unique, and you must either take an item or leave it behind (hence the name 0-1). This is the\nstandard variation of the knapsack problem.\n• Fractional Knapsack: Same as the 0-1 knapsack problem, but a fractional amount of each item can be taken.\n• Bounded Knapsack: Each of the 𝑛items is not guaranteed to be unique, and may have a bounded quantity available (each item 𝑖also\nhas a quantity 𝑞𝑖that identifies how many of that item exists).\n• Unbounded Knapsack: Same as the bounded knapsack, but you have an unbounded (infinite) quantity of each item type.\nSince each of these variants presents the problem in a different way, the best algorithm to use depends on the type of knapsack problem we are\ntrying to solve. In this chapter, we will look at the four types of knapsack problems listed above and use our knowledge of algorithm families to\ndevelop an efficient solution for each.", "word_count": 310, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "66c0e38f-828c-5145-968d-be03b67c27ac", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 940, "real_page_number": null, "text": "928\nChapter 24. The Knapsack Problem\n24.2\n0-1 Knapsack\n¸ 24.2.1\nSolving 0-1 Knapsack Using Brute Force\nThe 0-1 knapsack problem is the standard variation of the knapsack problem, where you are given a knapsack of capacity 𝐶and a collection of\nand items with value 𝑣𝑖and weight 𝑤𝑖. Your goal is to find the subset of items 𝑆that maximizes total value without𝑛unique unbreakable\nexceeding the the capacity of the knapsack:\nmaximize\n(\n∑value(𝑆)=\n𝑖∈𝑆\n𝑣𝑖\n)\nsuch that\n(∑\n𝑖∈𝑆\n𝑤𝑖≤𝐶\n)\nThe simplest solution to the knapsack problem is to use brute force and generate the entire solution space, and then find the viable solution\nwithin this solution space with the largest overall value. In a 0-1 knapsack problem, each of the 𝑛items can either be or fromincluded excluded\n2𝑛subsetsour solution; since there are two possibilities for each item, there are a total of that need to be generated. An example of the brute\nforce approach is applied below:\nItem\n1\n2\n3\n4\n5\nWeight\n1\n2\n5\n6\n7\nValue\n1\n6\n18\n22\n28\nKnapsack Capacity: 11\nSubset No.\n1\n2\n3\n4\n5\nWeight\nValue\nViable?\n1\n0\n0\n0\n0\n0\n0\n0\n✓\n2\n1\n0\n0\n0\n0\n1\n1\n✓\n3\n0\n1\n0\n0\n0\n2\n6\n✓\n4\n1\n1\n0\n0\n0\n3\n7\n✓\n5\n0\n0\n1\n0\n0\n5\n18\n✓\n6\n1\n0\n1\n0\n0\n6\n19\n✓\n7\n0\n0\n0\n1\n0\n6\n22\n✓\n8\n1\n0\n0\n1\n0\n7\n23\n✓\n9\n0\n1\n1\n0\n0\n7\n24\n✓\n10\n1\n1\n1\n0\n0\n8\n25\n✓\n11\n0\n0\n0\n0\n1\n7\n28\n✓\n12\n0\n1\n0\n1\n0\n8\n28\n✓\n13\n1\n0\n0\n0\n1\n8\n29\n✓\n14\n1\n1\n0\n1\n0\n9\n29\n✓\n15\n0\n1\n0\n0\n1\n9\n34\n✓\n16\n1\n1\n0\n0\n1\n10\n35\n✓\n17\n0\n0\n1\n1\n0\n11\n40\n✓\n18\n1\n0\n1\n1\n0\n12\n41\n✗\n19\n0\n0\n1\n0\n1\n12\n46\n✗\n20\n0\n1\n1\n1\n0\n13\n46\n✗\n21\n1\n0\n1\n0\n1\n13\n47\n✗\n22\n1\n1\n1\n1\n0\n14\n47\n✗\n23\n0\n0\n0\n1\n1\n13\n50\n✗\n24\n1\n0\n0\n1\n1\n14\n51\n✗\n25\n0\n1\n1\n0\n1\n14\n52\n✗\n26\n1\n1\n1\n0\n1\n15\n53\n✗\n27\n0\n1\n0\n1\n1\n15\n56\n✗\n28\n1\n1\n0\n1\n1\n16\n57\n✗\n29\n0\n0\n1\n1\n1\n18\n68\n✗\n30\n1\n0\n1\n1\n1\n19\n69\n✗\n31\n0\n1\n1\n1\n1\n20\n74\n✗\n32\n1\n1\n1\n1\n1\n21\n75\n✗\nHere, the optimal solution would be to take items 3 and 4 for a total value of 18 + 22 = 40 and a weight of 5 + 6 = 11. However, we had to\n2𝑛possibleenumerate over subsets and sum up the weight and value of each subset to get our solution — since each summation takes time,Θ(𝑛)\nΘ(𝑛2𝑛).the overall time complexity of the brute force approach is Is there a way to do better?\n¸ 24.2.2\nWhy Greedy Fails for 0-1 Knapsack\nAn intuitive strategy would be to try a greedy approach first, since it is simpler and often hard to beat if it works. Since we want to maximize the\ntotal value of our items while minimizing total weight, there are two greedy strategies that immediately jump out:\n1. Greedily select the items with the first.highest value\n2. Greedily select the items with the first.lowest weight\nHowever, if we apply either of these greedy strategies to the example, we will see that they do not work. If we greedily select items with higher\nvalue first, we would take item 5 first, and then item 2 (since this is the best we can do without going over capacity after selecting item 5), and\nthen item 1. The total value of this solution is 28 + 6 + 1 = 35, which is not optimal. Similarly, if we greedily select items with lower weight\nfirst, we would take item 1 first, then item 2, then item 3. The value of this solution is 1 + 6 + 18 = 25, which is also not optimal.", "word_count": 755, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9b1f17c9-195f-551f-864f-9a78fda83ffe", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 941, "real_page_number": null, "text": "24.2 0-1 Knapsack\n929\nIn general, the approach of selecting higher value items first may not work if large items have high value (such as item 5), which forces us to\nincur a large weight cost after greedily selecting a high-value item. On the other hand, the approach of selecting lower weight items first could\nfail if small items have low value (such as item 1), which causes us to gain little value for the weight we take on. Therefore, to find a valid\ngreedy approach (if it exists), we would need to consider both the weight and value of each item in tandem.\nThis leads us to a third approach: greedily selecting items with the highest (or ratio) first. The value density ofvalue density value-weight\nan item is its value divided by its weight (e.g., value/weight). This allows us to maximize the value we get from each unit of weight we take on,\ninstead of blindly taking on valuable or light items without considering the other variable. However, if we try this strategy on the example, it\nalso does not produce an optimal solution: we would end up taking item 5 first, then 2, then 1 for a total value of 35.\nItem\n1\n2\n3\n4\n5\nWeight\n1\n2\n5\n6\n7\nValue\n1\n6\n18\n22\n28\nRatio\n1\n3\n3.6\n3.67\n4\nUnfortunately, this greedy strategy could still fail for the 0-1 knapsack problem if large items have a high value density (such as item 5). By\ntaking item 5, we were prevented from taking the next best item (item 4) because our knapsack could not support it.\nIf considering the value-weight ratio failed to find a valid greedy solution, is there even a way to do better? It actually turns out that the\nanswer is no: problem! If we wanted to solve 0-1there exists no greedy strategy that guarantees an optimal solution for the 0-1 knapsack\nknapsack, we would have to rely on a slightly more complicated algorithm family.\n¸ 24.2.3\nSolving 0-1 Knapsack Using Dynamic Programming\nOne approach that we can use to solve the 0-1 knapsack problem is dynamic programming, as it is perfectly follows the decision making problem\nstructure discussed in the previous chapter. To solve 0-1 knapsack using dynamic programming, we will iterate over each item and use the\nsolutions of previously computed subproblems to determine we should take the item or leave it behind. To illustrate this, let’s define as𝐹(𝑖,𝑐)\n𝑖ththe optimal value attainable from considering the first 𝑖items with a knapsack of capacity 𝑐, with 𝑤𝑖and 𝑣𝑖as the weight and value of the\nitem. We can express recursively by noticing that there are only two possibilities for each item 𝑖: it can either be taken or left behind.𝐹(𝑖,𝑐)\n• If item 𝑖is behind, the maximum value attainable must be equal to 𝐹(𝑖−1,𝑐), or the optimal solution of the first items with a𝑖−1left\n𝑖thknapsack capacity 𝑐. This is because, if the item is not included, the best you can do is equal to the solution of the subproblem that\ndoes not consider this item.\n• If item 𝑖is taken, the maximum value attainable must be equal to 𝐹(𝑖−1,𝑐−𝑤𝑖)+𝑣𝑖, or the optimal solution of the first items with𝑖−1\n𝑖th 𝑖tha knapsack capacity 𝑐−𝑤𝑖, plus the value of the item. This is because, if you want to include the item in your knapsack, the earlier\nitems can only take up at most 𝑐−𝑤𝑖weight so you don’t go over capacity.\nTherefore, to compute 𝐹(𝑖,𝑐), we simply have to take the better of these two values. The base case of this problem occurs when (no items𝑖=0\nto choose from), which implies that the best value attainable is also 0 (since you cannot take any items). Putting this all together, we end up with\nthe following recurrence relation:\n𝐹(𝑖,𝑐)=\n{\n0,\nif 𝑖=0\nmax(𝐹(𝑖−1,𝑐), 𝐹(𝑖−1,𝑐−𝑤𝑖)+𝑣𝑖),\nif 𝑖>0, 𝑐>0\nA top-down solution is shown below — in this solution, we simply make the recursive calls reflected in the above recurrence relation and write\nour solutions to a memo, where memo[𝑖][𝑐] stores the solution to 𝐹(𝑖,𝑐).\n1\nstruct Item {\n2\nint32_t weight;\n3\nint32_t value;\n4\n};\n5\n6\nint32_t knapsack_01(int32_t constc, std::vector<Item>& items) {\n7\nconst size_t n = items.size();\n8\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(cmemo(n + 1, + 1, -1));\n9\nreturn knapsack_helper(c, items, n - 1, memo);\n10\n} // knapsack_01()\n11\n12\nint32_t knapsack_helper(int32_t const int32_tc, std::vector<Item>& items, i,\n13\nstd::vector<std::vector<int32_t>>& memo) {\n14\nif (i < 0) {\n15\nreturn 0;\n16\n} // if\n17\nif (memo[i][c] != -1) {\n18\nreturn memo[i][c];\n19\n} // if\n20\nint32_t val_excluded = knapsack_helper(c, items, i - 1, memo);\n21\n// optimization: if weight of item already over capacity, it must be excluded\n22\nif (items[i].weight > c) {\n23\nreturn memo[i][c] = val_excluded;\n24\n} // if\n25\nint32_t val_included = knapsack_helper(c - items[i].weight, items, i - 1, memo) + items[i].value;\n26\nreturn memo[i][c] = std::max(val_excluded, val_included);\n27\n} // knapsack_helper()", "word_count": 847, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "49e7be53-739c-571d-bdb0-eea41dfa5372", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 942, "real_page_number": null, "text": "930\nChapter 24. The Knapsack Problem\nA bottom-up approach follows the same idea, but starts with the base cases and builds upwards toward the final solution. To implemement a\nbottom-up solution, we will need a double nested loop: an outer loop that iterates over all the items, and an inner loop that iterates over all\nknapsacks from capacity 0 to 𝑐. A bottom-up implementation is shown below:\n1\nstruct Item {\n2\nint32_t weight;\n3\nint32_t value;\n4\n};\n5\n6\nint32_t knapsack_01(int32_t constc, std::vector<Item>& items) {\n7\nconst size_t n = items.size();\n8\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(cmemo(n + 1, + 1, 0));\n9\nfor (size_t i = 0; i < n; ++i) {\n10\nfor (size_t j = 0; j < c + 1; ++j) {\n11\nif (j < items[i].weight) {\n12\n// weight of item already over capacity, exclude\n13\nmemo[i + 1][j] = memo[i][j];\n14\n} // if\n15\nelse {\n16\nmemo[i + 1][j] = std::max(memo[i][j], memo[i][j - items[i].weight] + items[i].value);\n17\n} // else\n18\n} // for j\n19\n} // for i\n20\nreturn memo[n][c];\n21\n} // knapsack_01()\nTo illustrate how this works, we will use the example provided. We start off by initiating a memo of size to store the solutions of(𝑛+1)×(𝑐+1)\nour subproblems. Since any subproblem where has a solution of 0, row 0 of our memo must be entirely filled out with 0s.𝑛=0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n3\n4\n5\nCapacity\nItem\nWe will then consider row 1, which considers all items up to item 1. Item 1 has a weight of 1, which does not fit in a knapsack with capacity 0,\nso memo[1][0] gets set to 0.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n2\n3\n4\n5\nCapacity\nItem\nHowever, if our knapsack capacity increases to 1, we would be able to add item 1 to our knapsack. This increases our value to 1, which is\nassigned to memo[1][1].\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n2\n3\n4\n5\nCapacity\nItem", "word_count": 404, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dc82fa30-c0dc-5020-9e10-b240e09a4782", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 943, "real_page_number": null, "text": "24.2 0-1 Knapsack\n931\nSince row 1 involves subproblems that only consider up to item 1, the optimal solution will always stay at 1 for these subproblems even as\ncapacity increases, as you only have one item (of value 1) to pick from. The remainder of this row is filled out as follows:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n3\n4\n5\nCapacity\nItem\nNext, we move on to row 2, which brings item 2 (of weight 2 and value 6) into consideration. Similar to before, if our knapsack capacity\nis 0, then we cannot add any items, and our value is consequently 0. Therefore, memo[2][0] gets assigned to a value of 0. Notice that this\nifgets handled by the check on lines 11-13 of the bottom-up code; since the capacity (0) is less than the weight of item 2 (6), we just assign\nmemo[2][0] to the value of memo[1][0].\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n3\n4\n5\nCapacity\nItem\nThis also applies to the solution of memo[2][1]. Since item 2 does not fit if our knapsack capacity is 1, we simply assign memo[2][1] to the\nvalue of the cell directly above, or memo[1][1], which is the optimal value without considering item 2.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n3\n4\n5\nCapacity\nItem\nWhen we move to a knapsack capacity of 2, however, item 2 is able to fit in our knapsack. Thus, we are left with a choice: we can either exclude\nitem 2 or include it in our knapsack. If we exclude item 2, then the best we can do is the value of the cell directly above, or memo[1][2]. This is\nbecause memo[1][2] stores the optimal solution of considering only items up to item 1 with a knapsack of size 2 (i.e., the same problem, but\nwith item 2 omitted).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n3\n4\n5\nCapacity\nItem", "word_count": 454, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0f38c43c-193a-5428-9bfd-eb953b53c912", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 944, "real_page_number": null, "text": "932\nChapter 24. The Knapsack Problem\nOn the other hand, if we include item 2, then the best we can do is the value of item 2, plus the best value of choosing the remaining items with\nthe capacity left over. In this case, the capacity left over after adding item 2 is 0 (which we know from memo[1][0]), so we will add the value of\nitem 2 to memo[1][0] to get the best value attainable from including item 2.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n3\n4\n5\nItem\n1\n2\n3\n4\n5\nWeight\n1\n2\n5\n6\n7\nValue\n1\n6\n18\n22\n28\n+\nCapacity\nItem\nIf we include item 2, the best value we can get is 6, which is better than the best value of 1 if we exclude item 2. Therefore, the value of\nmemo[2][2] is determined to be 6.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n6\n3\n4\n5\nCapacity\nItem\nThis process is repeated for the rest of the table. For example, here are the two values we need to reference to solve memo[5][11]: the best value\nof excluding item 5 is 40 (memo[4][11]), and the best value of including item 5 is 7 + 28 = 35 (value[5] + memo[4][11 - weight[5]]). Since 40\nis better than 35, it is the solution for memo[5][11] (and also our entire problem).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n3\n0\n1\n6\n7\n7\n18\n19\n24\n25\n25\n25\n25\n4\n0\n1\n6\n7\n7\n18\n22\n23\n28\n29\n29\n40\n5\n0\n1\n6\n7\n7\n18\n22\n28\n29\n34\n35\nCapacity\nItem\nThe best value attainable from excluding item 5 is 40.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n3\n0\n1\n6\n7\n7\n18\n19\n24\n25\n25\n25\n25\n4\n0\n1\n6\n7\n7\n18\n22\n23\n28\n29\n29\n40\n5\n0\n1\n6\n7\n7\n18\n22\n28\n29\n34\n35\nItem\n1\n2\n3\n4\n5\nWeight\n1\n2\n5\n6\n7\nValue\n1\n6\n18\n22\n28\n+\nCapacity\nItem\nThe best value attainable from including item 5 is 7 + 28 = 35.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n3\n0\n1\n6\n7\n7\n18\n19\n24\n25\n25\n25\n25\n4\n0\n1\n6\n7\n7\n18\n22\n23\n28\n29\n29\n40\n5\n0\n1\n6\n7\n7\n18\n22\n28\n29\n34\n35\n40\nCapacity\nItem\n40 is better than 35, so we exclude item 5 to get our solution of 40.", "word_count": 632, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e0b32c7c-52d2-50a5-80aa-2c5893094c7b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 945, "real_page_number": null, "text": "24.2 0-1 Knapsack\n933\nGiven a knapsack of capacity 𝐶and 𝑛items to choose from, a dynamic programming solution to the 0-1 knapsack problem takes time,Θ(𝑛𝐶)\nsince there are at most subproblems that need to be solved once, each taking time. The auxiliary space used by the above solution isΘ(𝑛𝐶) Θ(1)\nalso Θ(𝑛𝐶), since a memo of this size is declared. However, if you are using a bottom-up approach, you can reduce this space complexity to\nusing the memo optimization strategy covered in the previous chapter (since you only need to keep track of the two most recent rows toΘ(𝐶)\nsolve any subproblem).\nIf we wanted to find the actual set of items we should take instead of just the optimal value, we can simply work backward using our\ncompleted memo. To reconstruct the knapsack items, we start at the solution cell and consider whether each item should be taken or left behind\nby referencing the subproblems where the item is and isn’t taken. We then use this decision to walk through our memo, all the way back to a\nbase case. For instance, to identify which items we should take for the provided example, we would start at memo[5][11] and first determine if\nitem 5 should be taken. The maximum value attainable from including item 5 is 35 (memo[𝑖−1][𝑐−𝑤𝑖] + 𝑣𝑖= memo[4][4] + 28 = 35), and\nthe maximum value attainable from excluding item 5 is 40 (memo[𝑖−1][𝑐] = memo[4][11] = 40), so item 5 should be excluded.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n3\n0\n1\n6\n7\n7\n18\n19\n24\n25\n25\n25\n25\n4\n0\n1\n6\n7\n7\n18\n22\n23\n28\n29\n29\n40\n5\n0\n1\n6\n7\n7\n18\n22\n28\n29\n34\n35\n40\nItem\n1\n2\n3\n4\n5\nTaken?\n✗\nCapacity\nItem\nNow that we have determined that item 5 should not be taken, we will now need to determine if item 4 should be included. If item 4 is excluded,\nthen the best value we can still obtain is 25, from the value of memo[3][11]. However, if item 4 is included, the best value we can obtain is\nmemo[3][5] + 22 = 18 + 22 = 40 (this is because taking item 4 would leave us with a remaining capacity of 11 - 6 = 5, and we would gain 22\nunits of value). Therefore, item 4 should be included in our knapsack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n3\n0\n1\n6\n7\n7\n18\n19\n24\n25\n25\n25\n25\n4\n0\n1\n6\n7\n7\n18\n22\n23\n28\n29\n29\n40\n5\n0\n1\n6\n7\n7\n18\n22\n28\n29\n34\n35\n40\nItem\n1\n2\n3\n4\n5\nTaken?\n✓\n✗\nCapacity\nItem\nWe now move on to item 3. If we exclude item 3, then the best value we can attain is 7, from memo[2][5]. If include item 3, then the best value\nwe can attain is memo[2][0] + 18 = 18. Therefore, item 3 should be included in our knapsack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n3\n0\n1\n6\n7\n7\n18\n19\n24\n25\n25\n25\n25\n4\n0\n1\n6\n7\n7\n18\n22\n23\n28\n29\n29\n40\n5\n0\n1\n6\n7\n7\n18\n22\n28\n29\n34\n35\n40\nItem\n1\n2\n3\n4\n5\nTaken?\n✓\n✓\n✗\nCapacity\nItem\nOur knapsack is out of space, so we can immediately conclude that items 1 and 2 must not be included. The final solution would therefore be to\ntake items 3 and 4 to obtain our optimal value of 40.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n0\n1\n6\n7\n7\n7\n7\n7\n7\n7\n7\n7\n3\n0\n1\n6\n7\n7\n18\n19\n24\n25\n25\n25\n25\n4\n0\n1\n6\n7\n7\n18\n22\n23\n28\n29\n29\n40\n5\n0\n1\n6\n7\n7\n18\n22\n28\n29\n34\n35\n40\nItem\n1\n2\n3\n4\n5\nTaken?\n✗\n✗\n✓\n✓\n✗\nCapacity\nItem", "word_count": 848, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "540f1e94-313e-53d6-bb0f-37dc3b4372a7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 946, "real_page_number": null, "text": "934\nChapter 24. The Knapsack Problem\nThe implementation of the knapsack reconstruction process is shown below.\n1\nstruct Item {\n2\nint32_t weight;\n3\nint32_t value;\n4\n};\n5\n6\nstd::vector<bool> reconstruct_knapsack(int32_t constc, std::vector<Item>& items) {\n7\nconst size_t n = items.size();\n8\nsize_t rem_cap = c; // remaining capacity\n9\nstd::vector<bool> false);taken(n,\n10\nfor (int32_t i = n - 1; i >= 0; --i) {\n11\nif (items[i].weight <= rem_cap) {\n12\nif (memo[i][rem_cap - items[i].weight] + items[i].value >= memo[i][rem_cap]) {\n13\ntrue;taken[i] =\n14\nrem_cap -= items[i].weight;\n15\n} // if\n16\n} // if\n17\n} // for i\n18\nreturn taken;\n19\n} // reconstruct_knapsack()\nSince reconstructing the knapsack from a completed memo only requires a single iteration over all the items, this process has a time complexity\nof Θ(𝑛). This doesn’t affect the overall time complexity of solving the entire 0-1 knapsack problem, however, since you still have to complete\nthe memo before you can reconstruct the solution.\n¸ 24.2.4\nSolving 0-1 Knapsack Using Branch and Bound\nThe knapsack problem is an example of an optimization problem, so branch and bound can also be used. Since knapsack is a maximization\nproblem, we will need an upper bound that identifies an optimistic estimate for each partial solution and a lower bound that identifies the best\ncomplete solution encountered so far. The components needed to solve 0-1 knapsack using branch and bound are summarized below:\npromising() true• The function should return if the collection of items chosen in a partial solution does not overfill the knapsack\n(i.e., total weight < 𝐶).\nThe lower bound should be initialized to the and then updated if a better solution is• highest possible underestimate of the optimal solution\nfound. After branch and bound runs to completion, the value of the lower bound is the optimal solution.\n– We want to start with an underestimate because we don’t want to prune off the actual best solution — if we end up with a starting\nestimate that is too high, then our algorithm would think that the optimal solution isn’t good enough and would therefore overprune.\nOn the other hand, we still want this underestimate to be as high as reasonably possible (without sacrificing performance to obtain\nthis estimate) since a closer bound allows us to prune faster.\n– A good strategy is to use the greedy 0-1 knapsack solution (by value density) as the initial lower bound. Although the greedy\napproach does not guarantee an optimal solution for 0-1 knapsack, it can be used to produce a \"close enough\" underestimate\nrelatively quickly.\nThe upper bound for any partial solution should be calculated by adding the sum of the existing items’ values with an of the• overestimate\nvalue that can fit in the remaining capacity.\n– We want an overestimate because the upper bound serves as a limit on the best outcome from extending a partial solution. If the\nupper bound is less than the lower bound (which is the best solution we know so far), then the partial solution be optimalcannot\nand thus can be pruned.\n– A good strategy is to use the greedy knapsack solution of the remaining items (by value density) as the upper bound.fractional\n(A fractional knapsack allows you to break items apart and take only a part of an item, where the proportion of weight taken\nreflects the proportion of value obtained.) Because the fractional knapsack problem gives you the ability to break items apart for\npartial value, its solution cannot be worse than that of the 0-1 knapsack problem (and would thus be a good bound on the value of\ncontinuing a partial solution).", "word_count": 616, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1ea105b5-a1d8-58f5-933a-8cd4cbfd1280", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 947, "real_page_number": null, "text": "24.3 Fractional Knapsack\n935\nIt should be noted that, unlike TSP, there is no additional check we need to make to determine if a promising solution is also a valid solution\n(since any promising solution can be a valid solution). Furthermore, we only need to generate all of items rather than permutations,combinations\nas the ordering of items in our knapsack does not matter. The state-space tree of the 0-1 knapsack problem is shown below, where each branch\nrepresents a decision to include (left branch) or exclude (right branch) the next item. This tree represents the solution space that we are exploring\nwhile running our branch-and-bound algorithm.\n[ ]\n[ ]\n[ ]\n[ ]\n...\n...\n[3]\n...\n...\ntakeitem3\nleaveitem3\n[2]\n[2]\n...\n...\n[2,3]\n...\n...\ntakeitem3\nleaveitem3\ntakeitem2\nleaveitem2\n[1]\n[1]\n[1]\n...\n...\n[1,3]\n...\n...\ntakeitem3\nleaveitem3\n[1,2]\n[1,2]\n...\n...\n[1,2,3]\n...\n...\ntakeitem3\nleaveitem3\ntakeitem2\nleaveitem2\ntakeitem1\nleaveitem1\nEach of the nodes of the tree represents the \"state\" of a partial solution that our algorithm may encounter. Since there are two choices we can\n2𝑛whenmake at each state, the total number of states in the tree is given 𝑛items to choose from. Furthermore, if we assume that a greedy\nfractional knapsack problem is solved at each state to estimate an upper bound, the overall worst-case time complexity of the branch-and-bound\nsolve.1𝑂(𝑛2𝑛),approach is bounded by since each fractional knapsack problem takes time to However, as mentioned in earlier chapters,𝑂(𝑛)\nthis worst-case complexity bound isn’t reflective of the actual performance of branch-and-bound on a 0-1 knapsack; this is because the worst\ncase only occurs if we get unlucky while pruning and end up exploring most of the search space, a scenario that almost never happens in practice.\n24.3\nFractional Knapsack\nThe fractional knapsack problem is a variation of the 0-1 knapsack problem that allows you to take a portion of an item for partial value. For\ninstance, if you had a knapsack of capacity 5 and an item with weight 15, you could break that item up and only take the portion that fits (for\n1∕3rd= of the value). To illustrate this, consider the same set of items provided earlier:5∕15\nItem\n1\n2\n3\n4\n5\nWeight\n1\n2\n5\n6\n7\nValue\n1\n6\n18\n22\n28\nKnapsack Capacity: 11\nPreviously, the optimal 0-1 knapsack solution was to take items 3 and 4 for a combined value of 40. However, if we allow items to be broken up,\n2∕3rdthe optimal solution would then be to take item 5 and of item 4 for a total value of 28 + (2∕3)(22) = 42 2∕3.\nUnlike the 0-1 knapsack problem, greedily selecting items with the highest value density for the fractional knapsack problem actually\nsolution! Consider the example above: if we compute the value density of each of the items, we would first add item 5 toguarantees an optimal\nour knapsack, and then item 4 (until we run out of capacity): this gives us the optimal solution of 42 2∕3. In fact, we can prove that the greedy\napproach (by value density) works for all cases of the fractional knapsack problem:\nFrame the problem as one in which only one subproblem remains after a greedy choice is made.\nIn this case, the greedy choice would be to consider the item with the highest value density and add as much of it as we can to our knapsack\nwithout going over capacity. After making this greedy choice, we are left with the subproblem of solving the fractional knapsack problem using\nthe remaining items with the capacity left over.\nShow that the greedy-choice property is satisfied, such that at least one optimal solution to the original problem includes the first greedy choice\nmade by the greedy algorithm.\nThis was the step that failed for the 0-1 knapsack problem, as there was no guarantee that the first greedy choice was optimal. However, things\nare a bit different if we allow items to be broken up for partial value. We can use a proof by contradiction to show that the greedy choice for the\nfractional knapsack problem is included in at least one optimal solution.\n𝑣𝑖Let’s assume that some item 𝑖has the highest value density\n𝑤𝑖among all the items available. Since the greedy approach takes as much\nof item 𝑖as possible, we first assume that the optimal solution does not take as much of item 𝑖as possible (this also implies the knapsack is\ncompletely filled, since you can add more of item 𝑖to get a better solution if the knapsack were not full). Since item 𝑖has the highest value\ndensity, the optimal solution must have taken some other item 𝑗such that\n𝑣𝑗\n𝑤𝑗≤𝑣𝑖\n𝑤𝑖. As a result, we can take out any 𝑥weight of item 𝑗in\n1Thisassumesastandardimplementationofthefractionalknapsackalgorithmwheretheitemshavealreadybeensortedbyvaluedensity. Iftheitemsarenot\nalreadysortedbyvaluedensity,thentheyshouldbesorted,whichtakes time. However,thissortingstepisonlydoneonceatthebeginningoftheΘ(𝑛log(𝑛))\nalgorithm,sothe workisalower-ordertermanddoesnotcontributetotheoveralltimecomplexityclassoftheproblem. (IfyouarereadingthisandΘ(𝑛log(𝑛))\ndon’tknowwhatfractionalknapsackevenis,don’tworry;itwillbecoveredinthenextsection.)", "word_count": 937, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0e16ce50-2063-541e-a472-29e1583a6fcc", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 948, "real_page_number": null, "text": "936\nChapter 24. The Knapsack Problem\nthe knapsack and replace it with 𝑥weight of item 𝑖; this changes the value of our knapsack by 𝑥\n( 𝑣𝑖\n𝑤𝑖−\n𝑣𝑗\n𝑤𝑗\n)\n≥0., which is This produces a\ncontradiction, as we can replace the contents of a knapsack without item 𝑖with item 𝑖to get a total value that is no worse than our original\nsolution! As a result, we have proven that there exists an optimal solution that includes the item with the highest value density, and that the\ngreedy-choice property holds.\nShow that the problem exhibits an optimal substructure, where the optimal solution of the remaining subproblem can be combined with the\ngreedy choice to obtain an optimal solution for the original problem.\nLet𝑆representtheoriginalfractionalknapsackproblemwearetryingtosolve, whichhasoptimalsolution𝑉forweight𝑊. Ifthegreedychoice\n𝑆′ 𝑊′selects item 𝑖, an optimal substructure would imply that the solution to the subproblem with knapsack capacity 𝑊−𝑊𝑖is𝑆−{𝑖}= =\n𝑉′equal to 𝑉−𝑉𝑖, where 𝑊𝑖and 𝑉𝑖represent the weight and value of item 𝑖.=\n𝑉′We will now use a proof by contradiction to show that an optimal substructure must exist. Let’s assume that is not the optimal solution\n𝑆′ 𝑉′′ 𝑉′. 𝑉′′of and that another solution exists that is better than If that were the case, we could simply take the items that give us a value of\n𝑉′′ 𝑉′and then add item 𝑖to get a value of +𝑉𝑖for our original problem 𝑆, which is better than +𝑉𝑖. However, we know that the optimal\n𝑉′ 𝑆′solution to 𝑆is 𝑉, which is a contradiction! As a result, we can conclude that the optimal solution of must be otherwise,+𝑉𝑖= 𝑉−𝑉𝑖…\nwe would be able to construct a solution for 𝑆that is better than the optimal solution of 𝑉. We have successfully proven that the fractional\nknapsack problem exhibits an optimal substructure.\nBecause the strategy of greedily selecting items with the highest value density satisfies the greedy-choice property and exhibits optimal\nsubstructure, we can conclude that it guarantees an optimal solution for any fractional knapsack problem.\nSimilar to other greedy algorithms, the greedy solution to fractional knapsack boils down to the following steps:\n• Sort the items in order of value density (this allows us to quickly determine which item we should consider at each step).\n• Greedily select the available item with the largest value density and add as much of it to the knapsack without going over capacity. Repeat\nthis until the knapsack is full.\nThe code for a greedy implementation is shown below:\n1\nstruct Item {\n2\nint32_t weight;\n3\nint32_t value;\n4\n};\n5\n6\nbool compare(const constItem& i1, Item& i2) {\n7\ndouble static_cast<double>(i1.value)ratio1 = / i1.weight;\n8\ndouble static_cast<double>(i2.value)ratio2 = / i2.weight;\n9\nreturn ratio1 > ratio2;\n10\n}; // compare()\n11\n12\ndouble fractional_knapsack(int32_t c, std::vector<Item>& items) {\n13\n// sort items in decreasing order of value density\n14\nstd::sort(items.begin(), items.end(), compare);\n15\nint32_t curr_weight = 0;\n16\ndouble curr_value = 0.0;\n17\nfor (size_t i = 0; i < items.size(); ++i) {\n18\n// add item completely if possible\n19\nif (curr_weight + items[i].weight <= c) {\n20\ncurr_weight += items[i].weight;\n21\ncurr_value += items[i].value;\n22\n} // if\n23\n// otherwise, add fractional part of it\n24\nelse {\n25\ndouble partial_weight = c - current_weight;\n26\ncurr_value += (items[i].value partial_weight / items[i].weight);*\n27\nbreak;\n28\n} // else\n29\n} // for i\n30\nreturn curr_value;\n31\n} // fractional_knapsack()\nGiven 𝑛items to choose from, the sorting step takes time and the item selection step takes time. Combining these two stepsΘ(𝑛log(𝑛)) Θ(𝑛)\ntogether, we get an overall time complexity of Θ(𝑛log(𝑛)), since the sorting step is the bottleneck of the algorithm (everything else is a lower\norder term). However, if the items are already presorted, then the time complexity would drop to Θ(𝑛), as a single linear pass would be sufficient\nfor computing the optimal solution.", "word_count": 675, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "46c9e3d7-21f2-5221-bf87-39184a203d49", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 949, "real_page_number": null, "text": "24.4 Bounded Knapsack\n937\n24.4\nBounded Knapsack (✽)\nThe standard 0-1 knapsack problem we discussed earlier involved only a instance of each item. However, what if of eachsingle identical copies\nitem are available? This leads us to a variation of the standard 0-1 knapsack problem known as the bounded knapsack problem, which restricts\nthe number of copies of each item 𝑖to some non-negative integer 𝑞𝑖. The following is an example of a bounded knapsack problem, where you\nare given 8 copies of item 1, 7 copies of item 2, 3 copies of item 3, 2 copies of item 4, 6 copies of item 5, and a knapsack capacity of 45.\nItem\n1\n2\n3\n4\n5\nWeight\n1\n2\n5\n6\n7\nValue\n1\n6\n18\n22\n28\nQuantity\n8\n7\n3\n2\n6\nKnapsack Capacity: 45\nThe optimal solution for this knapsack problem can be attained by taking 5 copies of item 5 (for a total value of 28 5 = 140 using weight 7 5× ×\n= 35) and then taking 2 copies of item 3 (for a total value of 18 2 = 36 using weight 5 2 = 10), which results in an overall value of 140 + 36× ×\n= 176. However, this solution isn’t inherently obvious, and it cannot be attained using the greedy by value density approach (which would give\nus a suboptimal solution of 175). How can we devise an algorithm that can efficiently solve the bounded knapsack problem?\nOne simple yet effective approach is to \"refactor\" the bounded knapsack problem into a standard 0-1 knapsack problem by considering each\nof the duplicate items as its own separate item. Instead of evaluating the above example as a collection of 5 items to choose from, we will\ninstead treat the input as a collection of 26 items: 8 of which have weight 1 and value 1, 7 of which have weight 2 and value 6, 3 of which have\nweight 5 and value 18, and so on.\nItem\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n…\n26\nWeight\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n5\n5\n5\n6\n…\n7\nValue\n1\n1\n1\n1\n1\n1\n1\n1\n6\n6\n6\n6\n6\n6\n6\n18\n18\n18\n22\n…\n28\nHowever, notice that the number of subproblems we need to consider (and thus the size of the memo) can get quite large as a consequence of\nthis strategy. Previously, the time complexity of the standard 0-1 knapsack problem using dynamic programming was Θ(𝑛𝐶), where 𝑛is the\nnumber of items we are given and 𝐶is the capacity of the knapsack. However, if we convert the bounded knapsack problem into a 0-1 knapsack\n∑𝑛problem, the overall time complexity would become Θ(𝑁𝐶), where 𝑁=\n𝑞𝑖.𝑖=1\nExactly how bad can this get? Even though the quantity of any item 𝑞𝑖can take on any non-negative integer in the bounded knapsack\n⌊𝑐∕𝑤𝑖⌋copiesproblem, it practically has a maximum value, since no more than of item 𝑖can fit in the knapsack.\n𝑞𝑖≤⌊𝑐\n𝑤𝑖⌋, 𝑖=1,…,𝑛\n⌊𝑐∕𝑤𝑖⌋for ⌊𝑐∕𝑤𝑖⌋withoutIf𝑞𝑖weresomehowlargerthan someitem𝑖,wecansimplyreplace𝑞𝑖with changingthesolution,sinceanyadditional\ncopies would have no chance of fitting in the knapsack in the first place (and therefore can be ignored). If we consider this cap on the quantity of\nitems available, we can conclude that\n𝑁≤⌊𝑐\n⌋+⌊𝑐𝑤1\n⌋+…+⌊𝑐𝑤2\n𝑤𝑛⌋<𝑛𝐶\nIn the worst case, 𝑂(𝑛𝐶). Therefore, the worst-case time complexity of solving the bounded knapsack problem, if you directly convert it𝑁=\n𝑂(𝑛𝐶2).into a 0-1 knapsack problem by considering each item individually, is However, it turns out there is a more𝑂(𝑁𝐶) 𝑂(𝑛𝐶×𝐶)= =\nefficient way to convert the bounded knapsack problem into a 0-1 knapsack problem.\nTo come up with this better method, note that every number can be written as the sum of distinct powers of two:\n1\n=\n20\n2\n=\n21\n3\n=\n21+20\n4\n=\n22\n5\n=\n22+20\n6\n=\n22+21\n7\n=\n22+21+20\n8\n=\n23\nSince the inclusion or exclusion of each power of two is a binary decision for each sum, we can consider our items in groups of powers of two\nrather than individually. This gives us the same behavior as considering every item individually, since any value we want can be simulated by\ncombining these powers of two. For instance, we are given 8 copies of item 1 in the previous example. The naïve approach would consider all 8\ncopies individually as its own item.\nItem\n1\n2\n3\n4\n5\n6\n7\n8\nWeight\n1\n1\n1\n1\n1\n1\n1\n1\nValue\n1\n1\n1\n1\n1\n1\n1\n1", "word_count": 815, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c43be10e-a74e-588b-8ae3-d29f1925adfb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 950, "real_page_number": null, "text": "938\nChapter 24. The Knapsack Problem\nHowever, the smarter approach would aggregate the items into groups of powers of two (combining any remaining items left over as its own\nitem at the very end):\nItem\n1\n2\n3\n4\n5\n6\n7\n8\nWeight\n1\n1\n1\n1\n1\n1\n1\n1\nValue\n1\n1\n1\n1\n1\n1\n1\n1\nItem\n1\n2\n3\n4\nWeight\n1\n2\n4\n1\nValue\n1\n2\n4\n1\n∑𝑛This still allows us to take any valid amount of the item that we want, just without having to increase our table size to\n𝑞𝑖. For instance, if𝑖=1\nwe wanted to take 6 copies of the original item, we can replicate this behavior using the new table by taking the aggregated items of weights 2\nand 4 (instead of considering each of the 6 items as separate and distinct, which we did before).\nUsing this strategy, we can convert the previous table of 26 columns into one with 14, halving a dimension of our problem!\nItem\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nWeight\n1\n2\n4\n1\n2\n4\n8\n5\n10\n6\n6\n7\n14\n21\nValue\n1\n2\n4\n1\n6\n12\n24\n18\n36\n22\n22\n28\n56\n84\nWhat is the worst-case time complexity of this modified strategy? Since we are grouping each item using powers of two, the total number of\n∑𝑛item groups 𝑁we may end up with is at most\n𝑖=1\n(⌊log2(𝑞𝑖)⌋+1) ∑𝑛instead of\n𝑞𝑖(since each item 𝑖can be split into groups,(𝑞𝑖)log2𝑖=1\nplus one for any amount left over after the largest possible power of two). Using the same rules as before and applying some logarithm rules,\nwe can see that 𝑁is at worst using this approach, giving our converted 0-1 knapsack problem a worst-case time complexity of𝑂(𝑛log(𝐶))\n𝑂(𝑛𝐶2)𝑂(𝑛𝐶log(𝐶)). This is better than the time we obtained earlier from considering every copy as a separate𝑂(𝑁𝐶) 𝑂(𝑛log(𝐶)×𝐶)= =\nknapsack_01()item! An implementation of this solution is shown below (reusing the method implemented in section 24.2.3):\n1\nstruct Item {\n2\nint32_t weight;\n3\nint32_t value;\n4\n};\n5\n6\nstruct ItemWithQuantity {\n7\nint32_t weight;\n8\nint32_t value;\n9\nint32_t quantity;\n10\n};\n11\n12\nint32_t bounded_knapsack(int32_t constc, std::vector<ItemWithQuantity>& items) {\n13\nstd::vector<Item> translated_items = translate_to_01(items);\n14\nreturn knapsack_01(c, translated_items);\n15\n} // bounded_knapsack()\n16\n17\ntranslate_to_01(conststd::vector<Item> std::vector<ItemWithQuantity>& items) {\n18\nstd::vector<Item> items_01;\n19\nfor (const ItemWithQuantity& item : items) {\n20\nint32_t rem_quantity = item.quantity;\n21\nint32_t curr_power_of_two = 1;\n22\nwhile (rem_quantity > 0) {\n23\nint32_t quantity_to_aggregate = std::min(rem_quantity, curr_power_of_two);\n24\nitems_01.push_back(Item{.weight = quantity_to_aggregate item.weight,*\n25\n.value\n= quantity_to_aggregate item.value});*\n26\nrem_quantity -= quantity_to_aggregate;\n27\ncurr_power_of_two *= 2;\n28\n} // while\n29\n} // for item\n30\nreturn items_01;\n31\n} // translate_to_01()\n32\n33\n// This function is identical to the bottom-up 0-1 solution provided earlier\n34\nint32_t knapsack_01(int32_t constc, std::vector<Item>& items) {\n35\nconst size_t n = items.size();\n36\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(cmemo(n + 1, + 1, 0));\n37\nfor (size_t i = 0; i < n; ++i) {\n38\nfor (size_t j = 0; j < c + 1; ++j) {\n39\nif (j < items[i].weight) {\n40\nmemo[i + 1][j] = memo[i][j];\n41\n} // if\n42\nelse {\n43\nmemo[i + 1][j] = std::max(memo[i][j], memo[i][j - items[i].weight] + items[i].value);\n44\n} // else\n45\n} // for j\n46\n} // for i\n47\nreturn memo[n][c];\n48\n} // knapsack_01()\nIt should be noted that there do exist other algorithms that can be used to solve the bounded knapsack problem in time, where 𝑛is theΘ(𝑛𝐶)\noriginal number of items given without double-counting identical copies. These algorithms won’t be discussed in this chapter, however, as they\nare a bit too complicated for the scope of this class.", "word_count": 651, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e0dc24d5-1c76-5d69-9a37-e7a83c50e9f3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 951, "real_page_number": null, "text": "24.5 Unbounded Knapsack\n939\n24.5\nUnbounded Knapsack\nThe final variant of the 0-1 knapsack problem we will discuss in this chapter is the unbounded knapsack problem, which provides an unlimited\nnumber of copies for each item. Although this variant may seem complicated at first, it is actually simpler than the other variants we have\ndiscussed so far! Previously, we had a constraint on the number of times each item can be used, which required us to consider the costs of\nincluding and excluding each item. However, since the unbounded knapsack problem removes this constraint, we no longer have to keep track\nof which items have already been used. Therefore, we can reduce our subproblems to only one dimension: the capacity of our knapsack.\nIf we define as the maximum profit achievable with a knapsack of capacity 𝑐, we can recursively compute by iterating over all𝐹(𝑐) 𝐹(𝑐)\nitems smaller than 𝑐and finding the ones we should add to maximize profit. To illustrate how this works, consider the previous example, but\nthis time with an unlimited quantity of each item:\nItem\n1\n2\n3\n4\n5\nWeight\n1\n2\n5\n6\n7\nValue\n1\n6\n18\n22\n28\nQuantity\n∞\n∞\n∞\n∞\n∞\nKnapsack Capacity: 45\nSince we no longer have to worry about which items have already been included, there are only five subproblems that our solution depends on\n(given 45), of which we take the largest:𝐶=\n• The best solution for a knapsack of capacity 44 + value of item 1 (𝐹(𝑐−𝑤1)+𝑣1, or in this example)𝐹(44)+1\n• The best solution for a knapsack of capacity 43 + value of item 2 (𝐹(𝑐−𝑤2)+𝑣2, or in this example)𝐹(43)+6\n• The best solution for a knapsack of capacity 40 + value of item 3 (𝐹(𝑐−𝑤3)+𝑣3, or in this example)𝐹(40)+18\n• The best solution for a knapsack of capacity 39 + value of item 4 (𝐹(𝑐−𝑤4)+𝑣4, or in this example)𝐹(39)+22\n• The best solution for a knapsack of capacity 38 + value of item 5 (𝐹(𝑐−𝑤5)+𝑣5, or in this example)𝐹(38)+28\nIn general, the solution to for any 𝑐can be expressed using the following recurrence relation:𝐹(𝑐)\n𝐹(𝑐)=\n{\n0,\nif 𝑐=0\n≤𝑖≤𝑛where 𝑤𝑖≤𝑐}),max({𝐹(𝑐−𝑤𝑖)+𝑣𝑖for all 1\nif 𝑖>0, 𝑐>0\nA top-down solution for the unbounded knapsack problem is shown below:\n1\nstruct Item {\n2\nint32_t weight;\n3\nint32_t value;\n4\n};\n5\n6\nint32_t unbounded_knapsack(int32_t constc, std::vector<Item>& items) {\n7\nstd::vector<int32_t> memo(c + 1, -1);\n8\nreturn knapsack_helper(c, items, memo);\n9\n} // unbounded_knapsack()\n10\n11\nint32_t knapsack_helper(int32_t const std::vector<int32_t>&c, std::vector<Item>& items, memo) {\n12\nif (c == 0) {\n13\nreturn 0;\n14\n} // if\n15\nif (memo[c] != -1) {\n16\nreturn memo[c];\n17\n} // if\n18\nint32_t best = 0;\n19\nfor (size_t i = 0; i < items.size(); ++i) {\n20\nif (items[i].weight <= c) {\n21\nbest = std::max(best, knapsack_helper(c - items[i].weight, items, memo) + items[i].value);\n22\n} // if\n23\n} // for i\n24\nreturn memo[c] = best;\n25\n} // knapsack_helper()\nA bottom-up solution is shown below:\n1\nint32_t unbounded_knapsack(int32_t constc, std::vector<Item>& items) {\n2\nstd::vector<int32_t> memo(c + 1);\n3\nfor (size_t j = 0; j <= c; ++j) {\n4\nfor (size_t i = 0; i <= items.size(); ++i) {\n5\nif (items[i].weight <= j) {\n6\nmemo[j] = std::max(memo[j], memo[j - items[i].weight] + items[i].value);\n7\n} // if\n8\n} // for i\n9\n} // for j\n10\nreturn memo[c];\n11\n} // unbounded_knapsack()\nThere are a total of 𝐶subproblems, each taking time to solve (since we have to loop over all items to determine which we should add toΘ(𝑛)\nour knapsack). Using dynamic programming, we only need to solve each subproblem once, which gives us a time complexity overall. InΘ(𝑛𝐶)\naddition, since we declare a memo of size in our solution, the auxiliary space usage is Θ(𝐶).𝐶+1", "word_count": 656, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "fb149bee-cf5f-5b7e-983a-c2b93d1a96b7", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 952, "real_page_number": null, "text": "940\nChapter 24. The Knapsack Problem\nChapter 24 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Which of the following algorithm families does NOT guarantee an optimal solution to the 0-1 knapsack problem?\nA) Branch and bound\nB) Dynamic programming\nC) Greedy\nD) Brute force\nE) None of the above\n2. Given a knapsack of capacity 𝑐and 𝑛items to choose from, what is the worst-case time complexity of an optimally-implemented dynamic\nprogramming solution to the 0-1 knapsack problem?\nA) Θ(𝑛)\nB) Θ(𝑛𝑐)\nΘ(𝑛𝑐2)C)\nD) Θ(𝑛log(𝑐))\nE) Θ(𝑛𝑐log(𝑐))\n3. Suppose you had the following five items, and you want to place them in a 0-1 knapsack of capacity 13.\nItem\n1\n2\n3\n4\n5\nWeight\n1\n3\n6\n7\n10\nValue\n9\n11\n23\n28\n30\nKnapsack Capacity: 13\nWhich of the following statements is TRUE?\nA) The greedy approach of selecting items with the highest value first will produce the optimal solution\nB) The greedy approach of selecting items with the lowest weight first will produce the optimal solution\nC) The greedy approach of selecting items with the highest value-to-weight ratio first will produce the optimal solution\nD) The greedy approach of selecting items with the lowest weight-to-value ratio first will produce the optimal solution\nE) None of the above\nIf you wanted to efficiently implement a branch and bound solution to the 0-1 knapsack problem, which of the following algorithm families4.\nwould be most ideal for estimating the upper bound used for pruning?\nI. Greedy\nII. Dynamic programming\nIII. Brute force\nA) I only\nB) II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n5. Assume you are implementing a dynamic programming approach to the 0-1 knapsack problem, and you are trying to find the maximum\npossible value you can take in your knapsack of weight capacity 5. You have the following items:\nItem ID\nWeight\nValue\n1\n2\n$35\n2\n1\n$16\n3\n4\n$61\n4\n3\n$53\nThe memo for this problem is shown below. Some of the values in the memo have already been filled in (ID represented by memo row,\nweight represented by memo column).\nID/Weight\n0\n1\n2\n3\n4\n5\n0\n$0\n$0\n$0\n$0\n$0\n$0\n1\n$0\n$0\n$35\n$35\n2\n$0\n3\n$0\n4\n$0\nT\nAfter the memo is completely filled in, what would be the value of 𝑇?\nA) $61\nB) $64\nC) $69\nD) $70\nE) $88", "word_count": 452, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "582f2dda-1493-5269-aae2-27e47a2ddcc7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 953, "real_page_number": null, "text": "24.5 Unbounded Knapsack\n941\n6. Assume you are implementing a dynamic programming approach to the 0-1 knapsack problem, and you are trying to find the maximum\npossible value you can take in your knapsack of weight capacity 5. You have the following items; the prices of items 3 and 4 are unknown:\nItem ID\nWeight\nValue\n1\n2\n$27\n2\n1\n$20\n3\n4\n4\n3\nT\nThe memo for this problem is shown below. Some of the values in the memo have already been filled in (ID represented by memo row,\nweight represented by memo column). You are given two conditions that need to be met:\n𝑋≥𝑌in• the memo below\n• all optimal solutions for this 0-1 knapsack problem (with weight capacity 5) must include item 4 in the final knapsack\nID/Weight\n0\n1\n2\n3\n4\n5\n0\n$0\n$0\n$0\n$0\n$0\n$0\n1\n$0\n$0\n$27\n$27\n2\n$0\n3\n$0\n𝑋\n𝑌\n4\n$0\nWhat is the lowest possible value of item 4 (denoted as 𝑇) that ensures these two conditions will always be true?\nA) $40\nB) $41\nC) $47\nD) $67\nE) $68\nFor questions 7-8, suppose you had the following five items, and you want to place them in a knapsack of capacity 16.\nItem\n1\n2\n3\n4\n5\nWeight\n4\n10\n6\n8\n2\nValue\n5\n7\n9\n6\n4\nKnapsack Capacity: 16\n7. If you are allowed to take a portion of these items for partial value (i.e., fractional knapsack), what is the maximum value that can be stored\ninto the knapsack without going over capacity?\nA) 18\nB) 19\nC) 20\nD) 21\nE) 22\n8. Which of the following statements is/are TRUE?\nI. If this were a 0-1 knapsack problem, the maximum value attainable in the knapsack is 18\nII. Item 1 is taken in the optimal fractional knapsack solution, but is left behind in the optimal 0-1 knapsack solution\nIII. If there were an unlimited supply of all five items, the maximum value attainable in the unbounded knapsack is 32\nA) I only\nB) II only\nC) III only\nD) I and III only\nE) II and III only\n9. Which of the following statements is/are TRUE regarding the greedy approach toward a knapsack problem?\nI. The greedy approach will not always give an optimal solution to the 0-1 knapsack problem\nII. The greedy approach will not always give an optimal solution to the fractional knapsack problem\nIII. Given 𝑛items to put into a fractional knapsack of capacity 𝑐, the worst-case time complexity of a greedy solution is Θ(𝑛log(𝑛))\nA) I only\nB) II only\nC) I and II only\nD) I and III only\nE) I, II, and III", "word_count": 455, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2bec62c9-7aae-5588-a41e-4b982efc8dcb", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 954, "real_page_number": null, "text": "942\nChapter 24. The Knapsack Problem\n𝑂(𝑛2𝑛)10. The 0-1 knapsack problem can be solved in time using brute force when given 𝑛items and knapsack capacity 𝑐. What is the tightest\ntime complexity bound on solving this 0-1 knapsack problem if you were to use a branch and bound approach instead?\nA) 𝑂(𝑛𝑐)\nB) 𝑂(𝑛log(𝑛))\n𝑂(𝑛2𝑐)C)\n𝑂(𝑛2𝑛)D)\nE) 𝑂(𝑛!)\n11. Which of the following is NOT an example of the 0-1 knapsack problem?\nA) Deciding on which landmarks to visit on a one-day trip to New York City to maximize enjoyment\nB) Determining if a box of toys can be filled to exactly its capacity, given a collection of toys with different sizes\nC) Choosing the questions you want to answer to optimize your score on a timed exam, where each question is worth a number of\npoints (with no partial credit) and can take a varying amount of time to solve\nD) Selecting which packages to ship on a cargo truck with limited capacity to maximize revenue\nE) All of the above are examples of the 0-1 knapsack problem\n12. In which of the following knapsack scenarios would a greedy algorithm that always selects the available item with the lowest individual\nweight return an optimal solution?\nA) When each item is unique\nB) When there is a finite amount of each item available\nC) When there is an infinite amount of each item available\nD) When you are allowed to take a fractional amount of each item\nE) None of the above\n13. Which of the following statements regarding the knapsack problem is/are FALSE?\nI. The 0-1 knapsack problem is a constraint satisfaction problem, rather than an optimization problem\nII. The top-down and bottom-up dynamic programming approaches to the 0-1 knapsack problem yield different worst-case time\ncomplexities\nIII. Given a list of 𝑛items that are already sorted by value density, the worst-case time complexity of the greedy solution for the\nfractional knapsack problem is Θ(𝑛)\nA) I only\nB) II only\nC) I and II only\nD) I and III only\nE) I, II, and III\n14. You are given an array of coin denominations, as well as the size of each type of coin. You have an infinite number of each kind of coin,\nand you want to find the maximum value you can fit in a bag of a limited capacity. Which type of knapsack problem is this most similar to?\nA) 0-1 knapsack\nB) Fractional knapsack\nC) Bounded knapsack\nD) Unbounded knapsack\nE) None of the above\n15. The dining hall is closed for the night, and you are the last person there. Before leaving, however, you notice a bunch of pizza slices on the\ncounter that have not been taken. Since you are worried that the pizzas would get thrown out after you leave, you decide to take as many\npizzas as you can in a lunchbox you brought in your backpack.\nstructGiven the following that stores information about the available pizzas that you can take:\nstruct Pizza {\ndouble size;\ndouble value;\nint32_t quantity;\n};\nsize valuewhere represents the size of the pizza slice, is a value that identifies the amount of enjoyment you would get from that slice\nquantityof pizza, and represents the number of slices of this particular type of pizza that are still available, implement a function that\nreturns the maximum pizza value you can stuff into your lunchbox. You are allowed to take a partial slice of pizza for partial value, if\nneeded.\ndouble max_pizza_value(const int32_tstd::vector<Pizza>& pizzas, max_capacity);\n46.5,Example: Given the following pizzas and lunchbox of capacity 27, you would return a total pizza value of since the optimal solution\nwould be to take 2 slices of pizza 2, 3 slices of pizza 3, and 1.5 slices of pizza 4 (for a total pizza value of 21 + 12 + 13.5 = 46.5):\nPizza Type\n1\n2\n3\n4\n5\nSize\n8\n3\n4\n6\n5\nValue\n10\n6\n7\n9\n5\nQuantity\n3\n2\n3\n2\n5\nKnapsack Capacity: 27", "word_count": 678, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "36597e6b-1731-510d-a59a-a908bec780a5", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 955, "real_page_number": null, "text": "24.5 Unbounded Knapsack\n943\n16. You are given access to proprietary, state-of-the-art trading software that allows you to view the value of certain stocks in the future. You\ncurrent_prices future_prices, current_prices[i]are given two arrays of stock prices, and where represents the price of\n𝑖th 𝑖thfuture_prices[i]the stock today, and represents the price of the stock a year from now. Knowing this information, you want\nto maximize your profits by buying these stocks today and selling them a year from now. However, the limitations of the software prevent\nbudgetyou from purchasing more than one share of an individual stock, and you have a that you cannot exceed. Implement a function\nthat returns the maximum total profit you can earn by selecting the stocks to buy within your budget constraints.\nint32_t max_stock_profit(const std::vector<int32_t>& current_prices,\nconst std::vector<int32_t>& int32_tfuture_prices, budget);\n19,Example: Given the following stocks and a budget of $40, you would return since that is the maximum profit you can earn without\ngoing over budget (buying stocks 1 and 3 for $38 today and selling for $57 a year from now):\nStock Number\n0\n1\n2\n3\n4\nCurrent Price\n$11\n$24\n$18\n$14\n$29\nFuture Price\n$17\n$35\n$13\n$22\n$41\n17. A massive blizzard blew over your neighborhood last evening, and everything is now covered with several feet of snow! As the head of your\nneighborhood’s homeowners association, you want to hire some snow removal companies to help clean up the snow before the morning\nwork commute. Unfortunately, due to the impact of the blizzard on snow removal service availability in your area, there are only two\ncompanies that are available to service your neighborhood at this time:\n𝑖th• The service cost and time required by this company to remove snow from the house is provided in twoBig-snOw Removal Inc.:\n𝑖thservice_fees service_times.integer arrays, and If you use this company’s services, removing snow from the house in\nservice_fees[i] service_times[i]the neighborhood costs units of money and takes units of time.\n• Due to a special promotion negotiated with the neighborhood at the beginning of winter, this snowSnowstack Underflow LLC:\nremoval company is able to remove snow from any house in 1 unit of time for free. However, this company can only be used for free\nservices if is already occupied with another house in the neighborhood at the same time.Big-snOw Removal Inc.\nImplement a function that returns the minimum possible cost to remove snow from all houses in the neighborhood.\nint32_t min_cost_for_snow_removal(const std::vector<int32_t>& service_fees,\nconst std::vector<int32_t>& service_times);\n6,Example: Given the following houses and the corresponding service fees and times from contractor A, you would return since that is the\nminimum possible cost of removing snow from all houses while adhering to the stated contractor rules above (using contractor A to service\nhouses 1 and 2 for a total cost of $2 + $4 = $6, and using contractor B to concurrently service the other four houses for free):\nHouse Number\n0\n1\n2\n3\n4\n5\nService FeeBig-snOw Removal Inc.\n$4\n$5\n$4\n$5\n$2\n$3\nService TimeBig-snOw Removal Inc.\n2\n4\n3\n2\n1\n2\nAlgorithm Family Practice Exercises\nQuestions 18-30 are generic algorithm family questions, which could cover content from any of the previous algorithm family chapters.\n18. Suppose you aretryingto breakinto yourprofessor’s computer topeek atthe finalexamsurprise themwith a thankyou note for allthework\nthat was put into the course. You noticed during lecture that your professor had a 9-character password, so you are randomly generating\npasswords of length 9. What algorithm family is this an example of?\nA) Backtracking\nB) Branch and bound\nC) Brute force\nD) Divide and conquer\nE) Dynamic programming\n19. You want to identify the student with the highest percentage score in the class. At the end of the semester, you receive the individual\nassignment grades received by every student (but not the overall percentage grade). You first calculate the percentage score earned by\nthe first student in the class. Then, for each of the remaining students, you calculate the maximum score that could be attained by that\nstudent after each assignment is individually considered. If this maximum score is lower than the current best, you stop and move on to the\nnext student on the roster. Otherwise, the student is promising, and you continue to check their grades until they either obtain the highest\nscore or it becomes impossible for them to attain a score higher than the current best score encountered. What algorithm family is this an\nexample of?\nA) Backtracking\nB) Branch and bound\nC) Brute force\nD) Divide and conquer\nE) Dynamic programming", "word_count": 787, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "efa20af1-8f71-5071-a6e1-ee8ba04ce423", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 956, "real_page_number": null, "text": "944\nChapter 24. The Knapsack Problem\n20. Your course registration is tomorrow, and you know that you must take five courses next semester to stay on track for graduation. Each of\nthese classes has multiple sections, all at different times and locations. You take out your paper and pencil and try to make a schedule that\nfits the following constraints:\n• No two classes can overlap\n• A half-hour block must be reserved between classes on Central and North Campus\n• Consecutive classes cannot last more than four hours in duration\nWhich of the following algorithm families would be best at accomplishing this task?\nA) Backtracking\nB) Branch and bound\nC) Brute force\nD) Divide and conquer\nE) Greedy\n21. Your favorite amusement park is closing in two hours, and you still haven’t gotten to most of the rides! You know that each of the rides left\nhave wait times of between 15 and 90 minutes. In addition, different rides give you different levels of satisfaction. Both wait time and\nsatisfaction take on integer values. You want to maximize the satisfaction you get from these last two hours, and you don’t want to ride the\nsame ride twice. Which of the following algorithm families would be best at accomplishing this task?\nA) Backtracking\nB) Brute force\nC) Divide and conquer\nD) Dynamic programming\nE) Greedy\n22. The dining hall just closed for the night, and the workers have started kicking everyone out. As the other students leave, you realize that you\nhave a problem on your hands — you got way too much food! You still have several uneaten dishes on your table, and each of these dishes\ngives you different levels of satisfaction. However, your stomach can only handle so much, and you prefer not the throw up after you leave.\nWhich algorithm family would be most efficient at determining what you should eat to maximize your satisfaction, while also considering\nthe capacity of your stomach?\nA) Backtracking\nB) Brute force\nC) Divide and conquer\nD) Dynamic programming\nE) Greedy\n23. You want to color a graph of the United States such that no adjacent states share the same color. Which of the following algorithm families\nwould be best at accomplishing this task?\nA) Backtracking\nB) Branch and bound\nC) Brute force\nD) Divide and conquer\nE) Greedy\n24. You are working to test the durability of a brand new phone case that you want to release. In order to test the case, you want to determine\nfrom how many feet a phone with the case can fall without breaking. If you are dropping the phone in increments of one foot up to 𝑛feet,\nwhich of the following algorithm families would allow you to complete the test with the minimal number of drops, if you are given 𝑛\nphones?\nA) Backtracking\nB) Branch and bound\nC) Brute force\nD) Divide and conquer\nE) Dynamic programming\n25. You are a developer for a web server that generates terabytes of access logs every day. You want to sort these logs by timestamp to analyze\nthe performance of the web server over time, but the total size of the logs is so large that the data you want to sort cannot fit in your\ncomputer’s main memory. Which of the following algorithm families can you use to most efficiently perform your desired task in this\nsituation?\nA) Backtracking\nB) Brute force\nC) Divide and conquer\nD) Dynamic programming\nE) Greedy", "word_count": 578, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7a37e83b-acde-5be2-a184-7671664ec64c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 957, "real_page_number": null, "text": "24.5 Unbounded Knapsack\n945\n26. You are attending a prestigious programming conference, where you have the opportunity to attend a bunch of lectures from some of the\nmost renowned experts of the field. However, there is a problem: many of these lectures overlap and have different lengths and start/end\ntimes. If your goal is to attend as many lectures as possible, which of the following algorithm families will allow you to most efficiently\ndetermine the optimal subset of lectures you should attend to reach your goal?\nA) Branch and bound\nB) Brute force\nC) Divide and conquer\nD) Dynamic programming\nE) Greedy\n𝑛227. You are given a 𝑛×𝑛board with several shaded cells. You are told to place the numbers 1 to in the cells in such a way that consecutive\nnumbers occupy neighboring cells (either horizontally or vertically) and shared cells only contain prime numbers. An example of a 10 x 10\nboard and its solution is shown below:\nIf you wanted to implement a program that can solve this puzzle, which approach would allow you to solve this problem most efficiently?\nA) Backtracking\nB) Branch and bound\nC) Brute force\nD) Dynamic programming\nE) Greedy\n28. You are given 𝑛coins that look visually identical. You are told that only one coin is real, and that the rest are in fact candy pieces that are\nmasquerading as a coin. The only way to determine if a coin is real or not is to bite into it to see if it is actually candy. Given that you can\nonly bite one coin at a time, which of the following algorithm families is most appropriate for finding the real coin as efficiently as possible?\nA) Backtracking\nB) Brute force\nC) Divide and conquer\nD) Dynamic programming\nE) Greedy\n29. You are given 𝑛coins that look visually identical. You are told that only one coin is real, and that the rest are in fact candy pieces that are\nmasquerading as a coin. However, you are also told that the real coin is also heavier than the fake candy coins, and that the fake candy coins\nall weigh the same. If you are given a scale to help you measure the coins, which of the following algorithm families is most appropriate for\nfinding the real coin as efficiently as possible?\nA) Backtracking\nB) Brute force\nC) Divide and conquer\nD) Dynamic programming\nE) Greedy\n30. You work at a logistics company that is in charge of scheduling daily appliance deliveries and installations for customers in a large city.\nGiven a list of customer addresses that need to be served on a given day, you want to devise a route for the delivery truck that minimizes the\nnumber of miles driven, while also handling special delivery instructions from certain customers that are needed for a successful delivery\n(e.g., a specific customer may not be able to accept a delivery after a point in time, so they may request to be included within the first five\ndeliveries of the day to be able to accept the delivery). Which of the following algorithm families would be most efficient in devising a\nroute that ensures a successful delivery to all customers while also minimizing the number of total miles driven by the delivery truck?\nA) Backtracking\nB) Branch and bound\nC) Brute force\nD) Divide and conquer\nE) Greedy", "word_count": 562, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab5501a1-bdba-5900-ad91-d46c5f5b5602", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 958, "real_page_number": null, "text": "946\nChapter 24. The Knapsack Problem\nChapter 24 Exercise Solutions\n1. The correct answer is (C). The greedy approach does not guarantee an optimal solution for the 0-1 knapsack problem (see section 24.2.2\nfor an explanation why). The other three algorithm families do guarantee an optimal solution (brute force because it tries everything; branch\nand bound because it is a smarter brute force that only explores promising search paths, which include the optimal solution; dynamic\nprogramming because the knapsack problem has an optimal substructure with overlapping subproblems).\n2. The correct answer is (B). There are a total of 𝑛𝑐subproblems that need to be solved, each taking time (see section 24.2.3). Since weΘ(1)\nonly solve each unique subproblem once using dynamic programming, the overall time complexity of a DP solution is Θ(𝑛𝑐).\n3. The correct answer is (E). The greedy approach fails to obtain the optimal solution in all four of these approaches. The greedy approach of\nselecting items with the highest value first (option A) would get you items 2 and 5, for a total value of 41. The greedy approach of selecting\nitems with the lowest weight first (option B) would get you items 1, 2, and 3, for a total value of 43. The greedy approach of selecting\nitems with the highest value-to-weight ratio first (option C) would get you items 1, 2, and 4 for a total value of 48. The greedy approach\nof selecting items with the lowest weight-to-value ratio first (option D), would also get you items 1, 2, and 4 for a total value of 48. The\noptimal solution, however, would be to take items 3 and 4 for a total value of 51 — none of the above.\n4. The correct answer is (A). The purpose of estimating an upper bound is to quickly get a close enough answer to make the actual branch\nand bound process more efficient. Dynamic programming and brute force actually obtain the correct answer, so they shouldn’t be used to\nestimate the upper bound (otherwise there is no point in actually running branch and bound after computing the initial upper bound).\n5. The correct answer is (C). The final memo looks like this:\nID/Weight\n0\n1\n2\n3\n4\n5\n0\n$0\n$0\n$0\n$0\n$0\n$0\n1\n$0\n$0\n$35\n$35\n$35\n$35\n2\n$0\n$16\n$35\n$51\n$51\n$51\n3\n$0\n$16\n$35\n$51\n$61\n$77\n4\n$0\n$16\n$35\n$53\n$69\n$88\n𝑋≥𝑌to6. The correct answer is (B). For be true, the optimal solution considering only items 1-3 with a knapsack of capacity 3 must be\ngreater than or equal to the optimal solution considering only items 1-3 with a knapsack of capacity 4. For this to happen, the value of item\n3 cannot exceed the value of 𝑋— otherwise, it would be chosen for the knapsack of capacity 4, which would force 𝑌to be larger than 𝑋.\nAfter completing the partial memo up to the position of 𝑋, we determine that 𝑋= $47:\nID/Weight\n0\n1\n2\n3\n4\n5\n0\n$0\n$0\n$0\n$0\n$0\n$0\n1\n$0\n$0\n$27\n$27\n$27\n$27\n2\n$0\n$20\n$27\n$47\n$47\n$47\n3\n$0\n$20\n$27\n$47\n𝑌\n4\n$0\nThus, for our first condition to be true, the value of item 3 cannot exceed $47. This leads us to our second condition: we want item 4\nto always end up in the optimal solution for a knapsack of capacity 5. For this to happen, the value of item 4 must be high enough that\ncombining it with item 1 ($27) would always exceed the value of taking items 2 and 3 instead (we do not need to consider the case of\ntaking item 4 with item 2 here, as this would never be optimal; you can only choose to take one of either items 1 or 2 alongside item 4 if the\nknapsack capacity is 5, and item 1 is worth more). We have determined that the largest possible value of item 3 is $47, so the total combined\nvalue of items 2 and 3 is at most $20 + $47 = $67. Thus, the combined value of taking items 4 and 1 must be at least $68 for item 4 to\nalways be in the optimal solution (a total of $67 would not work here, since we want item 4 to be contained in optimal solutions, whichall\nis not true in the case of a tie). Since item 1 is worth $27, the lowest possible value of item 4 to satisfy both conditions is $68 - $27 = $41.\n7. The correct answer is (D). For the fractional knapsack problem, the optimal solution can be obtained by greedily taking the items with the\nhighest value-to-weight ratio until you run out of space. In this case, the value-to-weight ratios are shown below:\nItem\n1\n2\n3\n4\n5\nWeight\n4\n10\n6\n8\n2\nValue\n5\n7\n9\n6\n4\nRatio\n1.25\n0.7\n1.5\n0.75\n2\nThus, the optimal solution would take item 5 first, then item 3, then item 1, then half of item 4 (to fill the knapsack up to capacity), yielding\na total value of 4 + 9 + 5 + 3 = 21.\n8. The correct answer is (E). The optimal solution to the 0-1 knapsack problem is 19, which is obtained by taking items 3, 4, and 5. This\nmakes statement I false and statement II true (since we do not take item 1 here, but did in the fractional knapsack solution in question 7). If\nthere is an unlimited supply of all five items, the unbounded knapsack solution would be to only take item 5 until the knapsack is full (8\ncopies), which yields a total value of 8 4 = 32.×", "word_count": 968, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "75c50df9-71bf-5d18-b908-acc9c211ae51", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 959, "real_page_number": null, "text": "24.5 Unbounded Knapsack\n947\n9. The correct answer is (D). Statement I is true because greedy does not give an optimal solution to 0-1 knapsack (see section 24.2.2 for an\nexplanation why). Statement II is false because greedily selecting items by highest value-to-weight ratio does guarantee an optimal solution\nto fractional knapsack (see section 24.3 for an explanation why). Statement III is true because the sorting step serves as the bottleneck of\nthe greedy fractional knapsack algorithm.\n10. The correct answer is (D). If we get incredibly unlucky, we could possibly generate permutations of items in a way such that every\npermutation we encounter is better than the last, preventing us from pruning anything since every path would be promising. In this case,\n𝑂(𝑛2𝑛).our branch and bound approach would essentially devolve to brute force, which would end up with a time complexity bound of\n11. The correct answer is (B). In the 0-1 knapsack problem, you must either take an item or leave it behind, with the goal of maximizing the\ntotalvalueobtainedwithoutexceedingthecapacityoftheknapsack. Options(A),(C),and(D)areallexamplesofthe0-1knapsackproblem,\nas you have to either \"take\" an item (e.g., visiting a landmark, answering a question, shipping a package) or leave it behind. However, this is\nnot the case with choice (B), which does not involve any optimization at all and is only concerned with determining whether a subset sum\nthat matches the knapsack capacity exists.\n12. The correct answer is (E). A greedy approach that only selects based on item weight cannot guarantee an optimal solution (regardless of\nwhether it is used on a 0-1 or fractional knapsack), since it is possible that the lightest item has a suboptimal value compared to heavier\nitems that can be chosen. The following scenario, for instance, is one where this greedy approach would always fail for all four knapsack\ndescriptions associated with options (A)-(D):\nItem\n1\n2\nWeight\n1\n10\nValue\n1\n20\n13. The correct answer is (C). Statement I is false because 0-1 knapsack is an optimization problem, since you want to maximize the value\nattainable from placing items in your knapsack. Statement II is false because both the top-down and bottom-up dynamic programming may\nneed to solve up to 𝑛𝑐subproblems for 𝑛items and a knapsack capacity of 𝑐, so the worst-case time complexity of both implementations\nis Θ(𝑛𝑐). Statement III is true since the item selection step of the greedy approach becomes the most expensive step if the sortingΘ(𝑛)\nbottleneck is no longer involved.\n14. The correct answer is (D). In this case, you are given an unlimited quantity of each item that you can place in your knapsack. This is best\ndescribed as an unbounded knapsack problem.\n15. Sinceyouareallowedtotakepartialslicesofpizzaforpartialvalue, thisisafractionalknapsackproblem, andwecansolveitusingagreedy\napproach that always selects the slice with the highest value-to-size ratio. One possible implementation of this solution is provided below:\n1\nstruct Pizza {\n2\ndouble size;\n3\ndouble value;\n4\nint32_t quantity;\n5\n};\n6\n7\nbool compare(const constPizza& p1, Pizza& p2) {\n8\nreturn (p1.value / p1.size) > (p2.value / p2.size);\n9\n};\n10\n11\ndouble max_pizza_value(const int32_tstd::vector<Pizza>& pizzas, max_capacity) {\n12\nstd::vector<Pizza> sorted_pizzas = pizzas;\n13\nstd::sort(sorted_pizzas.begin(), sorted_pizzas.end(), compare);\n14\nint32_t curr_size = 0;\n15\ndouble curr_value = 0.0;\n16\nfor (size_t i = 0; i < sorted_pizzas.size(); ++i) {\n17\ndouble total_pizza_size = sorted_pizzas[i].quantity sorted_pizzas[i].size;*\n18\ndouble total_pizza_value = sorted_pizzas[i].quantity sorted_pizzas[i].value;*\n19\nif (curr_size + total_pizza_size <= max_capacity) {\n20\ncurr_size += total_pizza_size;\n21\ncurr_value += total_pizza_value;\n22\n} // if\n23\nelse {\n24\ndouble partial_pizza_size = max_capacity - curr_size;\n25\ncurr_value += (total_pizza_value partial_pizza_size / total_pizza_size);*\n26\nbreak;\n27\n} // else\n28\n} // for i\n29\nreturn curr_value;\n30\n} // max_pizza_value()", "word_count": 657, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5c089f96-34c2-5b63-80b5-35f5bf90da1f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 960, "real_page_number": null, "text": "948\nChapter 24. The Knapsack Problem\n16. This is a modified 0-1 knapsack problem, where our budget acts as our constraint (the knapsack capacity), and the stocks we can choose\nhave a value of future price - current price. Thus, we can use dynamic programming to tackle this problem. For each stock 𝑖, we have two\nchoices: we can either skip the stock, for which our max profit would be the same as the max profit we can achieve with the first stocks𝑖−1\nand the same budget; or, we can buy the stock, for which the max profit is equal to the value of stock 𝑖plus the max profit we can achieve\nwith the first stocks with a reduced budget that is lower by the stock’s current price. A bottom-up implementation is shown below:𝑖−1\n1\nint32_t max_stock_profit(const std::vector<int32_t>& current_prices,\n2\nconst std::vector<int32_t>& int32_tfuture_prices, budget) {\n3\nconst size_t n = current_prices.size();\n4\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(budgetmemo(n + 1, + 1, 0));\n5\nfor (size_t i = 0; i < n; ++i) {\n6\nfor (size_t j = 0; j < budget + 1; ++j) {\n7\nif (j < current_prices[i]) {\n8\n// price of stock already over budget, exclude\n9\nmemo[i + 1][j] = memo[i][j];\n10\n} // if\n11\nelse {\n12\nint32_t curr_stock_profit = memo[i][j - current_prices[i]] + future_prices[i] - current_prices[i];\n13\nmemo[i + 1][j] = std::max(memo[i][j], curr_stock_profit);\n14\n} // else\n15\n} // for j\n16\n} // for i\n17\nreturn memo[n][budget];\n18\n} // max_stock_profit()\nThis problem is a twist on the standard 0-1 knapsack problem, where the cost paid to use the first snow removal company (Big-snOw)17.\nnot only allows you to remove snow from one house, but also potentially several other houses via the second snow removal company\n(Snowstack Overflow) for free. Thus, you can treat this as a inverted knapsack problem where the cost of using on house 𝑖hasBig-snOw\nservice_fees[i] 1 + service_times[i]a cost of and a value of (for the total number of houses that can be cleaned up by\npaying the upfront cost for house 𝑖, including any free houses that come bundled by using the second removal company), and we want to\nminimize total cost rather than maximize total value. We can solve this type of problem using the same \"take-it-or-leave-it\" style dynamic\nprogramming approach as for 0-1 knapsack. One possible top-down solution is shown below:\n1\nint32_t min_cost_for_snow_removal(const std::vector<int32_t>& service_fees,\n2\nconst std::vector<int32_t>& service_times) {\n3\nsize_t num_houses = service_fees.size();\n4\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(num_housesmemo(num_houses + 1, + 1, -1));\n5\nreturn helper(service_fees, service_times, 0, num_houses, memo);\n6\n} // min_cost_for_snow_removal()\n7\n8\nint32_t helper(const std::vector<int32_t>& const std::vector<int32_t>&service_fees, service_times,\n9\nint32_t int32_t std::vector<std::vector<int32_t>>&i, houses_left, memo) {\n10\nif (houses_left <= 0) {\n11\nreturn 0;\n12\n} // if\n13\nif (i >= service_fees.size()) {\n14\nreturn std::numeric_limits<int32_t>::max() / 4;\n// smaller to prevent overflow on addition\n15\n} // if\n16\nif (memo[i][houses_left] != -1) {\n17\nreturn memo[i][houses_left];\n18\n} // if\n19\n// best solution using paid company to clear the ith house, allowing you to use the\n20\n// free company for additional houses\n21\nint32_t use_paid = service_fees[i] + helper(service_fees, service_times, i + 1,\n22\nhouses_left - (1 + service_times[i]), memo);\n23\n// best solution if you do not use the paid company to clear the ith house\n24\nint32_t no_use_paid = helper(service_fees, service_times, i + 1, houses_left, memo);\n25\n// the optimal solution up to house i given houses_left houses left to clear is the minimum of the two\n26\nreturn memo[i][houses_left] = std::min(use_paid, no_use_paid);\n27\n} // helper()", "word_count": 605, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "18801b0c-56be-5b84-9919-17a4932da0f6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 961, "real_page_number": null, "text": "24.5 Unbounded Knapsack\n949\n18. The correct answer is (C). You are randomly trying all possible passwords of length 9, so this is an example of brute force. Knowing that\na specific 9-character combination does not work will give you any additional information on the remaining possible combinations.\n19. The correct answer is (B). You are trying to solve an optimization problem by dropping all solutions that are not good enough, so this is an\nexample of branch and bound.\n20. The correct answer is (A). This is a constraint satisfaction problem, where you are checking if you can build a schedule that meets all your\nconstraints. This is a problem that can be best solved using backtracking.\nThe correct answer is (D). This is similar to the 0-1 knapsack problem, where you have to place items (rides) of different costs (wait times)21.\ninto a knapsack of limited size (the time you have remaining until the park closes). Since you are dealing with integer values here, this type\nof problem can be most efficiently solved using a dynamic programming approach.\n22. The correct answer is (E). This is similar to the fractional knapsack problem; if you begin eating a plate of food, you aren’t obligated to\nfinish it. It is okay to only eat a portion of a dish to fill your stomach, so the best approach would be to eat the food you like the most until\nyou cannot eat anymore. This is an example of a greedy algorithm.\n23. The correct answer is (A). Since you want to color the United States under a constraint that two adjacent states cannot share the same\ncolor, you will want to select an algorithm that can be used to solve constraint satisfaction problems. Of the given options, this is best\nhandled using a backtracking approach.\n24. The correct answer is (D). The strategy that will allow you to minimize the total number of drops would be to use a binary search approach\nto eliminate half of the remaining test heights with each drop. (i.e., start the first drop at height 𝑛∕2; if the phone breaks, do the next drop at\nheight 𝑛∕4, otherwise do the next drop at height 3𝑛∕4, and so on). This is an example of divide-and-conquer.\n25. The correct answer is (C). Since you want to sort a ton of logs that do not fit in main memory, an external sorting algorithm (like external\nmergesort) would be most appropriate for this task. Such a sorting algorithm would be able to divide the data into smaller chunks that do fit\nin memory, sort each individual chunk and write the results to temporary files on disk, and then merge the sorted chunks in passes to get the\nfinal result. These efficient sorting methods fall under the divide-and-conquer paradigm.\n26. The correct answer is (E). This is a variation of the activity selection problem discussed in section 21.3.3, which can be solved by greedily\nselecting the next lecture with the earliest end time.\n27. The correct answer is (A). This is a constraint satisfaction problem, where you have to find a solution that meets several complex\nrequirements (in this case, every prime number must end up in a gray cell, and all adjacent numbers must be in neighboring cells). This\nis best handled by backtracking, which explores the solution space and rejects any partial solution that fails to meet any of the required\nconstraints (i.e., paths where a prime number falls on a non-shaded cell).\n28. The correct answer is (B). Knowing that a coin is candy does not give you any additional information on which coin is real, since the only\nway to determine if a coin is real or not is to bite into it, and you can only test each coin individually. Thus, there is no option but to try\nevery coin until you find the one that is not candy, which is a brute force approach.\n29. Thecorrectansweris(C).Withtheaddedknowledgethattherealcoinisheavierthantheothers, younowhaveawaytodividethesolution\nspace so that you do not have to try out each coin individually. The optimal strategy would be to split the coins into two even groups and\nplace each group on one side of the scale. The heavier side must contain the real coin, and all the coins in the lighter side must be fake. This\nis continued until the real coin is the only one that remains (in the case of an odd number of coins, one coin can be placed aside, and if the\ntwo sides of the scale are equal in weight, then the coin placed aside must be real). This is an example of a divide-and-conquer algorithm.\n30. The correct answer is (B). This is very similar to the traveling salesperson (TSP) problem, where you want to minimize the total travel\nneeded to visit a set of predetermined locations. TSP problems can be solved using branch-and-bound (where you try to explore the search\nspace and prune out any solutions that are not as good as the best solution encountered so far), which is the most applicable algorithm\nfamily for solving this problem. Backtracking does not apply here since this is an optimization problem, divide-and-conquer does not apply\nsince you cannot break up the TSP problem into independent subproblems, and there is no greedy method that can optimally solve TSP.", "word_count": 911, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "706c9385-4a79-5fe8-be2b-60357f948663", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 962, "real_page_number": null, "text": "950\nChapter 24. The Knapsack Problem\nThis page has been intentionally left blank.", "word_count": 13, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3f7f09b5-46fa-5e48-b064-a00816f88682", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 963, "real_page_number": null, "text": "Chapter 25\nShortest Path Algorithms\n25.1\nThe Shortest Path Problem\nBack in chapter 19, we introduced several algorithms that can be used to solve a variety of different graph problems. One of these algorithms\nwas the breadth-first search (BFS), which can be applied to find the shortest path between any two vertices of an unweighted graph. However,\nmany graphs in real-world applications are rather than unweighted. For weighted graphs, a simple BFS cannot be used to find theweighted\npath between two vertices with the minimal weight, as it is designed to pick the route with the fewest number of edges rather than the one that\nminimizes overall edge weight. As a result, we will need to consider other algorithms if we want to find the lowest-weighted path between two\nvertices of a weighted graph.\nBefore we begin exploring these algorithms, we will first define the problem we are trying to solve: the shortest path problem. In the\nshortest path problem, you are given an edge-weighted graph and two vertices 𝑣𝑠∈𝑉and 𝑣𝑑∈𝑉, and you want to find the path𝐺=(𝑉,𝐸)\nstarting at 𝑣𝑠and ending at 𝑣𝑑with the lowest overall path weight. For example, the shortest path between nodes 𝐴and 𝐸in the following\ngraph is 𝐴→𝐶→𝐷→𝐸with total weight 1 + 2 + 3 = 6. Notice that the of edges in our path is irrelevant here, as the value we wantnumber\nto minimize is the total of the edges from source to destination.weight\nA\nB\nC\nD\nE\n4\n5\n1\n2\n3", "word_count": 255, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "49093ff9-ae54-54a2-9de4-b0c396817ff1", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 964, "real_page_number": null, "text": "952\nChapter 25. Shortest Path Algorithms\nOne important feature shared by shortest path algorithms is a reliance on substructure, as the shortest path between any two points in aoptimal\ngraph must itself be composed of the shortest paths between intermediary nodes along the way. For instance, we know that the shortest path\nfrom 𝐴→𝐸is 𝐴→𝐶→𝐷→𝐸for a total weight of 1 + 2 + 3 = 6. However, this also implies that the shortest path from 𝐴→𝐷must be\n𝐴→𝐶→𝐷for a total weight 1 + 2 = 3. This is because, if there were a shorter path that goes from 𝐴to 𝐷without going through 𝐶, then the\nbest path from 𝐴to 𝐸could not possibly be a contradiction to a claim we already know to be true! Similarly, we also know𝐴→𝐶→𝐷→𝐸…\nthat the shortest path from 𝐶to 𝐸must be 𝐶→𝐷→𝐸. In general, given any shortest path →𝑣𝑘from to 𝑣𝑘, any subpath𝑣0 →𝑣1 𝑣0→…\n≤𝑖≤𝑗≤𝑘mustfrom vertex 𝑣𝑖to 𝑣𝑗such that also be a shortest path from 𝑣𝑖to 𝑣𝑗.0\nRecall that optimal substructure serves as a foundation for the greedy and dynamic programming algorithm families discussed in earlier\nchapters. Thus, it comes at no surprise that shortest path algorithms typically rely on these algorithmic techniques to solve the shortest path\nproblem. The reliance on an optimal substructure also leads to another consequence of shortest path algorithms: since smaller subproblems are\nused to build up to larger subproblems, many of these algorithms find the shortest path from the source vertex 𝑣𝑠to in theevery other vertex\ngiven graph 𝑉as a byproduct of solving the original problem from 𝑣𝑠to some destination 𝑣𝑑. Because of this, such algorithms also solve the\nsingle-source shortest path problem, which seeks to find the shortest paths from a single source vertex to in the graph.every other vertex\nFinally, there is one notable caveat to the shortest path problem that pertains to edges. If a graph contains anegatively weighted negative\nreachable from the source node 𝑣𝑠, then the shortest path is negatively infinite and the problem has no solution. For instance, consider thecycle\nfollowing graph, where the cycle 𝐵→𝐶→𝐷→𝐵has a total weight of 5 + 3 + (-9) = -1.\nA\nB\nC\nD\nE\n−9\n1\n3\n2\n5\nWhat is the shortest path from 𝐴→𝐸? It should be quickly apparent that the answer is negatively infinite, since you can repeatedly traverse the\ncycle 𝐵→𝐶→𝐷→𝐵to infinitely decrease your total path weight.\nIt is important to note the distinction between a graph with a negative and a graph with negative weights. If a graph has a negativecycle edge\ncycle reachable from the source vertex, then the shortest path solution is negatively infinite. However, a graph with negative edges still has a\nwell-defined shortest path if those edges do not produce a negative cycle. If we change the weight of 𝐵𝐷from -9 to -7, the weight of the cycle\n𝐵→𝐶→𝐷→𝐵becomes 3 + 5 + (-7) = +1. Therefore, it is no longer advantageous to continuously traverse the cycle, and the graph has a\nwell-defined shortest path of 𝐴→𝐶→𝐵→𝐷→𝐸.\nA\nB\nC\nD\nE\n−7\n1\n3\n2\n5\nNonetheless, the mere existence of negative edges in a graph — regardless of whether a negative cycle exists — restricts the types of shortest\npath algorithms that we can use. As we will see later on, graphs with negative edges invalidate the greedy-choice property, and thus cannot be\nsolved using algorithms that rely on a greedy approach (such as Dijkstra’s algorithm).\n25.2\nDijkstra’s Algorithm\n¸ 25.2.1\nImplementing Dijkstra’s Algorithm\nOne of the simplest and most efficient shortest path algorithms is Dijkstra’s algorithm, which utilizes a greedy approach to find the shortest\npath from a source vertex 𝑣𝑠to other vertex of a weighted graph, as long as the graph has no negative edges. The core idea behindevery\nDijkstra’s algorithm is to greedily explore vertices in order of increasing distance from the source vertex. To run Dijkstra’s algorithm, we need\nto keep track of three things for each vertex:\n• A vertex is visited if all of its neighbors have been fully processed (this will make more sense asWhether the vertex has been visited.\nwe go over an example). Dijkstra’s algorithm makes the assumption that, once we fully visit a vertex, its best known distance bemust\noptimal, and we do not have to visit it ever again.\n• Throughout the algorithm, we keep track of the best distance known so farThe best known distance to the vertex from the source vertex.\nto get to each vertex. This value will be used by the algorithm to determine which unvisited vertex we should visit next.\n• This is the vertex that directly precedes the current vertex along the path with the best known distance.The predecessor of the vertex.\nThis value can be used to reconstruct the shortest path at the end of the algorithm.", "word_count": 818, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b0ffe0ad-c7ea-5dca-b39b-f37e1bd6b339", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 965, "real_page_number": null, "text": "25.2 Dijkstra’s Algorithm\n953\nUsing this information, Dijkstra’s algorithm is implemented as follows:\n1. Given a graph, split its vertices into two groups: ones that are visited and ones that are unvisited. At the start of the algorithm, all vertices\nare marked as unvisited.\n2. Assign the source vertex with a best known distance of 0, and assign all other vertices with a best known distance of ∞.\n3. Mark the starting vertex as the \"current\" vertex.\n4. Mark the current vertex as visited.\n5. Iterate over all unvisited neighbors that are reachable from the current vertex and calculate each neighbor’s distance to the source vertex\nif you pass the current vertex. If this distance is ever better than the best known distance for a neighbor, that neighbor’s bestthrough\nknown distance is updated to this better solution.\n6. Select the unvisited vertex with the smallest best known distance and set it as the new current vertex.\n7. Repeat steps 4-6 until either the destination vertex is visited, or the best known distances of the remaining unvisited vertices are all ∞\n(which indicates there is no connection between the source vertex and the remaining unvisited vertices). Once you reach this step, the\nalgorithm is complete, and the best known distance of each vertex 𝑣is also the weight of the shortest path from the source vertex to 𝑣.\nTo illustrate this process, let’s use an example. Consider the following weighted graph, for which we want to find the shortest path from vertex\n𝐴to vertex 𝐹.\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\nAs mentioned, we need to keep track of three things for each vertex: whether it has been visited, its best known distance, and its predecessor.\nNotice that this is very similar to the table we used when running Prim’s algorithm to find a graph’s MST! In fact, we can use this same table\nstructure to help us implement Dijkstra’s algorithm — in this case, we will define 𝑘𝑣as whether a vertex has been visited, 𝑑𝑣as its best known\ndistance, and 𝑝𝑣as its predecessor. Since vertex 𝐴is our starting vertex, we assign 𝑑𝐴to 0 and all other best known distances to ∞.\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nF\n0\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\nFirst, we consider all of the unvisited neighbors of our starting vertex 𝐴and compute the optimal distance required to reach each of them. The\nunvisited neighbors of 𝐴are 𝐵and 𝐶, which can be reached with distances 6 and 4, respectively. This is better than the current best known\ndistances for these two vertices (∞), so we update their values and set their predecessors to 𝐴. We also mark vertex 𝐴as visited. Important\n𝐴𝐵and 𝐴𝐶arenote: the best known distances of B and C are being set to 6 and 4 because the edge weights of 6 and 4 — instead, it’snot\n𝐵and 𝐶frombecause the best known distances to are 6 and 4. This will make a bigger difference in later iterations as wethe source vertex\nmove past the starting vertex.\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nF\n6\nA\nF\n4\nA\n∞-F\n∞-F\n∞-F\nNow that vertex 𝐴has been visited, we will pick our next vertex by finding the unvisited vertex with the smallest best known distance. Here,\nvertex 𝐵has a best known distance of 6, vertex 𝐶has a best known distance of 4, and all remaining unvisited vertices have a best known\ndistance of ∞. The best known distance of vertex 𝐶is smallest, so we set 𝐶as our new current vertex and mark it as visited.\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nF\n6\n𝐴\nT\n4\n𝐴\n∞-F\n∞-F\n∞-F", "word_count": 681, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5cf8b59e-986b-5bf8-9ef1-c4cf59acd96b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 966, "real_page_number": null, "text": "954\nChapter 25. Shortest Path Algorithms\nWe will now repeat what we did earlier and iterate over all the unvisited neighbors of vertex 𝐶(which are 𝐵, 𝐷, and 𝐹) and compute the best\ndistance to reach these vertices while going through vertex 𝐶. Here, the best distance to get from the source vertex to 𝐵while passing through\nvertex 𝐶is equal to 𝑑𝐶+ 5. This is better than the best known distance of 6 for 𝐵so far, so we update 𝐵’s best known distance to𝐶𝐵=4+1=\n5 and set its predecessor to 𝐶. Similarly, the best distance to get to vertex 𝐷through vertex 𝐶is 𝑑𝐶+ (which is better than 𝐷’s𝐶𝐷=4+5=9\ncurrent best known distance of ∞), and the best distance to get to vertex 𝐹through vertex 𝐶is 𝑑𝐶+ (which is better than 𝐹’s𝐶𝐹=4+7=11\nbest known distance of ∞). Thus, we also update 𝐷and 𝐹’s best known distances and predecessors accordingly.\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nF\n5\nC\nT\n4\n𝐴\nF\n9\nC\n∞-F\nF 𝐶11\nThe unvisited vertex with the smallest best known distance is now vertex 𝐵, so we set it as our current vertex. We then iterate over its unvisited\nneighbors, 𝐷and 𝐸, and update our table accordingly.\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n5 𝐶\nT\n4\n𝐴\nF\n7\nB\nF\n7\nB\nF 11 𝐶\nBoth vertex 𝐷and 𝐸now have the smallest unvisited tentative distance, so we can choose either of them for our next vertex. Here, we will\nchoose vertex 𝐷first; however, since 𝐷has no unvisited neighbors, we only need to mark 𝐷as visited.\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n5 𝐶\nT\n4\n𝐴\nT\n7\n𝐵\nF\n7\n𝐵\nF 11 𝐶\nWe then choose 𝐸as our next vertex to visit. Vertex 𝐸has one unvisited vertex (𝐹), and the best distance to get to vertex 𝐹through vertex 𝐸is\n10. Therefore, we update the best known distance of 𝐹to 10 and its predecessor to 𝐸.𝑑𝐸+𝐸𝐹=7+3=\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n5 𝐶\nT\n4\n𝐴\nT\n7\n𝐵\nT\n7\n𝐵\nF 10 E\nVertex 𝐹is the only vertex that remains unvisited, and its best known distance is not ∞, so we mark it as visited and our algorithm is complete.\nThis is the final result of running Dijkstra’s algorithm on the graph:\nA\nB\nC\nD\nE\nF\n6\n4\n1\n2\n2\n5\n7\n3\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT\n5 𝐶\nT\n4\n𝐴\nT\n7\n𝐵\nT\n7\n𝐵\n10T 𝐸", "word_count": 512, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "eceb759e-27c6-50c5-95e1-b553f033abfa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 967, "real_page_number": null, "text": "25.2 Dijkstra’s Algorithm\n955\nAt this point, the 𝑑𝑣column of the table now gives us the shortest distance from the starting vertex 𝐴to every other vertex in the graph (for\ninstance, 10, so the shortest path from 𝐴to 𝐹has a total weight of 10). We can also use the predecessors to determine the path we took𝑑𝐹=\nfrom 𝐴to any other node: for instance, 𝐹’s predecessor is 𝐸, 𝐸’s predecessor is 𝐵, 𝐵’s predecessor is 𝐶, and 𝐶’s predecessor is 𝐴, so the\nshortest path from 𝐴to 𝐹must be 𝐴→𝐶→𝐵→𝐸→𝐹.\nRemark: Even though we won’t go over an example here, Dijkstra’s algorithm on a graph works the same way. When iterating overdirected\nthe neighbors of a vertex, make sure to consider edge direction and visit only the neighbors that are directly reachable along a directed edge.\nSimilar to Prim’s algorithm, there are two ways to implement Dijkstra’s algorithm. These two implementations differ in how the smallest\nunvisited 𝑑𝑣is found. One implementation relies on a of all the vertices to find the unvisited vertex with the smallest 𝑑𝑣. The codelinear search\nfor this approach is shown below:\n1\nstruct DijkstraData {\n2\ndouble d;\n3\nint32_t p;\n4\nbool k;\n5\nDijkstraData()\n6\nstd::numeric_limits<double>::infinity() false: d{ }, p{ -1 }, k{ } {}\n7\n};\n8\n9\n// graph is in the form of an adjacency matrix\n10\nint32_t dijkstra(const std::vector<std::vector<int32_t>>& int32_t int32_tgraph, src, dest) {\n11\nstd::vector<DijkstraData> dijkstra_table(graph.size());\n12\ndijkstra_table[src].d = 0;\n13\nfor (size_t count = 0; count < graph.size(); ++count) {\n14\n// set current vertex as unvisited vertex with smallest tentative distance\n15\nint32_t std::numeric_limits<int32_t>::max();min_dist =\n16\nsize_t idx = 0;\n17\nfor (size_t i = 0; i < dijkstra_table.size(); ++i) {\n18\nif (!dijkstra_table[i].k && dijkstra_table[i].d < min_dist) {\n19\nmin_dist = dijkstra_table[i].d;\n20\nidx = i;\n21\n} // if\n22\n} // for i\n23\n24\nif std::numeric_limits<int32_t>::max())(min_dist == {\n25\nreturn -1; // no solution\n26\n} // if\n27\n28\n// mark idx as visited\n29\ntrue;dijkstra_table[idx].k =\n30\n31\n// update table if distance is better\n32\nfor (size_t neighbor = 0; neighbor < graph.size(); ++neighbor) {\n33\nint32_t new_dist =\ndijkstra_table[idx].d + graph[idx][neighbor];\n34\nif (!dijkstra_table[neighbor].k && new_dist < dijkstra_table[neighbor].d) {\n35\ndijkstra_table[neighbor].d = new_dist;\n36\ndijkstra_table[neighbor].p = idx;\n37\n} // if\n38\n} // for neighbor\n39\n} // for count\n40\n41\nreturn dijkstra_table[dest].d;\n42\n} // dijkstra()\nΘ(|𝑉|2), |𝑉|The time complexity of the linear search approach is where is the number of vertices in the graph; this is because a linear pass is\n|𝑉|completed times to find the minimum 𝑑𝑣for every vertex in the graph (lines 17-22). This is ideal if the given graph is dense, since each\nvertex would have many connecting edges, and iterating over most of the neighbors would be necessary. However, if you are given a sparse\ngraph, each vertex would not have as many connections: this makes a linear traversal over all vertices a wasteful endeavor. In this case, much\nlike with Prim’s algorithm, it would be better to use a instead of a linear search to identify the unvisited vertex with the smallest 𝑑𝑣.min-heap", "word_count": 534, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6a0e1faa-1652-5876-a4b6-35ac540f7b59", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 968, "real_page_number": null, "text": "956\nChapter 25. Shortest Path Algorithms\nThe heap-implementation of Dijkstra’s algorithm is shown below:\n1\nstruct DijkstraData {\n2\ndouble d;\n3\nint32_t p;\n4\nbool k;\n5\nDijkstraData()\n6\nstd::numeric_limits<double>::infinity() false: d{ }, p{ -1 }, k{ } {}\n7\n};\n8\n9\nusing std::vector<std::vector<std::pair<int32_t, int32_t>>>;AdjList = // <neighbor, weight>\n10\nusing std::pair<int32_t, int32_t>;DistPair =\n// <d_v, v>\n11\n12\nint32_t dijkstra(const int32_t int32_tAdjList& graph, src, dest) {\n13\nstd::vector<DijkstraData> dijkstra_table(graph.size());\n14\nstd::priority_queue<DistPair, std::vector<DistPair>, std::greater<DistPair>> pq;\n15\ndijkstra_table[src].d = 0;\n16\npq.emplace(0, src);\n17\n18\nwhile (!pq.empty()) {\n19\nauto [min_dist, idx] = pq.top();\n20\npq.pop();\n21\n22\nif std::numeric_limits<int32_t>::max())(min_dist == {\n23\nreturn -1; // no solution\n24\n} // if\n25\n26\nif (!dijkstra_table[idx].k) {\n27\ntrue;dijkstra_table[idx].k =\n28\nfor (auto& neighbor_dist_pair : graph[idx]) {\n29\nauto [neighbor, dist] = neighbor_dist_pair;\n30\nint32_t new_dist = dijkstra_table[idx].d + dist;\n31\nif (new_dist < dijkstra_table[neighbor].d) {\n32\ndijkstra_table[neighbor].d = new_dist;\n33\ndijkstra_table[neighbor].p = idx;\n34\npq.emplace(new_dist, neighbor);\n35\n} // if\n36\n} // for neighbor_dist_pair\n37\n} // if\n38\n} // while\n39\n40\nreturn dijkstra_table[dest].d;\n41\n} // dijkstra()\n|𝐸| |𝑉|What is the time complexity of the heap implementation, defined in terms of the number of edges and vertices in the graph? If a binary\nΘ(log(|𝑉|))heap is used as the underlying implementation, it takes time to pop a vertex from the heap, which is done at most once for each of\n|𝑉| Θ(|𝐸|)the vertices in the graph. In addition, we iterate over all the neighbors of the current vertex (which takes at most time) and update\nΘ(log(|𝑉|))their best known distances and push them into the heap if needed (where each push takes time). Combining these together, the\nΘ(|𝑉|log(|𝑉|)+|𝐸|log(|𝑉|)). |𝐸|worst-case time complexity of Dijkstra’s algorithm using a min-binary heap is Since is significantly larger\n|𝑉| Θ(|𝐸|log(|𝑉|))than in the worst case, we can also express this complexity class as by removing lower order terms.\nRemark: Similar to Prim’s algorithm, if you use a Fibonacci heap as the underlying implementation of the priority queue (which allows an\nΘ(|𝑉|log(|𝑉|)+element’sprioritytobedecreasedinamortized time),theworst-casetimecomplexityofDijkstra’salgorithmdropsfromΘ(1)\nΘ(|𝑉|log(|𝑉|)+|𝐸|), Θ(log(|𝑉|))|𝐸|log(|𝑉|)) to as the cost of updating a vertex’s priority would no longer take time. This is why you\nΘ(|𝐸|+|𝑉|log(|𝑉|))might see the time complexity of Dijkstra’s as in other resources; in this class, however, we will assume that a binary\nheap is used unless otherwise specified.\n|𝐸|Similar to Prim’s algorithm, the heap approach is less performant than the linear search approach for dense graphs. This is because is on the\nΘ(|𝑉|2) Θ(|𝑉|2Θ(|𝐸|log(|𝑉|)) log(|𝑉|)).order of in a dense graph, so the time complexity of essentially becomes This is worse than the\nΘ(|𝑉|2).linear search time complexity of\nIn summary, you should use the linear search approach if you want to run Dijkstra’s algorithm on a dense graph, and the min-heap approach\nΘ(|𝑉|2),if you want to run it on a sparse graph. The worst-case time complexity of the linear search approach is and the worst-case time\nΘ(|𝐸|log(|𝑉|)).complexity of the min-heap approach is Notice that this result directly mirrors our analysis of Prim’s algorithm! It comes as no\nsurprise that these two algorithms share the same time complexities, as they apply a similar greedy strategy to a graph, even if the work done at\neach step is slightly different.\nSummary of Dijkstra’s Algorithm Time Complexities\nImplementation Method\nTime Complexity\nAdjacency matrix, linear search\nΘ(|𝑉|2)\nAdjacency list, binary heap\nΘ(|𝐸|log(|𝑉|))\nAdjacency list, Fibonacci heap\nΘ(|𝐸|+|𝑉|log(|𝑉|))", "word_count": 598, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2da34a1b-c676-5cb8-9548-4272dc7d283e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 969, "real_page_number": null, "text": "25.2 Dijkstra’s Algorithm\n957\n¸ 25.2.2\n(✽)Proving Dijkstra’s Correctness\nNow that we have discussed Dijkstra’s algorithm works, it is also worthwhile to take a look at it works. How do we know that Dijkstra’show why\nalgorithm always returns an optimal solution? To understand why, it is important to understand the core assumption that Dijkstra’s makes: once\nWe will use a proof by contradiction to show thata vertex is marked as visited, its best known distance from the source vertex must be optimal.\nthis assumption always holds, as long as there are no negative edges.\nLet us denote as the length of the actual optimal shortest path from to 𝑣𝑛. For Dijkstra’s to fail, we claim that there must exist𝛿(𝑣1,𝑣𝑛) 𝑣1\nsome vertex 𝑣𝑘reachable from that is the first to be marked as visited with a best known distance from the source vertex 𝑣1: i.e.,𝑣1 suboptimal\n𝛿(𝑣1,𝑣𝑘). Since a path must exist between and this first incorrectly marked vertex 𝑣𝑘, we will define a shortest path from to 𝑣𝑘as𝑑𝑣𝑘> 𝑣1 𝑣1\n𝑘th⟨𝑣1,𝑣2,𝑣3,…,𝑣𝑘⟩, where 𝑘is the number of vertices in this shortest path and 𝑣𝑘is the vertex along 𝑃𝑣1→𝑣𝑘. There are two key𝑃𝑣1→𝑣𝑘=\nobservations we can make here:\n1. Since 𝑣𝑘is the vertex to be marked as visited with a suboptimal best known distance, all vertices that precede it in its shortestfirst\npath (i.e., 𝑣1,𝑣2,…,𝑣𝑘−1) must have a best known distance that is optimal. In other words, 𝛿(𝑣1,𝑣1), 𝛿(𝑣1,𝑣2), …, and𝑑𝑣1 𝑑𝑣2= =\n𝛿(𝑣1,𝑣𝑘−1).1𝑑𝑣𝑘−1 =\n≤𝑖≤𝑗≤𝑘)⟨𝑣𝑖,𝑣𝑖+1,…,𝑣𝑗⟩ofAny subpath 𝑃𝑣1→𝑣𝑘(1 must itself be optimal, or we would be able to replace this subpath to2. 𝑃𝑣𝑖→𝑣𝑗=\nget a shorter path from to 𝑣𝑘(i.e., since there would be a shorter way to get from 𝑃𝑣𝑖to 𝑃𝑣𝑗).𝑣1\nRight before we mark vertex 𝑣𝑘as visited, there are two scenarios that are possible:\n1. 𝑣𝑘is 𝑃𝑣1→𝑣𝑘thatVertex the first vertex along is unvisited.\nIn this case, all vertices from to have been visited. From our initial claim that 𝑣𝑘is the first vertex to be visited without an optimal best𝑣1 𝑣𝑘−1\nknowndistance, weknowthatthebestknowndistanceofvertex (thevertexdirectlypreceding𝑣𝑘)mustbeoptimal, i.e., 𝛿(𝑣1,𝑣𝑘−1).𝑣𝑘−1 𝑑𝑣𝑘−1 =\nThus, when we update 𝑣𝑘’s best known distance while processing vertex 𝑣𝑘−1, 𝑑𝑣𝑘is set to a value no larger than [𝛿(𝑣1,𝑣𝑘−1) + 𝑊(𝑣𝑘−1,𝑣𝑘)]\n(where is defined as the weight of the edge connecting vertices 𝑥and 𝑦). However, since the edge connecting and 𝑣𝑘is on a𝑊(𝑥,𝑦) 𝑣𝑘−1\nshortest path from to 𝑣𝑘, the value of [𝛿(𝑣1,𝑣𝑘−1) + 𝑊(𝑣𝑘−1,𝑣𝑘)] must be equal to 𝛿(𝑣1,𝑣𝑘). Therefore, if we explore before 𝑣𝑘, the𝑣1 𝑣𝑘−1\n𝑑𝑣𝑘≠𝛿(𝑣1,𝑣𝑙),value of 𝑑𝑣𝑘cannot be greater than 𝛿(𝑣1,𝑣𝑘). We initially claimed that so this results in a contradiction.\n2. 𝑣𝑐, <𝑐<𝑘, 𝑃𝑣1→𝑣𝑘thatThere exists a vertex that is the first vertex along is unvisited.1\nIf some other vertex 𝑣𝑐is the first vertex along 𝑃𝑣1→𝑣𝑘that remains unvisited, we know that the predecessor of this vertex must be visited𝑣𝑐−1\nand that 𝛿(𝑣1,𝑣𝑐−1). Thus, after exploring vertex 𝑣𝑐−1, the best known distance of vertex 𝑣𝑐must be no greater than + 𝑊(𝑣𝑐−1,𝑣𝑐).𝑑𝑣𝑐−1 𝑑𝑣𝑐−1=\nTherefore,\n𝑑𝑣𝑐≤𝑑𝑣𝑐−1 +𝑊(𝑣𝑐−1,𝑣𝑐)\nSince 𝛿(𝑣1,𝑣𝑐−1), we can rewrite this inequality as𝑑𝑣𝑐−1 =\n𝑑𝑣𝑐≤𝛿(𝑣1,𝑣𝑐−1)+𝑊(𝑣𝑐−1,𝑣𝑐)\nNotice here that is the distance of the path 𝑃𝑣1→𝑣𝑐, which is a subpath of 𝑃𝑣1→𝑣𝑘. Because we defined 𝑃𝑣1→𝑣𝑘as an𝛿(𝑣1,𝑣𝑐−1)+𝑊(𝑣𝑐−1,𝑣𝑐)\noptimal path from to 𝑣𝑘at the beginning of the proof, we can use our second observation to conclude that the distance of this subpath must𝑣1\nitself be optimal between and 𝑣𝑐. In other words, = 𝛿(𝑣1,𝑣𝑐).𝑣1 𝛿(𝑣1,𝑣𝑐−1)+𝑊(𝑣𝑐−1,𝑣𝑐)\n𝑑𝑣𝑐≤𝛿(𝑣1,𝑣𝑐−1)+𝑊(𝑣𝑐−1,𝑣𝑐)\nand\n=𝛿(𝑣1,𝑣𝑐−1)+𝑊(𝑣𝑐−1,𝑣𝑐) 𝛿(𝑣1,𝑣𝑐)\nimplies\n𝑑𝑣𝑐≤𝛿(𝑣1,𝑣𝑐)\nUnder the assumption that there are graph, cannot be greater than 𝛿(𝑣1,𝑣𝑘), since 𝑣𝑐precedes 𝑣𝑘in our𝛿(𝑣1,𝑣𝑐)no negative edges in the\nshortest path (and thus there must be at least one more non-negative edge in the graph after 𝑣𝑐). Using our initial claim that 𝑑𝑣𝑘is not optimal,\nwe would get the following inequality.\n𝑑𝑣𝑐≤𝛿(𝑣1,𝑣𝑐)\nand\n𝛿(𝑣1,𝑣𝑐)<𝛿(𝑣1,𝑣𝑘)\nand\n𝛿(𝑣1,𝑣𝑘)<𝑑𝑣𝑘\nimplies\n𝑑𝑣𝑐<𝑑𝑣𝑘\nFrom here, we can see that 𝑑𝑣𝑘if 𝑣𝑘were marked as visited with a suboptimal 𝑑𝑣𝑘. However, this is a problem, since Dijkstra’s algorithm𝑑𝑣𝑐<\nwould never mark 𝑣𝑘as visited before 𝑣𝑐if 𝑑𝑣𝑘. Therefore, it is impossible for 𝑣𝑘to be marked as visited before any preceding vertex 𝑣𝑐if𝑑𝑣𝑐<\n𝑑𝑣𝑘were not optimal, which contradicts our claim that 𝑣𝑘is marked as visited with a suboptimal best known distance. From this proof, we have\nshown that vertex 𝑣𝑘— or a vertex whose best known distance is suboptimal when marked as visited — cannot possibly exist. Thus, we have\nproved the correctness of Dijkstra’s algorithm.\n¸ 25.2.3\nWhy Dijkstra’s Algorithm Fails on Negative Edges\nHowever, once we add negative edges into the equation, the proof falls apart, and Dijkstra’s algorithm can no longer guarantee an optimal\nsolution. Previously, when we marked a vertex as visited, we were sure that no other paths could yield a better solution — this is because any\nadditional paths not yet encountered by the algorithm would add a non-negative amount of weight, and thus cannot be optimal. When we allow\nnegative edges, this assumption is no longer true; even after marking a vertex as visited, there is still the off chance that another unencountered\npath has a negative edge that could give us a better solution.\n1For this observation to hold, we must also show that 𝑑𝑣cannot be than for any vertex 𝑣. A quick proof would show that, since the best known𝛿(𝑠,𝑣)better\ndistanceofanyvertexiscomputedusingthebestknowndistancesofearliervertices(allthewaybacktothesourcevertex),itwouldbeimpossibletosetany\nvertex’sbestknowndistancetoavaluebetterthantheactualdistancerequiredtoreachit.", "word_count": 968, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "271a5ef1-565c-57d9-b5ff-45f74f8c2a3e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 970, "real_page_number": null, "text": "958\nChapter 25. Shortest Path Algorithms\nFor example, consider the following graph, where we want to find the shortest path from 𝐴to 𝐶:\nA\nB\nC\n4\n2\n-3\nDijkstra’s algorithm would greedily select edge 𝐴𝐶for a distance of 2, since it is better than the distance of 4 from edge 𝐴𝐵. As a result, the\nalgorithm would mark vertex 𝐶as visited with a best known distance of 2… however, this is not the optimal solution! If we had gone through\nvertex 𝐵instead, we would have ended up with a better distance of 4 + (-3) = 1. However, Dijkstra’s algorithm failed to find this optimal path\nbecause it did not know about edge 𝐵𝐶and that its weight was negative enough to overcome the seemingly suboptimal choice of edge 𝐴𝐵.\nA\nB\nC\n4\n2\n-3\nDijkstra Solution\nA\nB\nC\n4\n2\n-3\nOptimal Solution\nIn general, if there are multiple ways to reach a vertex in a graph, and at least one of those ways involves a negative edge, then Dijkstra’s\nalgorithm could potentially mark that vertex as visited before encountering the negative edge. Since Dijkstra’s uses a greedy approach, it never\nreconsiders this decision and assumes that the solution it found is optimal, using it to construct the solutions of subsequent vertices that are\nencountered later on. By the time the algorithm discovers a negative edge that improves the solution of a previously visited vertex, it is too\nlate! This is why Dijkstra’s algorithm is avoided if a graph has negative edges. If you do have a graph with negative edges, it is better to use a\ndifferent algorithmic approach that can handle this scenario (specifically dynamic programming, which will be discussed in the next section).\n¸ 25.2.4\nChanging Edge Weights in the Shortest Path\nExample 25.1 Let 𝑃be the shortest path from vertex 𝑠to vertex 𝑡of a given graph. If the weight of every edge in the graph is multiplied\nby the same positive constant 𝑘(i.e., the weight of each edge is changed from 𝑥to 𝑘𝑥), is 𝑃still guaranteed to be the shortest path in the\ngraph? Or could the shortest path change?\nThe shortest path does not change if you multiply every edge by a constant factor. We can show this using a proof by contradiction: suppose\n𝑃′ 𝑃′there exists a different path from 𝑠to 𝑡that is shorter than 𝑃after all the edges are multiplied by 𝑘. However, if we divide all the edges in\n𝑃′by 𝑘, we would see that must have been shorter before all the edges were multiplied. Therefore, 𝑃would not have been the shortest path,\nwhich results in a contradiction. In general, multiplying all edges by a constant factor does not change the shortest path because multiplication is\ndistributive (unlike addition, which we will see in the next example).\nExample 25.2 Let 𝑃be the shortest path from vertex 𝑠to vertex 𝑡of a given graph. If the weight of every edge in the graph is incremented\nby the same positive constant 𝑘(i.e., the weight of each edge is changed from 𝑥to 𝑘+𝑥), is 𝑃still guaranteed to be the shortest path in the\ngraph? Or could the shortest path change?\nUnlike multiplication, addition does not guarantee that the shortest path will remain the same. This can be shown using a quick example: in the\nfollowing graph, the shortest path from 𝐴to 𝐶is to go through 𝐵for a total weight of 1 + 2 = 3.\nA\nB\nC\n1\n4\n2\nHowever, if we add 2 to every edge, the shortest path now travels from 𝐴to 𝐶directly for a total weight of 6.\nA\nB\nC\n3\n6\n4\nThe reason for this is that addition, unlike multiplication, does not distribute weight proportionally across all edges in the graph. This ends up\npunishing paths with more edges significantly more than paths with fewer edges, as longer paths have more edges on which additional weight\ncan be applied. As a result, a path that is initially optimal may be overtaken by a path with fewer edges if the same weight is applied equally to\nevery edge of the graph.", "word_count": 687, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2bbd8816-40c2-51e2-9b18-d5f1421cdf25", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 971, "real_page_number": null, "text": "25.2 Dijkstra’s Algorithm\n959\n¸ 25.2.5\nSolving Problems Using Dijkstra’s Algorithm\nExample 25.3 timesYou are given a network of 𝑛nodes, labeled to 𝑛, and a vector of travel times as directed edges, such that the1\ntimes[i] = (u, v, w), u v wvalue of where is the source node, is the target node, and is the time it takes for a signal to travel from\nsource to target. Write a function that takes in a vector of delay times, the number of nodes 𝑛, and a starting node 𝑠𝑟𝑐, and returns the time it\n-1.would take for all nodes to receive a signal that is sent from node 𝑘. If it is not possible for all nodes to receive the signal, return\nint32_t network_delay_time(const std::vector<std::vector<int32_t>>& int32_t int32_ttimes, n, src);\ntimes = [[2, 1, 1], [2, 3, 1], [3, 4, 1]], n = 4, src = 2, 2.Example: Given and you would return This is because it takes\ntwo units of time for all nodes in the network to receive the signal (where node to takes the longest).2 4\n2\n1\n3\n4\n1\n1\n1\nThis is a shortest-path problem on a weighted graph, so Dijkstra’s algorithm can be used to solve it. First, we will need to convert our input\ninto either an adjacency list or adjacency matrix so that Dijkstra’s can be applied. Whether a list or matrix is chosen depends on the input\n(e.g., whether the graph is dense or sparse, time complexity constraints, etc.), but since we are not given any additional information in the\nexample, we will implement our first solution using an adjacency list. After converting the input into an adjacency list, we can run the binary\nheap implementation of Dijkstra’s algorithm covered previously, as shown below. Then, when the algorithm completes, we can look through\neach of the vertices to identify which one requires the most time to receive a message from the source vertex (using the final value of 𝑑𝑣).\n1\nstruct DijkstraData {\n2\nint32_t d;\n3\nint32_t p;\n4\nbool k;\n5\nDijkstraData()\n6\nstd::numeric_limits<int32_t>::max() false: d{ }, p{ -1 }, k{ } {}\n7\n};\n8\n9\nusing std::pair<int32_t, int32_t>;DistPair =\n// <d_v, v>\n10\n11\nint32_t network_delay_time(const std::vector<std::vector<int32_t>>& int32_t int32_ttimes, n, src) {\n12\n// convert times vector into an adjacency list\n13\nstd::unordered_map<int32_t, std::vector<std::pair<int32_t, int32_t>>> graph;\n14\nfor (auto& time_vec : times) {\n15\ngraph[time_vec[0]].emplace_back(time_vec[1], time_vec[2]);\n16\n} // for time_vec\n17\n18\nstd::vector<DijkstraData> dijkstra_table(n + 1);\n19\nstd::priority_queue<DistPair, std::vector<DistPair>, std::greater<DistPair>> pq;\n20\ndijkstra_table[src].d = 0;\n21\npq.emplace(0, src);\n22\n23\nwhile (!pq.empty()) {\n24\nauto [min_dist, idx] = pq.top();\n25\npq.pop();\n26\nif (!dijkstra_table[idx].k) {\n27\ntrue;dijkstra_table[idx].k =\n28\nfor (auto& neighbor_dist_pair : graph[idx]) {\n29\nauto [neighbor, dist] = neighbor_dist_pair;\n30\nint32_t new_dist = dijkstra_table[idx].d + dist;\n31\nif (new_dist < dijkstra_table[neighbor].d) {\n32\ndijkstra_table[neighbor].d = new_dist;\n33\ndijkstra_table[neighbor].p = idx;\n34\npq.emplace(new_dist, neighbor);\n35\n} // if\n36\n} // for neighbor_dist_pair\n37\n} // if\n38\n} // while\n39\n40\n// return largest distance in the final Dijkstra table\n41\nint32_t std::numeric_limits<int32_t>::min();max_dist =\n42\nfor (auto it = dijkstra_table.begin() + 1; it != dijkstra_table.end(); ++it) {\n43\nmax_dist = std::max(max_dist, it->d);\n44\n} // for data\n45\nreturn std::numeric_limits<int32_t>::max()max_dist == ? -1 : max_dist;\n46\n} // network_delay_time()", "word_count": 559, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "992ab445-c197-5f6e-b93d-49531d58b4c4", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 972, "real_page_number": null, "text": "960\nChapter 25. Shortest Path Algorithms\nWe can write a similar solution using an adjacency matrix and linear search, as shown (this code is nearly identical to the linear search solution\nintroduced in our initial discussion of Dijkstra’s algorithm, where we perform a linear search over the vertices to determine which unvisited\nvertex has the minimal distance, rather than using a heap).\n1\nstruct DijkstraData {\n2\nint32_t d;\n3\nint32_t p;\n4\nbool k;\n5\nDijkstraData()\n6\nstd::numeric_limits<int32_t>::max() false: d{ }, p{ -1 }, k{ } {}\n7\n};\n8\n9\nint32_t network_delay_time(const std::vector<std::vector<int32_t>>& int32_t int32_ttimes, n, src) {\n10\n// convert times vector into an adjacency matrix\n11\nstd::vector<std::vector<int32_t>> std::vector<int32_t>(ngraph(n + 1, + 1, -1));\n12\nfor (auto& time_vec : times) {\n13\ngraph[time_vec[0]][time_vec[1]] = time_vec[2];\n14\n} // for time_vec\n15\n16\nstd::vector<DijkstraData> dijkstra_table(n + 1);\n17\ndijkstra_table[src].d = 0;\n18\nfor (size_t count = 0; count < graph.size(); ++count) {\n19\nint32_t std::numeric_limits<int32_t>::max();min_dist =\n20\nsize_t idx = 0;\n21\nfor (size_t i = 0; i < dijkstra_table.size(); ++i) {\n22\nif (!dijkstra_table[i].k && dijkstra_table[i].d < min_dist) {\n23\nmin_dist = dijkstra_table[i].d;\n24\nidx = i;\n25\n} // if\n26\n} // for i\n27\ntrue;dijkstra_table[idx].k =\n28\nfor (size_t neighbor = 0; neighbor < graph.size(); ++neighbor) {\n29\nint32_t new_dist = dijkstra_table[idx].d + graph[idx][neighbor];\n30\nif (!dijkstra_table[neighbor].k && graph[idx][neighbor] != -1\n31\n&& new_dist < dijkstra_table[neighbor].d) {\n32\ndijkstra_table[neighbor].d = new_dist;\n33\ndijkstra_table[neighbor].p = idx;\n34\n} // if\n35\n} // for neighbor\n36\n} // for count\n37\n38\n// return largest distance in the final Dijkstra table\n39\nint32_t std::numeric_limits<int32_t>::min();max_dist =\n40\nfor (auto it = dijkstra_table.begin() + 1; it != dijkstra_table.end(); ++it) {\n41\nmax_dist = std::max(max_dist, it->d);\n42\n} // for data\n43\nreturn std::numeric_limits<int32_t>::max()max_dist == ? -1 : max_dist;\n44\n} // network_delay_time()\nΘ(|𝑉|2).Θ(|𝑉|+|𝐸|),The time complexity of the adjacency list solution is and the time complexity of the adjacency matrix solution is\n25.3\nBellman-Ford Algorithm (✽)\nIf negative edges exist in a graph, Dijkstra’s algorithm cannot be used, since there is no guarantee that the greedy choice will always lead to\nan optimal solution. Instead, we will have to rely on a different algorithm to find a shortest path. One such algorithm is the Bellman-Ford\nalgorithm, which uses dynamic programming to solve the single-source shortest path problem.\nThe Bellman-Ford algorithm uses the following recurrence relation, where represents the weight of the shortest path from the source𝑑(𝑣,𝑘)\nvertex 𝑠to 𝑣using at most 𝑘edges. If and 𝑠(source vertex is also the destination), then the weight of the shortest path is trivially 0𝑘= 𝑣=0\n𝑣≠𝑠,since no edges are required to get from a vertex to itself. Similarly, if and then the weight of the shortest path is ∞, since there is𝑘=0\nno way to travel from one vertex to another using 0 edges. If neither of these cases apply, then the algorithm would take the of thesmaller\nfollowing to compute 𝑑(𝑣,𝑘):\n𝑘th1. This is the smaller value of the two if allowing aThe weight of the shortest path from s to v using at most k - 1 edges, i.e., d(v, k - 1).\nedge in the path from 𝑠to 𝑣does not improve the best known solution.\n2. This is equal to the minimum weight of the shortest path fromThe weight of the shortest path from s to v using exactly k edges.\n𝑠to 𝑢using at most edges, plus the weight of 𝑢to 𝑣, for all vertices 𝑢such that 𝑢→𝑣exists as an edge in the graph, (i.e.,𝑘−1\n𝑘thmin(𝑢→𝑣)∈𝐸{𝑑(𝑢,𝑘−1)+𝑊(𝑢,𝑣)}). This is the smaller value if allowing a edge in the path from 𝑠to 𝑣improves the solution.\n𝑑(𝑣,𝑘)=\n⎧\n⎪\n⎨\n⎪⎩\n0,\nif and𝑘= 𝑣=𝑠0\n∞,\n𝑣≠𝑠if and𝑘=0\nmin(𝑑(𝑣,𝑘−1), min(𝑢→𝑣)∈𝐸{𝑑(𝑢,𝑘−1)+𝑊(𝑢,𝑣)}),\notherwise", "word_count": 647, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7fad2b9d-6d64-5374-92cc-ac2b7690e640", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 973, "real_page_number": null, "text": "25.3 Bellman-Ford Algorithm\n961\nThe Bellman-Ford algorithm provides an optimized implementation of this recurrence relation by completing the following steps:\n|𝑉| |𝑉|1. Given vertices, initialize a table of size that stores\n• the of each vertex from the given source vertexbest known distance\n• the vertex, which precedes the current vertex along the path with the best known distancepredecessor\n2. Iterate over edges of the graph and compute the distance to reach the edge’s terminal vertex along a path that passes through that edgeall\n(updating any best known distances if necessary). Note that this is different from Dijkstra’s algorithm, which only updates the best known\nalldistances of vertices that are locally reachable from the current vertex. We need to consider edges with each iteration to handle the\npossibility of negative edges later on that may improve our solution.\n|𝑉|−13. Repeat step 2 a total of times. After doing this, the table now stores the optimal distance from the source vertex to every other\nvertex in the graph.\nTo visualize this process, consider the following graph, for which we want to find the shortest path from 𝐴to 𝐷.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\nFirst, we will instantiate a table that keeps track of each vertex’s best known distance (𝑑𝑣) as well as its predecessor (𝑝𝑣). From the base case,\nthe source vertex has its best known distance set to 0, while all others have theirs set to ∞.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(∞)\n(∞)\n(∞)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n∞-\n∞-\n∞-\n|𝑉|−1Next, we will iterate over all the edges of the graph times and determine if each edge yields a better solution for its terminal vertex. In\nother words, for each edge 𝑢→𝑣, we calculate if the best known distance to 𝑢, plus the weight of the edge 𝑢→𝑣, improves the best known\ndistance to 𝑣. The edges can be processed in any order, so for our example, we will process the edges in the following arbitrary ordering:\n⃖⃖⃖⃖⃖⃗\n⃖⃖⃖⃖⃖⃗𝐵𝐷,\n𝐴𝐵,⃖⃖⃖⃖⃖⃖⃗\n⃖⃖⃖⃖⃖⃗𝐶𝐷,\n⃖⃖⃖⃖⃖⃗𝐶𝐵,\n𝐴𝐶\n|𝑉| |𝑉|−1We will now begin iterating over the edges of the graph. Since the number of vertices is 4, we will perform iterations.= =3\nIteration 1:\n⃖⃖⃖⃖⃖⃗First, we will consider edge\n𝐵𝐷. The current best known distance to 𝐵is ∞, and the weight of the edge is 4. Thus, the best known distance to\n⃖⃖⃖⃖⃖⃗𝐷using edge\n𝐵𝐷is \"∞+4\" (or just ∞), which is not better than the current best known distance to 𝐷of ∞. Therefore, nothing in the table\ngets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(∞)\n(∞)\n(∞)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n∞-\n∞-\n∞-\n⃖⃖⃖⃖⃖⃗Next, we will consider edge\n𝐴𝐵. The current best known distance to 𝐴is 0, and the weight of the edge is 3. Thus, the best known distance to 𝐵\n⃖⃖⃖⃖⃖⃗using edge\n𝐴𝐵is 0 + 3, which is better than the current best known distance to 𝐵of ∞. The best known distance to 𝐵is thereby updated to 3,\nand its predecessor is set to 𝐴.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(∞)\n(3)\n(∞)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n3 A\n∞-\n∞-", "word_count": 559, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a241ca2c-e678-5bb7-9fed-7c59491b9b00", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 974, "real_page_number": null, "text": "962\nChapter 25. Shortest Path Algorithms\n⃖⃖⃖⃖⃖⃖⃗Next, we will consider edge\n𝐶𝐷. The current best known distance to 𝐶is ∞, and the weight of the edge is 2. Thus, the best known distance to\n⃖⃖⃖⃖⃖⃖⃗𝐵using edge\n𝐶𝐷is \"∞+2\" (or just ∞), which is not better than the current best known distance to 𝐷of ∞. Therefore, nothing in the table\ngets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(∞)\n(3)\n(∞)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n3 A\n∞-\n∞-\n⃖⃖⃖⃖⃖⃗Next, we will consider edge\n𝐶𝐵. The current best known distance to 𝐵is ∞, and the weight of the edge is -5. Thus, the best known distance to\n⃖⃖⃖⃖⃖⃗𝐵using edge\n𝐶𝐵is \"∞−5\" (or just ∞), which is not better than the best known distance to 𝐵of 3. Therefore, nothing in the table gets updated\nat this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(∞)\n(3)\n(∞)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n3 A\n∞-\n∞-\n⃖⃖⃖⃖⃖⃗Lastly, we will consider edge\n𝐴𝐶. The current best known distance to 𝐴is 0, and the weight of the edge is 6. Thus, the best known distance to\n⃖⃖⃖⃖⃖⃗𝐶using edge\n𝐴𝐶is 0 + 6, which is better than the best known distance to 𝐶of ∞. The best known distance to 𝐶is thereby updated to 6, and its\npredecessor is set to 𝐴.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(3)\n(∞)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n3 A\n6 A\n∞-\nIteration 2:\n⃖⃖⃖⃖⃖⃗We will repeat the same process, this time using the values obtained from iteration 1. First, we consider edge\n𝐵𝐷. The current best known\n⃖⃖⃖⃖⃖⃗distance to 𝐵is 3, and the weight of the edge is 4. Thus, the best known distance to 𝐷using edge\n𝐵𝐷is 7, which is better than the current best\nknown distance to 𝐷of ∞. The best known distance to 𝐷is updated to 7, and its predecessor set to 𝐵.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(3)\n(7)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n3 A\n6 A\n7 B\n⃖⃖⃖⃖⃖⃗Next, we will consider edge\n𝐴𝐵. The current best known distance to 𝐴is 0, and the weight of the edge is 3. Thus, the best known distance to 𝐵\n⃖⃖⃖⃖⃖⃗using edge\n𝐴𝐵is still 3, so nothing in the table gets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(3)\n(7)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n3 A\n6 A\n7 B", "word_count": 437, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3f40f8fe-000f-51bd-9a1d-81a14836e3a8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 975, "real_page_number": null, "text": "25.3 Bellman-Ford Algorithm\n963\n⃖⃖⃖⃖⃖⃖⃗Next up is edge\n𝐶𝐷. The current best known distance to 𝐶is 6, and the weight of the edge is 2. Thus, the best known distance to 𝐷using edge\n⃖⃖⃖⃖⃖⃖⃗\n𝐶𝐷is 8. This is not better than the current best known distance to 𝐷of 7, so nothing gets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(3)\n(7)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n3 A\n6 A\n7 B\n⃖⃖⃖⃖⃖⃗Next, we will consider edge\n𝐶𝐵. The current best known distance to 𝐶is 6, and the weight of the edge is -5. Thus, the best known distance to 𝐵\n⃖⃖⃖⃖⃖⃗using edge\n𝐶𝐵is 1. This is better than the current best known distance to 𝐵of 3, so 𝐵’s best known distance is updated to 1, and its predecessor\nis updated to 𝐶.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(1)\n(7)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n1 C\n6 A\n7 B\n⃖⃖⃖⃖⃖⃗Lastly, we will consider edge\n𝐴𝐶. The current best known distance to 𝐴is 0, and the weight of the edge is 6. Thus, the best known distance to\n⃖⃖⃖⃖⃖⃗𝐶using edge\n𝐴𝐶is still 6, so nothing gets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(1)\n(7)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n1 C\n6 A\n7 B\nIteration 3:\n⃖⃖⃖⃖⃖⃗We will now complete one final iteration of the edges of the graph, using the values we obtained from iteration 2. First, we will look at edge\n𝐵𝐷.\n⃖⃖⃖⃖⃖⃗The current best known distance to 𝐵is 1, and the weight of the edge is 4. Thus, the best known distance to 𝐷using edge\n𝐵𝐷is 5, which is\nbetter than the current best known distance to 𝐷of 7. The best known distance to 𝐷is thereby updated to 5, and its predecessor is set to 𝐵.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(1)\n(5)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n1 C\n6 A\n5 B\n⃖⃖⃖⃖⃖⃗Next, we will consider edge\n𝐴𝐵. The current best known distance to 𝐴is 0, and the weight of the edge is 3. Thus, the best known distance to 𝐵\n⃖⃖⃖⃖⃖⃗using edge\n𝐴𝐵is still 3, so nothing gets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(1)\n(5)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n1 C\n6 A\n5 B", "word_count": 422, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2d55c6ad-4a60-56f4-bb07-ed8e7d19ee66", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 976, "real_page_number": null, "text": "964\nChapter 25. Shortest Path Algorithms\n⃖⃖⃖⃖⃖⃖⃗Next, we will consider edge\n𝐶𝐷. The current best known distance to 𝐶is 6, and the weight of the edge is 2. Thus, the best known\n⃖⃖⃖⃖⃖⃖⃗distance to 𝐷using edge\n𝐶𝐷is 8. This is not better than the current known distance to 𝐷of 5, so nothing gets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(1)\n(5)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n1 C\n6 A\n5 B\n⃖⃖⃖⃖⃖⃗Next, we will consider edge\n𝐶𝐵. The current best known distance to 𝐶is 6, and the weight of the edge is -5. Thus, the best known distance to\n⃖⃖⃖⃖⃖⃗𝐵using edge\n𝐶𝐵is still 1, so nothing gets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(1)\n(5)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n1 C\n6 A\n5 B\n⃖⃖⃖⃖⃖⃗Lastly, we will consider edge\n𝐴𝐶. The current best known distance to 𝐴is 0, and the weight of the edge is 6. Thus, the best known distance to\n⃖⃖⃖⃖⃖⃗𝐶using edge\n𝐴𝐶is still 6, so nothing gets updated at this step.\nA\nB\nC\nD\n3\n6\n-5\n2\n4\n(0)\n(6)\n(1)\n(5)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n0\n-\n1 C\n6 A\n5 B\nAfter the third iteration, the values in our table have converged to the optimal solution, and we can conclude from the table that the shortest path\nfrom 𝐴to 𝐷has a total weight of 5, the shortest path from 𝐴to 𝐵has weight 1, and the shortest path from 𝐴to 𝐶has weight 6. How do we\n𝑘thknow that the table must be optimal after the third iteration? Notice that, after the iteration, we end up knowing the minimum distance\nto any vertex k. For instance, the best known distance to 𝐷is 7 after the second iteration — thiswhen restricted to paths of length at most\nindicates that the shortest path to 𝐷has a weight of 7 if we are limited to paths with a length of at most 2. This is because, for any shortest path\n→𝑣𝑘, the distance to is correctly computed after the first iteration, the distance to is correctly computed after the second𝑣0 →𝑣1 𝑣1 𝑣2→…\n|𝑉|−1, |𝑉|−1iteration, and so on. Since the maximum possible length of a shortest path is our solution must be optimal after iterations,\nwhich in this example is 4 - 1 = 3.\nJust because three iterations are needed to guarantee an optimal solution does not mean that all three iterations actually improve our solution.\nIn fact, the order of edges processed in our example was actually a worst-case ordering, as all three iterations were required before the optimal\ndistance to our destination vertex could be known. In the best case, only a single iteration of the edges is needed to discover the optimal solution,\n⃖⃖⃖⃖⃖⃗which happens if we iterate over the edges in a way that maintains the order of edges in the optimal path (i.e.,\n⃖⃖⃖⃖⃖⃗𝐴𝐶, then\n⃖⃖⃖⃖⃖⃗𝐶𝐵, then\n𝐵𝐷). However,\npractice.2this requires us to know what the optimal path is in the first place, and finding an optimal ordering beforehand is not feasible in\n|𝑉|−1There is also another reason why all iterations may still be necessary, regardless of the order of edges chosen: the detection of\nnegative cycles. Since Bellman-Ford can be used on a graph with negative edges, it needs to handle the case where a negative cycle exists in\n(|𝑉|−1)tha given graph. To detect the presence of a negative cycle, simply perform an additional iteration after the iteration — if any best\n|𝑉|−1,known distance is improved during this additional iteration, that means we have found a shortest path that is longer than which can only\nhappen if a negative cycle exists in the graph.\n2The Θ(|𝑉|+|𝐸|)exception,however,isifyouaregivenadirected,acyclicgraph(DAG).Insuchacase,youcanfindtheshortestpathin timebytopologically\nsortingthegraph,andtheniteratingoverthevertices(andprocessingtheiredges)intopologicalorder.", "word_count": 692, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1bdc35f3-3aa1-518e-a61d-266c14fffe21", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 977, "real_page_number": null, "text": "25.3 Bellman-Ford Algorithm\n965\nAn implementation of Bellman-Ford is shown below:\n1\nstruct BFData {\n2\ndouble d;\n3\nint32_t p;\n4\nBFData()\n5\nstd::numeric_limits<double>::infinity(): d{ }, p{ -1 } {}\n6\n};\n7\n8\nusing std::vector<std::vector<std::pair<int32_t, int32_t>>>;AdjList = // <neighbor, weight>\n9\n10\nint32_t bellman_ford(const int32_t int32_tAdjList& graph, src, dest) {\n11\nstd::vector<BFData> weight_table(graph.size());\n12\nweight_table[src].d = 0;\n13\n14\n// iterate over all edges |V|-1 times\n15\nfor (int32_t iteration = 1; iteration < graph.size(); ++iteration) {\n16\nfor (size_t idx = 0; idx < graph.size(); ++idx) {\n17\nfor (const auto& [neighbor, weight] : graph[idx]) {\n18\nif (weight_table[idx].d + weight < weight_table[neighbor].d) {\n19\nweight_table[neighbor].d = weight_table[idx].d + weight;\n20\nweight_table[neighbor].p = idx;\n21\n} // if\n22\n} // for neighbor, weight\n23\n} // for idx\n24\n} // for iteration\n25\n26\n// check for negative weight cycles by completing one more iteration\n27\n// if any weight improves, then we have a negative weight cycle\n28\nfor (size_t idx = 0; idx < graph.size(); ++idx) {\n29\nfor (const auto& [neighbor, weight] : graph[idx]) {\n30\nif (weight_table[idx].d + weight < weight_table[neighbor].d) {\n31\nthrow std::invalid_argument(\"Graph contains negative weight cycle\");\n32\n} // if\n33\n} // for neighbor, weight\n34\n} // for idx\n35\n36\nreturn weight_table[dest].d;\n37\n} // bellman_ford()\nΘ(|𝑉|)The Bellman-Ford algorithm iterates over all the edges of a graph times and, for each edge, computes whether it improves the best\nknown distance to a vertex of our graph. Each of these computations (which happen on lines 16-20) takes a constant amount of time and is done\nΘ(|𝐸|) |𝐸|a total of times per iteration, where is the number of edges in the graph. Therefore, the overall time complexity of Bellman-Ford is\nΘ(|𝑉||𝐸|). Θ(|𝑉|)In addition, since the algorithm initializes a table of size to keep track of the distances to each vertex, the auxiliary space\nΘ(|𝑉|).used by Bellman-Ford is\nOverall, from an asymptotic runtime perspective, the Bellman-Ford algorithm is not as efficient as Dijkstra’s algorithm. However, that is to\nbe expected, since Dijkstra’s operates using a greedy approach and only considers locally optimal edges, while Bellman-Ford considers all edges\nduring a single iteration. The main benefit of Bellman-Ford is its versatility: you can use it to solve the single-source shortest path problem for\nany graph without negative cycles, something that Dijkstra’s cannot guarantee.\nRemark: You may have noticed that the example we used for Bellman-Ford involves a directed graph. Can Bellman-Ford also be used for an\nundirected graph? It depends. Bellman-Ford can only be used on an undirected graph present. However, this isif there are no negative edges\ntrue for all shortest-path algorithms that can handle negative edges. This is because a negative edge in an undirected graph is inherently a\nnegative cycle! For example, if there is an undirected edge from vertex 𝐴to 𝐵with a weight of -1, you could continuously travel from 𝐴to\n𝐵and then back to 𝐴to infinitely reduce your total weight.\nHowever, if an undirected graph does have no negative edges, it also means that Dijkstra’s algorithm would work as well. Since Dijkstra’s\nalgorithm is asymptotically faster, Bellman-Ford is typically an inferior choice in these situations. That being said, there are cases where\nBellman-Ford is a preferable choice, such as in environments that require distributed computation (which is why you will see Bellman-Ford\nagain if you ever take a networking class, but that is a topic for another day). In addition, even if a graph does have a negative cycle,\nBellman-Ford can be flexibly adapted to identify where these negative cycles exist, as well as the shortest paths to vertices that are not\naffected by these negative cycles.", "word_count": 621, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4dd68d99-4ecd-5d53-b5bc-93547eeb5f78", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 978, "real_page_number": null, "text": "966\nChapter 25. Shortest Path Algorithms\n25.4\nFloyd-Warshall Algorithm (✽)\nBoth Dijkstra’s algorithm and Bellman-Ford are designed to solve the single-source shortest path problem, in that they find the shortest path\nfrom a given source vertex to every other vertex of a graph. However, there are situations where finding the shortest path to a vertex is notsingle\nenough. This leads us a problem known as the all-pairs shortest path problem, which seeks to find the shortest path between pairs ofall\nvertices in a graph. One obvious approach to this problem is to run a single-source algorithm like Dijkstra’s algorithm once for every vertex.\nHowever, this approach does not work in all situations, such as if a given graph contains negative edges. In the next two sections, we will discuss\ntwo additional algorithms that can be used to solve the all-pairs shortest path problem, each with its own ideal use case.\nThe first of these algorithms is the Floyd-Warshall algorithm, which applies dynamic programming to find the shortest path between all\npairs of vertices in a weighted graph. Floyd-Warshall is best suited for dense graphs, and it does support graphs with negative edges (as long as\nthere are no negative cycles). The key idea behind Floyd-Warshall is to use to build up the optimal paths between all pairsintermediate vertices\nof vertices. To illustrate how this works, consider the following graph:\nA\nB\nC\nD\n9\n3\n5\n4\n1\nLet’s suppose we want to find the shortest path from vertex 𝐴to 𝐷, under the constraint that we can use zero intermediate vertices. In this case,\n⃖⃖⃖⃖⃖⃗the answer is trivial: if we cannot travel through any intermediate vertices from 𝐴to 𝐷, then the shortest path must be the weight of edge\n𝐴𝐷,\nor 9. We will denote this path as 𝑃0(𝐴,𝐷), which represents the optimal path that uses at most zero intermediate vertices to get from 𝐴to 𝐷.\nA\nB\nC\nD\n9\n3\n5\n4\n1\nNow, let’s loosen our constraint and allow the shortest path to go through at most one intermediate vertex, 𝐵. With this change, we now have\ntwo choices: we can either route through vertex 𝐵to get to 𝐷, or we could travel to 𝐷directly. The optimal path would therefore be the smaller\nof these two choices.\nA\nB\nC\nD\n9\n3\n5\n4\n1\nA\nB\nC\nD\n9\n3\n5\n4\n1\n𝑃1(𝐴,𝐷)\n⏟⏞⏟⏞⏟\nmin(𝑃0(𝐴,𝐷)=\n⏟⏞⏟⏞⏟\n, 𝑃0(𝐴,𝐵)+𝑃0(𝐵,𝐷)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n)\nshortest path from 𝐴to 𝐷allowing\none intermediate vertex {𝐵}\nshortest path from 𝐴to 𝐷using\nzero intermediate vertices\nshortest path from 𝐴to 𝐷\nrouting through vertex 𝐵\nIf we continued this strategy and allowed the shortest path to go through at most two intermediate vertices, 𝐵and 𝐶, we would be able to\ndirectly compute this new shortest path using the solution we computed previously. If we add 𝐶to our list of allowable vertices, our best\nsolution would be the minimum of:\n• the shortest path when our path is routed through vertex 𝐶\n• the shortest path when we were only allowed to route through vertex 𝐵(this is smaller if routing through 𝐶does not produce a better\nsolution)\nA\nB\nC\nD\n9\n3\n5\n4\n1\nA\nB\nC\nD\n9\n3\n5\n4\n1\n𝑃2(𝐴,𝐷)\n⏟⏞⏟⏞⏟\nmin(𝑃1(𝐴,𝐷)=\n⏟⏞⏟⏞⏟\n, 𝑃1(𝐴,𝐶)+𝑃1(𝐶,𝐷)\n⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟\n)\nshortest path from 𝐴to 𝐷allowing\ntwo intermediate vertices {𝐵,𝐶}\nshortest path from 𝐴to 𝐷using\none intermediate vertex {𝐵}\nshortest path from 𝐴to 𝐷\nrouting through vertex 𝐶", "word_count": 582, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "558b7ea0-e7a9-54f5-835c-472c7056042c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 979, "real_page_number": null, "text": "25.4 Floyd-Warshall Algorithm\n967\nAs you can see, we could continue this process for all other vertices that exist in our graph, adding them one-by-one to a set of allowable\nintermediate vertices, and then computing if a shorter path can be obtained by routing through the newest vertex in our allowable set. This\nforms the core recurrence relation for the Floyd-Warshall algorithm. Given a set of vertices {𝑣1,𝑣2,…,𝑣𝑛}, we can use the following𝑉=\nrecurrence to compute 𝑃𝑘(𝑖,𝑗), which is the shortest path from 𝑖to 𝑗that is only allowed to go through intermediate vertices in the subset\n≤𝑘≤𝑛,for for all pairs (𝑖,𝑗):𝑉𝑘={𝑣1,𝑣2,…,𝑣𝑘} 0\n𝑃𝑘(𝑖,𝑗)=\n{\n𝑊(𝑖,𝑗),\nif 𝑘=0\nmin(𝑃𝑘−1(𝑖,𝑗), 𝑃𝑘−1(𝑖,𝑘)+𝑃𝑘−1(𝑘,𝑗)),\notherwise\nmatrix3,Let’s look at the Floyd-Warshall algorithm in action, using the following graph. We will represent the graph in the form of a adjacency\nwhere ∞is used to denote that no direct path exists between two vertices. We will also keep track of a matrix of predecessors so that we can\nreconstruct the shortest path between any two vertices at the end of the algorithm.\nA\nB\nC\nD\nE\nF\n3\n5\n1\n5\n2\n4\n1\n5\n𝑉0 ={}\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n0\n∞\n1\n∞\n∞\n∞\n𝐁\n3\n0\n5\n∞\n∞\n∞\n𝐂\n∞\n∞\n0\n2\n4\n∞\n𝐃\n5\n∞\n∞\n0\n∞\n5\n𝐄\n∞\n∞\n∞\n∞\n0\n1\n𝐅\n∞\n∞\n∞\n∞\n∞\n0\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n−\n−\nA\n−\n−\n−\n𝐁\nB\n−\nB\n−\n−\n−\n𝐂\n−\n−\n−\nC\nC\n−\n𝐃\nD\n−\n−\n−\n−\nD\n𝐄\n−\n−\n−\n−\n−\nE\n𝐅\n−\n−\n−\n−\n−\n−\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\nFirst, we will add vertex 𝐴to our set of allowable intermediate vertices. We will then iterate over all pairs of vertices in our graph and compute\nif routing through vertex 𝐴produces a solution better than what we know so far for each pair. In this case, the shortest paths between for the\nfollowing two pairs are improved if we are allowed to visit 𝐴as an intermediate vertex:\n• (𝐵,𝐶): previously, the best known path between 𝐵and 𝐶had a weight of 5, but now the pair has a better path with weight𝑃0(𝐵,𝐶)=\n4.𝑃0(𝐵,𝐴)+𝑃0(𝐴,𝐶)=3+1=\n• (𝐷,𝐶): previously, the best known path between 𝐷and 𝐶had a weight of ∞, but now the pair has a better path with weight𝑃0(𝐷,𝐶)=\n6.𝑃0(𝐷,𝐴)+𝑃0(𝐴,𝐶)=5+1=\nThe values of and are thereby updated in the matrix to reflect these newly improved paths.(𝐵,𝐶) (𝐷,𝐶)\nA\nB\nC\nD\nE\nF\n3\n5\n1\n5\n2\n4\n1\n5\n𝑉1 {𝐴}=\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n0\n∞\n1\n∞\n∞\n∞\n𝐁\n3\n0\n4\n∞\n∞\n∞\n𝐂\n∞\n∞\n0\n2\n4\n∞\n𝐃\n5\n∞\n6\n0\n∞\n5\n𝐄\n∞\n∞\n∞\n∞\n0\n1\n𝐅\n∞\n∞\n∞\n∞\n∞\n0\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n−\n−\nA\n−\n−\n−\n𝐁\nB\n−\nA\n−\n−\n−\n𝐂\n−\n−\n−\nC\nC\n−\n𝐃\nD\n−\nA\n−\n−\nD\n𝐄\n−\n−\n−\n−\n−\nE\n𝐅\n−\n−\n−\n−\n−\n−\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\nNext, we will add vertex 𝐵to our set of allowable intermediate vertices. We then iterate over all pairs of vertices in our graph and compute\nif routing through vertex 𝐵produces a solution better than what we know so far for each pair. Here, there exists no pair that has their path\nimproved if 𝐵is allowed as an intermediate vertex, so nothing gets updated during this iteration.\nA\nB\nC\nD\nE\nF\n3\n5\n1\n5\n2\n4\n1\n5\n𝑉2 {𝐴,𝐵}=\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n0\n∞\n1\n∞\n∞\n∞\n𝐁\n3\n0\n4\n∞\n∞\n∞\n𝐂\n∞\n∞\n0\n2\n4\n∞\n𝐃\n5\n∞\n6\n0\n∞\n5\n𝐄\n∞\n∞\n∞\n∞\n0\n1\n𝐅\n∞\n∞\n∞\n∞\n∞\n0\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n−\n−\nA\n−\n−\n−\n𝐁\nB\n−\nA\n−\n−\n−\n𝐂\n−\n−\n−\nC\nC\n−\n𝐃\nD\n−\nA\n−\n−\nD\n𝐄\n−\n−\n−\n−\n−\nE\n𝐅\n−\n−\n−\n−\n−\n−\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n3EventhoughFloyd-Warshallisbestdesignedfordensegraphs,asparsegraphisusedinthisexamplejusttomakethingseasiertofollow. Thisiswhywearestill\nusinganadjacencymatrixtorepresentthegraph,despitethegraphnotbeingdense.", "word_count": 861, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cb1ff6a2-c514-59a3-acb2-217d6149455b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 980, "real_page_number": null, "text": "968\nChapter 25. Shortest Path Algorithms\nNext, we will add vertex 𝐶to our set of allowable intermediate vertices. Again, we will iterate over all pairs of vertices in our graph to see if any\npair has their shortest known path improved when 𝐶is permitted as an intermediate vertex. During this iteration, there are five pairs that have a\nbetter solution:\n• (𝐴,𝐷): previously, the best known path between 𝐴and 𝐷had a weight of ∞, but now the pair has a better path with weight𝑃2(𝐴,𝐷)=\n3.𝑃2(𝐴,𝐶)+𝑃2(𝐶,𝐷)=1+2=\n• (𝐴,𝐸): previously, the best known path between 𝐴and 𝐸had a weight of ∞, but now the pair has a better path with weight𝑃2(𝐴,𝐸)=\n5.𝑃2(𝐴,𝐶)+𝑃2(𝐶,𝐸)=1+4=\n• (𝐵,𝐷): previously, the best known path between 𝐵and 𝐷had a weight of ∞, but now the pair has a better path with weight𝑃2(𝐵,𝐷)=\n6.𝑃2(𝐵,𝐶)+𝑃2(𝐶,𝐷)=4+2=\n• (𝐵,𝐸): previously, the best known path between 𝐵and 𝐸had a weight of ∞, but now the pair has a better path with weight𝑃2(𝐵,𝐸)=\n8.𝑃2(𝐵,𝐶)+𝑃2(𝐶,𝐸)=4+4=\n• (𝐷,𝐸): previously, the best known path between 𝐷and 𝐸had a weight of ∞, but now the pair has a better path with weight𝑃2(𝐷,𝐸)=\n10.𝑃2(𝐷,𝐶)+𝑃2(𝐶,𝐸)=6+4=\nThe values of these five pairs are thereby updated in the matrix to reflect these newly improved paths.\nA\nB\nC\nD\nE\nF\n3\n5\n1\n5\n2\n4\n1\n5\n𝑉3 {𝐴,𝐵,𝐶}=\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n0\n∞\n1\n3\n5\n∞\n𝐁\n3\n0\n4\n6\n8\n∞\n𝐂\n∞\n∞\n0\n2\n4\n∞\n𝐃\n5\n∞\n6\n0\n10\n5\n𝐄\n∞\n∞\n∞\n∞\n0\n1\n𝐅\n∞\n∞\n∞\n∞\n∞\n0\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n−\n−\nA\nC\nC\n−\n𝐁\nB\n−\nA\nC\nC\n−\n𝐂\n−\n−\n−\nC\nC\n−\n𝐃\nD\n−\nA\n−\nC\nD\n𝐄\n−\n−\n−\n−\n−\nE\n𝐅\n−\n−\n−\n−\n−\n−\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\nNext, we will add vertex 𝐷to our set of allowable intermediate vertices. Using the same process as before, we discover four new pairs of edges\nthat have their shortest paths improved with 𝐷permitted as an intermediate vertex:\n(𝐴,𝐹): previously, the best known path between 𝐴and 𝐹had a weight of ∞, but now the pair has a better path with weight• 𝑃3(𝐴,𝐹)=\n8.𝑃3(𝐴,𝐷)+𝑃3(𝐷,𝐹)=3+5=\n• (𝐵,𝐹): previously, the best known path between 𝐵and 𝐹had a weight of ∞, but now the pair has a better path with weight𝑃3(𝐵,𝐹)=\n11.𝑃3(𝐵,𝐷)+𝑃3(𝐷,𝐹)=6+5=\n(𝐶,𝐴): previously, the best known path between 𝐶and 𝐴had a weight of ∞, but now the pair has a better path with weight• 𝑃3(𝐶,𝐴)=\n7.𝑃3(𝐶,𝐷)+𝑃3(𝐷,𝐴)=2+5=\n• (𝐶,𝐹): previously, the best known path between 𝐶and 𝐹had a weight of ∞, but now the pair has a better path with weight𝑃3(𝐶,𝐹)=\n7.𝑃3(𝐶,𝐷)+𝑃3(𝐷,𝐹)=2+5=\nThe values of these four vertices are updated in the matrix as follows:\nA\nB\nC\nD\nE\nF\n3\n5\n1\n5\n2\n4\n1\n5\n𝑉4 {𝐴,𝐵,𝐶,𝐷}=\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n0\n∞\n1\n3\n5\n8\n𝐁\n3\n0\n4\n6\n8\n11\n𝐂\n7\n∞\n0\n2\n4\n7\n𝐃\n5\n∞\n6\n0\n10\n5\n𝐄\n∞\n∞\n∞\n∞\n0\n1\n𝐅\n∞\n∞\n∞\n∞\n∞\n0\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n−\n−\nA\nC\nC\nD\n𝐁\nB\n−\nA\nC\nC\nD\n𝐂\nD\n−\n−\nC\nC\nD\n𝐃\nD\n−\nA\n−\nC\nD\n𝐄\n−\n−\n−\n−\n−\nE\n𝐅\n−\n−\n−\n−\n−\n−\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\nContinuing this process, we then add vertex 𝐸to our set of allowable intermediate vertices. This improves the shortest paths between these\npairs of vertices:\n• (𝐴,𝐹): previously, the best known path between 𝐴and 𝐹had a weight of 8, but now the pair has a better path with weight𝑃4(𝐴,𝐹)=\n6.𝑃4(𝐴,𝐸)+𝑃4(𝐸,𝐹)=5+1=\n• (𝐵,𝐹): previously, the best known path between 𝐵and 𝐹had a weight of 11, but now the pair has a better path with weight𝑃4(𝐵,𝐹)=\n9.𝑃4(𝐵,𝐸)+𝑃4(𝐸,𝐹)=8+1=\n• (𝐶,𝐹): previously, the best known path between 𝐶and 𝐹had a weight of 7, but now the pair has a better path with weight𝑃4(𝐶,𝐹)=\n5.𝑃4(𝐶,𝐸)+𝑃4(𝐸,𝐹)=4+1=\nA\nB\nC\nD\nE\nF\n3\n5\n1\n5\n2\n4\n1\n5\n𝑉5 {𝐴,𝐵,𝐶,𝐷,𝐸}=\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n0\n∞\n1\n3\n5\n6\n𝐁\n3\n0\n4\n6\n8\n9\n𝐂\n7\n∞\n0\n2\n4\n5\n𝐃\n5\n∞\n6\n0\n10\n5\n𝐄\n∞\n∞\n∞\n∞\n0\n1\n𝐅\n∞\n∞\n∞\n∞\n∞\n0\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n−\n−\nA\nC\nC\nE\n𝐁\nB\n−\nA\nC\nC\nE\n𝐂\nD\n−\n−\nC\nC\nE\n𝐃\nD\n−\nA\n−\nC\nD\n𝐄\n−\n−\n−\n−\n−\nE\n𝐅\n−\n−\n−\n−\n−\n−\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦", "word_count": 947, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2b1ca337-89f3-505e-8631-8d975d6b4197", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 981, "real_page_number": null, "text": "25.5 Johnson’s Algorithm\n969\nVertex 𝐹is the final vertex we add to our set of allowable intermediate vertices. However, the inclusion of vertex 𝐹does not improve the\nshortest path between any pair of vertices, so our matrices stay the same as before. Since we have added all vertices to the allowable set, we are\nnow done, and this is the final result of the algorithm.\nA\nB\nC\nD\nE\nF\n3\n5\n1\n5\n2\n4\n1\n5\n𝑉6 {𝐴,𝐵,𝐶,𝐷,𝐸,𝐹}=\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n0\n∞\n1\n3\n5\n6\n𝐁\n3\n0\n4\n6\n8\n9\n𝐂\n7\n∞\n0\n2\n4\n5\n𝐃\n5\n∞\n6\n0\n10\n5\n𝐄\n∞\n∞\n∞\n∞\n0\n1\n𝐅\n∞\n∞\n∞\n∞\n∞\n0\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\n⎡\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎣\n𝐀\n𝐁\n𝐂\n𝐃\n𝐄\n𝐅\n𝐀\n−\n−\nA\nC\nC\nE\n𝐁\nB\n−\nA\nC\nC\nE\n𝐂\nD\n−\n−\nC\nC\nE\n𝐃\nD\n−\nA\n−\nC\nD\n𝐄\n−\n−\n−\n−\n−\nE\n𝐅\n−\n−\n−\n−\n−\n−\n⎤\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥⎦\nAt this point, the values in the distance matrix store the shortest paths between any two pairs of vertices. For instance, the value of\nweight_matrix[𝐴][𝐹] is 6, which indicates that the shortest path from 𝐴to 𝐹has a total weight of 6. We can also backtrack through\nthe predecessor matrix to reconstruct the actual vertices along our shortest path. For example, we know that the predecessor of 𝐹along the\nshortestpathfrom𝐴to𝐹ispredecessor_matrix[𝐴][𝐹]=𝐸. Thepredecessorof𝐸alongtheshortestpathfrom𝐴to𝐸is𝐶, andthepredecessor\nof 𝐶from 𝐴to 𝐶is 𝐴. Therefore, the shortest path from 𝐴to 𝐹is 𝐴→𝐶→𝐸→𝐹.\nFloyd-Warshall can also be adapted to detect negative cycles. Since negative cycles can be used to infinitely reduce the weight of a\npath, a negative cycle exists if the distance from a vertex to itself ever becomes negative during the algorithm (which indicates there exists a\nnegative-weighted path from a vertex back to itself). This check can be added in while computing the distances of all pairs of vertices during\neach iteration of the algorithm. An implementation is shown below:\n1\nstruct FWData {\n2\ndouble d;\n3\nint32_t p;\n4\nFWData()\n5\nstd::numeric_limits<double>::infinity(): d{ }, p{ -1 } {}\n6\n};\n7\n8\nusing AdjMat = std::vector<std::vector<FWData>>;\n9\n10\nfloyd_warshall(constAdjMat AdjMat& graph) {\n11\nAdjMat output{graph};\n12\nfor (size_t k = 0; k < graph.size(); ++k) {\n13\nfor (size_t i = 0; i < graph.size(); ++i) {\n14\nfor (size_t j = 0; j < graph.size(); ++j) {\n15\nif (output[i][k].d + output[k][j].d < output[i][j].d) {\n16\noutput[i][j].d = output[i][k].d + output[k][j].d;\n17\noutput[i][j].p = output[k][j].p;\n18\nif (i == j && output[i][j].d < 0)\n19\nthrow std::invalid_argument(\"Graph contains negative weight cycle\");\n20\n} // if\n21\n} // for j\n22\n} // for i\n23\n} // for k\n24\nreturn output;\n25\n} // floyd_warshall()\nΘ(|𝑉|3).forFrom the triple loop, we can see that the worst-case time complexity Floyd-Warshall is This is because the outer loop (line 12)\n|𝑉|2|𝑉|executes times to add each vertex to the allowable set, and the inner nested loop (lines 13-14) performs computations to identify\nimprovements between all pairs of vertices, where each computation (lines 15-20) takes constant time.\nΘ(|𝑉|2)|𝑉|How does this compare to running Dijkstra’s algorithm times? For a dense graph, each run of Dijkstra’s algorithm would take\nΘ(|𝑉|3)|𝑉|time, so running Dijkstra’s times would result in an overall time complexity of — the same as Floyd-Warshall! From a pure\ncomplexity standpoint, either approach would be efficient for solving the all-pairs shortest path problem when given a dense graph. However,\n|𝑉|since Floyd-Warshall involves less overhead than running Dijkstra’s times, is simpler to implement, and also supports negative edges, it is\ntypically the preferred choice out of the two.\n25.5\nJohnson’s Algorithm (✽)\nΘ(|𝑉|3)Floyd-Warshall’s time complexity makes it a good candidate for the all-pairs shortest path problem on dense graphs, but what if you\nΘ(|𝐸|log(|𝑉|))are given a sparse graph? Recall that the time complexity of Dijkstra’s on a sparse graph is using the min-heap implementation.\n|𝑉| Θ(|𝑉||𝐸|log(|𝑉|))This means that running Dijkstra’s times results in an overall time complexity of for a sparse graph, which is better than\nΘ(|𝑉|3)the time complexity of Floyd-Warshall. Therefore, if you are given a sparse graph, running Dijkstra’s multiple times seems to be a\nbetter choice for solving the all-pairs shortest path problem.\nHowever, there is a catch: Dijkstra’s algorithm cannot support negative edges. Thus, if you want to solve the all-pairs shortest path problem\nΘ(|𝑉|3)for a sparse graph with negative edges in less than time, you will need to rely on another shortest path algorithm. One such algorithm is\nJohnson’s algorithm, which can be used to efficiently solve the all-pairs shortest path problem for sparse graphs with negative edge weights.", "word_count": 847, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a0e144ca-8bd3-5075-8200-6855d5fcfd63", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 982, "real_page_number": null, "text": "970\nChapter 25. Shortest Path Algorithms\nJohnson’s algorithm uses techniques from two shortest-path algorithms we have discussed already: Dijkstra’s algorithm and the Bellman-Ford\nalgorithm. The aim of Johnson’s algorithm is twofold: first, it uses the Bellman-Ford algorithm to transform a graph negative edges intowith\n|𝑉|one negative edges; then, it invokes Dijkstra’s algorithm times to explore this newly transformed non-negative graph to find all pairs’without\nshortest paths. The implementation of Johnson’s algorithm can be summarized using the following steps:\n1. Insert a new vertex 𝑠into the graph and connect it to every other vertex using directed edges of weight 0. This ensures that 𝑠is able to\nreach every vertex of the graph, while at the same time keeping the other vertices unaware of its existence (this allows it to be safely\nadded without changing the solution).\n2. Run the Bellman-Ford algorithm once with 𝑠as the source vertex (terminating if a negative cycle is detected). This discovers the shortest\npath from 𝑠to every other vertex of the original graph.\n3. Remove vertex 𝑠, leaving behind just the remaining vertices and their corresponding weights computed by Bellman-Ford.\n4. Reweight all the edges in the graph using the following equation, where 𝑑𝑣represents the lowest cost from 𝑠to 𝑣discovered using the\nBellman-Ford algorithm. This transformation ensures that all edges become non-negative.\nnew_weight(⃖⃖⃖⃗\n𝑢𝑣) = old_weight(⃖⃖⃖⃗\n𝑢𝑣) + 𝑑𝑢- 𝑑𝑣\n|𝑉|Run Dijkstra’s algorithm times to discover the shortest path from each vertex to every other vertex of the reweighted graph. For each5.\nshortest distance between any two vertices returned by Dijkstra’s algorithm, undo the transformation by adding 𝑑𝑣−𝑑𝑢to get the(𝑢,𝑣)\nactual best distance between 𝑢and 𝑣.\nTo illustrate how Johnson’s algorithm works, consider the following graph for which we want to find all-pairs shortest paths.\nA\nB\nC\nD\nE\n4\n3\n4\n-2\n-1\n-2\n2\n-3\nFirst, we will add a vertex 𝑠to the graph that is connected to all other vertices with a directed edge of cost 0. This ensures that all the vertices in\nthe graph are reachable from 𝑠, which is required for the Bellman-Ford step of the algorithm (since it needs to discover the shortest path to every\nvertex of the graph).\n𝑠\nA\nB\nC\nD\nE\n4\n3\n4\n-2\n-1\n-2\n2\n-3\n0\n0\n0\n0\n0\nWe then run the Bellman-Ford algorithm on this new graph with a source vertex of 𝑠. This gives us the following result.\n𝑠\nA\nB\nC\nD\nE\n4\n3\n4\n-2\n-1\n-2\n2\n-3\n0\n0\n0\n0\n0\n(0)\n(-4)\n(-1)\n(0)\n(0)\n(-3)\n𝑣𝑑𝑣𝑝𝑣\n𝑠\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n0\n-\n-4 C\n0\ns\n-3 E\n-1 C\n0\ns", "word_count": 453, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "de65e418-e2a1-53a4-b519-11434689352e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 983, "real_page_number": null, "text": "25.5 Johnson’s Algorithm\n971\nVertex 𝑠is then removed, leaving us with the original graph, but each vertex is now associated with a Bellman-Ford weight 𝑑𝑣. This is the key\ninsight used by Johnson’s algorithm: we can now use these weights to transform our graph into one without negative edges without changing the\nshortest path between any pair of vertices.\nA\nB\nC\nD\nE\n4\n3\n4\n-2\n-1\n-2\n2\n-3\n(-4)\n(-1)\n(0)\n(0)\n(-3)\nThe edges in the graph are all reweighted using the following equation:\nnew_weight(⃖⃖⃖⃗\n𝑢𝑣) = old_weight(⃖⃖⃖⃗\n𝑢𝑣) + 𝑑𝑢- 𝑑𝑣\n⃖⃖⃖⃖⃖⃗For example, edge\nold_weight(⃖⃖⃖⃖⃖⃗𝐷𝐴would be updated to\n𝐷𝐴) + 𝑑𝐷- 𝑑𝐴= -2 + (-1) - (-4) = 1.\nA\nB\nC\nD\nE\n4\n3\n4\n1\n-1\n-2\n2\n-3\n(-4)\n(-1)\n(0)\n(0)\n(-3)\nApplying this equation to the remaining edges gives us the following reweighted graph:\nA\nB\nC\nD\nE\n0\n3\n5\n1\n0\n1\n0\n0\n(-4)\n(-1)\n(0)\n(0)\n(-3)\n|𝑉|We then run Dijkstra’s algorithm times, once with each vertex as the source, to discover the shortest path between all pairs of vertices. Each\nshortest path we discover is then rescaled back to its original weight by adding 𝑑𝑣−𝑑𝑢, where 𝑑𝑣is the Bellman-Ford distance of the destination\nvertex and 𝑑𝑢is the Bellman-Ford distance of the source vertex. Running Dijkstra’s algorithm with 𝐴as the source vertex gives us the following:\nA\nB\nC\nD\nE\n0\n3\n5\n1\n0\n1\n0\n0\n(-4)\n(-1)\n(0)\n(0)\n(-3)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n0\n-\n0 A\n1 B\n1 C\n3 B", "word_count": 270, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "06b6f0b6-4868-56ab-9e80-a1b99537f2a5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 984, "real_page_number": null, "text": "972\nChapter 25. Shortest Path Algorithms\nThis gives us the shortest paths from 𝐴to every other vertex in our reweighted graph. To determine the shortest paths for our graphoriginal\n(before reweighting), we can simply add back the difference between the Bellman-Ford weights of each destination vertex and the source vertex\n𝐴. For example, the shortest path from 𝐴to 𝐷has weight 1 in our reweighted graph. Since the Bellman-Ford weight of vertex 𝐴is -4 and\nthe Bellman-Ford weight of vertex 𝐷is -1, the shortest path from 𝐴to 𝐷in our original graph has a total weight of 1 + (-1) - (-4) = 4. The\nremaining paths are adjusted as follows:\nA\nB\nC\nD\nE\n0\n3\n5\n1\n0\n1\n0\n0\n(-4)\n(-1)\n(0)\n(0)\n(-3)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n0\n-\n0 A\n1 B\n1 C\n3 B\n+ 0 - (-4)\n+ (-3) - (-4)\n+ (-1) - (-4)\n+ 0 - (-4)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n0\n-\n4 A\n2 B\n4 C\n7 B\nTo find the shortest paths between all other pairs of vertices, Dijkstra’s is run with each of the remaining vertices as the source vertex using the\nsame process. The results of each vertex, after rescaling back to the graph’s original weights, are shown below:\nDijkstra with B as the source vertex:\nA\nB\nC\nD\nE\n0\n3\n5\n1\n0\n1\n0\n0\n(-4)\n(-1)\n(0)\n(0)\n(-3)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1 C\n0\n-\n1 B\n1 C\n3 B\n+ (-4) - 0\n+ (-3) - 0\n+ (-1) - 0\n+ 0 - 0\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n-3 C\n0\n-\n-2 B\n0 C\n3 B\nDijkstra with C as the source vertex:\nA\nB\nC\nD\nE\n0\n3\n5\n1\n0\n1\n0\n0\n(-4)\n(-1)\n(0)\n(0)\n(-3)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n0 C\n0 A\n0\n-\n0 C\n3 B\n+ (-4) - (-3)\n+ 0 - (-3)\n+ (-1) - (-3)\n+ 0 - (-3)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n-1 C\n3 A\n0\n-\n-2 C\n6 B\nDijkstra with D as the source vertex:\nA\nB\nC\nD\nE\n0\n3\n5\n1\n0\n1\n0\n0\n(-4)\n(-1)\n(0)\n(0)\n(-3)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1 D\n1 A\n2 B\n0\n-\n4 B\n+ (-4) - (-1)\n+ 0 - (-1)\n+ (-3) - (-1)\n+ 0 - (-1)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n-2 D\n2 A\n0 B\n0\n-\n5 B\nDijkstra with E as the source vertex:\nA\nB\nC\nD\nE\n0\n3\n5\n1\n0\n1\n0\n0\n(-4)\n(-1)\n(0)\n(0)\n(-3)\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n0 C\n0 A\n0 E\n0 C\n0\n-\n+ (-4) - 0\n+ 0 - 0\n+ (-3) - 0\n+ (-1) - 0\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n-4 C\n0 A\n-3 E\n-1 C\n0\n-", "word_count": 512, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cf8d00fe-db39-5e05-afc2-22127446e047", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 985, "real_page_number": null, "text": "25.5 Johnson’s Algorithm\n973\nPutting this all together, this is the final result of Johnson’s algorithm for our graph. Each value of 𝑑𝑣in the tables below represents the weights\nof the shortest path between the source vertex as 𝑣.\nA\nB\nC\nD\nE\n4\n3\n4\n-2\n-1\n-2\n2\n-3\nSource: A\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n0\n-\n4 A\n2 B\n4 C\n7 B\nSource: B\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n-3 C\n0\n-\n-2 B\n0 C\n3 B\nSource: C\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n-1 C\n3 A\n0\n-\n-2 C\n6 B\nSource: D\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n-2 D\n2 A\n0 B\n0\n-\n5 B\nSource: E\n𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n-4 C\n0 A\n-3 E\n-1 C\n0\n-\nThe code for Johnson’s algorithm is shown on the following page. As you can see, Johnson’s algorithm is an combination of two shortest-path\nΘ(|𝑉||𝐸|); |𝑉|algorithmsthatarerunsequentially: Bellman-Ford, whichisrunonceforatimecomplexityof andDijkstra’s, whichisrun times\n|𝑉|×Θ(|𝐸|log(|𝑉|)) Θ(|𝑉||𝐸|log(|𝑉|)). |𝑉|for a time complexity of Because the time complexity of running Dijkstra times contributes to=\nΘ(|𝑉||𝐸|log(|𝑉|)),a higher order term, the overall time complexity of Johnson’s algorithm is also assuming you use a binary min-heap as your\nimplementation.4priority queue\nAt this point, you may be wondering: how does Johnson’s algorithm even work? The idea that Bellman-Ford can be used to transform a\ngraph with negative edges into one without can seem counterintuitive, but we can prove its correctness using a bit of arithmetic. Let’s define\n⟨𝑣1,𝑣2,𝑣3,…,𝑣𝑛⟩representas the original weight of an edge between vertices 𝑥and 𝑦, and let a path from vertex to vertex𝑊(𝑥,𝑦) 𝑃𝑣1→𝑣𝑛= 𝑣1\n𝑣𝑛. After reweighting, the weight of the path 𝑃𝑣1→𝑣𝑛becomes\n|𝑃𝑣1→𝑣𝑛| (𝑊(𝑣1,𝑣2)+𝑑𝑣1 −𝑑𝑣2)+(𝑊(𝑣2,𝑣3)+𝑑𝑣2 −𝑑𝑣3)+…+(𝑊(𝑣𝑛−1,𝑣𝑛)+𝑑𝑣𝑛−1 −𝑑𝑣𝑛)=\nThis is a telescoping series, so all the Bellman-Ford distances of intermediate vertices cancel out. This leaves us with\n|𝑃𝑣1→𝑣𝑛| 𝑊(𝑣1,𝑣2)+𝑊(𝑣2,𝑣3)+…+𝑊(𝑣𝑛−1,𝑣𝑛)+𝑑𝑣1 −𝑑𝑣𝑛=\nThis implies that all possible paths between vertices and 𝑣𝑛have their total path weight increased by the of −𝑑𝑣𝑛. Because𝑣1 𝑑𝑣1same amount\nof this, the shortest path between and 𝑣𝑛before reweighting reweighting, since all alternative paths𝑣1 must remain the shortest path after\nbetween and 𝑣𝑛have their weights incremented by the same amount. Therefore, the reweighting process does not change the shortest path𝑣1\nbetween any two vertices in our graph.\nHowever, proving that reweighting does not change our solution alone is not enough to prove the correctness of Johnson’s algorithm. We\nmust also prove that the Dijkstra step also returns the shortest path between any two pairs of vertices in our reweighted graph. To do so, all we\nneed to show is that the reweighted graph does not contain any negative edges, which guarantees the correctness of Dijkstra’s algorithm (which\nwe proved earlier).\nRecall that our Bellman-Ford weights were computed using a starting vertex 𝑠, which was connected to all other vertices with an edge\nweight of 0. If we define 𝑑𝑢as the optimal distance from the artificial vertex 𝑠to vertex 𝑢(i.e., the Bellman-Ford weight that we calculate), then\nwe know that the value of 𝑑𝑣for some other vertex 𝑣connected to 𝑢cannot be worse than 𝑑𝑢plus the cost to get from 𝑢to 𝑣(otherwise, 𝑑𝑣\nwouldn’t be the shortest distance from 𝑠to 𝑣, which is a contradiction). In other words,\n≥𝑑𝑣𝑑𝑢+𝑊(𝑢,𝑣)\nRearranging this equation by subtracting 𝑑𝑣from both sides, we get\n𝑊(𝑢,𝑣)+𝑑𝑢−𝑑𝑣≥0\n𝑊(𝑢,𝑣)+𝑑𝑢−𝑑𝑣is simply the cost of the reweighted edge between 𝑢and 𝑣. Therefore, any edge between two vertices in our reweighted graph\nmust have an edge weight that is at least zero. Since we have proven that reweighting does not change the optimal path, and that the reweighted\ngraph cannot have negative edges, we have thereby proven that Dijkstra’s always returns the optimal path between any two vertices of our\nreweighted graph. This, in turn, proves that Johnson’s algorithm is correct.\n4If Θ(|𝐸|+|𝑉|log(|𝑉|)),youuseaFibonacciheaptoimplementDijkstra’salgorithm,eachiterationofDijkstra’swouldtake leadingtoanoveralltimecomplexity\nΘ(|𝑉||𝐸|+|𝑉|2|𝑉|×Θ(|𝐸|+|𝑉|log(|𝑉|)) log(|𝑉|))of forJohnson’salgorithm.=", "word_count": 695, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f13fb00a-f888-545b-ab81-20f4daa85ecd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 986, "real_page_number": null, "text": "974\nChapter 25. Shortest Path Algorithms\nAn implementation of Johnson’s algorithm is shown below:\n1\nusing std::vector<std::vector<std::pair<int32_t, int32_t>>>;AdjList =\n2\n3\nstruct JohnsonData {\n4\ndouble d;\n5\nint32_t p;\n6\nJohnsonData()\n7\nstd::numeric_limits<double>::infinity(): d{ }, p{ -1 } {}\n8\n};\n9\n10\n// returns Bellman-Ford weights for a graph starting from src\n11\n// e.g., vec[5] stores shortest cost from vertex src to vertex 5\n12\nstd::vector<int32_t> bellman_ford(const int32_tAdjList& graph, src);\n13\n14\n// returns Dijkstra weights/predecessors for a graph starting from src\n15\ndijkstra(const int32_tstd::vector<JohnsonData> AdjList& graph, src);\n16\n17\n// adds a new vertex 0 (i.e., s) with zero-weighted edges to all other vertices\n18\nadd_zero_vertex(constAdjList AdjList& graph);\n19\n20\njohnson(conststd::vector<std::vector<JohnsonData>> AdjList& graph) {\n21\nAdjList s_graph = add_zero_vertex(graph);\n22\nstd::vector<int32_t> bf_weights;\n23\n24\ntry {\n25\nbf_weights = bellman_ford(s_graph, 0);\n26\n} // try\n27\ncatch (const std::exception&) {\n28\nstd::cerr << \"Graph contains negative weight cycle, cannot compute shortest paths!\" << std::endl;\n29\nthrow;\n30\n} // catch\n31\n32\n// reweight each edge (u, v) by adding bf_weights[u] - bf_weights[v]\n33\nAdjList reweighted_graph{graph};\n34\nfor (size_t u = 0; u < graph.size(); ++u) {\n35\nfor (auto& [v, weight] : reweighted_graph[u]) {\n36\nweight += (bf_weights[u] - bf_weights[v]);\n37\n} // for v, weight\n38\n} // for u\n39\n40\n// invoke Dijkstra |V| times to find all pairs shortest paths\n41\nstd::vector<std::vector<JohnsonData>> result(graph.size());\n42\nfor (size_t i = 0; i < graph.size(); ++i) {\n43\nresult[i] = dijkstra(reweighted_graph, i);\n44\n} // for i\n45\n46\n// reweight the shortest path for each pair (u, v) by adding\n47\n// bf_weights[v] - bf_weights[u]\n48\nfor (size_t u = 0; u < graph.size(); ++u) {\n49\nfor (size_t v = 0; v < graph.size(); ++v) {\n50\nif std::numeric_limits<double>::infinity())(result[u][v].d != {\n51\nresult[u][v].d += (bf_weights[v] - bf_weights[u]);\n52\n} // if\n53\n} // for v\n54\n} // for u\n55\n56\nreturn result;\n57\n} // johnson()", "word_count": 334, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c20297c4-d283-5782-9279-a05225cdec59", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 987, "real_page_number": null, "text": "25.6 A* Search\n975\n25.6\nA* Search (✽)\nA* search (pronounced \"A-star\" search) is an optimized algorithm that can be used to find the shortest path between two vertices in a graph.\nThe A* algorithm can be seen as an extension of Dijkstra’s algorithm, using the steps of Dijkstra’s as a foundation for how it explores a graph.\nHowever, unlike Dijkstra’s algorithm, A* uses a to estimate the remaining cost required to reach the target vertex from theheuristic function\ncurrent position along the search. This information is then used by the A* algorithm to determine the order in which vertices are explored.\nMore formally, when trying to finding a shortest path from a source vertex to a target vertex in a graph, the A* algorithm applies the following\nfunction to determine which vertex to explore next along the search path:𝑓(𝑛)\n𝑓(𝑛) 𝑔(𝑛)+ℎ(𝑛)=\nwhere is the cost from the source vertex to vertex 𝑛, and is a heuristic function that estimates the cheapest cost required to reach the𝑔(𝑛) ℎ(𝑛)\ntarget vertex from vertex 𝑛. At each step of the search, the A* algorithm looks through each of the unvisited neighbors of the current vertex and\nextends the search path by visiting the one that minimizes 𝑓(𝑛). Note that this differs from Dijkstra’s algorithm, which simply chooses the vertex\nwith the smallest known distance from the source; in fact, you can think of Dijkstra’s as a special case of A* where for all vertices 𝑛.ℎ(𝑛)=0\n𝑔(𝑛)\nℎ(𝑛)\nsource vertex\nvertex 𝑛\ntarget vertex\nThere are a few details to note about the A* algorithm. First, unlike Dijkstra’s algorithm, A* does not find the shortest path from the source\nvertex to other vertices in the graph (i.e., it does not solve the single-source shortest path problem). This is the trade-off of applying aall\nheuristic to optimize the search, since an effective heuristic is used to guide the search toward a specific target vertex. Second, the correctness of\nA* search depends on the heuristic that is used to estimate the remaining cost to reach the target vertex. For A* search to guarantee the shortest\npath between two vertices, its heuristic must be — that is, it cannot overestimate the actual cost required to get to the desired goal.admissible\nBecause of its optimized performance, the A* search algorithm has several common applications related to pathfinding problems. A* can be\nused in robotics to guide a robot along the shortest path from one location to another while avoiding obstacles, in artificial intelligence settings\nto facilitate natural language processing, and in game design to guide non-playable characters toward a certain goal, just to name a few.\nAn example of the A* algorithm in action is shown in the image below. This screenshot depicts the progress of a search that is being\nperformed on a two-dimensional grid, where you are allowed to move in four directions: up, down, left, and right. The darker shaded objects\nrepresent obstacles that you cannot pass through, and the dots represent coordinates of the grid that have been visited along the search. The\nheuristic used by A* in this example is a simple Euclidean distance calculation between the current coordinate and the goal.\nAs you can see, the use of a heuristic to estimate additional cost helps guide the search in the direction of the desired goal, which is something\nthat Dijkstra’s alone does not provide. Because A* applies a heuristic to identify the most promising search path, it is often called a best-first\nor algorithm. If you choose a good heuristic, the A* search algorithm can help you find the shortest path between twosearch informed search\npoints in a graph faster than the non-informed search algorithms discussed earlier!", "word_count": 626, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "e63afe9a-3b20-5e38-9a24-bdf93790ddd3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 988, "real_page_number": null, "text": "976\nChapter 25. Shortest Path Algorithms\n25.7\nSummary of Shortest Path Algorithms\nIn this chapter, we mainly concerned ourselves with two different types of shortest path problems: the shortest path problem andsingle-source\nthe shortest path problem. The first of these variants, the shortest path problem, seeks to find the shortest path from aall-pairs single-source\nsingle source vertex to every other vertex in a graph. When working with single-source shortest path problems, there are two primary algorithms\nwe can use: Dijkstra’s algorithm and the Bellman-Ford algorithm.\nDijkstra’s algorithm employs a greedy approach to explore a graph, and it can be implemented using either a linear search or a min-heap to\nΘ(|𝑉|2), |𝑉|determine which vertices to explore. The linear search works best for dense graphs and has an overall time complexity of where is\nthe number of vertices in the graph. On the other hand, the min-heap approach works best for sparse graphs and has an overall time complexity\nΘ(|𝐸|log(|𝑉|)) |𝐸| |𝑉|of (assuming a binary heap is used), where is the number of edges in the graph and is the number of vertices.\nOne major pitfall of Dijkstra’s algorithm is its inability to handle graphs with negative edge weights. This is because the greedy nature of\nthe algorithm prevents it from ever going back to previous states, even if it discovers a negative edge that improves the solution of a previously\nvisited vertex. To address graphs with negative edge weights, we can instead use the Bellman-Ford algorithm to solve the single-source shortest\npath problem. Bellman-Ford utilizes dynamic programming, allowing it to consider subproblems that Dijkstra’s was unable to consider. Because\nΘ(|𝑉||𝐸|)Bellman-Ford needs to consider all subproblems rather than just the greedy choice, its overall time complexity of is asymptotically\nless efficient than that of Dijkstra’s.\nSingle-Source Shortest Path Algorithms\nDijkstra’s Algorithm\n(Linear Search)\nDijkstra’s Algorithm\n(Min Binary Heap)\nBellman-Ford Algorithm\nTime Complexity\nΘ(|𝑉|2)\nΘ(|𝐸|log(|𝑉|))\nΘ(|𝑉||𝐸|)\nRecommended Graph Type\nDense\nSparse\nEither\nSupports Negative Edge Weights\nNo\nNo\nYes\nCan Detect Negative Cycles\nNo\nNo\nYes\nOn the other hand, the shortest path problem seeks to find the shortest path between all pairs of vertices in a graph. We have discussedall-pairs\nthree primary techniques for solving the all-pairs shortest path problem: running Dijkstra’s once for each vertex, the Floyd-Warshall algorithm,\nand Johnson’s algorithm.\n|𝑉|Running Dijkstra’s algorithm times is the best strategy if a graph is sparse and has no negative edges. Recall that each invocation of\nΘ(|𝑉|2) Θ(|𝐸|log(|𝑉|)) |𝐸|Dijkstra’s algorithm takes time if a linear search is used and time if a min-binary heap is used. However, is on\nΘ(|𝑉|2) Θ(|𝑉|3)|𝑉|the order of in dense graphs — this means that running Dijkstra’s times would take time if a linear search is used, and\nΘ(|𝑉|3Θ(|𝑉||𝐸|log(|𝑉|)) log(|𝑉|)) time if a min-binary heap is used. As a result, Floyd-Warshall would be a better algorithm to use if you=\nΘ(|𝑉|3)are given a dense graph, since it also runs in time (which is the best you can do with repeated Dijkstra’s), is simpler to implement, and\nalso supports graphs with negative edge weights.\nIf the underlying graph is sparse, but negative edge weights are involved, then Johnson’s algorithm would be a preferable choice. Much like\nFloyd-Warshall, Johnson’s algorithm supports graphs with negative edge weights, as long as there are no negative cycles. However, the time\nΘ(|𝑉|3)Θ(|𝑉||𝐸|log(|𝑉|)),complexity of Johnson’s algorithm is which is better than Floyd-Warshall’s time complexity if a graph is sparse\nΘ(|𝑉|2|𝐸| Θ(|𝑉|) Θ(|𝑉||𝐸|log(|𝑉|)) log(|𝑉|))).(since wouldbeontheorderof inasparsegraph,whichimpliesthat isasymptoticallyequalto\nAll-Pairs Shortest Path Algorithms\nDijkstra’s Times|V|\n(Min Binary Heap)\nFloyd-Warshall Algorithm\nJohnson’s Algorithm\n(Min Binary Heap)\nTime Complexity\nΘ(|𝑉||𝐸|log(|𝑉|))\nΘ(|𝑉|3)\nΘ(|𝑉||𝐸|log(|𝑉|))\nRecommended Graph Type\nSparse\nDense\nSparse\nSupports Negative Edge Weights\nNo\nYes\nYes\nCan Detect Negative Cycles\nNo\nYes\nYes\nLastly, it is important to recognize that all of these shortest path algorithms are designed for graphs. If you want to find the shortestweighted\npath between two vertices in an graph, then your goal would simply be to find the path between the two vertices that traverses theunweighted\nfewest number of edges. In this scenario, a straightforward breadth-first search would suffice.\nShortest path algorithms play a critical role in much of the technology we rely on today, from determining the optimal route to take on a road\ntrip to routing data packets efficiently in a communication network. Additional shortest path algorithms build upon these concepts to perform an\nthat optimizes the performance of finding the shortest path between two vertices of a graph. As discussed, one such algorithminformed search\nis the algorithm, which is one of the most popular algorithms for solving pathfinding problems due to its efficiency. The A* algorithmA* search\nimproves upon the standard Dijkstra’s implementation by applying a heuristic that estimates the cost of the cheapest path from a vertex to the\ndestination; this heuristic value is then combined with the best known distance to make a more informed decision on which vertex to visit next.\nRemark: In addition to the single-source and all-pairs shortest path problems, you may also encounter the single-destination shortest path\nproblem, which seeks to find the shortest path from every other vertex in a graph. However, solving this type ofto a single destination vertex\nproblem does not require anything new: by simply reversing the direction of each edge in the graph, the single-destination shortest path\nproblem essentially reduces to a single-source shortest path problem, which we can solve using an algorithm like Dijkstra’s or Bellman-Ford.", "word_count": 932, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2d5e7781-6d54-555a-86a5-d1ff25bfcd2c", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 989, "real_page_number": null, "text": "25.7 Summary of Shortest Path Algorithms\n977\nChapter 25 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. If you run Dijkstra’s algorithm once on a directed graph, you will learn\nA) The shortest distance from every vertex to every other vertex\nB) The shortest distance from one vertex to every other vertex\nC) The shortest distance from every vertex to one vertex\nD) More than one of the above\nE) None of the above\n2. Which of the following statements is/are TRUE regarding Dijkstra’s algorithm?\nI. Dijkstra’s algorithm only works on positively weighted, undirected graphs\nII. The linear search implementation of Dijkstra’s algorithm is preferable to the priority queue implementation on dense graphs\nΘ(|𝐸|log(|𝑉|))III. A single run of Dijkstra’s algorithm can find the shortest path between every pair of vertices in a graph in time\nA) I only\nB) II only\nC) III only\nD) I and II only\nE) I, II, and III\n3. True or false? Given a directed graph 𝐺with non-negative edge weights, finding the shortest path from node 𝐴to another node 𝑍in the\ngraph using Dijkstra’s algorithm is always faster than finding the shortest path from node 𝐴to every other node in 𝐺.\nA) True\nB) False\n4. True or false? Dijkstra’s algorithm is not guaranteed to find the shortest path of a graph with negative edge weights.\nA) True\nB) False\n𝐺′,5. True or false? Consider a graph 𝐺where the shortest path between node 𝐴and node 𝐵has a weight of 5. Consider another graph which\n𝐺′ 𝐺′,is identical to 𝐺with the exception that the weight of every edge in the graph is larger by a value of 10. In graph the shortest path\nbetween node 𝐴and node 𝐵must have a weight of 15.\nA) True\nB) False\n6. You work for the food delivery startup DoorHash, and you need to deliver orders to a customer on the other side of town. You are given\n|𝑉| |𝐸|two undirected graphs with vertices and non-negative edges that represent the network of roads in your town. In both graphs, the\nvertices represent road intersections and the edges represent roads that connect these intersections. However, in the first graph, the edges\nare weighted by the total walking time between two intersections, while in the second graph, the edges are weighted by the total driving\ntime between two intersections. The two graphs share the same connections between intersections, with the only difference being the edge\nweights (due to differences in time between walking and driving). You want to find the path between your starting point and the intersection\nclosest to the destination house where the difference between total walking time and driving time is the greatest. Can you use Dijkstra’s\nalgorithm to solve this problem, and if so, what is the time complexity if you use the most optimal data structures?\nΘ(|𝑉|)A) Yes, you can use Dijkstra’s algorithm with time complexity\nΘ(|𝐸|log(|𝑉|))B) Yes, you can use Dijkstra’s algorithm with time complexity\nΘ(|𝑉|2)C) Yes, you can use Dijkstra’s algorithm with time complexity\nΘ(|𝑉||𝐸|log(|𝑉|)D) Yes, you can use Dijkstra’s algorithm with time complexity\nE) No, this problem cannot be solved with Dijkstra’s algorithm\n|𝑉|7. You are an engineer at an utility company tasked with analyzing substations on a power grid. You are given a directed graph with\n|𝐸|vertices and non-negative edges, where each vertex represents a substation and each edge represents a transmission line that connects\ntwo substations. The edges in the graph are weighted by the percentage loss in signal strength that occurs when power travels between two\nsubstations (where a lower weight indicates lower loss). If every substation in the graph is connected to nearly every other substation, what\nis the time complexity of finding the minimum-loss path between two substations if you use Dijkstra’s algorithm with the most optimal data\nstructures?\nΘ(|𝑉|)A)\nΘ(|𝐸|log(|𝑉|))B)\nΘ(|𝑉|2)C)\nΘ(|𝑉||𝐸|log(|𝑉|)D)\nΘ(|𝑉|3)E)", "word_count": 692, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "5972c452-3963-5c89-b141-3d7e19364d6c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 990, "real_page_number": null, "text": "978\nChapter 25. Shortest Path Algorithms\n8. Consider the following graph with positive, non-zero integer weights, with two mystery weights denoted as 𝑥and 𝑦:\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n2\n𝑦\n𝑥\n4\n2\nYou use Dijkstra’s algorithm to find the shortest path from 𝐴to every other vertex in the graph above, breaking ties using alphabetical\nordering. Let 𝑆be the set of vertices for which the minimum path weight from 𝐴has been found. While running Dijkstra’s algorithm, the\nvertices in the graph are added to 𝑆in the following order: 𝐴, 𝐵, 𝐶, 𝐸, 𝐷. Knowing this, which of the following is a possible value for 𝑦?\nI. 3\nII. 5\nIII. 7\nA) II only\nB) III only\nC) I and II only\nD) II and III only\nE) I, II, and III\n|𝑉| |𝐸|9. Given a sparse graph with vertices and non-negative edges, what is the worst-case time complexity of finding the shortest path\nbetween pairs of vertices in the graph using Dijkstra’s algorithm?all\nΘ(|𝑉|)A)\nΘ(|𝐸|log(|𝑉|))B)\nΘ(|𝑉|2)C)\nΘ(|𝑉||𝐸|log(|𝑉|))D)\nΘ(|𝑉|3)E)\n10. Consider the following three algorithms:\nI. Prim’s algorithm\nII. Kruskal’s algorithm\nIII. Dijkstra’s algorithm\nWhich of these algorithms can be considered as part of the greedy algorithm family?\nA) III only\nB) I and II only\nC) I and III only\nD) II and III only\nE) I, II, and III\n11. You are using Dijkstra’s algorithm to find the shortest path from vertex 𝐴to every other vertex in the graph below. 𝑆is the set of vertices\nfor which the minimum path weight from 𝐴has been found. Which of the following gives the correct order in which vertices are added to\n𝑆? Break any ties using alphabetical ordering, and only update predecessors for weights that are improvements.\n𝐶\n𝐸\n𝐺\n𝐵\n𝐹\n𝐷\n𝐴\n2\n3\n3\n2\n3\n6\n4\n2\n5\nA) 𝐴,𝐶,𝐵,𝐹,𝐷,𝐺,𝐸\nB) 𝐴,𝐶,𝐵,𝐹,𝐺,𝐷,𝐸\nC) 𝐴,𝐶,𝐷,𝐺,𝐹,𝐵,𝐸\nD) 𝐴,𝐶,𝐹,𝐵,𝐷,𝐺,𝐸\nE) 𝐴,𝐶,𝐹,𝐵,𝐺,𝐷,𝐸", "word_count": 321, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "65ab6885-118b-5b7f-ad32-8982e713480f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 991, "real_page_number": null, "text": "25.7 Summary of Shortest Path Algorithms\n979\n12. You are using Dijkstra’s algorithm to find the shortest path from vertex 𝐴to every other vertex in the graph below. 𝑆is the set of vertices\nfor which the minimum path weight from 𝐴has been found. Which of the following gives the correct order in which vertices are added to\n𝑆? Break any ties using alphabetical ordering, and only update predecessors for weights that are improvements.\n𝐵\n𝐶\n𝐷\n𝐸\n𝐺\n𝐹\n𝐻\n𝐼\n𝐴\n4\n1\n3\n2\n1\n2\n2\n5\n3\n1\n5\n3\nA) 𝐴,𝐸,𝐷,𝐵,𝐶,𝐻,𝐼,𝐹,𝐺\nB) 𝐴,𝐸,𝐷,𝐻,𝐼,𝐵,𝐶,𝐹,𝐺\nC) 𝐴,𝐸,𝐷,𝐻,𝐼,𝐵,𝐶,𝐺,𝐹\nD) 𝐴,𝐸,𝐷,𝐼,𝐻,𝐵,𝐶,𝐺,𝐹\nE) 𝐴,𝐸,𝐷,𝐼,𝐻,𝐶,𝐺,𝐵,𝐹\n13. You are using Dijkstra’s algorithm to find the shortest path from vertex 𝐴to every other vertex in the graph below. 𝑆is the set of vertices\nfor which the minimum path weight from 𝐴has been found. Which of the following gives the correct order in which vertices are added to\n𝑆? Break any ties using alphabetical ordering, and only update predecessors for weights that are improvements.\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\n𝐴\n2\n1\n2\n1\n2\n1\n5\n7\n1\n1\n6\n5\n1\n3\n1\nA) 𝐴,𝐹,𝐵,𝐺,𝐼,𝐽,𝐻,𝐾,𝐶,𝐸,𝐷\nB) 𝐴,𝐹,𝐺,𝐵,𝐼,𝐽,𝐻,𝐾,𝐶,𝐸,𝐷\nC) 𝐴,𝐹,𝐺,𝐵,𝐼,𝐽,𝐻,𝐾,𝐸,𝐷,𝐶\nD) 𝐴,𝐹,𝐺,𝐵,𝐼,𝐽,𝐾,𝐻,𝐶,𝐸,𝐷\nE) 𝐴,𝐹,𝐺,𝐵,𝐼,𝐽,𝐾,𝐻,𝐸,𝐷,𝐶\n14. Suppose we run Dijkstra’s Algorithm on the graph below, starting at node 𝐴. After Dijkstra’s Algorithm has finished executing, what are\nthe values of 𝑝𝑣(the predecessor column) from top to bottom? Break any ties using alphabetical ordering, and only update predecessors for\nweights that are improvements.\n𝐵\n𝐻\n𝐶\n𝐼\n𝐺\n𝐷\n𝐹\n𝐸\n𝐴\n4\n1\n5\n4\n3\n5\n5\n2\n1\n2\n1\n3\n1\n6\n𝑣\n𝑘𝑣\n𝑑𝑣\n𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0\n-\nA) −,𝐼,𝐵,𝐼,𝐹,𝐺,𝐻,𝐼,𝐴\nB) −,𝐼,𝐵,𝐼,𝐹,𝐼,𝐻,𝐼,𝐴\nC) −,𝐼,𝐷,𝐼,𝐹,𝐺,𝐹,𝐼,𝐴\nD) −,𝐼,𝐷,𝐼,𝐹,𝐺,𝐻,𝐼,𝐴\nE) −,𝐼,𝐷,𝐼,𝐹,𝐼,𝐻,𝐼,𝐴", "word_count": 301, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "31e7ae07-0989-5273-a249-7df58d1a3db7", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 992, "real_page_number": null, "text": "980\nChapter 25. Shortest Path Algorithms\nFor questions 15-16, consider a graph whose edge connections and weights are listed in the table on the left. You are running Dijkstra’s\nalgorithm on this graph, and a partial snapshot of the algorithm’s current status is shown in the table on the right.\nEdge\nWeight\n𝐴→𝐸\n4\n𝐴→𝐹\n2\n𝐴→𝐺\n4\n𝐵→𝐶\n2\n𝐵→𝐸\n1\n𝐵→𝐺\n5\n𝐷→𝐶\n4\n𝐷→𝐸\n1\n𝐷→𝐻\n3\n𝐸→𝐴\n1\n𝐸→𝐵\n2\n𝐹→𝐴\n3\n𝐹→𝐻\n5\n𝐺→𝐵\n4\n𝐺→𝐷\n2\n𝐺→𝐻\n2\n𝐻→𝐶\n7\n𝐻→𝐹\n4\n𝐻→𝐺\n1\n𝑣\n𝑘𝑣\n𝑑𝑣\n𝑝𝑣\n𝐴\nT\n3\n𝐹\n𝐵\nF\n?\n-\n𝐶\nF\n?\n-\n𝐷\nF\n?\n-\n𝐸\nT\n7\n𝐴\n𝐹\nT\n0\n-\n𝐺\nT\n6\n𝐻\n𝐻\nT\n5\n𝐹\n15. What is the order in which the first four vertices have their lowest-weighted paths from the source vertex known? Include the source vertex\nitself as the first vertex in the solution.\nA) 𝐴,𝐹,𝐻,𝐺\nB) 𝐴,𝐻,𝐺,𝐸\nC) 𝐹,𝐴,𝐸,𝐻\nD) 𝐹,𝐴,𝐻,𝐺\nE) 𝐹,𝐻,𝐺,𝐸\n16. After running Dijkstra’s algorithm to completion, what is the total weight of the shortest path from the source vertex to vertex 𝐶?\nA) 8\nB) 9\nC) 10\nD) 11\nE) 12\nFor questions 17-18, consider the following directed graph:\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n17\n12\n4\n3\n4\n15\n1\n5\n10\n5\n17. Consider the above directed graph. What path does Dijkstra’s algorithm discover from 𝐴to 𝐹? Only update predecessors for weights that\nare improvements.\nA) 𝐴𝐵𝐸𝐹\nB) 𝐴𝐶𝐹\nC) 𝐴𝐷𝐶𝐹\nD) 𝐴𝐷𝐸𝐹\nE) Any of the above\n18. Consider the above directed graph, but add 100 to each edge weight. What path does Dijkstra’s algorithm discover from 𝐴to 𝐹? Only\nupdate predecessors for weights that are improvements.\nA) 𝐴𝐵𝐸𝐹\nB) 𝐴𝐶𝐹\nC) 𝐴𝐷𝐶𝐹\nD) 𝐴𝐷𝐸𝐹\nE) Any of the above", "word_count": 304, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "4b188b33-fd7f-5a93-8719-1e5792890a7a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 993, "real_page_number": null, "text": "25.7 Summary of Shortest Path Algorithms\n981\nFlight19. Consider the following struct, which stores information about a flight’s origin and destination locations, as well as the price of\ntaking that flight.\nstruct Flight {\nstd::string origin;\nstd::string dest;\ndouble price;\n};\nFlight start end, startGiven a vector of connections and two airports and return the cheapest possible price to depart from airport\nendand finish at airport with any number of layovers. You may assume that a path exists between the origin and destination locations.\ndouble cheapest_price(const const conststd::vector<Flight>& flights, std::string& src, std::string& dest);\n195Example: Given the following graph, you would return if the source airport is DTW and the destination airport is MIA, since the\nshortest path would be to take a layover in ATL.\nDTW\nJFK\nMIA\nATL\nORD\nDFW\n130\n110\n200\n125\n185\n65\n145\n170\n120\n75\nelevation20. You are a hiker currently hiking in a national park. You are given an elevation map in the form of a 𝑚×𝑛array, where each\n(start_row, start_col)cell represents the elevation at that location. Knowing your starting location and your destination location\n(dest_row, dest_col), and that you can move to any cell directly above, below, to the left, or to the right of your current location,\nimplement a function that returns the minimum total elevation change required to reach your destination. The elevation change between two\nadjacent cells on the map is the absolute difference in elevation between the two cells (e.g., if you are at elevation 5, and the cell above you is\nat elevation 8, the total elevation change between these two cells is 3; note that this also applies in the other direction since elevation change\nis measured in absolute value, so the elevation change would still be 3 if the current cell is at elevation 8 and the next cell is at elevation 5).\nint32_t min_elevation(const std::vector<std::vector<int32_t>>& size_t size_televation, start_row, start_col,\nsize_t size_tdest_row, dest_col);\nExample 1: Given the following elevation map on the left and coordinates of\nstart_row = 0•\nstart_col = 0•\nend_row = 4•\nend_col = 4•\n11,you would return since that is the minimum total elevation change that is possible between these two coordinates (shown by the shaded\npath in the elevation map on the right):\n5\n7\n3\n9\n3\n6\n8\n1\n6\n2\n7\n5\n4\n3\n1\n8\n9\n2\n5\n4\n9\n6\n5\n7\n8\n5\n7\n3\n9\n3\n6\n8\n1\n6\n2\n7\n5\n4\n3\n1\n8\n9\n2\n5\n4\n9\n6\n5\n7\n8\nExample 2: Given the following elevation map on the left and coordinates of\nstart_row = 3•\nstart_col = 2•\nend_row = 1•\nend_col = 2•\n6,you would return since that is the minimum total elevation change that is possible between these two coordinates (shown by the shaded\npath in the elevation map on the right):\n2\n3\n7\n4\n4\n5\n5\n8\n9\n6\n1\n3\n5\n4\n3\n9\n2\n6\n2\n3\n7\n4\n4\n5\n5\n8\n9\n6\n1\n3\n5\n4\n3\n9\n2\n6", "word_count": 533, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9c05b8da-12a9-5428-9b1e-62f0cf3caf5e", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 994, "real_page_number": null, "text": "982\nChapter 25. Shortest Path Algorithms\nChapter 25 Exercise Solutions\n1. The correct answer is (B). A single run of Dijkstra’s algorithm can be used to discover the shortest distance from the source vertex to every\nother reachable vertex in a given graph. This is because Dijkstra’s algorithm explores outward from the source vertex, greedily identifying\neach vertex’s shortest path to the source and using that information to determine the shortest path of vertices encountered later during the\nsearch. At the end of the run, every vertex reachable from the source would have been discovered and its shortest path known.\n2. The correct answer is (B). Only statement II is true. Statement I is false because Dijkstra’s algorithm is not limited to undirected graphs,\nas directed graphs are supported without any issues. Statement III is false because Dijkstra’s algorithm can only find the shortest path to\neach vertex from a single source vertex, rather than for all pairs of vertices, if it is only run once. Statement II is true because the linear\n|𝑉|2,|𝐸|search implementation is preferred for dense graphs, as the number of edges is on the order of which would cause the priority\nΘ(|𝑉|2 Θ(|𝑉|2)Θ(|𝐸|log(|𝑉|)) log(|𝑉|)),queue implementation to run with a time complexity of as opposed to for a linear search.=\n3. The correct answer is (B). This statement is false, since a single run of Dijkstra’s algorithm will already find the shortest path from node 𝐴\nto every other vertex, which includes 𝑍if it is reachable from 𝐴.\n4. The correct answer is (A). This statement is true, since Dijkstra’s algorithm is not guaranteed to work on graphs with negative edge\nweights. This is because Dijkstra’s correctness rests on the assumption that after the shortest path to a vertex is discovered, it is not possible\nfor another path to have a better weight. However, once negative edges are introduced, this guarantee no longer holds, and it becomes\npossible for an unencountered negative edge to give us a better solution.\n5. The correct answer is (B). This statement is false, since addition to every edge does not guarantee that the shortest path will remain the\nsame (see example 25.2). Thus, you cannot conclude that the shortest path between node 𝐴and node 𝐵will go through the same edges.\n6. The correct answer is (E). This is a longest path problem rather than a shortest path problem (since you want to find the path with the\ngreatest difference between walking and driving), which cannot be solved using Dijkstra’s algorithm.\n7. The correct answer is (C). Since every substation in the graph is connected to nearly every other substation, this is a dense graph, so the\nΘ(|𝑉|) |𝑉|optimal implementation of Dijkstra’s algorithm involves a linear search. This implementation takes time with vertices.\n8. The correct answer is (B). Since Dijkstra’s algorithm greedily explores vertices in order of increasing distance from the source vertex 𝐴,\nfor vertex 𝐷to be discovered last, it must be farther from vertex 𝐴than all the other vertices. There are only two ways to reach vertex 𝐷\nfrom the source vertex 𝐴given the provided order that Dijkstra’s visited the vertices: either through the direct connection of weight 𝑦, or\nthrough vertex 𝐶, which is reachable from 𝐴with total weight 2+𝑥. We know that 𝐸is marked as visited before 𝐷, so the distance from 𝐴\nto 𝐷via vertex 𝐶must be strictly greater than the distance to get from 𝐴to 𝐸via vertex 𝐶(equal does not work since ties are broken\nalphabetically). Thus, must be true, or 𝐷would be visited before 𝐸. The smallest possible value of 𝑥is 1 since the graph has𝑦>2+𝑥+2\npositive, non-zero integer weights, so 𝑦must be strictly greater than 5.\n9. The correct answer is (D). To solve the all-pairs shortest path problem using Dijkstra’s algorithm, you would need to run the algorithm\n|𝑉| Θ(|𝐸|log(|𝑉|))time with each vertex as the source vertex. Each run would take time on a sparse graph with the priority queue\n|𝑉| Θ(|𝑉||𝐸|log(|𝑉|)).implementation, so running the algorithm times would yield an overall time complexity of\n10. The correct answer is (E). All of the algorithms can be considered as greedy algorithms. Prim’s greedily chooses the vertex with the\nsmallest tentative distance to another vertex, Kruskal’s greedily chooses the smallest edge that does not produce a cycle, and Dijkstra’s\ngreedily chooses the vertex with the shortest distance available from the source vertex.\n11. The correct answer is (A). The process of running Dijkstra’s is shown below. The order in which vertices are marked as visited is:\n𝐴,𝐶,𝐵,𝐹,𝐷,𝐺,𝐸.\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nF\n3\nA\nF\n2\nA\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nF\n3\nA\nT\n2\nA\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nF\n3\nA\nT\n2\nA\nF\n6\nC\n∞-F\nF\n5\nC\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nT\n3\nA\nT\n2\nA\nF\n6\nC\n∞-F\nF\n5\nC\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nT\n3\nA\nT\n2\nA\nF\n6\nC\nF\n9\nB\nF\n5\nC\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nT\n3\nA\nT\n2\nA\nF\n6\nC\nF\n9\nB\nT\n5\nC\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nT\n3\nA\nT\n2\nA\nF\n6\nC\nF\n9\nB\nT\n5\nC\nF\n7\nF\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nT\n3\nA\nT\n2\nA\nT\n6\nC\nF\n9\nB\nT\n5\nC\nF\n7\nF\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nT\n3\nA\nT\n2\nA\nT\n6\nC\nF\n9\nB\nT\n5\nC\nT\n7\nF\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nT\n0\n-\nT\n3\nA\nT\n2\nA\nT\n6\nC\nT\n9\nB\nT\n5\nC\nT\n7\nF", "word_count": 1062, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "18adff0d-9cfe-55b1-b9c3-1caa5d34bea2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 995, "real_page_number": null, "text": "25.7 Summary of Shortest Path Algorithms\n983\n12. The correct answer is (C). The process of running Dijkstra’s is shown below. The order in which vertices are marked as visited is:\n𝐴,𝐸,𝐷,𝐻,𝐼,𝐵,𝐶,𝐺,𝐹.\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nF 4 A\nF 5 A\nF 2 A\nF 1 A\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nF 4 A\n5F A\nF 2 A\n1T A\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nF 4 A\n5F A\nF 2 A\nT 1 A\n∞-F\n∞-F\nF 3 E\nF 4 E\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nF 4 A\n5F A\n2T A\nT 1 A\n∞-F\n∞-F\n3F E\nF 4 E\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nF 4 A\n5F A\nT 2 A\nT 1 A\nF 7 D\n∞-F\n3F E\nF 3 D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nF 4 A\n5F A\nT 2 A\nT 1 A\n7F D\n∞-F\n3T E\n3F D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nF 4 A\nF 4 H\nT 2 A\nT 1 A\n7F D\n∞-F\n3T E\n3F D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nF 4 A\nF 4 H\nT 2 A\nT 1 A\n7F D\n∞-F\n3T E\n3T D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\n4T A\nF 4 H\nT 2 A\nT 1 A\n7F D\n∞-F\n3T E\n3T D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nT 4 A\nF 4 H\nT 2 A\nT 1 A\n7F D\nF 7 B\n3T E\n3T D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nT 4 A\n4T H\nT 2 A\nT 1 A\n7F D\n7F B\n3T E\n3T D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nT 4 A\nT 4 H\nT 2 A\nT 1 A\n7F D\nF 6 C\n3T E\n3T D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nT 4 A\nT 4 H\nT 2 A\nT 1 A\n7F D\n6T C\n3T E\n3T D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n0T\n-\nT 4 A\nT 4 H\nT 2 A\nT 1 A\n7T D\n6T C\n3T E\n3T D\n13. The correct answer is (B). The process of running Dijkstra’s is shown below. The order in which vertices are marked as visited is:\n𝐴,𝐹,𝐺,𝐵,𝐼,𝐽,𝐻,𝐾,𝐶,𝐸,𝐷.\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nF\n5\nA\n∞-F\n∞-F\n∞-F\nT\n2\nA\nF\n6\nA\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nF\n5\nA\n∞-F\n∞-F\n∞-F\nT\n2\nA\nT\n4\nF\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nT\n5\nA\n∞-F\n∞-F\n∞-F\nT\n2\nA\nT\n4\nF\nF\n9\nG\nF\n5\nG\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nT\n5\nA\nF 12 B\n∞-F\n∞-F\nT\n2\nA\nT\n4\nF\nF\n9\nG\nT\n5\nG\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nT\n5\nA\nF 12 B\n∞-F\n∞-F\nT\n2\nA\nT\n4\nF\nF\n9\nG\nT\n5\nG\nT\n7\nI\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nT\n5\nA\nF 12 B\n∞-F\nJF 10\nT\n2\nA\nT\n4\nF\nT\n8\nJ\nT\n5\nG\nT\n7\nI\nF\n8\nJ\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nT\n5\nA\nF\n9 H\n∞-F\n10F J\nT\n2\nA\nT\n4\nF\nT\n8\nJ\nT\n5\nG\nT\n7\nI\nT\n8\nJ\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nT\n5\nA\nT\n9 H\n∞-F\nF\n9\nK\nT\n2\nA\nT\n4\nF\nT\n8\nJ\nT\n5\nG\nT\n7\nI\nT\n8\nJ\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nT\n5\nA\nT\n9 H\nF 10 C\nT\n9\nK\nT\n2\nA\nT\n4\nF\nT\n8\nJ\nT\n5\nG\nT\n7\nI\nT\n8\nJ\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\n𝐽\n𝐾\nT\n0\n-\nT\n5\nA\nT\n9 H\n10T C\nT\n9\nK\nT\n2\nA\nT\n4\nF\nT\n8\nJ\nT\n5\nG\nT\n7\nI\nT\n8\nJ", "word_count": 1043, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9899badb-f5b0-55bb-a1d7-63f16ad7fcbe", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 996, "real_page_number": null, "text": "984\nChapter 25. Shortest Path Algorithms\n14. The correct answer is (B). The process of running Dijkstra’s is shown below.\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\nF\n4\nA\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\nF\n5\nA\nT\n2\nA\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\nT\n3\nI\n∞-F\nF\n5\nI\nF\n8\nI\nF\n5\nI\n∞-F\nF\n3\nI\nT\n2\nA\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\nT\n3\nI\nF\n8\nB\nF\n5\nI\nF\n8\nI\nF\n5\nI\n∞-F\nT\n3\nI\nT\n2\nA\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\nT\n3\nI\nF\n8\nB\nT\n5\nI\nF\n8\nI\nF\n5\nI\nF\n5 H\nT\n3\nI\nT\n2\nA\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\nT\n3\nI\nF\n8\nB\nT\n5\nI\nF\n8\nI\nT\n5\nI\nF\n5 H\nT\n3\nI\nT\n2\nA\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\nT\n3\nI\nF\n8\nB\nT\n5\nI\nF\n6\nF\nT\n5\nI\nT\n5 H\nT\n3\nI\nT\n2\nA\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\nT\n3\nI\nF\n8\nB\nT\n5\nI\nT\n6\nF\nT\n5\nI\nT\n5 H\nT\n3\nI\nT\n2\nA\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nT\n0\n-\nT\n3\nI\nT\n8\nB\nT\n5\nI\nT\n6\nF\nT\n5\nI\nT\n5 H\nT\n3\nI\nT\n2\nA\n15. The correct answer is (D). The source vertex is 𝐹, since it has a best known distance of 0 (and no predecessor). The next vertices to be\nadded can be determined by taking the vertices in ascending order of 𝑑𝑣, which comes out to 𝐴,𝐻,𝐺,𝐸. Since we want to include the\nsource vertex as the first vertex in the solution, the first four discovered vertices therefore come in the order of 𝐹,𝐴,𝐻,𝐺.\n16. The correct answer is (D). There are three ways to get to vertex 𝐶: either from vertex 𝐵, 𝐷, or 𝐻. The path from vertex 𝐻has a total path\nweight of 5 + 7 = 12. However, is the path from 𝐵or 𝐷better? Using the available connections, we see that the best weight to get to 𝐵is\nequal to 7 + 2 = 9 (via 𝐸→𝐵). Going from 𝐵to 𝐶therefore yields a total path weight of 9 + 2 = 11. Alternatively, the best weight to get\nto 𝐷is equal to 6 + 2 = 8 (via 𝐺→𝐷), so going from 𝐷to 𝐶yields a total path weight of 8 + 4 = 12. The best weight would therefore be\nto go from 𝐹to 𝐺(which we know has cost 6), then from 𝐺to 𝐵with a cost of 3, then from 𝐵to 𝐶for a cost of 2, for a total cost of 11.\n17. The correct answer is (D). The process of running Dijkstra’s algorithm is shown below. You can see via the predecessor column that the\nshortest path from 𝐴to 𝐹is 𝐴𝐷𝐸𝐹(note that even though there are multiple shortest paths that share the same weight, the process by\nwhich we update for equal weights makes the path discovered by Dijkstra’s algorithm deterministic).\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nF 15 A\nF 17 A\nT 12 A\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\n15T A\n17F A\nT 12 A\nF 16 D\nF 22 D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\n15T A\n17F A\nT 12 A\n16T D\nF 22 D\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\n15T A\n17T A\nT 12 A\n16T D\nF 21 E\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\n15T A\n17T A\nT 12 A\n16T D\n21T E\n18. The correct answer is (B). The process of running Dijkstra’s algorithm on the updated graph is shown below. You can see via the\npredecessor column that the shortest path from 𝐴to 𝐹is now 𝐴𝐶𝐹(a good example that adding the same weight to every edge does not\nguarantee that the shortest path stays the same, since it disproportionately punishes paths that have a greater number of edges).\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\n∞-F\n∞-F\n∞-F\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nF A115\nF A117\nT A112\n∞-F\n∞-F\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT A115\nF A117\nT A112\nF D216\nF D222\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT A115\nT A117\nT A112\nF D216\nF D222\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT A115\nT A117\nT A112\nT D216\nF C221\n𝑣𝑘𝑣𝑑𝑣𝑝𝑣\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\nT\n0\n-\nT A115\nT A117\nT A112\nT D216\nT C221", "word_count": 975, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5c0a043f-656f-5825-a260-606971a89226", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 997, "real_page_number": null, "text": "25.7 Summary of Shortest Path Algorithms\n985\n19. This is a shortest path problem where you want to find the path from start to destination with the lowest total weight (which represents cost).\nYou can solve this problem by running Dijkstra’s algorithm on the graph — one possible solution is shown below:\n1\nstruct DijkstraData {\n2\ndouble d;\n3\nbool k;\n4\nDijkstraData()\n5\nstd::numeric_limits<double>::infinity() false: d{ }, k{ } {}\n6\n};\n7\n8\nusing std::pair<double, int32_t>;DistPair =\n// <d_v, v>\n9\n10\ndouble cheapest_price(const conststd::vector<Flight>& flights, std::string& src,\n11\nconst std::string& dest) {\n12\n// convert input into an adjacency list\n13\nint32_t>std::unordered_map<std::string, id_map;\n14\nstd::unordered_map<int32_t, std::vector<std::pair<int32_t, double>>> graph;\n15\nint32_t id = 0;\n16\nfor (const auto& flight : flights) {\n17\nif (id_map.find(flight.origin) == id_map.end()) {\n18\nid_map[flight.origin] = id++;\n19\n} // if\n20\nif (id_map.find(flight.dest) == id_map.end()) {\n21\nid_map[flight.dest] = id++;\n22\n} // if\n23\ngraph[id_map[flight.origin]].emplace_back(id_map[flight.dest], flight.price);\n24\n} // for flight\n25\n26\nstd::vector<DijkstraData> dijkstra_table(id_map.size() + 1);\n27\nstd::priority_queue<DistPair, std::vector<DistPair>, std::greater<DistPair>> pq;\n28\ndijkstra_table[id_map[src]].d = 0;\n29\npq.emplace(0, id_map[src]);\n30\n31\nwhile (!pq.empty()) {\n32\nauto [min_dist, idx] = pq.top();\n33\npq.pop();\n34\nif (!dijkstra_table[idx].k) {\n35\ntrue;dijkstra_table[idx].k =\n36\nfor (auto& neighbor_dist_pair : graph[idx]) {\n37\nauto [neighbor, dist] = neighbor_dist_pair;\n38\ndouble new_dist = dijkstra_table[idx].d + dist;\n39\nif (new_dist < dijkstra_table[neighbor].d) {\n40\ndijkstra_table[neighbor].d = new_dist;\n41\npq.emplace(new_dist, neighbor);\n42\n} // if\n43\n} // for neighbor_dist_pair\n44\n} // if\n45\n} // while\n46\n47\nreturn dijkstra_table[id_map[dest]].d;\n48\n} // cheapest_price()", "word_count": 262, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c02a8641-d880-5ade-b59e-0f383f5fa952", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 998, "real_page_number": null, "text": "986\nChapter 25. Shortest Path Algorithms\n20. You can think of each cell of the elevation map as an individual vertex of a graph, where connections exist to adjacent cells with a weight\nequal to the absolute elevation difference. Using this approach, we can treat this as a shortest path problem from the starting cell to the\ndestination cell, which can be solved using Dijkstra’s algorithm. One solution is shown below:\n1\nstruct DijkstraData {\n2\nint32_t d;\n3\nbool k;\n4\nDijkstraData()\n5\nstd::numeric_limits<int32_t>::max() false: d{ }, k{ } {}\n6\n};\n7\n8\nusing std::tuple<int32_t, int32_t, int32_t>;DistTuple =\n// <d_v, row, col>\n9\n10\nint32_t min_elevation(const std::vector<std::vector<int32_t>>& elevation,\n11\nsize_t size_tstart_row, start_col,\n12\nsize_t size_tdest_row, dest_col) {\n13\nsize_t m = elevation.size(), n = elevation[0].size();\n14\nstd::vector<std::vector<DijkstraData>> dijkstra_table(m + 1, std::vector<DijkstraData>(n + 1));\n15\nstd::priority_queue<DistTuple, std::vector<DistTuple>, std::greater<DistTuple>> pq;\n16\ndijkstra_table[start_row][start_col].d = 0;\n17\npq.emplace(0, start_row, start_col);\n18\n19\n// for easier iteration over adjacent cells\n20\nstd::vector<int32_t> dirs = {0, 1, 0, -1, 0};\n21\n22\nwhile (!pq.empty()) {\n23\nauto [min_dist, curr_row, curr_col] = pq.top();\n24\npq.pop();\n25\nif (!dijkstra_table[curr_row][curr_col].k) {\n26\ntrue;dijkstra_table[curr_row][curr_col].k =\n27\nfor (size_t i = 0; i < 4; ++i) {\n28\nint32_t next_row = curr_row + dirs[i];\n29\nint32_t next_col = curr_col + dirs[i + 1];\n30\nif (next_row >= 0 && next_row < m && next_col >= 0 && next_col < n) {\n31\ndouble new_dist = dijkstra_table[curr_row][curr_col].d +\n32\nstd::abs(elevation[next_row][next_col] - elevation[curr_row][curr_col]);\n33\nif (new_dist < dijkstra_table[next_row][next_col].d) {\n34\ndijkstra_table[next_row][next_col].d = new_dist;\n35\npq.emplace(new_dist, next_row, next_col);\n36\n} // if\n37\n} // if\n38\n} // for i\n39\n} // if\n40\n} // while\n41\n42\nreturn dijkstra_table[dest_row][dest_col].d;\n43\n} // min_elevation()", "word_count": 290, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b8a70d6c-949e-5dcb-a0a1-a8230a285308", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 999, "real_page_number": null, "text": "Chapter 26\nComputational Geometry\n26.1\nPoints, Segments, and Lines\nIt is quite difficult to discuss any topic without referencing the everlasting presence of geometry in our multidimensional world, and the topic of\nalgorithms is certainly no exception. In this chapter, we will introduce the field of computational geometry, which focuses on the design and\nanalysis of algorithms for problems involving geometry, often with inputs and outputs of multiple dimensions. Computational geometry plays\nan important role in many fields of computer science, such as computer vision, image processing, modeling, robotics, and many more. Although\nwe won’t be able to delve into everything this field has to offer in EECS 281, we will still provide a high-level overview of several important\ncomputational geometry algorithms in the following sections.\nTo start off, we will look at how we can represent the simplest geometric components in a program. The first of these components is the\npoint, which describes a single position in a dimensional space. Each point can be uniquely identified using a coordinate for each dimension\nthat it resides in. For example, we identify each 2-D point using a pair of coordinates (𝑥,𝑦). In 3-D, we will need to add an additional coordinate\nto represent the position of a point along the third dimension, i.e., (𝑥,𝑦,𝑧).\n𝑦\n𝑥\n(2, 2)\n1\nstruct Point {\n2\ndouble x;\n3\ndouble y;\n4\n};\n5\n6\nPoint pt{2, 2};", "word_count": 234, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "816e10fe-c704-502e-a7a5-cd4265966ec3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1000, "real_page_number": null, "text": "988\nChapter 26. Computational Geometry\nA segment describes the portion of a line between two distinct points, and it can be uniquely identified by its two endpoints.\n𝑦\n𝑥\n(2, 2)\n(1, 1.5)\n1\nstruct Segment {\n2\nPoint pt1;\n3\nPoint pt2;\n4\n};\n5\n6\nSegment sg{Point{1, 1.5}, Point{2, 2}};\nIf we remove the endpoints of a segment and continue both ends infinitely in either direction, we would get a line. In two dimensions, a straight\nline can be uniquely identified using a slope and an y-intercept. The slope of a line is a measure of its steepness (i.e., Δ𝑦∕Δ𝑥, or the change in 𝑦\nfor each change in 𝑥), and the y-intercept is the value of 𝑦at which the line crosses the y-axis. Given a slope 𝑚and intercept 𝑏, all points (𝑥,𝑦)\non the line must satisfy the equation 𝑚𝑥+𝑏.𝑦=\n𝑦\n𝑥\n0.5𝑥+1𝑦=\n1\nstruct Line {\n2\ndouble slope;\n3\ndouble intercept;\n4\n};\n5\n6\nLine ln{0.5, 1};\nIt should be noted that a segment can also be used to define a line. Given two points and (𝑥1,𝑦1), we can solve for the slope (𝑚) and(𝑥0,𝑦0)\n𝑦-intercept (𝑏) of the line that crosses those two points by using the following equations:\n𝑦1−𝑦0𝑚=\n𝑥1−𝑥0\n𝑏=𝑦0−𝑚𝑥0\nHowever, different segments may be used to describe the exact same line. For instance, the segment from to identifies the same(1,1) (2,2)\nline as the segment from to (281,281). In fact, there are an infinite number of segments we can choose to represent any given(280,280)\n𝑦-intercept).1two-dimensional line (but only one slope and\nExample 26.1 You are given two straight 2-D lines in the form of a slope and a 𝑦-intercept. How would you determine if these two lines\nintersect, and what the intersection point is if they do intersect?\nLet’s denote the two lines we are given as 𝑎𝑥+𝑏and 𝑐𝑥+𝑑, where 𝑎and 𝑏are the slope and intercept values of the first line and 𝑐and𝑦= 𝑦=\n𝑑are the slope and intercept values of the second line. First, we would check if the two lines have the same slope, which would indicate that\nthey are parallel. If the two lines do have the same slope, then there are two cases that may arise depending on their intercept values:\n1. If their intercepts are equal, then they are the exact same line (and thus they \"intersect\" at all points).\n2. If their intercepts are not equal, then the two lines parallel each other and never intersect.\nIf the two lines do not have the same slope, then there must exist a single intersection point somewhere, which we will denote as (𝑥𝑖,𝑦𝑖). For the\ntwo lines to meet at (𝑥𝑖,𝑦𝑖), the following equations must be satisfied:\n𝑦𝑖=𝑎𝑥𝑖+𝑏\n(the point must be on line 1)(𝑥𝑖,𝑦𝑖)\n𝑦𝑖=𝑐𝑥𝑖+𝑑\n(the point must be on line 2)(𝑥𝑖,𝑦𝑖)\nSince the right-hand sides of both equations are equal to 𝑦𝑖, we can set them equal to each other, as shown:\n𝑎𝑥𝑖+𝑏=𝑐𝑥𝑖+𝑑\nSolving for 𝑥𝑖, we would get the following:\n𝑑−𝑏𝑥𝑖=\n𝑐−𝑎\nIf we plug the value of 𝑥𝑖into the original equation 𝑎𝑥𝑖+𝑏, we would be able to compute 𝑦𝑖, and thus the intersection point.𝑦𝑖=\nExample 26.2 You are given two segments, each represented by the coordinates of its endpoints. How would you determine if these two\nline segments intersect?\nTo determine if two segments intersect, we will first compute the lines that the two segments reside on. If the two lines have the same slope but\nintercepts, then the two segments cannot intersect (since they are parallel). If the two lines have the same slope but intercepts, wedistinct equal\nwould then need to compare the coordinates of the segments’ endpoints to determine if the segments themselves overlap. Otherwise, if the two\nlines have different slopes, we can use the formulas in example 26.1 to calculate the intersection point, and then check if the intersection point\nlies inside both of the given segments.\n1Wewon’tbeworryingtoomuchaboutlinesinmorethantwodimensionsinthischapter,sinceEECS281isn’tamathclass.", "word_count": 691, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "18674061-e70b-56dc-a72d-e3fc62d36606", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1001, "real_page_number": null, "text": "26.1 Points, Segments, and Lines\n989\nExample 26.3 You are given three points 𝑃1, 𝑃2, and on a 2-D plane. How would you determine if these three points all line on the𝑃3\nsame straight line? (Points lying on the same straight line are defined to be collinear.)\nTo determine if all three points lie on the exact same line, we can check if the lines formed by the segments — and — are the𝑃1 𝑃2 𝑃2 𝑃3\nsame. This can be done by using the endpoints of each segment to compute the slope and 𝑦-intercept of its corresponding line (via the equations\ncovered previously). If the slope and intercept values of both segments are the same, then both segments lie on the same line, which implies that\nall three points must also lie on the same line.\nRemark: Many of the algorithms that we will discuss in this chapter involve floating point comparisons. However, because of how they are\noperator== operator!=represented in memory, comparing two floating point numbers using or can be tricky due to imprecision.\nInstead, a better convention to is to check if two floating point values are rather than exactly equal; if the two values are closeclose enough\nenough within a reasonable error, then we consider them to be equal. This margin of allowable error is known as epsilon.\n1\ndouble a = ..., b = ...;\n2\n3\nif (a == b) { /* do stuff */ }\n// not ideal\n4\n5\nconstexpr double epsilon = 0.0001;\n6\nif (std::fabs(a - b) < epsilon) { /* do stuff */ }\n// better\nThe same idea applies to comparisons. When checking whether one floating-point value is less than or greater than another, it is typically a\ngood idea to add or subtract an epsilon value to handle any imprecision.\n1\ndouble a = ..., b = ...;\n2\n3\nconstexpr double epsilon = 0.0001;\n4\nif (a < b + epsilon) { /* do stuff */ }\n5\nif (a > b - epsilon) { /* do stuff */ }\n<limits> 1.0The C++ library actually provides an epsilon value for floating point types. This value stores the difference between and\nT.the next smallest floating-point value that can be represented using the floating point type\nstd::numeric_limits<T>::epsilon()\n×10−16.doubleFor values, this epsilon value is 2.22 Depending on the problem you are trying to solve, however, your optimal epsilon\nvalue may differ. For instance, if you are working with extremely large values and are relatively lax toward precision differences, a larger\nepsilon value may be ideal. On the other hand, if even the smallest differences can make a big impact on the problem you are trying to solve,\na more fine-grained choice of epsilon may be a better option.\nExample 26.4 .xYou are given a vector of points on a Cartesian (x-coordinate, y-coordinate) plane. Each point’s member variable is its\n.y Pointx-coordinate, and its member variable is its y-coordinate (the definition of a object is provided below). Write a function that\ncounts the number of unique rectangles that can be formed by these points. Do not count a rectangle more than once (e.g., a rectangle\nwith points (𝐴, 𝐵, 𝐶, is equivalent to a rectangle with points (𝐷, 𝐴, 𝐶, 𝐵)). Only consider rectangles whose sides are parallel to the𝐷)\npointsx-axis and y-axis in your count. All points in the vector are unique, but they may be given in any order.\n1\nstruct Point {\n2\nint32_t x;\n3\nint32_t y;\n4\n};\nExample: Given the following points: (1,1), (2,1), (3,1), (3,2), (2,3), (1,3), and (1,2), you would return𝐴= 𝐵= 𝐶= 𝐷= 𝐸= 𝐹= 𝐺=\n2, since there are two unique rectangles that can be formed using these points (𝐴𝐵𝐸𝐹and 𝐴𝐶𝐷𝐺).\nint32_t count_rectangles(const std::vector<Point>& points);\n≠𝑥2 ≠𝑦2.Hint: A rectangle is defined as a quadruple of points where and Rectangles can be(𝑥1,𝑦1),(𝑥1,𝑦2),(𝑥2,𝑦1),(𝑥2,𝑦2) 𝑥1 𝑦1\ndescribed using either two vertical segments or two horizontal segments. When counting rectangles, any pair of vertical segments with the\nsame pair of 𝑦-coordinates, or any pair of horizontal lines with the same pair of 𝑥-coordinates, forms a rectangle.\nThe key insight is to notice that a rectangle can only be formed if there is a pair of vertical segments with the same 𝑦-coordinates, or a pair of\nhorizontal segments with the same 𝑥-coordinates, as described by the hint. We can use this observation to our advantage when solving this\nproblem. To begin our approach, it may be helpful to look at an illustration; consider the following two points that form a vertical segment:\n𝑦\n𝑥", "word_count": 774, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "24fdc157-249b-5f99-ac27-472c8b52722c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1002, "real_page_number": null, "text": "990\nChapter 26. Computational Geometry\nHow can we determine the number of rectangles that can be formed using this vertical segment? For a rectangle to be constructed, there must\nexist another pair of points that form a segment with the same 𝑦-coordinates, as shown. In this case, we can identify a single rectangle that can\nbe formed using the two segments.\n𝑦\n𝑥\n𝑦2\n𝑦1\n𝑦\n𝑥\n𝑦2\n𝑦1\nHowever, what if we encounter another vertical segment with the same pair of 𝑦-coordinates? How would that affect our solution? Notice that\nencountering a third segment ends up increasing the total number of possible rectangles from one to three! This is because this new segment\ncan be used to construct two new, distinct rectangles: one each with the two vertical segments before it.\n𝑦\n𝑥\n𝑦2\n𝑦1\n𝑦\n𝑥\n𝑦2\n𝑦1\nThis observation can be extended with each new vertical segment we discover with the same pair of 𝑦-coordinates. If there was a fourth vertical\nsegment with the same pair of 𝑦-coordinates, then we would be able to create three new rectangles: one each with the three segments that\nprecede it. If there was a fifth vertical segment with the same pair of 𝑦-coordinates, we would be able to add four new rectangles, and so on. In\n𝑛thgeneral, encountering an vertical segment with a given pair of 𝑦-coordinates would allow you to create brand new rectangles.𝑛−1\nThis idea forms the crux of our solution. To solve the problem, we will iterate over all the vertical segments that can be constructed using\nthe given points (this can be done using a nested loop over the points to identify pairs of points that share the same 𝑥-coordinate). For each\nvertical segment we encounter, we increment a counter by the number of times we have seen another vertical segment with the same pair of\n𝑦-coordinates. This is shown using an example below (all of the segments below share the same 𝑦-coordinates of and for simplicity, but𝑦1 𝑦2\nthis process also works when there are multiple vertical segments that may have different 𝑦-coordinates):\nNumber of rectangles: 0\n𝑦\n𝑥\n𝑦2\n𝑦1\nNumber of rectangles: 0 + 1 = 1\n𝑦\n𝑥\n𝑦2\n𝑦1\nNumber of rectangles: 1 + 2 = 3\n𝑦\n𝑥\n𝑦2\n𝑦1\nNumber of rectangles: 3 + 3 = 6\n𝑦\n𝑥\n𝑦2\n𝑦1", "word_count": 395, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "90375fe3-3f44-5514-8f81-75b80c70e2a5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1003, "real_page_number": null, "text": "26.2 Sweep Line Algorithm\n991\nstd::unordered_map<>We can keep track of vertical segments with the same 𝑦-coordinates by using an associative container like a\nstd::map<>or to map each distinct pair of 𝑦-coordinates to the number of vertical line segments that share those coordinates. To avoid\nstd::map<>having to hash a custom object to keep things simple, we will implement our solution using a of pairs, where each pair stores\nthe 𝑦-coordinates of a vertical segment that can be formed using the given points (to avoid double counting, we will ensure that the smaller\n𝑦-coordinate will always be first in the pair). An implementation of this solution is shown below:\n1\nint32_t count_rectangles(const std::vector<Point>& points) {\n2\n// maps each pair of y-coordinates to the number of distinct\n3\n// vertical segments that have those coordinates\n4\nstd::map<std::pair<int32_t, int32_t>, int32_t> segments;\n5\n6\nint num_rectangles = 0;\n7\nfor (const Point& p1 : points) {\n8\nfor (const Point& p2 : points) {\n9\nif (p1.x == p2.x && p1.y < p2.y) {\n10\nstd::pair<int32_t, int32_t> vertical_segment{p1.y, p2.y};\n11\n// add the number of times this pair of y-coordinates was seen before\n12\n// to the count, then increment the value in the map\n13\nnum_rectangles += segments[vertical_segment]++;\n14\n} // if\n15\n} // for p2\n16\n} // for p1\n17\n18\nreturn num_rectangles;\n19\n} // count_rectangles()\nΘ(𝑛2The time complexity of this solution is for the number of points 𝑛, since we are performing a map lookup within a nestedlog(𝑛)) Θ(log(𝑛))\nΘ(𝑛2) Θ(𝑛2)loop (although it is possible to reduce this time complexity to if we use a hash table instead, but this would require us to implement\nΘ(𝑛2)a custom hashing function). The auxiliary space used by this solution is for the number of pairs we may have to insert into our map.\nIt should be noted that we could have counted the number of horizontal segments with the same 𝑥-coordinates to arrive at the same solution.\nBoth methods accomplish the same task of identifying all the rectangles that can be formed, so we only needed implement one of the two. We\nwill look at similar approaches toward solving geometry problems in the next section using a strategy known as the sweep line algorithm.\n26.2\nSweep Line Algorithm (✽)\n¸ 26.2.1\n(✽)Applications of the Sweep Line Algorithm\nOne of the most important algorithms in computational geometry is the sweep line algorithm. As its name implies, the sweep line algorithm\nsolves a geometric problem by sweeping an imaginary line across a plane, stopping at certain points (or \"events\") to perform necessary\ncomputations for the given problem. Once the line has swept through all the objects on the plane, the solution to the problem is obtained. The\nsweep line algorithm can be used to design efficient implementations for many important computational geometry problems. Just as an example,\ndiagrams.2one important application of the sweep line algorithm comes from the construction of Voronoi Given a set of 𝑛points on a plane, a\npartitions the plane into 𝑛convex polygons (often called \"cells\") that each represent regions of the plane that are closer to oneVoronoi diagram\nof the given points than all others. An example Voronoi diagram is shown below on the right, for the given set of points on the left.\nVoronoi diagrams are useful for a variety of fields, from biology to urban planning. For instance, a Voronoi diagram can be used to devise an\nalgorithm for diverting airplanes to the closest airport in the case of an emergency. One of the most efficient algorithms for building a Voronoi\ndiagram, algorithm, uses the sweep line technique to accomplish this task in worst-case time, given a set of 𝑛points.Θ(𝑛log(𝑛))Fortune’s\nThe Voronoi diagram is just one example of the sweep line algorithm’s utility in computational geometry. We will not be discussing Voronoi\ndiagrams and Fortune’s algorithm any further in this chapter, since the knowledge required to formulate this algorithm is beyond the scope of\nthe class. However, we will still look at other applications of the sweep line algorithm in this section, and how it can be used to solve simpler\ncomputational geometry problems.\n2Youarenotresponsibleforknowingwhatthisisfortheclass;thisisjustoneexampleofanapplicationofthesweeplinealgorithm.", "word_count": 724, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "80a61ed3-7b1b-549e-b5f1-cab3cc931757", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1004, "real_page_number": null, "text": "992\nChapter 26. Computational Geometry\n¸ 26.2.2\n(✽)Solving Problems Using the Sweep Line Algorithm\nExample 26.5 You are given a collection of 𝑛line segments, each represented using the coordinates of its two endpoints. You may make\nthe following assumptions about this collection of segments:\n• No line segment is vertical.\n• Any two segments intersect in at most one point.\n• No three segments intersect at the exact same point.\nDevise an algorithm that returns the coordinates of all intersection points within this collection of segments.\nA naïve approach would be to consider all pairs of line segments and compute if an intersection exists between each pair. However, this would\nΘ(𝑛2) Θ(𝑛2)require us to compare pairs of segments, and our algorithm would always run in time. Instead, we can do better if we utilize a\nsweep line approach. To illustrate how this works, consider the following example:\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nTo start our algorithm, we will create a sweep line at the left edge of our plane. This line will then sweep rightwards, stopping only when it\nexperiences the following events:\n1. The sweep line encounters a brand new segment (by hitting a left endpoint).\n2. The sweep line encounters the end of an active segment (by hitting a right endpoint).\n3. The sweep line encounters the intersection of two segments.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nA key observation to make here is that two segments can only ever intersect if they are adjacent to each other at the same horizontal position. For\nexample, line segments 𝐶and 𝐷cannot physically intersect at the following position of the sweep line, since they are not adjacent (lines 𝐵and\n𝐸are situated in between them). As a result, there is no need to check if an intersection exists between 𝐶and 𝐷at this horizontal position of the\nsweep line. A check only needs to be made if segments 𝐵and 𝐸terminate before 𝐶and 𝐷, or if an intersection occurs that changes the relative\norder of the four segments, since these are the events that could cause 𝐶to be adjacent to 𝐷(and thus raise the possibility of intersection).\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nTo track this information, we will need a container that can store the relative ordering of the active segments at any point along our algorithm, so\nstd::set<>)that we can easily check if any two segments are adjacent. This can be done using a self-balancing binary search tree (in C++, a\nthat stores the active segments at any point in time, ordered by the 𝑦-coordinate at which the segment encountered the sweep line. For example,\nat the position of the sweep line above, the segments in the container should be ordered 𝐷, 𝐵, 𝐸, and 𝐶(since this is the vertical order of the\nsegments at this position).\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐷\n𝐵\n𝐸\n𝐶\n(largest 𝑦-position)\n(smallest 𝑦-position)\nLet’s look at the sweep line in action. We will sweep the line rightward, stopping only if any of the three events occurs (left endpoint, right\nendpoint, or intersection). At each event, we then check if the segment(s) adjacent to the event generate an intersection. In our example, the first\nevent we encounter is the left endpoint of segment 𝐴. Since no other active segments exist at this position, 𝐴is inserted into our active segment\ncontainer, and no intersection checks need to be made.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐴\nDiscovered Intersections", "word_count": 595, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8fa7e57a-9806-5543-9599-d7f94669ab0f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1005, "real_page_number": null, "text": "26.2 Sweep Line Algorithm\n993\nThe next event we encounter is the left endpoint of segment 𝐵, which we add to the container of active segments — since 𝐵’s left endpoint had\na higher 𝑦-position when it encountered the sweep line, it is ordered above 𝐴in the container. Since 𝐵is a new segment and it is adjacent to\nsegment 𝐴, we check if an intersection exists between segments 𝐴and 𝐵(using the process defined in example 26.2). In this case, 𝐴and 𝐵do\nnot intersect, so we continue with our sweep.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐵\n𝐴\nDiscovered Intersections\nThe next event we encounter is the right endpoint of segment 𝐴. This indicates that 𝐴has terminated, so we can remove 𝐴from our container\nof active segments. 𝐵is the only segment remaining in the container, so no intersection checks need to be made.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐵\nDiscovered Intersections\nThe next event we encounter is the left endpoint of segment 𝐶, which we add to our container of active segments. Segment 𝐶is adjacent to\nsegment 𝐵, so we check if an intersection exists between these two segments. In this case, 𝐵and intersect, so we add the coordinates of𝐶do\nthe intersection point to our solution.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐵\n𝐶\nDiscovered Intersections\n(𝐵,𝐶)\nThe next event we encounter is the left endpoint of segment 𝐷, which is added to our container of active segments. Segment 𝐷is adjacent to\nsegment 𝐵, so we check if an intersection exists between these two segments. In this case, 𝐵and 𝐷do not intersect, so we continue our sweep.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐷\n𝐵\n𝐶\nDiscovered Intersections\n(𝐵,𝐶)\nThe next event we encounter is the left endpoint of segment 𝐸, which is added to our container of active segments. Segment 𝐸is adjacent to\nboth 𝐵and 𝐶, so we check if intersections exists between 𝐸and both 𝐵and 𝐶. 𝐸and 𝐶do not intersect, but 𝐸and 𝐵do; we therefore add the\ncoordinates of this intersection point to our solution.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐷\n𝐵\n𝐸\n𝐶\nDiscovered Intersections\n(𝐵,𝐶)\n(𝐵,𝐸)\nThe next event we encounter is the intersection of segments 𝐵and 𝐸. At this point, notice that segments 𝐵and 𝐸switch their vertical positions,\nso the relative order of these two segments must also be changed in our container of active segments. This swap also creates new adjacent\nsegments: 𝐸is now adjacent to 𝐷, and 𝐵is now adjacent to 𝐶. Thus, we will need to check if an intersection exists between these pairs of\nsegments. The intersection between 𝐵and 𝐶has already been discovered, but the intersection between 𝐷and 𝐸is brand new. We therefore add\nthe coordinates of this new intersection point to our solution.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐷\nE\nB\n𝐶\nDiscovered Intersections\n(𝐵,𝐶)\n(𝐵,𝐸)\n(𝐷,𝐸)", "word_count": 500, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "98b5149c-66f4-5527-bdaf-d540d0c01c35", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1006, "real_page_number": null, "text": "994\nChapter 26. Computational Geometry\nThe next event we encounter is the intersection of segments 𝐵and 𝐶. These two segments switch positions in our active segments container,\nand any new adjacent pairs of segments are checked for intersection. In this case, 𝐶is now adjacent to 𝐸, but we already discovered before that\n𝐶and 𝐸do not intersect.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐷\n𝐸\nC\nB\nDiscovered Intersections\n(𝐵,𝐶)\n(𝐵,𝐸)\n(𝐷,𝐸)\nThe next event we encounter is the intersection of segments 𝐷and 𝐸. These two segments switch positions in our container, and the new\nadjacent segment pair of 𝐶and 𝐷is checked for intersection. 𝐶and 𝐷do not intersect, so we continue with our sweep.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\nE\nD\n𝐶\n𝐵\nDiscovered Intersections\n(𝐵,𝐶)\n(𝐵,𝐸)\n(𝐷,𝐸)\nThe next event we encounter is the right endpoint of segment 𝐵. This indicates that 𝐵has terminated, so we remove 𝐵from our container of\nactive segments. We then check for intersection between any new pairs of adjacent segments. Since 𝐵was the bottom-most segment when it\nterminated, there are no new adjacent pairs, and no additional check is needed at this event.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐸\n𝐷\n𝐶\nDiscovered Intersections\n(𝐵,𝐶)\n(𝐵,𝐸)\n(𝐷,𝐸)\nThe next event we encounter is the right endpoint of segment 𝐷. This indicates that 𝐷has terminated, so we remove 𝐷from our container of\nactive segments. After 𝐷’s removal, 𝐶and 𝐸are now adjacent, so we check if an intersection exists between these two segments. These two\nsegments do not intersect, so we continue with our sweep.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\n𝐸\n𝐶\nDiscovered Intersections\n(𝐵,𝐶)\n(𝐵,𝐸)\n(𝐷,𝐸)\nIf we follow this process for all of the remaining events, we will eventually discover every intersection point once our sweep line reaches the\nright endpoint of the final segment, and thus the solution to our original problem.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nActive Segments\nDiscovered Intersections\n(𝐵,𝐶)\n(𝐵,𝐸)\n(𝐷,𝐸)\n(𝐹,𝐺)\n(𝐹,𝐻)\n(𝐻,𝐼)\nAt this point, you may be wondering: how do we instantiate a sweep line and move it across the plane? It turns out that we don’t have to create a\nphysical sweep line at all — the line we used in the example was just a visualization tool for understanding how the algorithm works. When it\ncomes to implementing a solution, we do not need to keep track of a line at all; we only need to keep track of the that we need to visit.events\nThis can be done using a min-priority queue of events, where the priority of an event is determined by its horizontal position (i.e., 𝑥-\ncoordinate). At the beginning of the algorithm, the priority queue is initialized with the endpoints of all the provided segments. During the\nalgorithm, we also insert any new intersection points that are discovered. By using this priority queue approach, we can easily determine the next\nevent to visit by popping off the element at the top of our priority queue, essentially emulating the process of sweeping a line across the plane.", "word_count": 531, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0656eedc-ca9a-59a8-913a-c6cae390ac6b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1007, "real_page_number": null, "text": "26.2 Sweep Line Algorithm\n995\nTo summarize, our algorithm to find all segment intersections is as follows:\n1. Create a min-priority queue of events based on the 𝑥-coordinate of the event, and initialize it with the endpoints of all the given segments.\nThis priority queue will allow us to visit all the events from left to right, akin to sweeping a line.\n(std::set<>2. Initialize a self-balancing binary search tree in C++) that stores the active segments at the current position of the sweep\nline, ordered by 𝑦-position.\n3. While there are still events to visit, pop the next event off the priority queue:\n• If the event is a segment’s left endpoint:\n– Add the segment to the balanced binary search tree of active segments.\n– Check segments adjacent to the new segment on the sweep line for intersection.\n– If a new intersection point is discovered, add it to the solution and push it into the event priority queue.\n• If the event is a segment’s right endpoint:\n– Remove the segment from the active segments container.\n– If two other segments become adjacent after the removal, check those two segments for intersection.\n– If a new intersection point is discovered, add it to the solution and push it into the event priority queue.\n• If the event is an intersection point:\n– Change the order of the two intersecting segments in the active segment container (this can be done by updating their\n𝑦-coordinates so that their orderings are switched).\n– If the switch causes two segments to become newly adjacent, check those segments for intersection.\n– If a new intersection point is discovered, add it to the solution and push it into the event priority queue.\n4. Once the event priority queue is empty and all segments have been processed, the solution is now complete with all segment intersections.\nWhat is the worst-case time complexity of this algorithm? At every event, we complete the following steps successively:\n1. We pop from the priority queue (which takes worst-case time).Θ(log(𝑛))\n2. We may have to check for segment intersection (which takes time using the strategy covered in example 26.2).Θ(1)\n3. We push and pop from the active segment binary search tree (which takes worst-case time).Θ(log(𝑛))\nThus, the total work we need to do for each event is bounded by Θ(log(𝑛)), which is the highest-order term. Since an event can either occur\nat the endpoints of a segment or an intersection point between two segments, the total number of events we encounter is 2𝑛+𝑘if there are 𝑛\nsegments and 𝑘intersections among these segments. Because there are a total of events, and each event can take up to time,Θ(𝑛+𝑘) Θ(log(𝑛))\nthe worst-case time complexity of the overall algorithm is Θ((𝑛+𝑘)log(𝑛)).\nThe algorithm we discussed is formally known as the algorithm. Although we made some simplifying assumptions for ourBentley-Ottman\nexample, there are additional modifications we can make to support line segments that do not adhere to our initial restrictions. For instance, we\ncantiebreaktwoeventswiththesame𝑥-coordinateusingtheir𝑦-coordinates—thisallowsustohandlemultipleeventsatthesame𝑥-coordinate,\nwhich in turn allows the algorithm to handle vertical line segments.\nExample 26.6 You are given 𝑛rectangles whose edges are parallel to the 𝑥- and 𝑦-axes. Devise an algorithm that can be used to find the\ntotal area covered by their union. If more than two rectangles cover the exact same area, that area should not be counted more than once.\nThis is another problem that can be efficiently solved using a sweep line approach. Consider the following input:\nOne important detail to notice is that any area covered by a union is bounded above and below by the 𝑦-coordinate of one of the given rectangles’\nhorizontal edges. Because of this, we can partition our graph into several intervals using the 𝑦-coordinates of our rectangles, as shown:\nSimilar to the previous example, we start our sweep line at the left end of our plane and sweep it rightwards across the rectangles. For each of\nthe intervals, we keep track of a counter that indicates how many rectangles exist in that interval at the current position of the sweep line. Every\ntime the sweep line crosses a left edge of a rectangle, the intervals that are spanned by that rectangle have their counter(s) incremented; every\ntime the sweep line crosses a right edge, the corresponding counter(s) are decremented. By following this rule, intervals that have a positive\ncounter value at the position of the sweep line must be a part of a rectangle union.", "word_count": 771, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ea9a5176-3869-5f06-ab1d-f68a19fb41b8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1008, "real_page_number": null, "text": "996\nChapter 26. Computational Geometry\nFor example, these are the values of the counters when the sweep line is at the following position:\n0\n1\n2\n3\n2\n2\n2\n1\n0\nOne potential strategy for implementing this solution is as follows:\n1. Initialize a running value that stores the area of the rectangle union encountered by the sweep line so far.\n2. Sorttheleftandrightedgesofeachrectangleinascendingorderof𝑥-coordinate(oruseapriorityqueue, similartothepreviousexample).\nEach of these edges represents an event that our sweep line needs to stop at, and sorting them allows the algorithm to easily determine\nwhich event to stop at next.\n3. \"Sweep\" the line through the plane by visiting each of the events one-by-one. At each event:\n• Iterate over the counters to determine the length of the sweep line that currently resides in a rectangle union (i.e., the combined\n≥1).length of intervals whose counter\n• Multiply this length with the horizontal distance to the previous event (this gets us the total area that the sweep line visited between\nthe previous and current events).\n• Add this result to the running total, and increment or decrement any counters as necessary.\n4. After the sweep line reaches the rightmost edge among the given rectangles, the value of the running total is equal to the total area\noccupied by the rectangle union.\nRemark: One data structure that is helpful for solving this problem is the tree, which is essentially a balanced binary tree that storessegment\ninformation about intervals, allowing you to efficiently query information for a given of values. A segment tree would make it possiblerange\nto query/update the counters and identify the length of the sweep line inside the rectangle union in time, where 𝑖is the total numberΘ(log(𝑖))\nof intervals. Since the number of intervals 𝑖is on the order of the number of rectangles 𝑛, and the number of events is also on the order of 𝑛,\nthe overall worst-case time complexity of finding the area of a rectangle union using a segment tree is equal to the number of events the𝑛×\ntime required for each event log(𝑖), or Θ(𝑛log(𝑛)). We will discuss segment trees in greater detail in a later section.Θ(𝑛×log(𝑖))=\n26.3\nThe Closest Pair of Points Problem\n¸ 26.3.1\nThe Closest Pair of Points Problem in One Dimension\nGiven a set of 𝑛points on a two-dimensional plane, how can you efficiently find the two points that are separated by the smallest distance? A\nnaïve brute force algorithm would calculate the distance between all pairs of points and return the pair whose distance is minimal. However, the\nΘ(𝑛2) Θ(𝑛2)since there are pairs on the graph. Can we do better?time complexity of this approach would be\nΘ(𝑛2)To solve this problem in better than time, we can use a divide-and-conquer approach. However, the idea behind this improved strategy\nmay not be immediately clear at first. To understand how we can utilize divide-and-conquer to solve this problem, it is helpful to consider the\nsame version of this problem, but on a one-dimensional line rather than a two-dimensional plane. If we are given a set of 𝑛points on a line, we\ncan find the closest pair of points by following these steps:\n1. Presort all the points in order by their coordinate position.\n2. Find the median of all points and partition the points into two halves based on this median.\n3. Recursively find the closest pair of points to the left of the median.\n4. Recursively find the closest pair of points to the right of the median.\n5. Find the closest pair of points that either cross the median (i.e., the rightmost point to the left of the median and the leftmost point to the\nright of the median) or include the median point (if there is one).\nAfter we recursively compute the closest pair of points (1) to the left of the median, (2) to the right of the median, and (3) among two points that\ncross the median, the closest pair among these three would also be the closest pair overall. (Notice that this is the same strategy we used to solve\nthe maximum subarray problem using divide-and-conquer in chapter 21.)\n𝑑𝐿, 𝑑𝑅, 𝑑𝑀.The closest pair of points is the pair corresponding to the minimum of and\n⏟⏟⏟\n𝑑𝐿\n⏟⏟⏟\n𝑑𝑅\n⏟⏞⏞⏞⏞⏟⏞⏞⏞⏞⏟\n𝑑𝑀", "word_count": 741, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b5b72c00-0b42-5e45-b8d4-920451918450", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1009, "real_page_number": null, "text": "26.3 The Closest Pair of Points Problem\n997\nTo implement this one-dimensional problem, we will need to first sort the points so that we can find the median; given 𝑛points, this step takes\ntime if we use a standard comparison-based sorting algorithm. Then, we use divide-and-conquer to recursively split the input in halfΘ(𝑛log(𝑛))\nand compute the closest pair among the three options listed previously; finding this closest pair takes time if the points are sorted. We canΘ(𝑛)\nthereby express the divide-and-conquer steps using the following recurrence:\n𝑇(𝑛)=\n{\nΘ(1),\n𝑛≤2if\n2𝑇(𝑛∕2)+Θ(𝑛),\nif 𝑛>2\nApplying the Master Theorem, we would see that the divide-and-conquer portion of the algorithm takes time. If we add this to theΘ(𝑛log(𝑛))\nsorting step, we can conclude that the overall time complexity of the algorithm is also Θ(𝑛log(𝑛)). It is this approach that we willΘ(𝑛log(𝑛))\ngeneralize to solve our original two-dimensional problem.\n¸ 26.3.2\nThe Closest Pair of Points Problem in Two Dimensions\nIn the one-dimensional case, we were able to divide our points in half using the median point. If we move up to two-dimensions, we can achieve\nsimilar behavior by splitting the points in half using a median line. This is done by sorting all the given points in order by their 𝑥-coordinate,\nand finding the vertical line that lies on the median of these 𝑥-coordinates:\nWe then follow the same procedure as before: we recursively find the closest pair in the left half, the closest pair in the right half, and the closest\npair that either crosses the median line or involves a point on the median line. Finding the closest pair to the left and right of the median line is\nrather straightforward, but finding the closest pair that crosses the median line is trickier. This is because the pair closest to the median line in\nthe 𝑥direction may be far away in the 𝑦direction (as shown in the example).\nThe two connected points above\nare the closest to the median line.\nHowever, they are not the closest\npair that crosses the median line!\nBecause of this, we need to consider pairs that are close enough to the median line, rather than just the two points that are closest to theall\nmedian line. But how do we define \"close enough\"? It turns out we can use the closest pair to the left and right of the median line to help us\ndetermine how far we need to look around the median line. Let us define 𝑑𝐿as the distance of the closest pair to the left of the median line, and\n𝑑𝑅as the distance of the closest pair to the right of the median line, as shown.\n𝑑𝐿\n𝑑𝑅\nLet us define the better of these two values as 𝛿; that is, min(𝑑𝐿,𝑑𝑅). When looking for pairs that cross the median line, we can ignore any𝛿=\npoint that is further than 𝛿away from the median line. Why? If a point is more than 𝛿away, the distance required to connect it to a point on the\nother side of the median line must be worse than either 𝑑𝐿or 𝑑𝑅, and thus cannot be optimal! This is represented by the shaded region below,\nwhich we will refer to as the 𝛿-strip.\n𝛿\n𝛿", "word_count": 546, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6a04b808-e81c-593a-b6ea-f68321aa51ae", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1010, "real_page_number": null, "text": "998\nChapter 26. Computational Geometry\nHowshouldweexaminethe𝛿-strip? Anaïveapproachwouldcomputethedistancebetweenallpairsofpointsthatcrossorinvolveapointonthe\nmedian line. However, in the worst case, every single point could reside in the 𝛿-strip! In such a scenario, we would have to compare all pairs of\nΘ(𝑛2)points in the plane. As discussed earlier, comparing all pairs of points takes time, so this leaves us with the following divide-and-conquer\nrecurrence in this worst-case scenario:\n𝑇(𝑛)=\n{\nΘ(1),\n𝑛≤2if\n2𝑇(𝑛∕2)+Θ(𝑛2),\nif 𝑛>2\nΘ(𝑛2),ApplyingtheMasterTheoremtothisrecurrencerelationgivesus sotheworst-casetimecomplexityofour\"improved\"divide-and-conquer\nΘ(𝑛2)approach is no better than the time complexity of brute force. If we want to achieve a better time complexity in the worst case, we will\nneed to find a more efficient way to examine points near the median line.\nAlthough it is not obvious, there exists a way to find the closest pair that crosses the median line in time, regardless of how the pointsΘ(𝑛)\nare positioned. Let us consider an arbitrary point 𝑝in the 𝛿-strip, located at coordinates (𝑥𝑝,𝑦𝑝). For the closest pair of points to cross the\nmedian line, the distance between the points must be at most 𝛿, since 𝛿is the best distance we know so far from computing the closest pair\namong points to the left and to the right of the median line.\nWhat is the maximum number of points located point 𝑝(i.e., with a 𝑦-coordinate less than 𝑦𝑝) that can be within a distance of 𝛿awaybelow\nfrom 𝑝? Notice that, in order for a point to be below 𝑝but still be within 𝛿away from 𝑝, its 𝑦-coordinate must reside in the range [𝑦𝑝−𝛿,𝑦𝑝].\n𝛿\n𝛿\n𝛿\n(𝑥𝑝,𝑦𝑝)\n𝑦𝑝−𝛿\nUsing this information, we can thereby construct a 𝛿×2𝛿rectangle situated below point 𝑝, bounded by the left and right boundaries of the\n𝛿-strip. This rectangle is shown with a dark gray shading below:\n𝛿\n𝛿\n𝛿\n(𝑥𝑝,𝑦𝑝)\n𝑦𝑝−𝛿\n𝑦𝑝\nFor another point below 𝑝to be within 𝛿away from the median line 𝛿away from point 𝑝itself, it be situated inside this 𝛿×2𝛿rectangle.and must\nIf a point 𝑞exists below this rectangle, then its distance from 𝑝must be farther than 𝛿— thus, there would be no reason to compute the distance\nbetween 𝑝and 𝑞, since the pair be the solution to the problem.(𝑝,𝑞) cannot\n𝛿\n𝛿\n𝛿\n(𝑥𝑝,𝑦𝑝)\n𝑦𝑝−𝛿\n𝑦𝑝\n(𝑥𝑞,𝑦𝑞)\n>𝛿", "word_count": 416, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "ab19151b-8104-5ddb-86c4-17e83c030bad", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1011, "real_page_number": null, "text": "26.4 The Clockwise Test and Triangle Area\n999\nAdditionally, we will make the claim that 𝛿×2𝛿rectangle, including point 𝑝. To see why this is the case,at most eight points can exist inside the\nwe will partition this rectangle into eight smaller squares with dimensions (𝛿∕2)×(𝛿∕2).\n𝛿\n𝛿\n𝛿\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝛿∕2\n𝛿∕2\nOnly point can feasibly exist in each of these eight squares. Why? Recall that we defined 𝛿as the minimum of 𝑑𝐿and 𝑑𝑅, whichat most one\nrespectively represent the closest pairs to the left and right of the median line. This means that no pair entirely contained on one side of the\nmedianlinecanhaveadistancelessthan𝛿—otherwise,𝛿wouldnotbethedistanceoftheclosestknownpair,contradictingourinitialdefinition.\nHowever, there is no way to fit two points in a single square without their distance being less than 𝛿. Thus, we can conclude that it(𝛿∕2)×(𝛿∕2)\nis impossible to fit more than one point in each of these eight squares, and that our original 𝛿×2𝛿rectangle can only fit at most eight points.\nIn the worst case, a point exists directly on the median line, requiring us to compute its distance to potentially seven other points in the\nrectangle.3 Thus, if we want to find the closest pair of points that cross the median line, we only need to compute the distance from each point in\nour original 𝛿-strip to at most seven other points; since seven is a constant, the computation for each point takes time. Because there can beΘ(1)\nat most 𝑛points in the 𝛿-strip, the total time complexity of finding the closest pair that crosses the median line is now Θ(𝑛), provided𝑛×Θ(1)=\nthat the points in the strip are sorted beforehand.\nIn summary, we can find the closest pair of points among a two-dimensional point set using the following algorithm:\n1. Presort the set of points by their 𝑥-coordinate (so that finding the median line takes constant time) their 𝑦-coordinate (so that we canand\neasily iterate over the 𝛿-strip to find the closest pair that crosses the median line for each recursive call). Given 𝑛points, this step takes\ntime.Θ(𝑛log(𝑛))\n2. Compute the median line that splits the points in half by 𝑥-coordinate (this takes constant time if the points are presorted by 𝑥-coordinate\nbeforehand).\n3. Recursively find 𝑑𝐿and 𝑑𝑅, the closest pairs of points to the left and right of the median line. Define 𝛿= min(𝑑𝐿, 𝑑𝑅).\n4. Iterate over all points in the 𝛿-strip from top to bottom (this can be done efficiently if the points are presorted by 𝑦-coordinate beforehand).\nFor each point 𝑝in the 𝛿-strip, compute its distance to the points within the 𝛿×2𝛿rectangle situated directly below 𝑝. The best distance\nencountered is stored as 𝑑𝑀, which is the closest pair of points that either crosses the median line or involves a point directly on the\nmedian line. This step takes time.Θ(𝑛)\n5. The closest pair of points is the minimum of 𝑑𝐿, 𝑑𝑅, and 𝑑𝑀.\nPresorting the points in step 1 takes time. Steps 2-5 represent a divide-and-conquer algorithm that can be expressed using theΘ(𝑛log(𝑛))\nfollowing recurrence relation:\n𝑇(𝑛)=\n{\nΘ(1),\n𝑛≤2if\n2𝑇(𝑛∕2)+Θ(𝑛),\nif 𝑛>2\nApplying the Master Theorem to this recurrence relation gives us Θ(𝑛log(𝑛)). We have therefore devised an algorithm that solves the two-\nΘ(𝑛2)dimensional closest pair of points problem in time, which is an improvement over the time of the brute force approach.Θ(𝑛log(𝑛))\n(Although we will not be discussing it in detail here, the divide-and-conquer pattern we used to solve the problem can be generalized to higher\ndimensions as well, all while retaining its time complexity.)Θ(𝑛log(𝑛))\n26.4\nThe Clockwise Test and Triangle Area\nSuppose you are given three non-collinear points, 𝑃1, 𝑃2, and on a two-dimensional plane. How can you determine if these three points are𝑃3\noriented in a or direction?clockwise counterclockwise\n𝑃1\n𝑃2\n𝑃3\nclockwise\n𝑃1\n𝑃3\n𝑃2\ncounterclockwise\n3Sevenisactuallynotexactinthiscase. However,forouranalysis,thisdoesnotmatter,sincetheexactupperboundisstillaconstant.", "word_count": 695, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c7c8dbff-6bfa-58d4-8282-48423bddec8c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1012, "real_page_number": null, "text": "1000\nChapter 26. Computational Geometry\nTo solve this problem, we will have to rely on a bit of linear algebra, specifically the concept of determinants. A determinant is a scalar value\n|𝑀|that is a function of the contents of a square matrix. The determinant of a 2 2 matrix 𝑀is equal to:×\n|𝑀|=\n|||||\n𝑎\n𝑏\n𝑐\n𝑑\n|||||\n𝑎𝑑−𝑏𝑐=\n|𝑀|For a 3 3 matrix 𝑀, the determinant is equal to:×\n|𝑀|=\n|||||||\n𝑎\n𝑏\n𝑐\n𝑑\n𝑒\n𝑓\n𝑔\nℎ\n𝑖\n|||||||\n𝑎=\n|||||\n𝑒\n𝑓\nℎ\n𝑖\n|||||\n−𝑏\n|||||\n𝑑\n𝑓\n𝑔\n𝑖\n|||||\n+𝑐\n|||||\n𝑑\n𝑒\n𝑔\nℎ\n|||||\n𝑎𝑒𝑖+𝑏𝑓𝑔+𝑐𝑑ℎ−𝑐𝑒𝑔−𝑏𝑑𝑖−𝑎𝑓ℎ=\nSo, why are we talking about determinants? It turns out there is a formula involving determinants that can be used to compute the area of a\ntriangle given three points:\n1Area of Triangle =\n2\n|||||||\n𝑥1\n𝑦1\n1\n𝑥2\n𝑦2\n1\n𝑥3\n𝑦3\n1\n|||||||\n1=\n2\n(𝑥1𝑦2−𝑦1𝑥2+𝑦1𝑥3−𝑥1𝑦3+𝑥2𝑦3−𝑥3𝑦2\n) 1=\n2\n((𝑥2 −𝑦1))−𝑥1)(𝑦3−𝑦1)−(𝑥3−𝑥1)(𝑦2\nHowever, this is not all: the sign of the result also determines that orientation of the points in the triangle! If the result of the computation\nis negative, then the points are in a clockwise orientation; if the result is positive, then the points are in a𝑃1 →𝑃2 →𝑃3 𝑃1 →𝑃2 →𝑃3\ncounterclockwise orientation. If the result is zero, then the three points are collinear. The directional orientation of a set of vertices defining a\nbelow.4polygon is sometimes also known as its order. An implementation of this equation is shown in the functionwinding\n1\n// returns a pair containing the area of triangle formed by the three points,\n2\n// as well as a bool that indicates whether the points are in a clockwise\n3\n// or counterclockwise orientation (true = counterclockwise, false = clockwise)\n4\nstd::pair<double, bool> area_of_triangle(Point a, Point b, Point c) {\n5\ndouble area = 0.5 (b.x - a.x) (c.y - a.y) - (c.x - a.x) (b.y - a.y);* * *\n6\nif (area > 0) {\n7\nreturn true};{area,\n8\n} // if\n9\nelse if (area < 0) {\n10\nreturn false};{std::fabs(area),\n11\n} // else if\n12\nelse {\n13\nthrow std::invalid_argument(\"The given points are collinear.\");\n14\n} // else\n15\n} // area_of_triangle()\nExample 26.7 You are given three points: (0,1.5), (1,0), and (−1,0.6). Are these three points oriented in𝑃1 𝑃2 𝑃3 𝑃1 →𝑃2 →𝑃3= = =\na clockwise or counterclockwise order? Also, what is the area of the triangle formed by these three points?\n𝑃1\n𝑃2\n𝑃3\nTo solve this problem, we can use the formula for the area of a triangle defined above:\n1Area=\n2\n((𝑥2 −𝑦1))−𝑥1)(𝑦3−𝑦1)−(𝑥3−𝑥1)(𝑦2\n1=\n((1−0)(0.6−1.5)−(−1−0)(0−1.5))2\n1=\n((1−0)(0.6−1.5)−(−1−0)(0−1.5))2\n1=\n((1×−0.9)−(−1×−1.5))2\n1=\n(−0.9−1.5)2\n1=\n(−2.4)2\n−1.2=\nThe area of the triangle formed by the given three points is therefore 1.2. Since the result of the calculation was negative, the points must have a\nclockwise orientation.\n4Ifyouneedtoimplementthisformulainaprogram,solvingfor ispreferredoversolvingfor(𝑥2−𝑥1)(𝑦3−𝑦1)−(𝑥3−𝑥1)(𝑦2−𝑦1) 𝑥1𝑦2−𝑦1𝑥2+𝑦1𝑥3−𝑥1𝑦3+\nsinceitinvolvesfewermultiplicationsandadditions.𝑥2𝑦3−𝑥3𝑦2", "word_count": 556, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "234a10c3-34ac-5829-bb3d-35b4e62150d8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1013, "real_page_number": null, "text": "26.5 Area of a Polygon\n1001\n26.5\nArea of a Polygon\n¸ 26.5.1\nThe Shoelace Formula\nThe formula detailed in the previous section gives us the area of a triangle formed by three non-collinear points. However, what if we wanted to\npolygon?5find the area of any simple If the shape we are given is simple enough, such as a square, rectangle, trapezoid, etc., then we have\nformulas that can be used to solve for the area. In certain cases though, we could be given a polygon that is irregular and does not have a specific\nformula designed to find its area. For example, consider the following polygon:\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\nPoint\nCoordinate\n𝐴\n(0.8,1.2)\n𝐵\n(1,2.3)\n𝐶\n(2,2.6)\n𝐷\n(2.7,1.6)\n𝐸\n(1.8,0.8)\nHow can we find the area of this polygon? One intuitive strategy is to break up the polygon into separate triangles, apply the formula in the\nprevious section to find each of their areas, and then combine the areas to get the area of the overall polygon.\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\nPoint\nCoordinate\n𝐴\n(0.8,1.2)\n𝐵\n(1,2.3)\n𝐶\n(2,2.6)\n𝐷\n(2.7,1.6)\n𝐸\n(1.8,0.8)\nHowever, finding a way to break a polygon into triangles is not always an easy task, especially as the given polygon becomes more complex.\nInstead, an easier method for finding polygon area relies on instead of triangles. To see why, take a look at what happens if we taketrapezoids\neach point of the polygon and draw a vertical line down to the 𝑥-axis.\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\nPoint\nCoordinate\n𝐴\n(0.8,1.2)\n𝐵\n(1,2.3)\n𝐶\n(2,2.6)\n𝐷\n(2.7,1.6)\n𝐸\n(1.8,0.8)\nNotice that this deconstructs our polygon into a set of trapezoids whose slanted edges are formed from the polygon’s edges.\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\nThe area of our polygon would be the total area of the darker trapezoids, minus the total area of the lighter trapezoids.\n5A isapolygonthatdoesnotintersectitselforhaveanyholes. Inthissection,wewillonlybedealingwithsimplepolygons,andwewillassumesimplepolygon\nthatanypointswearegiventracetheedgesofthepolygoninavalidorder.", "word_count": 431, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9cadb507-0e76-58d6-84b6-83b33b077262", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1014, "real_page_number": null, "text": "1002\nChapter 26. Computational Geometry\nMuch like triangles, trapezoids are easy to work with, as we can use a simple formula to find the area of any trapezoid:\n𝑤\nℎ1\nℎ2\n𝑤(ℎ1+ℎ2)Area =\n2\nIn our case, we are given the coordinate points of the trapezoid instead of the edge lengths. Here, we can use a redefined version of the formula\nthat uses the four coordinates of the trapezoid to compute its area:\n(𝑥0,0)\n(𝑥1,0)\n(𝑥0,𝑦0)\n(𝑥1,𝑦1)\n(𝑥1−𝑥0)(𝑦0+𝑦1)Area =\n2\n𝑥1𝑦0−𝑥0𝑦0+𝑥1𝑦1−𝑥0𝑦1=\n2\nEven though we have a formula that can be used to find the area of a trapezoid, we still need to determine which trapezoids should be added, and\nwhich trapezoids should be subtracted. Recall from our example that the first three (darker) trapezoids should have their areas added, while the\nlast two (lighter) trapezoids should have their areas subtracted.\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\nTo accomplish this task, let us look at what happens if we number the points of the polygon in a clockwise order. Note that it does not matter\nwhichpointgetsassignedvertex0, aslongastheremainingpointsarenumberedconsecutivelyinaclockwisemanner(youcanusetheclockwise\ntest from the previous section to determine if you are moving in a clockwise direction or not). For our example, we will assign vertex 𝐴to 0,\nvertex 𝐵to 1, vertex 𝐶to 2, vertex 𝐷to 3, and vertex 𝐸to 4:\n𝑦\n𝑥\n0\n1\n2\n3\n4\n1\n2\n1\n2\nTo determine if we need to add or subtract a trapezoid’s area, we need to pay attention to how the 𝑥-coordinate changes as we move around the\nshape in a clockwise order, from vertex 0 to vertex 1, from vertex 1 to vertex 2, and so on. Notice that, if the value of the 𝑥-coordinate increases\nas we move in a clockwise manner, then the trapezoids formed along the corresponding edges should have their areas added:\n𝑦\n𝑥\n0\n1\n1\n2\n𝑦\n𝑥\n1\n2\n1\n2\n𝑦\n𝑥\n2\n3\n1\n2", "word_count": 395, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a6f9f6f0-f4d0-565f-92a4-e253a5951757", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1015, "real_page_number": null, "text": "26.5 Area of a Polygon\n1003\nContrarily, if the value of the 𝑥-coordinate as we move in a clockwise manner, the trapezoid areas should be subtracted:decreases\n𝑦\n𝑥\n3\n4\n1\n2\n𝑦\n𝑥\n0\n1\n2\n4\nThese two situations do not need to be handled separately. Given 𝑛points that form a simple polygon, the sign of will always(𝑥(𝑖+1) 𝑛−𝑥𝑖)mod\nindicate whether the corresponding trapezoid area should be added or subtracted, as long as you are visiting the vertices of the polygon in\nformula:6clockwise order. Therefore, we can define the trapezoid area using the followingsigned\n(𝑥𝑖+1−𝑥𝑖)(𝑦𝑖+𝑦𝑖+1)Signed Area of Trapezoid (Clockwise) =\n2\n𝑥𝑖+1𝑦𝑖−𝑥𝑖𝑦𝑖+𝑥𝑖+1𝑦𝑖+1−𝑥𝑖𝑦𝑖+1=\n2\nThe area of the polygon would therefore be the sum of the signed areas of all the trapezoids:\nArea of Polygon (Clockwise) =\n𝑛−1\n∑\n𝑖=0\n(𝑥𝑖+1−𝑥𝑖)(𝑦𝑖+𝑦𝑖+1)\n2\n=\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖+1𝑦𝑖−𝑥𝑖𝑦𝑖+𝑥𝑖+1𝑦𝑖+1−𝑥𝑖𝑦𝑖+1\n2\nAdding these trapezoids together gives us an interesting result. For instance, consider a polygon with three vertices, numbered 0, 1, and 2.\nSumming up the trapezoid signed areas gives us the following:\n1Area of Polygon (Clockwise) =\n2(𝑥1𝑦0 −𝑥2𝑦0)−𝑥0𝑦0+𝑥1𝑦1−𝑥0𝑦1+𝑥2𝑦1−𝑥1𝑦1+𝑥2𝑦2−𝑥1𝑦2+𝑥0𝑦2−𝑥2𝑦2+𝑥0𝑦0\nNotice that several of these terms actually cancel each other out:\n1Area of Polygon (Clockwise) =\n2(𝑥1𝑦0 −𝑥2𝑦0)−𝑥0𝑦0+𝑥1𝑦1−𝑥0𝑦1+𝑥2𝑦1−𝑥1𝑦1+𝑥2𝑦2−𝑥1𝑦2+𝑥0𝑦2−𝑥2𝑦2+𝑥0𝑦0\nThis leaves us with the following:\n1Area of Polygon (Clockwise) =\n2(𝑥1𝑦0 −𝑥2𝑦0)−𝑥0𝑦1+𝑥2𝑦1−𝑥1𝑦2+𝑥0𝑦2\nIn fact, we can generalize this outcome to any number of points. Given a set of 𝑛points that form a simple polygon in clockwise order, the total\narea of the polygon can be expressed using the following abbreviated formula:\n1Area of Polygon (Clockwise) =\n2\n(\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖+1𝑦𝑖−\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖𝑦𝑖+1\n)\nIf the points are given in a counterclockwise order instead, we can use a similar approach to get a nearly identical formula (just with the signs\nflipped). If the 𝑥-coordinate value as we move in a manner, then the trapezoids formed along the edges should haveincreases counterclockwise\ntheir areas subtracted (the opposite of what happened when the points were in clockwise order):\n𝑦\n𝑥\n0\n1\n2\n1\n𝑦\n𝑥\n2\n1\n1\n2\n6The\"mod𝑛\"attheendof isimportant,sincewestillwanttoconsidervertex0asthevertexthatcomesaftervertex𝑛−1. Intheexample,if 4,thenwe𝑥𝑖+1 𝑖=\nwanttoconsider asequalto0(or5mod5),sincethatisthevertexthatfollows4. Thisadditional\"mod𝑛\"hasbeenremovedfromtheformulasinthis𝑖+1\nsectionjusttomaketheformattingcleaner,butitisstillimplicitlythereevenifitisnotexplicitlywrittenout.", "word_count": 461, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7859703e-5df3-5858-8273-84d7e519ff58", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1016, "real_page_number": null, "text": "1004\nChapter 26. Computational Geometry\nSimilarly, if the 𝑥-coordinate value as we move in a manner, the trapezoid areas should be added:decreases counterclockwise\n𝑦\n𝑥\n3\n2\n1\n2\n𝑦\n𝑥\n4\n3\n1\n2\n𝑦\n𝑥\n0\n4\n1\n2\nIn the counterclockwise case, the sign of determines if a trapezoid area should be added or subtracted. This allows us to define(𝑥𝑖−𝑥(𝑖+1) 𝑛)mod\nthe signed trapezoid area using the following formula. Notice that this is just the negated version of the clockwise signed area of a trapezoid!\n(𝑥𝑖−𝑥𝑖+1)(𝑦𝑖+1+𝑦𝑖)Signed Area of Trapezoid (Counterclockwise) =\n2\n𝑥𝑖𝑦𝑖+1−𝑥𝑖+1𝑦𝑖+1+𝑥𝑖𝑦𝑖−𝑥𝑖+1𝑦𝑖=\n2\nIf we sum up all the trapezoids using this formula and remove all terms that cancel each other out, we would get this formula for the area of a\npolygon whose points are listed in a counterclockwise order:\n1Area of Polygon (Counterclockwise) =\n2\n(\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖𝑦𝑖+1−\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖+1𝑦𝑖\n)\nFormally, this equation is known as the Shoelace formula, and it can be used to find the area of any simple polygon whose vertices are listed (in\nclockwise or counterclockwise order) as coordinates on a two-dimensional Cartesian plane. To invoke the Shoelace formula on a polygon, we\nhave to solve either or for all 𝑛vertices of the polygon. Since this arithmetic can be done in constant time,(𝑥𝑖+1𝑦𝑖−𝑥𝑖𝑦𝑖+1) (𝑥𝑖𝑦𝑖+1−𝑥𝑖+1𝑦𝑖)\nand it needs to be performed for each vertex, the total time complexity of the Shoelace formula is linear on the number of points that a polygon\nhas, or Θ(𝑛). While the equation for the Shoelace formula may seem complicated, you luckily do not have to memorize it since there is an\nintuitive way to remember how it works. This process will be covered using the following example.\nExample 26.8 Apply the Shoelace formula to find the area of the example polygon used in this section (reproduced below):\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n2\nPoint\nCoordinate\n𝐴\n(0.8,1.2)\n𝐵\n(1,2.3)\n𝐶\n(2,2.6)\n𝐷\n(2.7,1.6)\n𝐸\n(1.8,0.8)\nThe points in the polygon are provided in a clockwise order, so we can use the following formula to find its area:\n1Area of Polygon (Clockwise) =\n2\n(\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖+1𝑦𝑖−\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖𝑦𝑖+1\n)\nPlugging in the coordinates gives us the following:\n1Area=\n2((𝑥𝐵𝑦𝐴+𝑥𝐶𝑦𝐵+𝑥𝐷𝑦𝐶+𝑥𝐸𝑦𝐷+𝑥𝐴𝑦𝐸)−(𝑥𝐴𝑦𝐵+𝑥𝐵𝑦𝐶+𝑥𝐶𝑦𝐷+𝑥𝐷𝑦𝐸+𝑥𝐸𝑦𝐴))\n1=\n2((1×1.2+2×2.3+2.7×2.6+1.8×1.6+0.8×0.8)−(0.8×2.3+1×2.6+2×1.6+2.7×0.8+1.8×1.2))\n1=\n2((1.2+4.6+7.02+2.88+0.64)−(1.84+2.6+3.2+2.16+2.16))\n1=\n2(16.34−11.96)\n1=\n2(4.38)\n2.19=\nThe area of the polygon is therefore 2.19.", "word_count": 412, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "824a8adb-791a-5a0f-a4a6-db3caae14191", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1017, "real_page_number": null, "text": "26.5 Area of a Polygon\n1005\n¸ 26.5.2\nVisualizing the Shoelace Formula\nFrom the example, we can actually see how the Shoelace formula got its name. Instead of writing out a giant formula, let’s list out each of the\nvertices in a single column, repeating the first point at the end.\n𝐴\n(0.8\n1.2)\n𝐵\n(1\n2.3)\n𝐶\n(2\n2.6)\n𝐷\n(2.7\n1.6)\n𝐸\n(1.8\n0.8)\n𝐴\n(0.8\n1.2)\nIf we draw a diagonal from each 𝑥-coordinate to the 𝑦-coordinate of the vertex directly below it, multiply the numbers that are connected, and\n∑𝑛−1sum all of these products, we would get the value of\n𝑥𝑖𝑦𝑖+1:𝑖=0\n𝐴\n(0.8\n1.2)\n𝐵\n(1\n2.3)\n𝐶\n(2\n2.6)\n𝐷\n(2.7\n1.6)\n𝐸\n(1.8\n0.8)\n𝐴\n(0.8\n1.2)\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖𝑦𝑖+1 0.8×2.3+1×2.6+2×1.6+2.7×0.8+1.8×1.2 11.96= =\nIf we draw a diagonal from each 𝑦-coordinate to the 𝑥-coordinate of the vertex directly below it, multiply the numbers that are connected, and\n∑𝑛−1sum all of these products, we would get the value of\n𝑥𝑖+1𝑦𝑖.𝑖=0\n𝐴\n(0.8\n1.2)\n𝐵\n(1\n2.3)\n𝐶\n(2\n2.6)\n𝐷\n(2.7\n1.6)\n𝐸\n(1.8\n0.8)\n𝐴\n(0.8\n1.2)\n𝑛−1\n∑\n𝑖=0\n𝑥𝑖+1𝑦𝑖=1×1.2+2×2.3+2.7×2.6+1.8×1.6+0.8×0.8 16.34=\nIf we halve the difference between these two values and take the absolute value, we would get the area of the polygon. This is why this procedure\nis known as the Shoelace formula: the connections between the coordinates resemble shoelaces of a shoe. Visualizing the shoelace formula\nusing this process also makes it much easier to remember!\n𝐴\n(0.8\n1.2)\n𝐵\n(1\n2.3)\n𝐶\n(2\n2.6)\n𝐷\n(2.7\n1.6)\n𝐸\n(1.8\n0.8)\n𝐴\n(0.8\n1.2)\n1Area =\n2|11.96−16.34| 2.19=\n¸ 26.5.3\nShoelace Formula Edge Cases: Axis Intersection and Concavity\nThe shoelace formula works for any simple polygon, but there are two edge cases that are worth discussing. First, what happens if a polygon\nintersects with the 𝑥-axis? Would the trapezoid strategy we used earlier still be valid?\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n-1\nPoint\nCoordinate\n𝐴\n(0.8,−0.8)\n𝐵\n(1,0.3)\n𝐶\n(2,0.6)\n𝐷\n(2.7,−0.4)\n𝐸\n(1.8,−1.2)", "word_count": 348, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c990b95b-6864-5f11-91b5-e3ea7b9406c3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1018, "real_page_number": null, "text": "1006\nChapter 26. Computational Geometry\nThe answer is yes — even though we do not end up with a nicely formed trapezoid when considering two points on opposite ends of the 𝑥-axis,\nthe total area obtained by the Shoelace formula is still correct because all excess areas eventually cancel out. For example, consider the edge\nfrom 𝐶to 𝐷. It we apply the signed area of a trapezoid formula to points 𝐶and 𝐷, we end up getting the signed area of the region below:\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n-1\n(𝑥𝐷−𝑥𝐶)(𝑦𝐶+𝑦𝐷)Signed Area =\n2\n(2.7−2)(0.6−0.4)=\n2\n(0.7)(0.2)=\n2\n= 0.07\nIf we manually calculated the areas of these two triangles, we would see that the left triangle (within the polygon) has an area of 0.126 while the\nright triangle (outside the polygon) has an area of 0.056. So why does the signed area come out to 0.07? This is because the left and right\ntriangles are on opposite sides of the 𝑥-axis, so the area of the right triangle \"cancels out\" a portion of the left triangle. However, we know that\nthe area of 0.126 from the left triangle is all part of our total polygon area, so how can we make up this area of 0.056 that was cancelled out?\nThis missing area is actually regained when we look at points 𝐷and 𝐸. If we apply the signed area of a trapezoid formula to points 𝐷and\n𝐸, we end up getting the signed area of the following trapezoid:\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n-1\n(𝑥𝐸−𝑥𝐷)(𝑦𝐷+𝑦𝐸)Signed Area =\n2\n(1.8−2.7)(−0.4−1.2)=\n2\n(−0.9)(−1.6)=\n2\n= 0.72\nThe signed area of the above trapezoid is 0.72, including the excess area near vertex D. As a result, this trapezoid the area of ourovercounts\npolygon by the area of the excess region, which is 0.056. However, since the region we previously considered with points 𝐶and 𝐷undercounted\nthe polygon area by 0.056, we end up back at the correct area in the end.\nThis phenomenon happens again when we find the area of the trapezoid formed by points 𝐸and 𝐴. Because edge 𝐴𝐵crosses the 𝑥-axis,\nthe trapezoid formed by 𝐸and 𝐴under the 𝑥-axis overcounts the area of the polygon.\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n-1\n(𝑥𝐴−𝑥𝐸)(𝑦𝐸+𝑦𝐴)Signed Area =\n2\n(0.8−1.8)(−1.2−0.8)=\n2\n(−1)(−2)=\n2\n= 1\nHowever, the signed area of the region formed from points 𝐴and 𝐵undercounts the polygon area by the same amount, as the excess area under\nthe 𝑥-axis offsets the area above the 𝑥-axis, as shown:\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n1\n2\n1\n-1\n(𝑥𝐵−𝑥𝐴)(𝑦𝐴+𝑦𝐵)Signed Area =\n2\n(1−0.8)(−0.8+0.3)=\n2\n(0.2)(−0.5)=\n2\n= −0.05\nIn general, the Shoelace formula still works even if the polygon intersects the 𝑥-axis. This is because any area that is overcounted by a trapezoid\nextending outside the polygon will always be offset by the signed area of another region that undercounts the polygon area by the same amount.", "word_count": 511, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a1f74631-4e81-5692-9b6f-544171a427d5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1019, "real_page_number": null, "text": "26.5 Area of a Polygon\n1007\nAnother edge case that may initially appear problematic is the case of a concave polygon (i.e., a polygon whose edges cave inward), such as the\none below. Concave polygons are interesting because multiple trapezoid areas may overlap.\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n1\n2\n1\n2\nFor instance, when considering 𝐴-𝐵and 𝐶-𝐷, we would add the areas of the following trapezoids:\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n1\n2\n1\n2\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n1\n2\n1\n2\nHowever, there is a region that is counted by both of these trapezoids when we sum up their areas. Does this mean that this region is overcounted\nin our final polygon area?\n𝑦\n𝑥\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n1\n2\n1\n2\nNot quite. Even though the region above is added twice when summing the trapezoid areas, it is also subtracted once to ensure that the total area\nremains correct. In our example, the darker shaded region of the above polygon is added twice (when considering 𝐴-𝐵and 𝐶-𝐷) and subtracted\nonce (when considering 𝐵-𝐶). The net effect is that this region is counted only once, which is the result we want. This outcome happens with\nany type of concave polygon — even if trapezoids overlap and a region’s area is counted more than once, these excess areas will eventually be\nsubtracted when considering other edges, and the polygon area will still be correct at the very end.\nYou can play around with many different types of simple polygons positioned anywhere on a two-dimensional coordinate plane, and the\nShoelace formula will always find the correct area. This makes the Shoelace formula quite versatile in finding the area of many different shapes,\nas it not only honesa time complexity linear on the number ofpolygonpoints, butit also uses a constantamount of auxiliary space. The shoelace\nvisualization covered earlier also makes this formula quite easy to remember!\nRemark: The Shoelace formula can find the area of any simple polygon. However, if you want to find the areas of more advanced\nnon-polygonal shapes, you may need to use a different algorithm. For shapes like circles and ellipses (ovals), there are existing formulas that\ncan be used to compute their areas. Unfortunately, for more irregular shapes that do not support the Shoelace formula and also do not have a\nseparate formula to calculate their areas, things can get a bit tricky. We will not be discussing this situation any further in this chapter, but\nthere are several ways to compute the area of a shape that does not adhere to a formula or algorithm that is already known. One common\ntechnique for dealing with these irregular shapes is to first break them down into smaller shapes that you are able to find the areas of (such as\ntriangles, circles, or even fractions of these shapes), and then combine these individual areas to estimate the area of the original shape.", "word_count": 512, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "07d3c163-0044-595e-8ede-905956853dfd", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1020, "real_page_number": null, "text": "1008\nChapter 26. Computational Geometry\n26.6\nPoint Inside Polygon\nIn this section, we will look at a problem that has useful applications in a variety of fields such as computer graphics, geographical systems, and\ngame design: determining if a point lies inside a given polygon. Suppose you are given a polygon and coordinate point on a two-dimensional\nplane — how can you efficiently identify whether this point lines inside or outside the polygon?\nA solution to this problem is actually quite straightforward. To determine if a point 𝑃lies inside a polygon, consider a horizontal ray that\nemanates rightward from 𝑃(the direction of the ray does not matter, but we will consider a rightward ray to make things simple). We then count\nthe number of times the ray intersects the polygon. If this number is even, then the point is outside the polygon. If this number is odd, then the\npoint is inside the polygon. (Note: this is actually an oversimplication that does not handle all edge cases, but we will address these later in this\nFor example, the following ray intersects the polygon in two places. Two is even, so point 𝑃lies outside the polygon.section, so keep reading.)\n𝑃\n⊙⊙\nIn the figure below, the ray intersects the polygon in three places. Three is odd, so point 𝑃lies inside the polygon.\n𝑃\n⊙\n⊙\n⊙\nHowever, there is a catch! For example, the following ray only intersects the polygon in one location. One is odd, so we would presume that its\ncorresponding point is inside the polygon… but it isn’t!\n𝑃\n⊙\nSimilarly, the following ray intersects the polygon in two locations. Two is even, so we would presume that its corresponding point is outside\nthe polygon… but once again, it isn’t!\n𝑃\n⊙\n⊙\nNotice that this issue only comes up if the ray directly intersects with a vertex of the polygon, or if the ray covers the entirety of an edge. This\nscenario a special edge case that we need to handle. If the ray crosses over a vertex of the polygon, we cannot count it as a single intersection if\nwe want to arrive at the correct answer. Instead, we need to adjust the ray so that it avoids the vertex completely. This is done by wiggling the\nray up and down by some small distance epsilon and then recounting the number of times the ray intersects the polygon. Eventually, you will\nend up with an even or odd number without this edge case, which you can then safely apply to determine if the point lies inside or outside the\npolygon using the odd-even rule discussed above.", "word_count": 441, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b85a7ba5-0df5-54cd-bfea-cdca770f4a27", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1021, "real_page_number": null, "text": "26.7 Convex Hull Algorithms\n1009\nFor instance, consider the following point 𝑃whose ray intersects a vertex of the polygon.\n𝑃\n⊙\nIf we move the ray a tiny distance upward, we would see that it intersects the polygon twice without directly crossing over a polygon vertex.\nLikewise, if we move the ray a tiny distance downward, it would intersect the polygon zero times. Both of these values are even, so we can\nconclude that 𝑃resides outside the polygon.\n𝑃\n⊙\n⊙\n𝑃\nSimilarly, let us consider the following point 𝑃, whose ray also intersects a vertex of the polygon.\n𝑃\n⊙\n⊙\nIf we move the ray a tiny distance upward, we would see that it intersects the polygon three times without directly crossing over a polygon\nvertex. Likewise, if we move the ray a tiny distance downward, it would intersect the polygon one time. Both of these values are odd, so we can\nconclude that 𝑃resides inside the polygon.\n𝑃\n⊙⊙\n⊙\n𝑃\n⊙\nThis algorithm is often known as the algorithm. There are definitely other ways to determine if a point lies within a polygon, butray-casting\nthis algorithm is definitely one of the simplest, especially in two dimensions. The worst-case time complexity of this algorithm is Θ(𝑛), where 𝑛\nis the number of polygon points you are given. This is because the algorithm may need to perform an intersection analysis for each edge ofΘ(1)\nthe polygon, and the total number of edges in the polygon is Θ(𝑛).\nRemark: Another edge case that we have not discussed yet occurs when the given point lies on a vertex or edge of the polygon itself (which\nwe will consider as inside the polygon). You can easily check if a given point lies on a polygon vertex by comparing the point with the\ncoordinates of the vertices that make up the polygon. However, determining if a point lines on an edge is slightly more complicated. One\napproach is to do some preprocessing and compute the line segments that make up each of the polygon’s edges, and then checking to see if\nthe point lies on any of these segments. This does not change the overall time complexity of the algorithm, since this preprocessing step also\ntakes time.Θ(𝑛)\n26.7\nConvex Hull Algorithms (✽)\nGiven a set of points, a convex hull is the smallest convex polygon that fully encloses the given pointset. Intuitively, you can think of a convex\nhull as the polygon that is formed if you surround a set of points with a rubber band and then let go, allowing the rubber band to wrap tightly\naround the outermost points. An example is shown below:\nGiven a set of points...\n...stretch out a rubber band...\n...and let go to form a convex hull.", "word_count": 465, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2e6a6261-834b-5f0d-8df3-0d17bbaf8f9e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1022, "real_page_number": null, "text": "1010\nChapter 26. Computational Geometry\nThe convex hull is one of the most important structures in the field of computational geometry, as it is one of the simplest ways to approximate a\nshape for a given set of points. In this section, we will explore two common algorithms that can be used to find the convex hull of a 2-D pointset:\nand scan.Jarvis’s march Graham’s\n¸ 26.7.1\n(✽)Jarvis’s March\nOne elementary algorithm for computing a convex hull is Jarvis’s march, also known as the algorithm. Jarvis’s march isgift-wrapping\nimplemented as follows:\n1. Pick the leftmost point among the given set of points and set it as the current vertex (as this point is guaranteed to be on the convex hull).\n2. Find the point that makes the smallest counterclockwise (leftward) turn relative to the current vertex, and connect the two vertices\ntogether. Then, set the new vertex as the current vertex.\n3. Repeat until you return back to the starting vertex (this ends up \"gift-wrapping\" the points in a counterclockwise order).\nUsing the previous example, Jarvis’s march would connect the vertices as follows:\n1\n1\n2\n1\n2\n3\n1\n2\n3\n4\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\n7\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\nWhat is the time complexity of Jarvis’s march? Let us define 𝑛as the total number of points we are given, and ℎas the total number of points\nthat make up the convex hull. Notice that, for each of the ℎpoints on the convex hull, we have to compare it with other vertices to findΘ(𝑛)\nthe one that yields the smallest leftward turn. This requires us to find the relative angle between two points, which can be done in time.Θ(1)\nTherefore, the overall time complexity of Jarvis’s march is Θ(𝑛ℎ).\n¸ 26.7.2\n(✽)Graham’s Scan\nAnother algorithm that can be used to compute a convex hull is Graham’s scan. Graham’s scan is implemented as follows:\n1. Find the lowest point in the pointset (i.e., the one with the smallest 𝑦-coordinate) and use it as the starting point. If there are multiple\nlowest points, choose the one with the smallest 𝑥-coordinate.\n2. Sort the remaining points by the angle each point forms with the lowest point, relative to the horizontal axis. If multiple points have the\nsame angle, only the point farthest from the bottom-most point needs to be considered (as all inner points cannot be on the convex hull).\n3. Initialize an empty stack. Push the bottom-most vertex and the point with the smallest angle into the stack.\n4. Iterate over the remaining points in sorted order and perform the following:\n• If the point under consideration makes a (leftward) turn relative to the previous two points on the stack, push thecounterclockwise\npoint into the stack.\n• If the point under consideration makes a (rightward) turn or no turn relative to the previous two points on the stack, popclockwise\noff the point at the top of the stack, and continue popping points from the stack until you end up with a counterclockwise turn.\n5. Once you return to the starting point, the contents of the stack holds the vertices of the convex hull.", "word_count": 556, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "dc02a3ff-8e1a-55d9-80ea-74c580fc5d3b", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1023, "real_page_number": null, "text": "26.7 Convex Hull Algorithms\n1011\nTo see this algorithm in action, consider the same set of points as above. Point 0 is the lowest point, so we select it as our starting vertex. The\nremaining points are sorted in order of the angle it makes with point 0 relative to the 𝑥-axis (labeled from 1-10).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nWe initialize a stack and push in points 0 and 1. Then, we iterate over the remaining points in sorted order, pushing each point into the stack if it\nyields a counterclockwise/leftward turn, and popping from the stack if it yields a clockwise/rightward turn.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\nStack\nFirst, we consider point 2 and compare it with the line formed by the two points at the top of the stack (points 0 and 1). To reach point 2 from\nthis line, we need to make a counterclockwise turn, so we push point 2 into the stack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\nStack\nNext, we consider point 3 and compare it with the line formed by the two points at the top of the stack (points 1 and 2). To reach point 3 from\nthe line, we need to make a counterclockwise turn, so we push point 3 into the stack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n3\nStack\nNext, we consider point 4 and compare it with the line formed by the two points at the top of the stack (points 2 and 3). To reach point 4 from\nthe line, we need to make a clockwise turn, so point 3 is popped off the top of the stack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n3\nStack\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\nStack\nThe two points at the top of the stack are now points 1 and 2. If we compare point 4 with the line formed by points 1 and 2, we can see that a\ncounterclockwise turn is now needed to reach point 4. Thus, we can safely push point 4 onto the stack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\nStack", "word_count": 402, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "39e861da-3939-55a3-8e7f-11f438ec8321", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1024, "real_page_number": null, "text": "1012\nChapter 26. Computational Geometry\nThe next point we consider is point 5. To reach point 5 from the line formed by the two points at the top of the stack (points 2 and 4), we need to\nmake a counterclockwise turn, so point 5 is pushed into the stack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\n5\nStack\nNext up, we consider point 6 and compare it with the line formed by points 4 and 5. To reach point 6 from this line, we need to make a\ncounterclockwise turn, so point 6 is pushed into the stack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\n5\n6\nStack\nNext, we consider point 7 and compare it with the line formed by points 5 and 6. To reach point 7 from this line, we need to make a clockwise\nturn. Thus, the point at the top of the stack (point 6) is popped off.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\n5\n6\nStack\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\n5\nStack\nHowever, reaching point 7 still requires a clockwise turn from the line formed by points 4 and 5. As a result, we have to pop another value (point\n5) off the top of the stack. Remember that we need to continuously pop values off the stack until the point under consideration can be reached\nusing a counterclockwise turn!\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\n5\nStack\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\nStack\nAfter popping off point 5, we can now reach point 7 using a counterclockwise turn from the line formed by points 2 and 4, which are the two\npoints at the top of our stack. Thus, we can now safely push point 7 onto the stack.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\n7\nStack", "word_count": 359, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "728197c3-a77e-50c2-abc6-c25d1c60728c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1025, "real_page_number": null, "text": "26.7 Convex Hull Algorithms\n1013\nIf we continue this approach for the remaining points, we would end up with the following stack when we return to point 0. At this point, the\nalgorithm ends, and the contents of the stack store the vertices of our convex hull.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n4\n7\n8\n9\n10\nStack\nWhat is the time complexity of Graham’s scan? The implementation of Graham’s scan can be split up into three subsequent components: finding\nthe lowest point in the pointset, sorting the points by relative angle, and then processing all the points in sorted order. Given 𝑛points, finding the\nlowest point takes time (since you need to consider all 𝑛points to determine which one has the lowest 𝑦-coordinate), and sorting the pointsΘ(𝑛)\nby angle takes time. However, the time required to process all the points after sorting is a bit more involved. It turns out that thisΘ(𝑛log(𝑛))\nfinal step actually takes time.Θ(𝑛)\nWhy is this the case? Let us denote 𝑑𝑖as the total number of points that are popped from the stack while processing point 𝑝𝑖. In this case,\nthe amount work spent processing 𝑝𝑖is proportional to 𝑑𝑖+1, since we need to perform a clockwise test for each of the 𝑑𝑖points we pop,Θ(1)\nas well as the last point that needs to be tested before we can safely push point 𝑝𝑖onto the stack. If we sum up this value for all 𝑛, we would get a\nbound on the total work we need to do to process all 𝑛points:\n𝑛∑\n𝑖=1\n(𝑑𝑖+1)\n=\n𝑛+\n𝑛∑\n𝑖=1\n𝑑𝑖\n∑𝑛The key observation to notice here is that\n𝑑𝑖cannot be larger than 𝑛. This is because each of the 𝑛points is pushed onto the stack only𝑖=1\nonce, so a single point cannot be popped more than once. Thus, the total number of points we can pop cannot be greater than the number of\npoints in the entire pointset itself!\n𝑛+\n𝑛∑\n𝑖=1\n𝑑𝑖\n≤\n𝑛+ 𝑛\nWe can thereby conclude that the time complexity of processing all the points after sorting takes time. Out of the three steps, sorting hasΘ(𝑛)\nthe dominant time complexity of Θ(𝑛log(𝑛)), so the overall time complexity of Graham’s scan is also Θ(𝑛log(𝑛)).\n¸ 26.7.3\n(✽)Comparing Convex Hull Algorithms\nWhich convex hull algorithm is better, Jarvis’s march or Graham’s scan? It depends. Given 𝑛points, the time complexity of Graham’s scan is\nΘ(𝑛log(𝑛)). However, the time complexity of Jarvis’s march is Θ(𝑛ℎ), where ℎis the number of vertices on the convex hull. Therefore, whether\none algorithm is better than another depends on the relative values of and ℎ. If ℎis asymptotically smaller than log(𝑛), then Jarvis’slog(𝑛)\nmarch is better; otherwise, Graham’s scan is better.\nHowever, this opens up a brand new can of worms, since we do not know what ℎis until we compute the convex hull! In fact, weafter\ndeviated a bit from normal convention when describing the time complexity of Jarvis’s march, since previous algorithms were analyzed in\nterms of input size alone. However, because the performance of Jarvis’s march varies greatly depending on the output of the algorithm, its\nrunning time is also expressed in terms of its output size to accurately capture its performance. Such algorithms are known as output sensitive\nalgorithms, and they are fairly common in the field of computational geometry.\nSince neither Jarvis’s march nor Graham’s scan is asymptotically optimal in all cases, you may be wondering if there exists an algorithm\nthat is asymptotically optimal for all 𝑛and ℎ. It turns out that there is: can be used to compute a convex hull in Θ(𝑛log(ℎ))Chan’s algorithm\ntime by combining components from both Jarvis’s march and Graham’s scan. This algorithm is fairly involved, so we will not be discussing it\nin detail here. It should be noted that, while Chan’s algorithm is asymptotically optimal, the improvement from to isΘ(𝑛log(𝑛)) Θ(𝑛log(ℎ))\noften very small in practice. As a result, the simpler Graham’s scan algorithm should typically suffice for finding a convex hull, even if it is not\nasymptotically optimal in all cases.\nTime Complexity of Convex Hull Algorithms\nJarvis’s March\nGraham’s Scan\nChan’s Algorithm\nΘ(𝑛ℎ)\nΘ(𝑛log(𝑛))\nΘ(𝑛log(ℎ))", "word_count": 710, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a803cc3d-ce11-580d-ab6e-f67d6bada93f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1026, "real_page_number": null, "text": "1014\nChapter 26. Computational Geometry\n26.8\nBounding Box Algorithms (✽)\nGiven a set of points, a bounding box is a rectangular border that fully encloses all of the points in the set. For instance, the following is a\nbounding box for the given set of points on a two-dimensional plane.\nA minimum bounding box is the bounding box that encloses all the points with the smallest area (or volume in three dimensions, and so on for\nlarger dimensions). If we confine our bounding box so that its edges are parallel to the 𝑥- and 𝑦-axes, then finding the minimum bounding box is\nfairly straightforward: for each axis, we simply have to find the minimum and maximum values among the given points and use these as the\nboundary of the box. For instance, the following points (labeled 𝐴and 𝐵) have the smallest and largest 𝑥-coordinates respectively, so these\ncoordinates become the vertical boundaries of our bounding box.\n𝐴\n𝐵\nSimilarly, the following points (labeled 𝐶and 𝐷) have the smallest and largest 𝑦-coordinates respectively, so these coordinates become the\nhorizontal boundaries of our bounding box.\n𝐶\n𝐷\nGiven 𝑛points, the time complexity of this algorithm is Θ(𝑛), since we can build a bounding box by simply iterating over all 𝑛points and\nkeeping track of the smallest and largest values of 𝑥and 𝑦.\nRemark: To solve the above problem, we will need to find the smallest and largest coordinate values of each dimension. This sounds fairly\nstraightforward: to find the minimum of 𝑛values, simply keep track of a running minimum and continuously update it while iterating over\nthe values (the same idea applies for finding the maximum).\n1\nint32_t get_min_value(const std::vector<int32_t>& vec) {\n2\nint32_t running_min = vec[0]; // assuming non-empty\n3\nfor (size_t i = 1; i < vec.size(); ++i) {\n4\nrunning_min = std::min(running_min, vec[i]);\n5\n} // for i\n6\nreturn running_min;\n7\n} // get_min_value()\n8\n9\nint32_t get_max_value(const std::vector<int32_t>& vec) {\n10\nint32_t running_max = vec[0]; // assuming non-empty\n11\nfor (size_t i = 1; i < vec.size(); ++i) {\n12\nrunning_max = std::max(running_max, vec[i]);\n13\n} // for i\n14\nreturn running_max;\n15\n} // get_max_value()\nHowever, if you want to find the minimum and maximum of a set of values, running this algorithm twice (once to find the minimum,both\nonce to find the maximum) is not actually the most efficient way to do things. Notice that each of the loops above require comparisons,𝑛−1\nsorunningbothloopsinsuccessionwouldrequireatotalof comparisons. Thereisawaytodobetter2(𝑛−1) ifwecomputeboththeminimum\nonce! The idea behind this optimized approach is to compare values in pairs rather than one by one — by doing so, we canand maximum at\nuse the relative ordering of the values in the pair to judiciously decide whether we want to compare each value with the running minimum or\nmaximum. To illustrate how this works, consider the following array of values, for which we want to find both the minimum and maximum:\n7\n3\n2\n6\n9\n5\n4\n1\n10\n8", "word_count": 520, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "23031762-2bed-5b2b-93c1-f5156c9f5b52", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1027, "real_page_number": null, "text": "26.8 Bounding Box Algorithms\n1015\nFirst, we will initialize a running minimum and running maximum, whose values are set to the minimum and maximum of the first two\nelements. In this case, the running minimum and maximum values are set to 3 and 7, respectively.\n7\n3\n2\n6\n9\n5\n4\n1\n10\n8\n3\nmin\n7\nmax\nWe then iterate over the remaining elements in pairs, comparing the minimum of the pair with the running minimum, and the maximum of\nthe pair with the running maximum. In our example, the next pair we consider includes 2 and 6. 2 is the smaller of the pair, so it is compared\nwith the running minimum; 6 is the larger of the pair, so it is compared with the running maximum (updating any values if necessary).\n7\n3\n2\n6\n9\n5\n4\n1\n10\n8\n2\nmin\n7\nmax\nThe next pair we consider includes 9 and 5. 5 is the smaller of the two, so it is compared with the running minimum, and 9 is compared with\nthe running maximum (updating any values if necessary).\n7\n3\n2\n6\n9\n5\n4\n1\n10\n8\n2\nmin\n9\nmax\nThe next pair includes 4 and 1. 1 is smaller, so it is compared with the minimum, and 4 is compared with the maximum.\n7\n3\n2\n6\n9\n5\n4\n1\n10\n8\n1\nmin\n9\nmax\nThe next pair includes 10 and 8. 8 is smaller, so it is compared with the minimum, and 10 is compared with the maximum.\n7\n3\n2\n6\n9\n5\n4\n1\n10\n8\n1\nmin\n10\nmax\nAt the end of the algorithm, the running minimum and maximum would store the minimum and maximum of the entire set of values. By\nlooking at values in pairs before comparing with the running minimum and maximum, we avoid having to make fruitless comparisons that\nwill never lead us to a new minimum or maximum. This ends up dropping the total number of comparisons we need to make from 2(𝑛−1)\n𝑛−21+⌈3×to\n⌉(this is because we need one comparison between the first two values to initialize the running minimum and maximum,2\n𝑛−2and then three comparisons for each of the remaining\n2\npairs: (1) comparing the values in the pair to determine which is smaller, (2)\ncomparing the smaller value with the running minimum, and (3) comparing the larger value with the running maximum). Note: although\nthis approach still takes time, the coefficient of the linear term ends up being smaller since we are completing ∼1.5𝑛steps rather thanΘ(𝑛)\n∼2𝑛steps — as a result, we still have an improved algorithm even though the asymptotic complexity class does not change.\nIn practice, you will never have to implement a function like this on your own. Recall from chapter 11 that there is already a function in the\nstd::minmax_element().STL that finds the minimum and maximum values of a range:\nIf we remove the restriction that the bounding box needs to be parallel to the axes, then the problem becomes a bit trickier. The key insight to\nnotice here is that the orientation of the bounding box must be aligned with one of the edges of the pointset’s convex hull. This is because a\nconvex hull is, by definition, the smallest convex shape that encloses all of the given points. Therefore, we can find the minimum bounding box\nby considering the bounding boxes aligned with each edge of the convex hull and taking the one with the minimum area. This procedure is\nsummarized using the following steps:\n1. Compute the convex hull of the given set of points.\n2. Foreachedgeoftheconvexhull, \"rotate\"thepointsontheplanesothatthisedgeisparalleltotheaxes. Then, usingtheprocessdiscussed\nearlier, compute the minimum bounding box that is parallel with the 𝑥- and 𝑦-axes.\n• The word \"rotate\" is included in quotes here because we do not need to physically rotate the points in our algorithm. Instead, if we\nwant to rotate our pointset to align with a certain edge of our convex hull, we can find the unit vector ⃗𝑢corresponding to that edge\nof the hull and the unit vector ⃗𝑣that is orthogonal (i.e., perpendicular) to ⃗𝑢. We can then take the dot products of ⃗𝑢and ⃗𝑣with the\noriginal vertices of the convex hull to get the new rotated coordinates. (Do not worry if you have no idea what this all means; the\ntakeaway here is that we can use math to compute the rotated coordinates of our convex hull without having to physically rotate the\npoints in our plane.)\n3. Store the orientation with the minimum area encountered so far, updating it whenever a better solution is found. After all edges of the\nconvex hull are considered, return the best solution that is stored, which corresponds to the minimum bounding box of the entire pointset.", "word_count": 829, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "73354b99-b799-525a-bba5-cde3550ebb54", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1028, "real_page_number": null, "text": "1016\nChapter 26. Computational Geometry\nThe minimum bounding box for the example pointset, when allowing orientation, is the smallest of the eight boxes below:\n26.9\nk-d Trees (✽)\n¸ 26.9.1\n(✽)Partitioning Data in a k-d Tree\nA k-dimensional tree, also known as a k-d tree for short, is a binary search tree that can be used to efficiently organize points in a 𝑘-dimensional\nspace. In a k-d tree, each node represents a 𝑘-dimensional point, and each level of the tree recursively partitions a dimension of the point space\ninto two halves. An example of a k-d tree of dimension 𝑘= 2 is shown below:\nPoint\nCoordinate\n𝐴\n(3.7,3.9)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐷\n(3.0,3.0)\n𝐸\n(1.5,2.5)\n𝐹\n(4.0,2.0)\n𝐺\n(4.3,1.0)\n𝐻\n(1.7,0.9)\n𝐼\n(3.5,0.5)\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nk-d tree\n𝐷\n(3.0,3.0)\n𝐹\n(4.0,2.0)\n𝐴\n(3.7,3.9)\n𝐼\n(3.5,0.5)\n𝐺\n(4.3,1.0)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\nx\ny\nx\ny\nIn a standard binary search tree, items that are smaller than a given node end up in its left subtree, and items that are larger end up in its right\nsubtree. A k-d tree works in a very similar way, just generalized to multiple dimensions. You can think of each node of a k-d tree as a partition\npoint that splits a dimension into two halves: points in the smaller half are sent to its left subtree, and points in the larger half are sent to its right\nsubtree. The dimension to partition on alternates at each level of the tree, so a given node that partitions along the 𝑥-dimension would have\npartitioned).7children that partition along the 𝑦-dimension, and so on (circling back to the 𝑥-dimension after all dimensions are The dimension\nto partition on is known as the discriminator.\nFor example, consider thetree above. Point 𝐷serves asthe rootof the tree, whichdiscriminateson the 𝑥-dimension. Visually, this partitions\nthe pointset along (the 𝑥-coordinate of 𝐷), as shown. Points that have an 𝑥-coordinate are in the left subtree of 𝐷, and points that𝑥= <3 3\n≥3have an 𝑥-coordinate are in the right subtree (recall from chapter 18 that duplicates can be handled arbitrarily as long as you are consistent,\nbut we will default to the right subtree in these notes).\nPoint\nCoordinate\n𝐴\n(3.7,3.9)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐷\n(3.0,3.0)\n𝐸\n(1.5,2.5)\n𝐹\n(4.0,2.0)\n𝐺\n(4.3,1.0)\n𝐻\n(1.7,0.9)\n𝐼\n(3.5,0.5)\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nk-d tree\n𝐷\n(3.0,3.0)\n𝐹\n(4.0,2.0)\n𝐴\n(3.7,3.9)\n𝐼\n(3.5,0.5)\n𝐺\n(4.3,1.0)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\nx\ny\nx\ny\n7Alternating the partition dimension (discriminator) in a round-robin fashion is not the only way we can build a k-d tree, and it may actually be sub-optimal\ndependingonthedatadistribution. Anothermethodistoselectthediscriminatorasthedimensiononwhichthepointshavethegreatestspread,ordifference\nbetweenthelargestandsmallestcoordinatevalues. However,tokeepthingssimple,wewillonlylookattheround-robinversioninthischapter.", "word_count": 530, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "56746802-6f19-5428-9485-616f712f736d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1029, "real_page_number": null, "text": "26.9 k-d Trees\n1017\nFurthermore, let’s look at the right subtree of 𝐷, which represents the region to the right of point 𝐷. Point 𝐹serves as the root of this subtree,\nand it is on a level of the tree that discriminates on the 𝑦-dimension. Visually, this partitions the region to the right of 𝐷along (the𝑦= 2\n𝑦-coordinate of 𝐹), as shown. Points to the right of 𝐷that have a 𝑦-coordinate are in the left subtree of 𝐹, and points to the right of 𝐷that<2\n≥2have a 𝑦-coordinate are in the right subtree of 𝐹. (The same analysis applies to the left subtree rooted at 𝐸, which partitions the region to\nthe left of 𝐷along 2.5).𝑦=\nPoint\nCoordinate\n𝐴\n(3.7,3.9)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐷\n(3.0,3.0)\n𝐸\n(1.5,2.5)\n𝐹\n(4.0,2.0)\n𝐺\n(4.3,1.0)\n𝐻\n(1.7,0.9)\n𝐼\n(3.5,0.5)\n𝐴\n𝐹\n𝐺\n𝐼\n𝐵\n𝐶\n𝐷\n𝐸\n𝐻\nk-d tree\n𝐷\n(3.0,3.0)\n𝐹\n(4.0,2.0)\n𝐴\n(3.7,3.9)\n𝐼\n(3.5,0.5)\n𝐺\n(4.3,1.0)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\ny\nx\nx\ny\nWe can follow the same process to see how the remaining points in the tree are partitioned. For example, point 𝐼is the root of the left subtree of\n𝐹, and it is on a level of the tree that discriminates on the 𝑥-dimension. Therefore, point 𝐼partitions the region of the point space to the right of\n𝐷and below 𝐹(i.e., the darker shaded region above) along its 𝑥-coordinate value of 3.5. Any point in this darker shaded region to the left of\nis sent to the left subtree of 𝐼, and any point in this region to the right of is sent to the right subtree of 𝐼.𝑥=3.5 𝑥=3.5\nFrom this example, we can see that multiple distinct k-d trees can be built for the same set of points, based on the manner in which we\ncomplete our partitions. Below is an example of another k-d tree that partitions the points in a different order.\nPoint\nCoordinate\n𝐴\n(3.7,3.9)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐷\n(3.0,3.0)\n𝐸\n(1.5,2.5)\n𝐹\n(4.0,2.0)\n𝐺\n(4.3,1.0)\n𝐻\n(1.7,0.9)\n𝐼\n(3.5,0.5)\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nk-d tree\n𝐼\n(3.5,0.5)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐹\n(4.0,2.0)\n𝐶\n(2.2,3.3)\n𝐵\n(1.0,3.6)\n𝐷\n(3.0,3.0)\n𝐻\n(1.7,0.9)\n𝐸\n(1.5,2.5)\nx\ny\nx\ny\nx\nOne thing you may have noticed here is that the way in which we partition the points may affect the efficiency of searching through the tree.\nRecall that the best tree structure we can have is a balanced tree, which ensures worst-case search, insertion, and removal. However,Θ(log(𝑛))\nthe balancing techniques that we introduced back in chapter 18 do not work on k-d trees because each level of the tree strictly specifies the\ndimension that a given point discriminates on (and thus you cannot trivially change the level a node is situated on while rebalancing). Because\nof this, the time complexities of search, insertion, and removal from the standard k-d tree discussed above are each worst-case Θ(𝑛).\n¸ 26.9.2\n(✽)Inserting into a k-d Tree\nHowever, if you know all the points that you need to insert into a k-d tree beforehand, there is a way to build the tree so that it always ends up\nbeing balanced (and thus supports tree operations). The idea is to always insert the point (or either of the two middle points)Θ(log(𝑛)) middle\nfor the discriminator being considered, which allows the remaining points to be evenly split to the left and right of the newly inserted node. In\nour example, if we want to build a k-d tree with an initial discriminator on the 𝑥-dimension, we would insert point 𝐷first because 𝐷has the\nmedian 𝑥-coordinate of the available points.\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\n𝐻\n𝐼\nk-d tree\n𝐷\n(3.0,3.0)\nx", "word_count": 652, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "1855b603-b453-54c8-83b1-c86c52bf0f00", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1030, "real_page_number": null, "text": "1018\nChapter 26. Computational Geometry\nThe next level of the k-d tree partitions the points based on 𝑦-coordinate. For the left subtree of 𝐷, we want to insert the point to the left of 𝐷\nwith the middle 𝑦-coordinate. There are four points to the left of 𝐷: 𝐵, 𝐶, 𝐸, and 𝐻. The two middle points are 𝐶and 𝐸, so we can insert\neither as the root of the left subtree (in this case, we will choose the smaller one, which happens to be 𝐸).\n𝐴\n𝐹\n𝐺\n𝐼\n𝐷\n𝐵\n𝐶\n𝐸\n𝐻\nk-d tree\n𝐷\n(3.0,3.0)\n𝐸\n(1.5,2.5)\nx\ny\nWe can apply a similar idea for the points right of 𝐷. Of the four points to the right of 𝐷, the points with the middle 𝑦-coordinates are 𝐹and 𝐺,\nso we can insert either of these points as the root of the right subtree of 𝐷. We will select the point with the smaller coordinate value, or 𝐺.\n𝐴\n𝐹\n𝐺\n𝐼\n𝐵\n𝐶\n𝐷\n𝐸\n𝐻\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐸\n(1.5,2.5)\nx\ny\nContinuing this process, we would get the following. Each point that we insert into the k-d tree subdivides its region such that the number of\npoints on its left and right subtrees differs by no more than one, ensuring that our final k-d tree is balanced.\n𝐻\n𝐷\n𝐸\n𝐴\n𝐹\n𝐺\n𝐼\n𝐵\n𝐶\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐸\n(1.5,2.5)\n𝐻\n(1.7,0.9)\nx\ny\nx\n𝐵\n𝐶\n𝐷\n𝐸\n𝐻\n𝐴\n𝐹\n𝐺\n𝐼\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐻\n(1.7,0.9)\nx\ny\nx", "word_count": 284, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a700dbf9-bec7-50af-98ce-9f7abba41123", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1031, "real_page_number": null, "text": "26.9 k-d Trees\n1019\n𝐼\n𝐷\n𝐸\n𝐻\n𝐴\n𝐹\n𝐺\n𝐵\n𝐶\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐻\n(1.7,0.9)\nx\ny\nx\n𝐷\n𝐸\n𝐻\n𝐺\n𝐵\n𝐶\n𝐼\n𝐴\n𝐹\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐻\n(1.7,0.9)\nx\ny\nx\n𝐷\n𝐸\n𝐻\n𝐺\n𝐵\n𝐼\n𝐴\n𝐹\n𝐶\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\nx\ny\nx\ny\n𝐷\n𝐸\n𝐻\n𝐺\n𝐵\n𝐼\n𝐴\n𝐶\n𝐹\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐹\n(4.0,2.0)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\nx\ny\nx\ny\nBy inserting the points in this manner, we get a balanced k-d tree that supports search.Θ(log(𝑛))", "word_count": 151, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "57fc442b-fcfb-52f1-8944-7e5a4cbe59b0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1032, "real_page_number": null, "text": "1020\nChapter 26. Computational Geometry\nMuch like inserting into a standard binary search tree, inserting a point into a k-d tree is relatively straightforward. If the coordinate value of the\nnew point (as indicated by the discriminator) is smaller than the corresponding coordinate value of a node in the tree, make a recursive call on\nits left subtree; otherwise, make a recursive call on its right subtree. In either case, the discriminator is alternated among all the dimensions as\nyou traverse down the tree. For example, suppose we wanted to insert point 𝐽below, located at the coordinate (2.4, 1.9).\n𝐷\n𝐸\n𝐻\n𝐺\n𝐵\n𝐼\n𝐴\n𝐶\n𝐹\n𝐽\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐹\n(4.0,2.0)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\nx\ny\nx\ny\nWe first compare 𝐽with the root node of 𝐷. Since the root level of the k-d tree has a discriminator of 𝑥, we will compare the 𝑥-coordinate of 𝐽\nwith the 𝑥-coordinate of 𝐷to determine which side of the tree 𝐽should go. In this case, 2.4 3.0, so 𝐽belongs in the left subtree of 𝐷.<\n𝐷\n𝐸\n𝐻\n𝐺\n𝐵\n𝐼\n𝐴\n𝐶\n𝐹\n𝐽\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐹\n(4.0,2.0)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\nx\ny\nx\ny\n𝐽\n(2.4,1.9)\nNext, we compare 𝐽with 𝐸. Since this level of the tree has a discriminator of 𝑦, we will compare the 𝑦-coordinate of 𝐽with the 𝑦-coordinate of\n𝐸to determine which side of the tree 𝐽should go. In this case, 1.9 2.5, so 𝐽belongs in the left subtree of 𝐸.<\n𝐷\n𝐸\n𝐻\n𝐺\n𝐵\n𝐼\n𝐴\n𝐶\n𝐹\n𝐽\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐹\n(4.0,2.0)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\ny\nx\nx\ny\n𝐽\n(2.4,1.9)\nNext, we compare 𝐽with 𝐻. Since this level of the tree has a discriminator of 𝑥, we will compare the 𝑥-coordinate of 𝐽with the 𝑥-coordinate\nof 𝐻to determine which side of the tree 𝐻should go. In this case, 2.4 1.7, so 𝐽belongs in the right subtree of 𝐻.>\n𝐷\n𝐸\n𝐻\n𝐺\n𝐵\n𝐼\n𝐴\n𝐶\n𝐹\n𝐽\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐹\n(4.0,2.0)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\nx\nx\ny\ny\n𝐽\n(2.4,1.9)", "word_count": 410, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "75ffec8c-77b0-5bba-b030-5488df259e5d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1033, "real_page_number": null, "text": "26.9 k-d Trees\n1021\nSince 𝐻does not have anything in its right subtree, we can insert 𝐽in this position.\n𝐷\n𝐸\n𝐻\n𝐺\n𝐵\n𝐼\n𝐴\n𝐶\n𝐹\n𝐽\nk-d tree\n𝐷\n(3.0,3.0)\n𝐺\n(4.3,1.0)\n𝐴\n(3.7,3.9)\n𝐹\n(4.0,2.0)\n𝐼\n(3.5,0.5)\n𝐸\n(1.5,2.5)\n𝐵\n(1.0,3.6)\n𝐶\n(2.2,3.3)\n𝐻\n(1.7,0.9)\n𝐽\n(2.4,1.7)\ny\nx\ny\nx\nAs you can see, the time complexity of insertion depends on the number of elements you have to traverse before you find an open position for\nthe point you want to insert. In the worst-case, your k-d tree could be in the form of a stick, which could require you to traverse every node of\nthe tree before you find an open position; hence, the worst-case time complexity of insertion is if given 𝑛points. On average, however, youΘ(𝑛)\ncan expect the tree to be fairly balanced, and the average-case time complexity of insertion turns out to be Θ(log(𝑛)).\n¸ 26.9.3\n(✽)Deleting from a k-d Tree\nDeleting from a k-d tree is slightly trickier because each level of the tree may have a different discriminator. To delete a point from a k-d tree\nwith a discriminator dimension 𝑑, you can follow these steps:\n1. If the node to delete is a leaf node, delete the node and return.\n2. Otherwise, if the node is not a leaf node:\n• If the node has a right subtree, find the node in the right subtree with the smallest 𝑑value and replace the node you want to delete\nwith this 𝑑-minimum node. Then, recursively delete the original 𝑑-minimum node.\n• Otherwise, if the node has no right subtree, move the left subtree so that it becomes the new right subtree. Then, find the node with\nthe smallest 𝑑value in this new right subtree and replace the node you want to delete with this 𝑑-minimum node. Then, recursively\ndelete the original 𝑑-maximum node.\nFor instance, consider the following k-d tree, from which we want to delete point 𝐴:\nk-d tree\n𝐴\n(3.5,6.0)\n𝐶\n(6.0,8.0)\n𝐸\n(8.0,4.0)\n𝐻\n(9.0,6.0)\n𝐺\n(5.0,3.0)\n𝐼\n(6.0,2.0)\n𝐽\n(7.0,1.0)\n𝐵\n(2.0,4.5)\n𝐷\n(1.0,3.5)\n𝐹\n(2.0,2.0)\nx\ny\nx\ny\nx\ny\nThe discriminator of 𝐴is 𝑥. Since 𝐴is not a leaf node, we will find the node in the right subtree of 𝐴with the smallest 𝑥value, which happens\nto be point 𝐺:\nk-d tree\n𝐴\n(3.5,6.0)\n𝐶\n(6.0,8.0)\n𝐸\n(8.0,4.0)\n𝐻\n(9.0,6.0)\n𝐺\n(5.0,3.0)\n𝐼\n(6.0,2.0)\n𝐽\n(7.0,1.0)\n𝐵\n(2.0,4.5)\n𝐷\n(1.0,3.5)\n𝐹\n(2.0,2.0)\nx\ny\nx\ny\nx\ny", "word_count": 427, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "62863e9f-1fac-50ac-a3fb-39abd6519576", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1034, "real_page_number": null, "text": "1022\nChapter 26. Computational Geometry\nWe then replace point 𝐴with point 𝐺, and then we recursively delete the original node for point 𝐺:\nk-d tree\n𝐺\n(5.0,3.0)\n𝐶\n(6.0,8.0)\n𝐸\n(8.0,4.0)\n𝐻\n(9.0,6.0)\n𝐺\n(5.0,3.0)\n𝐼\n(6.0,2.0)\n𝐽\n(7.0,1.0)\n𝐵\n(2.0,4.5)\n𝐷\n(1.0,3.5)\n𝐹\n(2.0,2.0)\nx\ny\nx\ny\nx\ny\nThe discriminator of the original node 𝐺is 𝑦. Since the original 𝐺node is not a leaf node and does not have a right subtree, we will swap the\nleft subtree of 𝐺so that it becomes the new right subtree. Then, we replace node 𝐺with the node in this new subtree with the smallest 𝑦value,\nwhich happens to be point 𝐽:\nk-d tree\n𝐺\n(5.0,3.0)\n𝐶\n(6.0,8.0)\n𝐸\n(8.0,4.0)\n𝐻\n(9.0,6.0)\n𝐺\n(5.0,3.0)\n𝐼\n(6.0,2.0)\n𝐽\n(7.0,1.0)\n𝐵\n(2.0,4.5)\n𝐷\n(1.0,3.5)\n𝐹\n(2.0,2.0)\nx\ny\nx\ny\nx\ny\nk-d tree\n𝐺\n(5.0,3.0)\n𝐶\n(6.0,8.0)\n𝐸\n(8.0,4.0)\n𝐻\n(9.0,6.0)\n𝐽\n(7.0,1.0)\n𝐼\n(6.0,2.0)\n𝐽\n(7.0,1.0)\n𝐵\n(2.0,4.5)\n𝐷\n(1.0,3.5)\n𝐹\n(2.0,2.0)\nx\ny\nx\ny\nx\ny\nThe node for point 𝐽is a leaf node, so we can safely delete it without processing any other nodes in the k-d tree. This gives us our final k-d tree\nafter point 𝐴is deleted.\nk-d tree\n𝐺\n(5.0,3.0)\n𝐶\n(6.0,8.0)\n𝐸\n(8.0,4.0)\n𝐻\n(9.0,6.0)\n𝐽\n(7.0,1.0)\n𝐼\n(6.0,2.0)\n𝐵\n(2.0,4.5)\n𝐷\n(1.0,3.5)\n𝐹\n(2.0,2.0)\nx\ny\nx\ny\nx", "word_count": 238, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "346f3b45-a327-5323-84c0-f428f28e52f5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1035, "real_page_number": null, "text": "26.9 k-d Trees\n1023\nRemark: Why do we need to swap in the left subtree if a node to delete does not have a right subtree? Wouldn’t it be easier to replace the\nnode to delete with the descendant in the subtree with the discriminator value?left largest\nThe reason we perform the swap is to handle the case of equal discriminator values. Recall from our previous definition that duplicates will\nalways be sent to the right subtree. Although it does not matter which side duplicates go on, we prefer it to be consistent so that our k-d\ntree operations can be fully efficient. However, replacing the deleted node with the largest discriminator value in the left subtree does not\nguarantee this invariant. For instance, consider the following k-d tree:\nk-d tree\n𝐴\n(4.0,1.0)\n𝐵\n(3.0,3.0)\n𝐶\n(4.0,2.0)\nx\ny\nx\nIf we attempt to delete 𝐵by replacing the deleted node with the descendant in the left subtree with the largest discriminator, we would get\nthe following. However, this tree is invalid per the rules we initially defined for our k-d tree, since 𝐴and 𝐶share the same 𝑥-coordinate but\n𝐶is to the left of 𝐴!\nk-d tree\n𝐴\n(4.0,1.0)\n𝐶\n(4.0,2.0)\nx\ny\nBy swapping the left subtree so that it becomes the new right subtree, we ensure that duplicate discriminator values that were previously in\nthe left subtree correctly end up in the right subtree after we perform the delete.\nk-d tree\n𝐴\n(4.0,1.0)\n𝐶\n(4.0,2.0)\nx\ny\nSimilar to insert, the amount of work we need to complete during a deletion depends on the height of the tree. In the worst-case, you could have\na stick, which could require you to traverse every node of the tree before you eventually delete a leaf node — this would yield a worst-case time\ncomplexity of for a single deletion (where 𝑛is the number of points in the tree). However, much like insertion, you can expect the tree toΘ(𝑛)\nbe fairly balanced in the average case, and the average-case time complexity of deletion again turns out to be Θ(log(𝑛)). A summary of k-d tree\noperations is provided in the table below.\nNot Balanced\nBalanced\nAverage Case\nWorst Case\nAverage Case\nWorst Case\nTime Complexity of Search\nΘ(log(𝑛))\nΘ(𝑛)\nΘ(log(𝑛))\nΘ(log(𝑛))\nTime Complexity of Insert\nΘ(log(𝑛))\nΘ(𝑛)\nΘ(log(𝑛))\nΘ(log(𝑛))\nTime Complexity of Delete\nΘ(log(𝑛))\nΘ(𝑛)\nΘ(log(𝑛))\nΘ(log(𝑛))\nAuxiliary Space\nΘ(𝑛)\nΘ(𝑛)\nΘ(𝑛)\nΘ(𝑛)\nThe efficiency of k-d trees in representing a multidimensional space makes it a versatile tool for solving geometrical problems. One such\nproblem is the problem, which seeks to find the point in a pointset that is nearest to a given query point. k-d trees arenearest neighbor search\nalso useful for performing over multiple parameters. For example, if we are given a list of EECS 281 students with their examrange searches\nscores and the number of credits they are taking, we could perform a k-d tree range search to efficiently find the set of all students that are taking\nover 15 credits and have a score of 93 or higher on their EECS 281 exam.", "word_count": 524, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "af1b78c4-cf50-5e4f-b3e5-ec9f23757c5d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1036, "real_page_number": null, "text": "1024\nChapter 26. Computational Geometry\n26.10\nSegment Trees (✽)\n¸ 26.10.1\n(✽)Segment Trees in One Dimension\nAnother tree data structure that is useful for working with ranges is the segment tree, which allows you to efficiently query and update values\nwithin a specific interval of data. Segment trees are useful for solving problems that require you to query some information within a given\nrange of values (such as the sum, or the minimum and maximum value) that may be mutable in between different queries. To understand the\nmotivation behind why a segment tree may be useful, consider the following example:\nExample 26.9 nums.You are given an array of integers Implement a class that can handle the following two updates to this array:\nupdate_index(size_t idx, int32_t new_value), nums[idx] new_value.• which updates the value at to\nget_min_value_in_range(size_t left, size_t right),• which returns the minimum value within the index range\nleft right,from to inclusive.\n[4, 7, -13, 19, -12, -14, 1, 10]. get_min_value_in_range(2, 4)Example: Suppose you are given the array Calling\n-13, [-13, 19, -12]).would return which is the smallest value within the range from index 2 to 4 (or However, if you were to then call\nupdate_index(3, -15), 19 -15,the value at index 3 would change from to which would mean that the minimum value in the range\n-15 -13.[2, 4] would now become instead of\nA simple approach toward solving this problem would be to store the values in an array and update the values directly in time wheneverΘ(1)\nupdate_index() is called. However, this would mean that queries to find the minimum value in a range would each need to take time,Θ(𝑛)\nsince you would have to perform a linear pass to find the minimum value with every query (recomputation is necessary because values may be\nchanged between queries). A similar problem also applies if you try to cheapen the cost of queries: you could try to precompute the minimum\nvalues of all ranges so that queries can take time, but this would cause updates to take time since you would also need to perform aΘ(𝑛)Θ(1)\nlinear pass to update your precomputed values. This is where the segment tree comes into play: we can use this data structure to ensure that\nupdates and queries both take time. To construct a segment tree, we will need to identify the following two things:Θ(log(𝑛))\n1. The that should be stored at each node of the segment tree. This is typically the value that we want to query for.value\n2. The that will be used to merge two siblings in the segment tree together. This is used if a query specifies a range thatmerge operation\nspans multiple nodes of the segment tree. We will discuss this process in more detail later.\nIn this example, the value we want to query for is the minimum value of a range. To construct our segment tree, we will start by computing the\nminimum value of the entire array, from index 0 to 𝑛−1. This is inserted as the root of the segment tree (note that only -14 is actually inserted\ninto the tree; [0, 7] just indicates that this node represents the minimum value between indices 0 and 7 to make understanding the tree easier).\n[0,7]\n-14\nNext, we will split the array into two halves consisting of the index ranges of [0, 𝑛∕2−1] and [𝑛∕2, 𝑛−1]. The minimum value is computed for\neach of these ranges and inserted as the left and right subtrees of the root.\n[0,7]\n-14\n[4,7]\n-14\n[0,3]\n-13\nThis process of halving the index ranges continues until all the ranges only contain a single value, each of which comprises a leaf of the segment\ntree. The final segment tree created from the previous example is shown below:\n[0,7]\n-14\n[4,7]\n-14\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\nNotice that the first level has a single node, the second level contains two nodes, the third level contains four nodes, and so on, until the number\nof nodes in a level reaches 𝑛, the original size of the array. Therefore, we can conclude that the size of a segment tree for 𝑛values is bounded\nabove by 4𝑛using the following sum (this inequality was determined using the sum of a finite geometric series):\n1+2+4+…+2⌈log2(𝑛)⌉<2⌈log2(𝑛)⌉+1 <4𝑛\nTo implement our segment tree, we will also need to devise a method that can be used to merge two nodes in the segment tree together. This is\nneeded if a query requires information from multiple nodes in our segment tree (e.g., a query for the interval [2, 4] would need to combine the\ninformation stored in the nodes of [2, 3] and [4, 4]). For our example, our merge operation just needs to take the minimum value of the two\nnodes we want to combine (e.g., the solution for [2, 4] is the minimum of the solution for [2, 3] and [4, 4]).", "word_count": 871, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "997daad6-1b1c-56cd-a9a8-8e7716770174", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1037, "real_page_number": null, "text": "26.10 Segment Trees\n1025\nWith this information, we are now able to query the minimum value of any range in worst-case time. If we are given any index range,Θ(log(𝑛))\nthere are three outcomes that can happen:\n1. The range matches the segment of a node in the segment tree. In this case, you can just return the value associated with this node.\n2. The range falls entirely within the domain of either the left or right child (i.e., in the left or right half of values). In this case, recurse into\nthe child that covers the range.\n3. The range is split across both the left and right children. In this case, you will have to make two recursive calls: one into the left child\nwith its portion of the range, and one into the right child with its portion of the range. Then, combine the two solutions together to get the\nsolution for the original range.\nFor example, consider what happens when we make a query to find the minimum value within the index range [2, 4]. We start at the root of the\ntree and see which side the range [2, 4] belongs to. Because the range [2, 4] is split between both the left and right children, we will perform two\nrecursive calls, one on the left child, and one on the right child.\n[0,7]\n-14\n[4,7]\n-14\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\nLet’s first consider the recursive call on the left child, which stores the range of [0, 3]. The portion of our original range that falls in the left half\nis [2, 3], which falls entirely in the right half of the range [0, 3]. Thus, we will perform a recursive call on the right child with this range of [2, 3].\n[0,7]\n-14\n[4,7]\n-14\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\nWe have encountered a node associated with the interval range of [2, 3], which matches the range that we were looking for. Therefore, the value\nof this node, -13, is the solution of the original recursive call on the root’s left child.\n[0,7]\n-14\n[4,7]\n-14\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\nWe can repeat this process for the right subtree, as shown. The solution of this recursive call is -12.\n[0,7]\n-14\n[4,7]\n-14\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\nThe minimum sum in the range [2, 3] is -13, and the minimum sum in the range [4, 4] is -12. We can combine these two solutions to get the\nminimum sum in the desired range of [2, 4], which is min(-13, -12) = -13.", "word_count": 575, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "740b7e77-355d-5c97-b3e7-d9511273f022", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1038, "real_page_number": null, "text": "1026\nChapter 26. Computational Geometry\nUpdating a value in the array also takes time in a segment tree. This is because any element in the original data is only associatedΘ(log(𝑛))\nwith a single node at each level of the tree. Since there are only levels in the tree, each update would only need to changeΘ(log(𝑛)) Θ(log(𝑛))\nnodes in the tree, which takes time if each update takes constant time. An example is shown for the given example: if we change theΘ(log(𝑛))\n19 -19,value at index 3 from to only the following nodes of the segment tree need to be updated:\n[0,7]\n-19\n[4,7]\n-14\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-19\n[2,3]\n-19\n[3,3]\n-19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\nNow that we have the basic structure of how our segment tree can be used to solve the problem, we can begin implementing it in code. To\ninitially build our tree, we can use recursion to construct the values of the segment tree in a postorder fashion. This is done by recursively\nbuilding the values of a node’s two children and then merging the results together to get the value of the current node. An example is shown\nbelow (note that we store our segment tree as a binary tree that is represented as an 1-indexed array, which allows us to access the left and right\nchildren of the node at index 𝑖using index 2𝑖and 2𝑖+1, respectively — see section 18.2.1 for more detail on this procedure).\n1\nclass SegmentTree {\n2\nprivate:\n3\nstd::vector<int32_t> segment_tree;\n// segment tree represented using an array (1-indexed)\n4\nstd::vector<int32_t> values;\n// the values we want to query range data from\n5\n6\n// helper function that can be used to construct a segment tree node at curr_idx of the array\n7\nvoid build_segment_tree(size_t size_t size_tcurr_idx, left, right) {\n8\nif (left == right) {\n// base case of single index in range (leaf node)\n9\nsegment_tree[curr_idx] = values[left];\n10\n} // if\n11\nelse {\n12\nsize_t middle = left + (right - left) / 2;\n13\n// recursively build left subtree (left child of idx is located at 2 idx)*\n14\nbuild_segment_tree(2 curr_idx, left, middle);*\n15\n// recursively build right subtree (right child of idx is located at 2 idx + 1)*\n16\nbuild_segment_tree(2 curr_idx + 1, middle + 1, right);*\n17\n// combine the solutions to get the value of the node at idx\n18\nsegment_tree[curr_idx] = std::min(segment_tree[2 curr_idx], segment_tree[2 curr_idx + 1]);* *\n19\n} // else\n20\n} // build_segment_tree()\n21\n22\npublic:\n23\n// Segment tree ctor, the segment tree array is initialized to a size of 4n since this is the\n24\n// maximum potential size needed (the vector is also initialized to the largest possible\n25\n// integer since we do not want unused positions to interfere in finding the smallest value)\n26\nSegmentTree(const std::vector<int32_t>& nums)\n27\nstd::numeric_limits<int32_t>::max()),: segment_tree(4 nums.size(), values(nums) {*\n28\n// 1 is passed in as the initial vertex since that is index of the root, which we start from\n29\nbuild_segment_tree(1, 0, nums.size() - 1);\n30\n} // SegmentTree()\n31\n};", "word_count": 556, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "5dead0cd-2cd4-5d48-9208-846202c66233", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1039, "real_page_number": null, "text": "26.10 Segment Trees\n1027\nget_min_value_in_range(),To implement we will use the input range to decide which side of the tree to recurse into. If the input\nrange matches exactly with a range in our segment tree, we will return its value directly. Otherwise, we will search in the left and/or right\nsubtrees to get the solution(s), combining them if necessary. One possible implementation of this method is shown below:\n1\nclass SegmentTree {\n2\nprivate:\n3\n/* ... other members same as before ... */\n4\n5\n// helper function, seg_left and seg_right represent the boundaries of the current segment\n6\n// tree node, and query_left and query_right represent the boundaries of the current query\n7\nint32_t get_min_value_helper(size_t size_t size_tcurr_idx, seg_left, seg_right,\n8\nsize_t size_tquery_left, query_right) {\n9\n// query range matches range of segment tree node, so return the node's value\n10\nif (query_left == seg_left && query_right == seg_right) {\n11\nreturn segment_tree[curr_idx];\n12\n} // if\n13\nsize_t seg_middle = seg_left + (seg_right - seg_left) / 2;\n14\n// query entirely in left side, so recurse into left child\n15\nif (query_right <= seg_middle) {\n16\nreturn get_min_value_helper(2 curr_idx, seg_left, seg_middle, query_left, query_right);*\n17\n} // if\n18\n// query entirely in right side, so recurse into right child\n19\nif (query_left > seg_middle) {\n20\nreturn get_min_value_helper(2 curr_idx + 1, seg_middle + 1, seg_right,*\n21\nquery_left, query_right);\n22\n} // if\n23\n// else recurse into both and take the minimum (i.e., merge the two results together)\n24\nreturn std::min(\n25\nget_min_value_helper(2 curr_idx, seg_left, seg_middle, query_left, seg_middle),*\n26\nget_min_value_helper(2 curr_idx + 1, seg_middle + 1, seg_right, seg_middle + 1, query_right)*\n27\n);\n28\n} // get_min_value_helper()\n29\n30\npublic:\n31\n/* ... other members same as before ... */\n32\nint32_t get_min_value_in_range(size_t size_tleft, right) {\n33\nreturn get_min_value_helper(1, 0, values.size() - 1, left, right);\n34\n} // get_min_value_in_range()\n35\n};\nTo update a value at a specific index, we will recurse down the branch of the segment tree that contains the index we want to update. This can\nalso be done in a postorder fashion, as shown in the implementation below:\n1\nclass SegmentTree {\n2\nprivate:\n3\n/* ... other members same as before ... */\n4\nvoid update_index_helper(size_t size_t size_tcurr_idx, seg_left, seg_right,\n5\nsize_t int32_tidx_to_update, new_value) {\n6\nif (seg_left == seg_right) {\n// base case of single index in range (leaf node)\n7\nsegment_tree[curr_idx] = new_value;\n8\n} // if\n9\nelse {\n10\nsize_t seg_middle = seg_left + (seg_right - seg_left) / 2;\n11\nif (idx_to_update <= seg_middle) {\n12\n// update left child\n13\nupdate_index_helper(2 curr_idx, seg_left, seg_middle, idx_to_update, new_value);*\n14\n} // if\n15\nelse {\n16\n// update right child\n17\nupdate_index_helper(2 curr_idx + 1, seg_middle + 1, seg_right, idx_to_update, new_value);*\n18\n} // else\n19\n// combine solutions\n20\nsegment_tree[curr_idx] = std::min(segment_tree[2 curr_idx], segment_tree[2 curr_idx + 1]);* *\n21\n} // else\n22\n} // update_index_helper()\n23\n24\npublic:\n25\n/* ... other members same as before ... */\n26\nvoid update_index(size_t int32_tidx, new_value) {\n27\nupdate_index_helper(1, 0, values.size() - 1, idx, new_value);\n28\n} // update_index()\n29\n};\nThis concludes our implementation of the problem. By using a segment tree, we can ensure that values can be updated and that the minimum\nvalue can be queried from any range in worst-case time! This is better than the alternative options of recomputing the minimum valueΘ(log(𝑛))\nduring every query or during every update, both of which involve an operation that takes time.Θ(𝑛)", "word_count": 598, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b081d1eb-6138-570a-b8fd-b5c9a376f709", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1040, "real_page_number": null, "text": "1028\nChapter 26. Computational Geometry\nExample 26.10 In finance, an is a contract that gives its owner the right to buy or sell a quantity of an underlying asset at a specificoption\nprice on or before a specified date. This date is known as the option contract’s date, and the distance to expiration plays a role inexpiration\ndetermining that option’s value and overall risk. For this problem, implement a class that can be used to efficiently retrieve a trader’s total\nposition on an asset’s options that expire within a given time range (note that there are multiple types of options contracts in real life, but for\nthe sake of simplicity, we will consider all options together in this problem). An outline of this class is shown below:\n1\nclass OptionsExpirationManager {\n2\nprivate:\n3\nint32_t farthest_expiration;\n4\n// TODO: Add any data structures here!\n5\npublic:\n6\n// Constructor takes in the number of days until the farthest expiration\n7\nOptionsExpirationManager(int32_t farthest_expiration_in)\n8\n: farthest_expiration{farthest_expiration_in} {}\n9\n10\n// This method takes in an asset name, the number of days until that option expires\n11\n// (<= farthest_expiration), and an integer quantity (can be negative if options are\n12\n// sold), and places a trade for 'quantity' options with that asset name and expiration\n13\nvoid make_trade(const int32_t int32_tstd::string& asset_name, days_to_exp, quantity);\n14\n15\n// This method takes in an asset name and two integers, and returns\n16\n// the total position of all assets with expiration in [start, end] days\n17\nint32_t get_position(const int32_t int32_tstd::string& asset_name, start, end);\n18\n};\nExample: Consider the following sequence of operations, where the farthest expiration occurs 7 days from now:\nmake_trade(\"QQQ\", 4, 100):• makes a trade for 100 QQQ options that expire in 4 days.\nmake_trade(\"QQQ\", 2, -200):• makes a trade for -200 QQQ options that expire in 2 days.\nmake_trade(\"QQQ\", 5, 300):• makes a trade for 300 QQQ options that expire in 5 days.\nmake_trade(\"QQQ\", 6, -100):• makes a trade for -100 QQQ options that expire in 6 days.\nget_position(\"QQQ\", 3, 6)At this point, calling would return the total number of options in the position that expire between 3\nand 6 days from now, inclusive, which would be 100 + 300 - 100 = 300.\nThen, if we make the following trade:\nmake_trade(\"QQQ\", 5, -500):• makes a trade for -500 QQQ options that expire in 5 days.\nget_position(\"QQQ\", 3, 6)ourtotalpositioninoptionsthatexpirein5daysdropsfrom300to-200. Thus,anysubsequentcallsto\nwould return a new position of 100 - 200 - 100 = -200.\nSimilar to before, this problem requires us to efficiently query information about a range of data that may be modified. However, unlike the\nprevious problem, this question involves querying the of a range of data rather than the minimum value. Therefore, each node of oursum\nsegment tree will need to store the sum of a given range instead of its minimum value, and the merge operation will have to sum up the values of\nthe two nodes we want to combine. An illustration of the segment tree constructed using the provided example is shown below (first four trades):\n[0,7]\n100\n[4,7]\n300\n[6,7]\n-100\n[7,7]\n0\n[6,6]\n-100\n[4,5]\n400\n[5,5]\n300\n[4,4]\n100\n[0,3]\n-200\n[2,3]\n-200\n[3,3]\n0\n[2,2]\n-200\n[0,1]\n0\n[1,1]\n0\n[0,0]\n0\nmake_trade()We will start our implementation with the function. When a trade is made on a specific expiration, we recurse down the\nbranch of the segment tree that contains the expiration we want to update and add the new quantity to the existing position. The following\nillustrates what happens when we make a -500 trade on QQQ options that expire in 5 days:\n[0,7]\n-400\n[4,7]\n-200\n[6,7]\n-100\n[7,7]\n0\n[6,6]\n-100\n[4,5]\n-100\n[5,5]\n-200\n[4,4]\n100\n[0,3]\n-200\n[2,3]\n-200\n[3,3]\n0\n[2,2]\n-200\n[0,1]\n0\n[1,1]\n0\n[0,0]\n0", "word_count": 693, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a6e40ebd-5454-5c1c-b5e4-de36abf984fa", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1041, "real_page_number": null, "text": "26.10 Segment Trees\n1029\nThis behavior is implemented below (note that this is very similar to our implementation in the previous example with the minimum value):\n1\nclass OptionsExpirationManager {\n2\nprivate:\n3\nint32_t farthest_expiration;\n4\n// each asset name needs its own segment tree, so we will use an unordered map\n5\nstd::vector<int32_t>>std::unordered_map<std::string, segment_trees_by_asset;\n6\n7\nvoid trade_helper(const size_t size_tstd::string& asset_name, curr_idx, seg_left,\n8\nsize_t size_t int32_tseg_right, idx_to_update, quantity) {\n9\n// initialize segment tree if no trades have been made for the asset yet\n10\nauto segment_tree_it = segment_trees_by_asset.find(asset_name);\n11\nif (segment_tree_it == segment_trees_by_asset.end()) {\n12\nsegment_trees_by_asset[asset_name].resize(4 farthest_expiration);*\n13\n} // if\n14\nauto& segment_tree = segment_trees_by_asset[asset_name];\n15\nif (seg_left == seg_right) {\n16\nsegment_tree[curr_idx] += quantity;\n17\n} // if\n18\nelse {\n19\nsize_t seg_middle = seg_left + (seg_right - seg_left) / 2;\n20\nif (idx_to_update <= seg_middle) {\n21\ntrade_helper(asset_name, 2 curr_idx, seg_left, seg_middle, idx_to_update, quantity);*\n22\n} // if\n23\nelse {\n24\ntrade_helper(asset_name, 2 curr_idx + 1, seg_middle + 1,*\n25\nseg_right, idx_to_update, quantity);\n26\n} // else\n27\n// combine solutions\n28\nsegment_tree[curr_idx] = segment_tree[2 curr_idx] + segment_tree[2 curr_idx + 1];* *\n29\n} // else\n30\n} // trade_helper()\n31\n32\npublic:\n33\n// Constructor takes in the number of days until the farthest expiration\n34\nOptionsExpirationManager(int32_t farthest_expiration_in)\n35\n: farthest_expiration(farthest_expiration_in) {}\n36\n37\nvoid make_trade(const int32_t int32_tstd::string& asset_name, days_to_exp, quantity) {\n38\ntrade_helper(asset_name, 1, 0, farthest_expiration - 1, days_to_exp, quantity);\n39\n} // make_trade()\n40\n};\nget_position() get_min_value_in_range()The function can be implemented similarly to the function in the previous example.\nWe will use the input range to determine which side of the tree to recurse into, and if the input range spans multiple intervals, we will recurse\ninto each of those intervals and combine their solutions. This is implemented below:\n1\nclass OptionsExpirationManager {\n2\nprivate:\n3\nint32_t farthest_expiration;\n4\n// each asset name needs its own segment tree, so we will use an unordered map\n5\nstd::vector<int32_t>>std::unordered_map<std::string, segment_trees_by_asset;\n6\n7\nint32_t position_helper(const size_t size_tstd::string& asset_name, curr_idx, seg_left,\n8\nsize_t size_t size_tseg_right, query_left, query_right) {\n9\n// return 0 if no trades have been made for the asset yet\n10\nauto segment_tree_it = segment_trees_by_asset.find(asset_name);\n11\nif (segment_tree_it == segment_trees_by_asset.end()) {\n12\nreturn 0;\n13\n} // if\n14\nif (query_left == seg_left && query_right == seg_right) {\n15\nreturn segment_tree_it->second[curr_idx];\n16\n} // if\n17\nsize_t seg_middle = seg_left + (seg_right - seg_left) / 2;\n18\nif (query_right <= seg_middle) {\n19\nreturn position_helper(asset_name, 2 curr_idx, seg_left,*\n20\nseg_middle, query_left, query_right);\n21\n} // if\n22\nif (query_left > seg_middle) {\n23\nreturn position_helper(asset_name, 2 curr_idx + 1, seg_middle + 1,*\n24\nseg_right, query_left, query_right);\n25\n} // if\n26\nreturn position_helper(asset_name, 2 curr_idx, seg_left, seg_middle, query_left, seg_middle) +*\n27\nposition_helper(asset_name, 2 curr_idx + 1, seg_middle + 1,*\n28\nseg_right, seg_middle + 1, query_right);\n29\n} // position_helper()\n30\n31\n/* ... continued on next page ... */", "word_count": 504, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "954cec19-702e-59b2-a437-77513743bb33", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1042, "real_page_number": null, "text": "1030\nChapter 26. Computational Geometry\n32\npublic:\n33\n// Constructor takes in the number of days until the farthest expiration\n34\nOptionsExpirationManager(int32_t farthest_expiration_in)\n35\n: farthest_expiration(farthest_expiration_in) {}\n36\n37\nint32_t get_position(const int32_t int32_tstd::string& asset_name, start, end) {\n38\nreturn position_helper(asset_name, 1, 0, farthest_expiration - 1, start, end);\n39\n} // get_position()\n40\n};\nmake_trade() get_position()With the help of a segment tree, both and are able to run in worst-case time, where 𝑛is theΘ(log(𝑛))\nnumber of days until the farthest expiration.\n¸ 26.10.2\n(✽)Lazy Propagation Segment Trees\nIn the previous section, we discussed a segment tree implementation that can support efficient updates to a single value. However, what\nif we wanted to update an entire range of values (for example, add 10 to every value in the range from index 1 to 5)? Using the previous\nimplementation, we would need to call the update function multiple times, once for every index that needs to be updated. Given 𝑛values to\nupdate, this would take time. It turns out that this is not the most efficient way to do things: we can speed up range updates toΘ(𝑛log(𝑛))\ntime by using a strategy known as lazy propagation, which postpones value updates until they are actually required. To illustrate howΘ(log(𝑛))\n[4, 7, -13, 19, -12, -14, 1, 10],this works, consider our first example of where our segment tree stores the minimum in a range.\n[0,7]\n-14\n[4,7]\n-14\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\nSuppose we wanted to increment all the values in the index range [4, 7] by 5. Without lazy propagation, our segment tree would look this after\nthe update, which would take time. A lot of this work, however, can be avoided.Θ(𝑛log(𝑛))\n[0,7]\n-13\n[4,7]\n-9\n[6,7]\n6\n[7,7]\n15\n[6,6]\n6\n[4,5]\n-9\n[5,5]\n-9\n[4,4]\n-7\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\nWith lazy propagation, we only update a node if we need to use its value during a query. In our example, we only need to reference the values up\nto the [4, 7] node in the right subtree, so we do not need to recurse any further into its children. Instead, we mark them with a \"+5\" value to\nindicate to future queries that an increment of 5 should be applied if the value of those nodes are ever needed. By using this strategy, we are able\nto drop the time complexity of each range update from to Θ(log(𝑛)), as the number of updates required is now proportional to theΘ(𝑛log(𝑛)\nheight of the segment tree.\n[0,7]\n-13\n[4,7]\n-9\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\n+5\n+5", "word_count": 531, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c2d7614a-6e64-5e9e-b010-2c8661b44cc2", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1043, "real_page_number": null, "text": "26.10 Segment Trees\n1031\nIncrements that have not yet been applied to the tree are additive. For instance, if the interval range [4, 7] were incremented by 1 after the\nprevious +5 update, the children of [4, 7] would have their values updated from +5 to +6.\n[0,7]\n-13\n[4,7]\n-8\n[6,7]\n1\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-14\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\n+6\n+6\nWheneverweneedtousethevalueofanodeduringaquery, wemustmakesuretopropagateanychangesthatmaystillbepending. Forinstance,\nsuppose we make a query to increment each value in the range [6, 7] by 3. When recursing into the segment tree, we end up encountering the [6,\n7] interval, which still has a pending update of +6. Thus, we will need to apply the +6 in addition to the +3 requested in the current query, for a\ntotal of +9. Since we returned the value in the [6, 7] node, we do not need to further recurse into any of its children, so we will assign each child\nwith a \"+9\" value to reflect changes that will need to be made in the future. (Notice that the node associated with the [4, 5] interval ended up\nbeing updated as well, since its value was needed to recompute the [4, 7] node after the [6, 7] node was modified. Its children, however, do not\noverlap with our initial query of [4, 7], so we do not need to update them immediately.)\n[0,7]\n-13\n[4,7]\n-8\n[6,7]\n10\n[7,7]\n10\n[6,6]\n1\n[4,5]\n-8\n[5,5]\n-14\n[4,4]\n-12\n[0,3]\n-13\n[2,3]\n-13\n[3,3]\n19\n[2,2]\n-13\n[0,1]\n4\n[1,1]\n7\n[0,0]\n4\n+9\n+9\n+6\n+6\nLazy propagation segment trees are implemented similarly to the standard segment trees we introduced previously. However, in addition to\nan array of size 4𝑛to represent the segment tree, we also keep track of a separate array of size 4𝑛to store pending updates (for instance, any\npending updates to the node at index 5 of the segment tree array is also stored at index 5 of this additional array). Then, in our query and update\nfunctions, we must look into this additional array and apply any pending changes before we use the value of any node in our segment tree.\n¸ 26.10.3\n(✽)Segment Trees in Multiple Dimensions\nSegment trees can also be used to represent data in multiple dimensions. This can be done using a segment tree of segment trees, one for each\ndimension of the data. For example, consider the following matrix, for which we want to support efficient sum queries and updates to any given\nsubmatrix (such as the one outlined below, for which a query would return a sum of 7 + 2 + 1 + 3 = 13).\n1\n3\n6\n9\n4\n7\n2\n5\n8\n1\n3\n4\n6\n5\n8\n7\nTo build a two-dimensional segment tree, we start by constructing one-dimensional segment trees on one of the dimensions while keeping the\nother dimension fixed. For our example, we will keep 𝑦fixed and build a segment tree for each value of 𝑥(i.e., for each row of the matrix). The\n([6, 5, 8, 7]),following is the segment tree for the row where each node stores the sum of an interval of 𝑥:𝑦=0\n𝑥∈[0,3]\n26\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n7\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n5\n𝑥∈[0,0]\n6", "word_count": 624, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bbfbb9e9-91f9-544c-91c1-e71e30f9c5f0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1044, "real_page_number": null, "text": "1032\nChapter 26. Computational Geometry\n([8, 1, 3, 4]):The following is the segment tree for the row 𝑦=1\n𝑥∈[0,3]\n16\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n4\n𝑥∈[2,2]\n3\n𝑥∈[0,1]\n9\n𝑥∈[1,1]\n1\n𝑥∈[0,0]\n8\n([4, 7, 2, 5]):The following is the segment tree for the row 𝑦=2\n𝑥∈[0,3]\n18\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n5\n𝑥∈[2,2]\n2\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n7\n𝑥∈[0,0]\n4\n([1, 3, 6, 9]):The following is the segment tree for the row 𝑦=3\n𝑥∈[0,3]\n19\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n9\n𝑥∈[2,2]\n6\n𝑥∈[0,1]\n4\n𝑥∈[1,1]\n3\n𝑥∈[0,0]\n1\nThen, we will merge these segment trees together to get new segment trees that correspond to intervals of the dimension we fixed (in this case,\n𝑦). For instance, merging together the segment trees for and would give us a segment tree for the interval 𝑦∈[0,1].𝑦= 𝑦=0 1\n𝑥∈[0,3]\n26\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n7\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n5\n𝑥∈[0,0]\n6\n𝑦∈[0, 0]\n𝑥∈[0,3]\n16\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n4\n𝑥∈[2,2]\n3\n𝑥∈[0,1]\n9\n𝑥∈[1,1]\n1\n𝑥∈[0,0]\n8\n𝑦∈[1, 1]\n𝑥∈[0,3]\n42\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n11\n𝑥∈[2,2]\n11\n𝑥∈[0,1]\n20\n𝑥∈[1,1]\n6\n𝑥∈[0,0]\n14\n𝑦∈[0, 1]", "word_count": 231, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3f88b7d2-4918-536f-a523-b3b95c702dff", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1045, "real_page_number": null, "text": "26.10 Segment Trees\n1033\nSimilarly, merging together the segment trees for and would give us a segment tree for the interval 𝑦∈[2, 3].𝑦= 𝑦=2 3\n𝑥∈[0,3]\n18\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n5\n𝑥∈[2,2]\n2\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n7\n𝑥∈[0,0]\n4\n𝑦∈[2, 2]\n𝑥∈[0,3]\n19\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n9\n𝑥∈[2,2]\n6\n𝑥∈[0,1]\n4\n𝑥∈[1,1]\n3\n𝑥∈[0,0]\n1\n𝑦∈[3, 3]\n𝑥∈[0,3]\n37\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n14\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n15\n𝑥∈[1,1]\n10\n𝑥∈[0,0]\n5\n𝑦∈[2, 3]\nWe can then merge together the segment trees for and to get a combined segment tree for 𝑦∈[0,3], as shown.𝑦∈[0,1] 𝑦∈[2,3]\n𝑥∈[0,3]\n42\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n11\n𝑥∈[2,2]\n11\n𝑥∈[0,1]\n20\n𝑥∈[1,1]\n6\n𝑥∈[0,0]\n14\n𝑦∈[0, 1]\n𝑥∈[0,3]\n37\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n14\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n15\n𝑥∈[1,1]\n10\n𝑥∈[0,0]\n5\n𝑦∈[2, 3]\n𝑥∈[0,3]\n79\n𝑥∈[2,3]\n44\n𝑥∈[3,3]\n25\n𝑥∈[2,2]\n19\n𝑥∈[0,1]\n35\n𝑥∈[1,1]\n16\n𝑥∈[0,0]\n19\n𝑦∈[0, 3]", "word_count": 187, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "a9b849a6-8e4f-5e20-bc12-7403d73351c0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1046, "real_page_number": null, "text": "1034\nChapter 26. Computational Geometry\nThis gives us our final segment tree for the entire two-dimensional matrix, which is shown below:\n𝑥∈[0,3]\n26\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n7\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n5\n𝑥∈[0,0]\n6\n𝑦∈[0,0]\n𝑥∈[0,3]\n16\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n4\n𝑥∈[2,2]\n3\n𝑥∈[0,1]\n9\n𝑥∈[1,1]\n1\n𝑥∈[0,0]\n8\n𝑦∈[1,1]\n𝑥∈[0,3]\n18\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n5\n𝑥∈[2,2]\n2\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n7\n𝑥∈[0,0]\n4\n𝑦∈[2,2]\n𝑥∈[0,3]\n19\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n9\n𝑥∈[2,2]\n6\n𝑥∈[0,1]\n4\n𝑥∈[1,1]\n3\n𝑥∈[0,0]\n1\n𝑦∈[3,3]\n𝑥∈[0,3]\n42\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n11\n𝑥∈[2,2]\n11\n𝑥∈[0,1]\n20\n𝑥∈[1,1]\n6\n𝑥∈[0,0]\n14\n𝑦∈[0,1]\n𝑥∈[0,3]\n37\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n14\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n15\n𝑥∈[1,1]\n10\n𝑥∈[0,0]\n5\n𝑦∈[2,3]\n𝑥∈[0,3]\n79\n𝑥∈[2,3]\n44\n𝑥∈[3,3]\n25\n𝑥∈[2,2]\n19\n𝑥∈[0,1]\n35\n𝑥∈[1,1]\n16\n𝑥∈[0,0]\n19\n𝑦∈[0,3]\nThis segment tree can now be used to efficiently query the sum of any region of our original matrix. To illustrate, let us consider an example of\nsumming the region spanning and 𝑦∈[1,3]:𝑥∈[0,2]\n1\n3\n6\n9\n4\n7\n2\n5\n8\n1\n3\n4\n6\n5\n8\n7\nSince the 𝑦region we want to query spans the intervals of both the left and right subtrees, we will need to(𝑦∈[1,3]) (𝑦∈[0,1]) (𝑦∈[2,3])\nrecurse into both sides of the segment tree to compute our solution. In the left subtree, we end up recursing into the node with interval 𝑦∈[1,1],\nand in the right subtree, we end up recursing into the node with interval 𝑦∈[2,3].\n𝑥∈[0,3]\n26\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n7\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n5\n𝑥∈[0,0]\n6\n𝑦∈[0,0]\n𝑥∈[0,3]\n16\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n4\n𝑥∈[2,2]\n3\n𝑥∈[0,1]\n9\n𝑥∈[1,1]\n1\n𝑥∈[0,0]\n8\n𝑦∈[1,1]\n𝑥∈[0,3]\n18\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n5\n𝑥∈[2,2]\n2\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n7\n𝑥∈[0,0]\n4\n𝑦∈[2,2]\n𝑥∈[0,3]\n19\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n9\n𝑥∈[2,2]\n6\n𝑥∈[0,1]\n4\n𝑥∈[1,1]\n3\n𝑥∈[0,0]\n1\n𝑦∈[3,3]\n𝑥∈[0,3]\n42\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n11\n𝑥∈[2,2]\n11\n𝑥∈[0,1]\n20\n𝑥∈[1,1]\n6\n𝑥∈[0,0]\n14\n𝑦∈[0,1]\n𝑥∈[0,3]\n37\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n14\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n15\n𝑥∈[1,1]\n10\n𝑥∈[0,0]\n5\n𝑦∈[2,3]\n𝑥∈[0,3]\n79\n𝑥∈[2,3]\n44\n𝑥∈[3,3]\n25\n𝑥∈[2,2]\n19\n𝑥∈[0,1]\n35\n𝑥∈[1,1]\n16\n𝑥∈[0,0]\n19\n𝑦∈[0,3]", "word_count": 458, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "22f99322-a239-5d11-9ead-f0af391c506e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1047, "real_page_number": null, "text": "26.10 Segment Trees\n1035\nIn these two smaller segment trees, we will query the sum using the 𝑥dimension of our desired range. Since the 𝑥dimension of our initial\nquery spans the the interval [0, 2], we end up recursing into the node associated with the interval in the left subtree, and the node(𝑥∈[0,1])\nassociated with the interval in the right subtree. For the segment tree node, this gets us a value of 9 + 3 = 12.(𝑥∈[2,2]) 𝑦∈[1,1]\n𝑥∈[0,3]\n16\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n4\n𝑥∈[2,2]\n3\n𝑥∈[0,1]\n9\n𝑥∈[1,1]\n1\n𝑥∈[0,0]\n8\n𝑦∈[1, 1]\nFor the segment tree node, this gets us a value of 15 + 8 = 23.𝑦∈[2,3]\n𝑥∈[0,3]\n37\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n14\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n15\n𝑥∈[1,1]\n10\n𝑥∈[0,0]\n5\n𝑦∈[2, 3]\nCombining these two results together, we get a total sum of 12 + 23 = 35, which is the solution to our initial query.\n𝑥∈[0,3]\n26\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n7\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n5\n𝑥∈[0,0]\n6\n𝑦∈[0,0]\n𝑥∈[0,3]\n16\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n4\n𝑥∈[2,2]\n3\n𝑥∈[0,1]\n9\n𝑥∈[1,1]\n1\n𝑥∈[0,0]\n8\n𝑦∈[1,1]\n𝑥∈[0,3]\n18\n𝑥∈[2,3]\n7\n𝑥∈[3,3]\n5\n𝑥∈[2,2]\n2\n𝑥∈[0,1]\n11\n𝑥∈[1,1]\n7\n𝑥∈[0,0]\n4\n𝑦∈[2,2]\n𝑥∈[0,3]\n19\n𝑥∈[2,3]\n15\n𝑥∈[3,3]\n9\n𝑥∈[2,2]\n6\n𝑥∈[0,1]\n4\n𝑥∈[1,1]\n3\n𝑥∈[0,0]\n1\n𝑦∈[3,3]\n𝑥∈[0,3]\n42\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n11\n𝑥∈[2,2]\n11\n𝑥∈[0,1]\n20\n𝑥∈[1,1]\n6\n𝑥∈[0,0]\n14\n𝑦∈[0,1]\n𝑥∈[0,3]\n37\n𝑥∈[2,3]\n22\n𝑥∈[3,3]\n14\n𝑥∈[2,2]\n8\n𝑥∈[0,1]\n15\n𝑥∈[1,1]\n10\n𝑥∈[0,0]\n5\n𝑦∈[2,3]\n𝑥∈[0,3]\n79\n𝑥∈[2,3]\n44\n𝑥∈[3,3]\n25\n𝑥∈[2,2]\n19\n𝑥∈[0,1]\n35\n𝑥∈[1,1]\n16\n𝑥∈[0,0]\n19\n𝑦∈[0,3]\n12returnscallrecursive\nrecursivecallreturns23\nQueries in this two-dimensional segment tree take time in the worst case, where 𝑚and 𝑛are the dimensions of the data storedΘ(log(𝑚)log(𝑛))\nin the segment tree. This is because a query will need to descend down the outer tree for the first coordinate, and then, for each traversed vertex\nin this outer tree, it may need to descend down its inner tree for the second coordinate. Both of these descents take logarithmic time on the\nnumber of nodes in its tree, which is on the order of 𝑚for one dimension and on the order of 𝑛for the other.\nWhile not discussed in detail here, the time complexity of updating this two-dimensional segment tree is also in the worstΘ(log(𝑚)log(𝑛))\ncase for the same reason. This is because a single update to the tree will only affect nodes whose interval includes the coordinate to be updated,\nand the process of searching for these nodes follows the same method that we used while querying for a value.", "word_count": 501, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9bb8cce2-b671-58e9-88a8-501f179295c9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1048, "real_page_number": null, "text": "1036\nChapter 26. Computational Geometry\n26.11\nImages and Graphics\n¸ 26.11.1\nRaster and Vector Graphics\nIn this final section, we will discuss one of the most prevalent features of our digital landscape: the world of images and graphics. Although this\nconcept may feel a bit disjoint from all of the other topics we have discussed so far, the study of computer graphics is nonetheless one of the\ncore components of computational geometry, and also one of the most relevant in practice. (As a disclaimer, we are not going to delve too deep\ninto the vast scope of computer graphics in this section, as advanced exploration of this field is more fitting for a technical class beyond EECS\n281. Rather, the goal is to provide a brief, high-level overview of how visual data can be represented in two dimensions.)\nThere are two main categories that image files fall into: and graphics. The more common form is raster (bitmap)raster graphics vector\ngraphics, which represent images on a screen using tiny colored pixels that, when combined together, form the basis of more detailed images\nsuch as photographs. The greater the number of pixels involved, the higher the quality of the overall image (and vice versa). A vast quantity of\ngraphics files that we deal with on a daily basis (such as JPEG, PNG, and GIF) use raster graphics to represent their underlying visual content.\nRaster graphics are created using pixels that are arranged to collectively compose an image.\nOn the other hand, vector graphics represent images using a set of fixed points and mathematical equations that define how these points should\nbe connected. The mathematical formulas that comprise a vector file capture most of the details of a vector image, from lines and curves to\nshapes and fills. Other visual features such as colors and line widths may also be tracked alongside these formulas. Because the details of a\nvector image can be generated from its underlying points and formulas, no physical pixels are needed to store a vector image. Vector graphics are\noften used to represent fonts, logos, and an assortment of other digital illustrations that involve geometric shapes and solid, non-blended colors.\nThe shape/outline of\nthe letter is generated\nusing mathematical\nformulas instead of\nindividual pixels.\nVector graphics are created using paths and shapes that are generated using mathematical formulas.\nRaster and vector graphics are two distinct formats that can be used to represent an image, and they have several notable differences. One of\nthe biggest differences between the two formats involves scalability. Because raster images are pixel based, they are not easily scalable, and\nenlarging a raster image may result in substantial quality loss. This is because the pixels in the original image end up being stretched out over a\nlarger area, making the resultant image appear more pixelated and less sharp.", "word_count": 473, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "0b6db929-b2ff-550a-a121-653fd1ea987e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1049, "real_page_number": null, "text": "26.11 Images and Graphics\n1037\nVector graphics, on the contrary, do not deal with pixels and instead compute an image using mathematical formulas and other non-resolution\ndependent information. Thus, when you enlarge a vector image, its features are recomputed to adjust for its new size. This allows the resultant\nimage to maintain the same sharpness prior to the size change. Because of this, vector graphics are often preferred for objects that need to be\neasily scaled without quality loss, such as fonts and logos.\nBecause vector graphics are represented using mathematical formulas, they are more lightweight compared to raster graphics, which may require\na large number of pixels to represent the same image (and thus also more storage). However, raster graphics are more versatile in terms of the\nspectrum of images they can represent. For instance, blended color schemes and gradients are much easier to depict with raster graphics (which\nis why photographs are often stored in a raster format). Additionally, raster files are easier to view and edit compared to vector files, which may\nrequire specialized software to use.\n¸ 26.11.2\nColor Representation in Images\nThere are several ways to represent color in an image, but the most common is the RGB color model. In an RGB image, varying amounts of red\n(R), green (G), and blue (B) are combined to form all the colors in the image. You can essentially think of an RGB image as three versions of the\nsame image — one filtered red, one filtered green, and one filtered blue — that are stacked on top of each other to produce a full color image.\nRemark: Why are red, green, and blue selected as the primary colors in the RGB color model? The answer lies in the physiology of how\nour eyes work. In the retina of our eyes, there are photoreceptor cells (known as cone cells) that allow us to perceive color. There are three\nprimary types of cone cells, each sensitive to a different wavelength of light roughly corresponding to the colors of red, green, and blue. The\ncombination of the light signals received from these cone cells allows our brains to differentiate among a wider variety of colors. Thus, the\nRGB color model essentially applies the process by which our eyes perceive color to represent color on a digital image!\nWhen representing RGB raster images on a computer, each pixel is typically associated with three unsigned 8-bit integers that indicate(𝑅,𝐺,𝐵)\nhow much red, green, and blue should be added to produce its color (note that an unsigned 8-bit integer supports a range from 0 to 255, so a\nvalue of 255 represents the maximum amount of red, green, or blue). For instance, the RGB triplet of (255, 0, 0) represents red, (0, 255, 0)\nrepresents green, and (0, 0, 255) represents blue. In addition, the RGB triplet (0, 0, 0) represents black, and (255, 255, 255) represents white. A\ntable of several important colors in the RGB space is provided below:\nColor\n(R, G, B)\nBlack\n(0,0,0)\nRed\n(255,0,0)\nGreen\n(0,255,0)\nBlue\n(0,0,255)\nYellow\n(255,255,0)\nCyan\n(0,255,255)\nMagenta\n(255,0,255)\nWhite\n(255,255,255)\nRed\nGreen\nBlue\nCyan\nYellow\nMagenta\nIt is common to see the three RGB values of a color represented in the hexadecimal (base-16) notation #RRGGBB, where RR represents the\nblue.8base-16 numeric value for red, GG represents the base-16 numeric value for green, and BB represents the base-16 numeric value for For\nexample, the color crimson has the hexadecimal notation #DC143C. This corresponds to the RGB values (220, 20, 60), since \"DC\" is 220 in\nbase-16, \"14\" is 20 in base-16, and \"3C\" is 60 in base-16.\nRGB is not the only color model that exists. Many other color models can be used, each often serving a different purpose. However, RGB is\nconsidered as one of the go-to standards for color representation due to its relative simplicity and ubiquity compared to these other models.\n¸ 26.11.3\nImage Compression\nImage compression is the process of reducing the size of a digital image, typically for lighter storage or more efficient transmission. There are\ntwo primary classes of image compression: lossless and lossy compression. If lossless compression is applied to an image, the file size is\nreduced, but the original image can be perfectly reconstructed from the compressed file. Because of this, lossless compression is more limited\nwith how much it can actually compress, as it must maintain enough information for a compressed image to be restored to its original quality.\nOn the other hand, if lossy compression is applied, data may be permanently discarded to produce the compressed image. This allows lossy\ncompression to compress more, but at the expense of losing some of the original image data. Examples of lossless and lossy images are pretty\ncommon: for instance, PNG image files utilize lossless compression methods, while JPEG image files utilize lossy compression methods.\nNote that both compression categories have their own use cases, so lossy compression is not always inferior depending on what you want to\naccomplish. If you want a high compression rate and do not need to retain the all of the original image’s data, a lossy compression method may\nbe better. This is why JPEG is a popular method for representing digital photographs: for most high-resolution images, a fair amount of image\ndata can be safely compressed away without a perceivable loss in image quality, and the enhanced compression rate of lossy files typically\nallows JPEG image files to be smaller than their PNG counterparts. On the contrary, images that place a greater importance on smaller details,\nsuch as digital art and images created through graphic design, are more often found in a lossless PNG format rather than a lossy JPEG one.\n8Base-16notationusesthelettersA,B,C,D,E,andFtorepresentthenumbers10,11,12,13,14,and15,respectively.", "word_count": 978, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "011f34d2-1293-5914-bf7b-618c8008fab5", "section_ids": ["400e6ab5-21b1-5550-b977-5104e002e4e1"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1050, "real_page_number": null, "text": "1038\nChapter 26. Computational Geometry\nChapter 26 Practice Exercises\nDisclaimer: These practice questions are not official and may not have been vetted for course material. You may use these for practice, but\nyou should prioritize official resources from current staff for a more accurate depiction of what you need to know for assignments and exams.\n1. Consider the following statements regarding raster and vector graphics. Which of the these statements is/are TRUE?\nI. Images that are created using raster graphics can be easily enlarged without losing quality or detail\nII. Vector graphics are resolution independent, as they are constructed using anchor points that are connected using mathematical\nequations and other non-resolution dependent information\nIII. If a raster image utilizes the RGB color model, each pixel of that image stores three integers in the range [0, 255] that can be used\nto identify its color\nA) I only\nB) III only\nC) I and II only\nD) II and III only\nE) I, II, and III\n2. You are given an array of 𝑛points on an 𝑥𝑦-plane, and you want to find the closest pair of points in the array. Which of the following\nstatements is TRUE?\nΘ(𝑛2)An optimal brute force solution for this problem runs in time, and an optimal divide-and-conquer solution runs inA) Θ(𝑛log(𝑛))\ntime\nΘ(2𝑛)B) An optimal brute force solution for this problem runs in time, and an optimal divide-and-conquer solution runs in Θ(𝑛log(𝑛))\ntime\nΘ(𝑛2)C) An optimal brute force solution for this problem runs in time, and an optimal divide-and-conquer solution runs in Θ(log(𝑛))\ntime\nΘ(2𝑛)D) An optimal brute force solution for this problem runs in time, and an optimal divide-and-conquer solution runs in Θ(log(𝑛))\ntime\nΘ(2𝑛) Θ(𝑛2)E) An optimal brute force solution for this problem runs in time, and an optimal divide-and-conquer solution runs in time\n𝑥≥1 𝑦≥1.3. Youaregiven𝑛linesegments, whoseendpointshaveintegercoordinates where and Thetwoendpointsofanylinesegment(𝑥,𝑦)\nare distinct, and 𝑛is guaranteed to be non-zero. Consider the following function:\n1\nstruct int32_t int32_tPoint { x; y; };\n2\nstruct Segment { Point a; Point b; };\n3\n4\ndouble foo(const std::vector<Segment>& segments) {\n5\nstd::vector<std::pair<double, int32_t>> ends;\n6\nfor (const Segment& s : segments) {\n7\ndouble static_cast<double>(s.a.y) static_cast<double>(s.a.x);slope_a = /\n8\ndouble static_cast<double>(s.b.y) static_cast<double>(s.b.x);slope_b = /\n9\nends.push_back({std::min(slope_a, slope_b), -1});\n10\nends.push_back({std::max(slope_a, slope_b), +1});\n11\n} // for s\n12\nstd::sort(ends.begin(), ends.end());\n13\nint32_t count = 0, best = 0;\n14\ndouble slope = 0;\n15\nfor (const auto& p : ends) {\n16\ncount -= p.second;\n17\nif (count > best) {\n18\nbest = count;\n19\nslope = p.first;\n20\n} // if\n21\n} // for end\n22\nreturn slope;\n23\n} // foo()\nWhat does this function attempt to do, given a vector of line segments?\nA) It returns the slope of the longest line segment in the vector\nB) It returns the slope of a line through the origin that crosses as few line segments as possible\nC) It returns the slope of a line through the origin that crosses as many line segments as possible\nD) It returns the slope of a line that crosses as many segment endpoints as possible\nE) It returns the slope that is shared by the greatest number of line segments in the vector\n4. Given 𝑛line segments and a disk (represented as a center point and a radius) in an 𝑥𝑦-plane, you want to determine whether any two line\nsegments intersect inside of the disk. What is the worst-case time complexity of solving this problem, if you use the most efficient algorithm?\nA) Θ(log(𝑛))\nB) Θ(\n√\n𝑛)\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)", "word_count": 622, "has_chapter": false, "has_section": false, "has_question": true, "has_answer": false, "text_embedding": null}
{"id": "8f830789-b1e5-55d8-a181-130f367e88f5", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1051, "real_page_number": null, "text": "26.11 Images and Graphics\n1039\n5. A horizontal line is projected out from a point 𝑄that lies on the same plane as an unknown polygon 𝑃. If the horizontal line touches 𝑃\nexactly seven times as it travels away from 𝑄, what can you conclude?\nA) Point 𝑄must lie inside the polygon 𝑃\nB) Point 𝑄must lie outside the polygon 𝑃\nC) Point 𝑄must lie on an edge of polygon 𝑃\nD) Either 𝑄must lie inside the polygon 𝑃, or on an edge of polygon 𝑃\nE) You cannot make any of the above conclusions with certainty\n6. Youaregiven𝑛pointswithintegercoordinates, andyouwanttofindtheverticesoftherectangleparalleltothe𝑥-and𝑦-axesthatcontainsall\n𝑛points with the smallest area. Assume that 𝑛is at least 3 and that not all of the points are collinear. What is the worst-case time complexity\nof solving this problem, if you use the most efficient algorithm?\nA) Θ(log(𝑛))\nB) Θ(\n√\n𝑛)\nC) Θ(𝑛)\nD) Θ(𝑛log(𝑛))\nΘ(𝑛2)E)\n7. Which of the these statements is/are TRUE regarding the process of computing the area of a polygon using the Shoelace formula?\nI. Given a polygon with 𝑛points, the Shoelace formula can compute the area of this polygon in timeΘ(𝑛)\nII. While running the Shoelace formula, double-covered trapezoidal regions must be avoided, with explicit checks needed to ensure\nthat no two trapezoids ever cover the same area of the polygon\nIII. The Shoelace formula cannot be used if the polygon does not have an edge that is parallel with either the 𝑥- or 𝑦-axis\nA) I only\nB) II only\nC) I and II only\nD) II and III only\nE) I, II, and III\n8. Which of the following shapes CANNOT have its area computed using the Shoelace formula? Select all that apply.\nA)\n𝑦\n𝑥\nB)\n𝑦\n𝑥\nC)\n𝑦\n𝑥\nD)\n𝑦\n𝑥\nE)\n𝑦\n𝑥\n.x9. You are given an array of points on a Cartesian (x-coordinate, y-coordinate) plane. Each point’s member variable is its x-coordinate, and\n.yits member variable is its y-coordinate. Write a function that counts the number of rectangles that can be formed by these points.unique\nDo not count a rectangle more than once (e.g., a rectangle with points is equivalent to a rectangle with points (𝐷,𝐴,𝐶,𝐵)).(𝐴,𝐵,𝐶,𝐷)\npointsOnly consider rectangles whose sides are parallel to the x-axis and y-axis in your count. All points in the array are unique, but\nthey may be given in any order.\n≠𝑥2 ≠𝑦2.Hint: A rectangle is defined as a quadruple of points where and Rectangles can be(𝑥1,𝑦1),(𝑥1,𝑦2),(𝑥2,𝑦1),(𝑥2,𝑦2) 𝑥1 𝑦1\ndescribed using either two vertical lines or two horizontal lines. When counting rectangles, any pair of vertical lines with the same pair of\n𝑦-coordinates, or any pair of horizontal lines with the same pair of 𝑥-coordinates, forms a rectangle.\nstruct int32_t int32_tPoint { x; y; };\nint32_t count_rectangles(const std::vector<Point>& points);\nExample: Given the following points: and (1,2), you would return𝐴=(1,1),𝐵=(2,1),𝐶=(3,1),𝐷=(3,2),𝐸=(2,3),𝐹=(1,3), 𝐺=\n2, since there are two unique rectangles that can be formed using these points (𝐴𝐵𝐸𝐹and 𝐴𝐶𝐷𝐺).", "word_count": 528, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "6699dc2e-fabe-566e-9dbf-ddfc52198b0c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1052, "real_page_number": null, "text": "1040\nChapter 26. Computational Geometry\n10. You are given two triangles in the form of its coordinate points. Implement a function that can be used to determine if the two triangles can\n1e-9)be transformed to have identical coordinates (in the same order, with decimal precision by using a combination of only the two\noperations below:\n• Translation: moves all three vertices of the triangle by an equal amount\n• rotates the triangle around one of its verticesRotation:\nstruct int32_t int32_tPoint { x; y; };\nstruct Triangle { Point a; Point b; Point c; };\nbool can_transform(const constTriangle& triangle1, Triangle& triangle2);\ntrue,Example 1: Given the following two triangles, you would return since you can transform one to the other using just translation and\nrotation:\ntriangle1: [(1, 2), (5, 2), (5, 5)]•\ntriangle2: [(3, -3), (3, 1), (0, 1)]•\nfalse,Example 2: Given the following two triangles, you would return since you cannot transform one to the other using just translation\nand rotation (you would need a reflection, which is not permitted):\ntriangle1: [(5, 2), (1, 2), (1, 5)]•\ntriangle2: [(3, -3), (3, 1), (0, 1)]•\nfalse,Example 3: Given the following two triangles, you would return since you cannot transform one to the other using just translation\nand rotation (the vertices would never line up):\ntriangle1: [(1, 2), (5, 2), (5, 5)]•\ntriangle2: [(3, 1), (0, 1), (3, -3)]•\n𝑦\n𝑥\n𝐴1\n𝐵1\n𝐶1\n𝐴2\n𝐵2\n𝐶2\nExample 1\n𝑦\n𝑥\n𝐴1\n𝐵1\n𝐶1\n𝐴2\n𝐵2\n𝐶2\nExample 2\n𝑦\n𝑥\n𝐴1\n𝐵1\n𝐶1\n𝐶2\n𝐴2\n𝐵2\nExample 3\nHint: The following formulas may be useful in solving this problem:\n• Distance formula between two points and (𝑥2,𝑦2):(𝑥1,𝑦1) 𝑑=\n√\n−𝑥1)2 −𝑦1)2(𝑥2 +(𝑦2\n1• Area of triangle (with signed orientation information): 𝐴=\n2\n|||||||\n𝑥1\n𝑦1\n1\n𝑥2\n𝑦2\n1\n𝑥3\n𝑦3\n1\n|||||||\n1=\n2\n((𝑥2 −𝑦1))−𝑥1)(𝑦3−𝑦1)−(𝑥3−𝑥1)(𝑦2\nThe year is 2810, and humanity has established a network of space stations scattered across the vast expanse of the galaxy. You are an11.\ninterstellar logistics engineer who is tasked to optimize emergency supply routes. To minimize travel time and fuel, you need to identify the\ntwo closest space stations and compute the minimum distance between any pair of them; this will allow for quick docking and resource\nsharing between these nearest neighbors.\nYou are given an array of space stations, each represented as a point in a 2-D coordinate plane, where the 𝑥- and 𝑦-axes are measured in\nlight-hours away from Earth’s position. Implement the following function, which returns the minimum distance between the two closest\nspace stations. You may assume that no two stations occupy the exact same position.\nstruct int32_t int32_tStation { x; y; };\ndouble min_space_distance(const std::vector<Station>& stations);\nYou should try to solve this problem in time, when given 𝑛space stations as the input.Θ(𝑛log(𝑛))\nExample: Given the following five space station coordinates: (1,3),(4,7),(1,6),(4,1),(6,3), you would return\n√\n≈2.82843, since this is8\nthe smallest distance between any two space stations (in this case, and (6,3)).(4,1)\n𝑦\n𝑥\n√\n8", "word_count": 522, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "80b294ad-df85-5c1e-a7c1-91e21346d275", "section_ids": ["03040db0-8c7a-579f-b3e6-0760934cf89e"], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1053, "real_page_number": null, "text": "26.11 Images and Graphics\n1041\nChapter 26 Exercise Solutions\n1. The correct answer is (D). Statement I is false because that quality exists for vector images and not raster images; instead, raster images\ndeal with pixels and thus may lose quality when the original size is changed. Statement II is true, since vector graphics do not deal with\npixels and instead generate an underlying image using points connected using mathematical formulas and other non-resolution dependent\ninformation. Statement III is true since the RGB color model uses a number between 0 and 255 to determine the mix of red, green, and blue\nthat is used to generate a specific color.\n2. The correct answer is (A). With a brute force approach, you would need to check the distances between every pair of points and take the\nΘ(𝑛2) Θ(𝑛2)minimum; this would take since there are pairs that will need to be checked. With divide-and-conquer, the time complexity of\nsolving this problem can be brought down to Θ(𝑛log(𝑛)), with an explanation of this process covered in section 26.3.2.\n3. The correct answer is (C). This function returns the slope of the line through the origin that crosses as many line segments as possible. On\nlines 7-8, the code computes the range of slopes that a line through the origin would need to cross a given line segment (e.g., if a line with\nslope 5 crossing the origin intersects with one end of the segment, and a line with slope 7 crossing the origin intersects with the other end\nof the segment, then any slope between 5 and 7 through the origin would cross that line segment). Then, it iterates over the slopes and\nincrements a counter whenever it encounters the min slope required to cross a segment, and decrements the counter whenever it encounters\nthe max slope that can cross that segment (which is the purpose of the -1 and +1 values in the pair on lines 9 and 10). If the counter is\nlarger than the best encountered (i.e., more line segments are crossed with the new slope), then we keep track of that slope with a running\nbest value. After iterating over all the slopes, the running best value is returned as the slope through the origin that crosses the most line\nsegments.\n4. The correct answer is (D). This problem can be solved using the sweep-line algorithm, where we scan over the line segments in ascending\norder of 𝑥-coordinate and check for intersections (the same algorithm described on section 26.2.2), with the added check of ensuring that\nthe intersection falls within with a disk (determining whether a point lies within a circle is a constant-time calculation). The time complexity\nof running the sweep-line algorithm optimally is Θ(𝑛log(𝑛)).\n5. The correct answer is (E). Just knowing that 𝑄touches 𝑃seven times is not enough to make any conclusion without performing epsilon\ntests in case 𝑄intersects a vertex of polygon 𝑃. If you wiggle 𝑄up and down by some small distance and the number of intersections\nremains odd, you can then conclude that 𝑄is in the polygon; otherwise, if the number of intersections become even, then you can conclude\nthat 𝑄is outside the polygon.\n6. The correct answer is (C). This is a bounding box problem, and we can solve it by iterating over all the points and keeping track of the\nsmallest and largest values of 𝑥and 𝑦(since we want the resulting rectangle to be parallel to the 𝑥- and 𝑦-axes). This solution would take\ntime given 𝑛points.Θ(𝑛)\n7. The correct answer is (A). Only statement I is true, since the Shoelace formula performs a constant-time calculation for each vertex of the\npolygon, which takes time overall. Statement II is false so no separate check needs to be done for double-covered trapezoidal regions,Θ(𝑛)\nsince they regions automatically cancel each other out during the summation. Statement III is false since the Shoelace formula can find the\narea of any simple polygon, without any restriction on shape or orientation.\n8. The correct answer is (E). The Shoelace formula can be used to find the area of any simple polygon, regardless of position and concavity\n(see section 26.5.3 for a demonstration). The only shape that the Shoelace formula cannot compute the area for is option (E), which is not a\nsimple polygon.\n9. Rectangles can be identified using either two vertical or horizontal lines. Thus, for each vertical line formed by two 𝑦-coordinates, a\nrectangle can be formed if another line that can be created using the same two 𝑦-coordinates is found. To obtain our solution, we can loop\nthrough the points and keep track of the 𝑦-coordinate pairs we encounter and a running total of the number of rectangles that can be created\nusing that pair as the vertical edges of a rectangle. One implementation of this solution is shown below:\n1\nint32_t count_rectangles(const std::vector<Point>& points) {\n2\nstd::map<std::pair<int32_t, int32_t>, int32_t> lines;\n3\nint32_t count = 0;\n4\nfor (const auto& p1 : points) {\n5\nfor (const auto& p2 : points) {\n6\nif (p1.x == p2.x && p1.y < p2.y) {\n7\nstd::pair<int32_t, int32_t> vertical_line{ p1.y, p2.y };\n8\ncount += lines[vertical_line]++;\n9\n} // if\n10\n} // for p2\n11\n} // for p1\n12\nreturn count;\n13\n} // count_rectangles()", "word_count": 884, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": true, "text_embedding": null}
{"id": "11292952-adcd-502d-b9d9-d2a0846276d8", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1054, "real_page_number": null, "text": "1042\nChapter 26. Computational Geometry\n10. To determine if two triangles can be transformed to be identical using only translation and rotation, we can verify that their edge lengths are\nthe same and that their points are listed in the same orientation/winding order (if one triangle has its points in a clockwise direction and the\nother in a counterclockwise direction, a reflection would be needed to transform one to the other). To determine if two triangles have their\ncoordinates in the same orientation, we can simply compute the signed area using the provided equation and check that the two triangle\nareas share the same sign. One implementation of a solution is shown below:\n1\ndouble dist_sq(const constPoint& a, Point& b) {\n2\n// solution can also take sqrt, but it does not make a difference for this problem\n3\nreturn (b.x - a.x) (b.x - a.x) + (b.y - a.y) (b.y - a.y);* *\n4\n} // dist_sq()\n5\n6\ndouble signed_area(const Triangle& t) {\n7\nreturn 0.5 ((t.b.x - t.a.x) (t.c.y - t.a.y) - (t.c.x - t.a.x) (t.b.y - t.a.y));* * *\n8\n} // signed_area()\n9\n10\nbool can_transform(const constTriangle& triangle1, Triangle& triangle2) {\n11\nstd::vector<double> sides1 = {dist_sq(triangle1.a, triangle1.b), dist_sq(triangle1.b, triangle1.c),\n12\ndist_sq(triangle1.c, triangle1.a)};\n13\nstd::vector<double> sides2 = {dist_sq(triangle2.a, triangle2.b), dist_sq(triangle2.b, triangle2.c),\n14\ndist_sq(triangle2.c, triangle2.a)};\n15\nfor (size_t i = 0; i < 3; ++i) {\n16\nif (std::abs(sides1[i] - sides2[i]) > 1e-9) {\n17\nreturn false;\n// check for congruence (equal side lengths)\n18\n} // if\n19\n} // for i\n20\ndouble area1 = signed_area(triangle1);\n21\ndouble area2 = signed_area(triangle2);\n22\nreturn (area1 > 0) == (area2 > 0);\n// check for orientation/winding order\n23\n} // can_transform()\nThis is a closest pair of points problem, which can be solved using the divide-and-conquer approach discussed in section 26.3.2. An11.\nimplementation of this solution is shown below:\n1\ndouble dist(const constStation& a, Station& b) {\n2\nreturn std::sqrt((b.x - a.x) (b.x - a.x) + (b.y - a.y) (b.y - a.y));* *\n3\n} // dist()\n4\n5\nbool compare_x(const const returnStation& s1, Station& s2) { s1.x < s2.x; }\n6\nbool compare_y(const const returnStation& s1, Station& s2) { s1.y < s2.y; }\n7\n8\ndouble doublemin_strip(std::vector<Station>& strip, min_lr) {\n9\ndouble min_dist = min_lr;\n10\nstd::sort(strip.begin(), strip.end(), compare_y);\n11\nfor (size_t i = 0; i < strip.size(); ++i) {\n12\nfor (size_t j = i + 1; j < strip.size(); ++j) {\n13\nif ((strip[j].y - strip[i].y) < min_dist) {\n14\nmin_dist = std::min(min_dist, dist(strip[i], strip[j]));\n15\n} // if\n16\n} // for j\n17\n} // for i\n18\nreturn min_dist;\n19\n} // min_strip()\n20\n21\ndouble min_dist(const size_t size_tstd::vector<Station>& stations, left, right) {\n22\nif (right - left <= 2) {\n// brute force for two or fewer points\n23\ndouble std::numeric_limits<double>::infinity();min_dist =\n24\nfor (size_t i = left; i < right; ++i) {\n25\nfor (size_t j = i + 1; j < right; ++j) {\n26\nmin_dist = std::min(min_dist, dist(stations[i], stations[j]));\n27\n} // for j\n28\n} // for i\n29\nreturn min_dist;\n30\n} // if\n31\nsize_t mid = (left + right) / 2;\n32\ndouble min_left = min_dist(stations, left, mid), min_right = min_dist(stations, mid + 1, right);\n33\ndouble min_lr = std::min(min_left, min_right);\n34\nstd::vector<Station> strip;\n// build the strip of points within a distance of min_lr from mid\n35\nfor (size_t i = left; i < right; ++i) {\n36\nif (std::abs(stations[i].x - stations[mid].x) < min_lr) {\n37\nstrip.push_back(stations[i]);\n38\n} // if\n39\n} // for i\n40\nreturn std::min(min_lr, min_strip(strip, min_lr));\n41\n} // min_dist()\n42\n43\ndouble min_space_distance(const std::vector<Station>& stations) {\n44\nstd::vector<Station> sorted_stations{stations};\n45\nstd::sort(sorted_stations.begin(), sorted_stations.end(), compare_x);\n46\nreturn min_dist(sorted_stations, 0, sorted_stations.size());\n47\n} // min_space_distance()", "word_count": 638, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "c272a89d-0d8b-5823-8369-92c98ee9777f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1055, "real_page_number": null, "text": "Chapter 27\nSmart Pointers and Memory Management\n27.1\nMemory Allocation and Storage Duration Classes (✽)\n¸ 27.1.1\n(✽)The Memory Model\nWhen a process first begins running on your machine, it is assigned memory by the machine’s system, which manages the physicaloperating\nhardware resources that a running process is able to use. This procedure of assigning memory to running processes is fairly complex, especially\nsince hundreds of processes may be running at the same time on a single machine, so we will not discuss it in detail here. Instead, we will focus\non the memory management procedures of a single running process and the mechanisms it can use to allocate memory for its own program data.\nAs we touched upon in the first chapter, every running process on a machine has its own memory layout. This memory layout depicts the\naddress space of that process, which is used to store information the process may need to run. An example of a typical layout is shown below:\nstack\nheap\nstatic\ntext\nhigh addresses\nlow addresses\nEach of these regions correspond to a different method of storage, of which include automatic, static, and dynamic storage. We will go over\neach of these storage categories in greater detail in this section.", "word_count": 209, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "b24a51f8-7a90-5450-9802-2007a9e70d8e", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1056, "real_page_number": null, "text": "1044\nChapter 27. Smart Pointers and Memory Management\n¸ 27.1.2\n(✽)The Stack and Automatic Storage\nThe stack region of the address space is used for automatic storage. Objects that are created in automatic storage are locally scoped — that is,\ntheir lifetime is restricted within the scope they are initialized in. For more on scope, see section 1.7.\nstack\nheap\nstatic\ntext\nAttempting to reference a local variable outside of its scope would result in undefined behavior. The lifetime of objects allocated in automatic\nstorage is maintained by the compiler, so you as the programmer will not need to be responsible for cleaning up the memory allocated for these\nx, y, zobjects. Some example code is shown below; here, and are all initialized on the stack with the following local scopes:\n1\nvoid func(int32_t x) {\n2\nint32_t y = 281;\n3\n{\n4\nint32_t z = x + y;\n5\n}\n// end scope of z\n6\n} // end scope of x and y\n¸ 27.1.3\n(✽)Global Variables and Static Storage\nObjects allocated in static storage may be expected to last throughout the duration of an entire program, and thus should be accessible at any\npoint during a program’s lifetime. Global variables, static member variables, and static local variables are all types of variables that may be\nallocated in static storage. These variables only have a single copy in memory that is referenced by the entire program.\nstack\nheap\nstatic\ntext\nSome example code involving global and static variables is shown below:\n1\nint32_t GLOBAL_VAR = 281;\n2\n3\nstruct Foo {\n4\nstatic const int32_t static_member = 370;\n5\n};\n6\n7\nint32_t increment() {\n8\nstatic int32_t counter = GLOBAL_VAR;\n9\nreturn ++counter;\n10\n} // increment()\n11\n12\nint main() {\n13\nstd::cout << GLOBAL_VAR << '\\n';\n// prints 281\n14\nstd::cout << Foo::static_member << '\\n';\n// prints 370\n15\nstd::cout << increment() << '\\n';\n// prints 282\n16\nstd::cout << increment() << '\\n';\n// prints 283\n17\n} // main()\nGLOBAL_VAR Foo::static_member counterHere, is a global variable, is a static member variable, and is a static local variable. All\nthree of these objects live in the static region of the address space, and they are accessible for the entire duration of the program.\nSimilar to objects in automatic storage, objects in static storage are also managed by the compiler. When the compiler translates your C++\nsource code into a machine-code executable that can be run on a machine, it is able to identify the variables that should be placed in static\nstorage duration. Storage for these variables is then allocated at the start of the program (although the actual initialization of these variables may\nbe deferred to when they are first used, depending on the programming language).", "word_count": 467, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "295d5b30-fc80-5cf5-99f2-3ab120b976c0", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1057, "real_page_number": null, "text": "27.1 Memory Allocation and Storage Duration Classes\n1045\n¸ 27.1.4\n(✽)Linking Global Variables Across Multiple Translation Units\nIf a global variable needs to be referenced for an entire program, what happens if you have multiple implementation files? In C++, each symbol\n(e.g., variable or function name) can be multiple times within its scope, but it can only be once (this is known as thedeclared defined one\n(ODR)). Declaring a symbol is akin to introducing the symbol with bare bones information needed to establish its existence, whiledefinition rule\ndefining a symbol actually specifies instructions on how it can be created. A comparison between the two is provided with the examples below:\nDeclaration:\nDefinition:\nint32_t x;\nint32_t x{281};\nvoid func(int32_t y);\nvoid func(int32_t y) { std::cout << y << '\\n'; }\nstruct Z;\nstruct int32_t doubleZ { z1; z2; };\n.cpp)It may be possible for a program to consist of multiple units, which is a unit that comprises an implementation file (e.g.,translation\n.h, .hpp)along with all the headers (e.g., that it includes directly or indirectly. The compiler compiles each translation unit independently on\nits own, and then links the results together into a single executable that can be run. Most global variables exhibit a property known as external\nby default, in that they can be visible from any translation unit in a program — thus, no two global objects within the same program canlinkage\nshare the same name. If you want a global variable to exhibit (i.e., the variable is only visible within its translation unit orinternal linkage\nstaticimplementation file), you can specify it as to restrict its use to the unit it is declared in (as briefly covered in section 1.8.3).\n.cppThat being said, even though a global variable may be visible to all the implementation files of a program, it can only be defined once\nexterndue to the one definition rule. To satisfy this rule, you can use the keyword to indicate that the definition of a symbol exists in another\ntranslation unit. For non-const global variables, this is done by declaring defining the variable normally in one file, and then declaring theand\nexternvariable in all the other files in the program with the keyword (and no definition). An example is shown below:\nfile1.cpp\n// file1.cpp\nint32_t GLOBAL_VAR = 281;\n// declare and define (defining can only be done once)\n/*\n... other code ...\n*/\nfile2.cpp\n// file2.cpp (another file to be compiled for the same program)\n// cannot redefine GLOBAL_VAR due to ODR, so specify it as extern to let the\n// compiler know that the definition can be found in another translation unit\nextern int32_t GLOBAL_VAR;\n/*\n... other code ...\n*/\nvoid func() {\nstd::cout << GLOBAL_VAR << '\\n';\n// prints 281\n} // func()\nconst constexprThere is one notable exception: objects declared as or exhibit internal linkage by default, so constant global variables can\nconstonly be visible within the translation unit it is defined in. To allow a global variable to exhibit external linkage, you will need to apply\nexternthe keyword to both the definition as well as all declarations in any other files used by the program. An example is shown below:\nfile1.cpp\n// file1.cpp\nextern const int32_t GLOBAL_VAR = 281;\n// declare and define (extern needed since this is const)\n/*\n... other code ...\n*/\nfile2.cpp\n// file2.cpp (another file to be compiled for the same program)\nextern const int32_t GLOBAL_VAR;\n/*\n... other code ...\n*/\nvoid func() {\nstd::cout << GLOBAL_VAR << '\\n';\n// prints 281\n} // func()", "word_count": 606, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "7c4a9022-4c8b-554a-8a24-9bcceb6c67ea", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1058, "real_page_number": null, "text": "1046\nChapter 27. Smart Pointers and Memory Management\nextern, extern \"C\",On the topic of if you ever work with C++ in industry, you may also end up seeing functions that are qualified as such\nas the one shown below:\n1\nextern int\"C\" foo(/* ... args ... */) {\n2\n/*\n3\n... code here ...\n4\n*/\n5\n} // foo()\nThis indicates to the compiler that the function should be compiled and linked using C naming conventions. This is necessary because C and\nC++ compilation is not exactly the same. When compiling and linking C code, each function can be uniquely identified by its name. However,\nC++ cannot do this, as it supports function overloading, where functions with different parameters can share the name. To account for this,same\nC++ compilers use a technique known as to generate a unique identifier symbol for each function, even if multiple functionsname mangling\nextern \"C\"may share the same name. By using to mark a function, you are essentially instructing the compiler to use that function’s name\nas its own identifier symbol without any additional mangling (although this implies that these functions cannot be overloaded).\nThis is not really something you will need to worry about in most cases. That being said, there are scenarios where this keyword can be\nuseful. One such example occurs if you are working on a shared library whose functions can be dynamically loaded at runtime. By defining a\nextern \"C\",function with you prevent the compiler from name mangling the function name during compilation, allowing you to dynamically\nextern \"C\"link and use that function from another executable, regardless of how that executable was compiled. Note that does not imply\nthat the code must be written in C — in fact, it can be used to dynamically link C++ libraries that may have been compiled with separate C++\ncompilers that mangle names in different ways (as the process used for name mangling is not defined by the standard).\nRemark: C++20 introduced the concept of modules, which make it easier to share functionality across different translation units. With\na module, you can compile shared functionality independently from the translation units that need them. By compiling each module as a\nseparate unit, you are then able to import them more easily from other components, reduce compilation times, and avoid many existing\nproblems associated with header files. Each module only needs to be compiled once, and the compiler is able to reuse its binary file wherever\nthat module is imported into a project. A simple example using modules is shown below:\nhelloworld.ixx\n// helloworld.ixx (module file)\n#include <iostream>\nexport module helloworld;\n// module declaration\nexport void print_hello_world() {\nstd::cout << \"Hello World!\\n\";\n} // print_hello_world()\nmain.cpp\n// main.cpp\nimport helloworld;\n// import declaration\nint main() {\nprint_hello_world();\n// prints \"Hello World!\"\n} // main()\nThere is a lot to explore about modules, and we will not be able to cover it all here. You are definitely encouraged to learn more about\nmodules on your own if you are curious about how they work.\n¸ 27.1.5\n(✽)Executable Instructions and Text Storage\nThe text segment is a fixed-size segment of memory that is used to store executable instructions provided by the compiler.\nstack\nheap\nstatic\ntext\nThis section typically contains the compiled machine code, the functions that make up the program, and other read-only instructional data.\nWhen a program begins execution, the contents of this section are read to determine how the program should run.", "word_count": 588, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "955cd616-5a9a-5582-a32e-d0b993fca805", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1059, "real_page_number": null, "text": "27.1 Memory Allocation and Storage Duration Classes\n1047\n¸ 27.1.6\n(✽)The Heap and Dynamic Storage\nFor most of this chapter, we will be primarily concerned with a region of memory known as the heap. This portion of memory is used for\ndynamic storage, where objects can be explicitly created and destroyed by the programmer. An object is allocated in dynamic storage using the\nnew new[] delete delete[]C++ operators of and (for arrays), and it is deallocated using the C++ operators of and (for arrays).\nstack\nheap\nstatic\ntext\nSome example code that uses dynamic memory is shown below:\n1\nint main() {\n2\nint32_t* new int32_t{281};ptr =\n// allocates memory for an integer (281) on the heap\n3\nstd::cout << *ptr << '\\n';\n// prints 281\n4\ndelete ptr;\n// deletes the memory allocated for the integer 281\n5\n6\nint32_t* new int32_t[281];arr =\n// allocates memory for an array of size 281\n7\nfor (size_t i = 0; i < 281; ++i) {\n8\narr[i] = 281;\n// populates the array with the value 281\n9\n} // for i\n10\ndelete[] arr;\n// deletes the memory for the array of size 281\n11\n} // main()\nBy allocating objects in dynamic memory, you ensure that they can still remain valid outside the scope in which they were created (and will\nremain so untilthey are explicitly deleted). In the example below, the object createdby the function canstill be used after the functionexits. This\ncan be useful if you want a function to create objects of multiple derived types based on runtime information (this is known as a pattern).factory\n1\nint32_t* get_number(int32_t number) {\n2\nint32_t* new int32_t{number};ptr =\n3\nreturn ptr;\n4\n} // get_object()\n5\n6\nint main() {\n7\nint32_t* ptr = get_number(281);\n8\nstd::cout << *ptr << '\\n';\n// prints 281\n9\ndelete ptr;\n10\n} // main()\nIn C++, objects allocated in dynamic memory are a frequent source of bugs when they are not handled correctly. Unlike objects with static or\nautomatic storage durations, whose lifetimes are managed by the compiler, objects allocated with dynamic memory must be explicitly managed\nby the programmer. If the programmer fails to properly manage this memory, then issues may arise. Examples of common issues involving\ndynamic memory include memory leaks, double deletions, and dangling pointers.\n¸ 27.1.7\n(✽)Common Issues Involving Dynamic Memory: Memory Leaks\nA memory leak is an error that occurs when a program fails to deallocate memory that it no longer needs. This typically happens when a\npointer to dynamic memory goes out of scope before the memory it pointed to is cleaned up. This memory is thereby orphaned, and there is no\nlonger any way to access it. The orphaned object sits in memory and cannot be cleaned up until the leaking program terminates, at which its\nmemory is reclaimed by the operating system. A simple example of a memory leak is shown in the example code below:\n1\nvoid leaky_func() {\n2\nint32_t* new int32_t{281};ptr =\n3\nstd::cout << *ptr << '\\n';\n4\n} // leaky_func()\n281. deleteIn the above code, dynamic memory is allocated on line 2 to store the integer However, is never called on this memory, so it is\nptrnever cleaned up. When goes out of scope on line 4, access to this dynamic memory is lost and therefore leaked.\nptr\n0x5370\npointer goes out of scope when the function exits...\n...but its dynamic memory has not been cleaned up, and we can no longer access it!\nstack\nheap\n281\n0x5370\nMemory leaks are one of the most common dynamic error bugs, and they can be a nuisance to deal with when they occur. A program exhibiting\na memory leak may run fine for a while, leading to a false sense of security that there are no problems. However, as more and more memory\ngets leaked as time goes by, the amount of available memory decreases, and the program’s performance deteriorates until it eventually crashes.\nLuckily, there are many tools available (such as Valgrind) that allow you to detect memory leaks in your code, and the usage of smart pointers\ncan help you write safer code that is less prone to memory leaks. We will go over smart pointers in a later section of this chapter.", "word_count": 728, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f02d61ee-6b44-5537-be56-354d4222936d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1060, "real_page_number": null, "text": "1048\nChapter 27. Smart Pointers and Memory Management\n¸ 27.1.8\n(✽)Common Issues Involving Dynamic Memory: Double Deletions\nA double delete, also known as a double free, is a dynamic memory error that occurs when a program attempts to delete an object in dynamic\nmemory more than once. An example of a double delete is shown below:\n1\nvoid double_delete() {\n2\nint32_t* new int32_t{281};ptr1 =\n3\nint32_t* ptr2 = ptr1;\n4\ndelete ptr1;\n5\ndelete ptr2;\n6\n} // double_delete()\nptr1 ptr2 ptr1In this code, both and point to the same object in dynamic memory. When is deleted on line 4, the object it points to is\nptr2 ptr2cleaned up and invalidated. However, now points to the deallocated object, so deleting on line 5 tries to delete the same object\nagain. This results in a double delete, which produces undefined behavior.\nptr1\n0x5370\nptr2\n0x5370\nstack\nheap\n281\n0x5370\nptr1delete\nptr1\n0x5370\nptr2\n0x5370\nstack\nheap\n???\n0x5370\nptr2delete (ERROR! Memory is already deleted!)\nptr1\n0x5370\nptr2\n0x5370\nstack\nheap\n???\n0x5370\nDouble deletions can be dangerous if you introduce them in your code. While you may get a runtime crash when they happen, you may also\ncorrupt other memory that may have been allocated at the location of the previously deallocated object. The outcome of a double delete is\nunpredictable because, as mentioned, its behavior is undefined. To avoid double deletions, you must be vigilant when transferring ownership of\nobjects allocated in dynamic memory, so that two entities will not ever delete the same object. This is also something that can be prevented with\nthe usage of smart pointers, as we will see later.\n¸ 27.1.9\n(✽)Common Issues Involving Dynamic Memory: Dangling Pointers\nA dangling pointer is a pointer that points to a dead object after it has already been deleted. If you try to dereference a dangling pointer, you\nwould get undefined behavior. An example of a dangling pointer is shown below:\n1\nvoid dangling_pointer() {\n2\nint32_t* new int32_t{281};ptr =\n3\nstd::cout << *ptr << '\\n';\n// prints 281\n4\ndelete ptr;\n5\nstd::cout << *ptr << '\\n';\n// undefined behavior, ptr is a dangling pointer!\n6\n} // dangling_pointer()\nptrIn this case, the object that points to is deleted on line 4. However, line 5 still tries to reference the value of the object, which has already\nbeen invalidated. There is no saying what could be at the memory address that was deleted: perhaps the memory could have been reused for\nsomething else. As a result, the output of line 5 is undefined.\nptr\n0x5370\nstack\nheap\n???\n0x5370\nIf you know that a pointer to dynamic memory will last longer than the object it points to in dynamic memory, a good strategy is to set it to\nnullptr deleteafter you call on it. By doing this, you remove the possibility of dereferencing an invalid address in memory. Note that\nnullptrdereferencing a would still be undefined behavior, but this can be detected more easily when it happens (and potentially give you a\nnullptrbenign segmentation fault instead of potentially corrupting other memory). Setting a deleted pointer to can also reduce the risk of a\nnullptrdouble delete, since deleting a does nothing by default (and is thus safe to do).", "word_count": 555, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "99ece06e-87dd-59a5-bbdd-90efc1638778", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1061, "real_page_number": null, "text": "27.2 Resource Acquisition is Initialization (RAII)\n1049\n¸ 27.1.10\n(✽)Garbage Collection\nLanguages such as C and C++ require the programmer to manage their own dynamic memory. As a result, if you explicitly create an object in\ndynamic memory, you must also remember to delete the object and free its memory after you are done using it. Although giving the programmer\nthe responsibility of managing their own memory has its benefits (as it provides more flexibility and control), it can also lead to memory bugs\nlike the ones we discussed previously and can make the language harder to learn. To address this, many other languages (such as Python, Java,\nand C#) utilize a technique known as garbage collection to automatically detect and free up memory that is no longer being used. When using\nthese garbage-collected languages, you do not need to worry about managing dynamic memory manually — a garbage collection process is\nperformed in the background to reclaim memory that you no longer need. Many strategies for garbage collection exist, but we will not be\ndiscussing all of them here (although one strategy, reference counting, will be covered when we introduce shared pointers in a later section).\n27.2\nResource Acquisition is Initialization (RAII) (✽)\n¸ 27.2.1\n(✽)Motivations for RAII\nIn C++, the programmer is responsible for managing their own dynamic memory. This should be simple, right? Whenever you allocate memory\non the heap, just remember to also clean up this memory after you are done using it. In a perfect world, this would not be an issue… but alas,\nthe world is not perfect, and a solution of saying is not an effective way to prevent potentially dangerous memory issues. In\"just remember it\"\nfact, dynamic memory problems may not even be caused by a programmer’s carelessness or negligence: consider the following function, which\nupon first glance appears to abide by the rules of proper memory management:\n1\nvoid foo() {\n2\nint32_t* new int32_t{281};ptr =\n3\nbar(ptr);\n4\n/* ... other stuff ... */\n5\ndelete ptr;\n6\n} // foo()\nbar() foo()? delete ptrHowever, what happens if throws an exception that propagates to Even though the programmer remembered to\non line 5, this statement is never reached due to the exception thrown on line 3. Thus, the function terminates without cleaning up the memory\nptr, deleteallocated for resulting in a memory leak. As a result, simply relying on a programmer to appropriately call is not enough to\nprevent all memory issues from occuring. Instead, as savvy developers, a better method would be to design our code in a way to prevent these\nissues from happening at all, regardless of how careful a programmer is while managing dynamically allocated memory.\nTo begin exploring how this can be done, let us look back at the lifetime of a local object that is allocated on the stack. When the local\nobject is created within a scope, its constructor is automatically called, and when it goes out of scope, its destructor is automatically called.\nWhat if we could somehow give ownership of dynamically-allocated memory to an object that is automatically allocated and deallocated on the\nnew)stack? All we have to do is allocate the dynamic memory in the object’s constructor (using and deallocate the dynamic memory in the\ndelete).object’s destructor (using Then, when we create that object as a local variable on the stack, its destructor would be automatically\ncalled when it goes out of scope, which in turn frees the dynamic memory in its destructor. That way, dynamically allocated resources will\nalways be deallocated automatically when they go out of scope, and there would be no need to explicitly delete this memory on your own!\nThis idiom is known as resource acquisition is initialization, or RAII. With RAII, an object that needs to use a resource (such as dynamic\nmemory) acquires it when it is initialized (via its constructor) and releases it when it is destroyed (via its destructor). The idea here is that all\nresources need to be released at some point, and a lot of common issues are caused by resources being maintained even when they are no longer\nused. We can ensure that resources are managed properly throughout our program by tying their usage directly with the lifetime of any object\nthat needs them. RAII is not just a concept that is relevant to dynamic memory: it also applies to other resources that need to be shared, such as\nnetwork connections and locks (which is a mechanism that can be used to restrict access to a shared resource that may be needed by multiple\nrunning threads at the same time — you will learn more about this if you take a class that covers concurrency).\n¸ 27.2.2\n(✽)Managing Dynamic Memory Using RAII\nLet us look at an example of RAII in action. Consider our initial example of a leaky function, where we allocate an object on the heap and then\nforget to delete it.\n1\nvoid foo() {\n2\nint32_t* new int32_t{281};ptr =\n3\nstd::cout << *ptr << '\\n';\n// prints 281\n4\n} // foo()\ndelete ptrAs previously indicated, dynamic memory is allocated on line 2 to store the integer, but is never called on this memory. When\ngoes out of scope on line 4, this memory is leaked.\nptr\n0x5370\nstack\nheap\n281\n0x5370", "word_count": 898, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8fb06c8c-67b9-5a71-86b9-fcf3b561d87d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1062, "real_page_number": null, "text": "1050\nChapter 27. Smart Pointers and Memory Management\nHowever, we can fix this problem by defining a separate class that can be used to manage this pointer. This class takes in the pointer upon\nconstruction and automatically deletes it in its destructor. We will also overload additional operators to allow our class to behave as if it were a\n->pointer (supporting dereferencing using the and operators). This is shown below:*\n1\nclass IntPtr {\n2\npublic:\n3\n// constructor, takes in the value to allocate dynamic memory for\n4\nIntPtr(int32_t ptr{new int32_t{val}}val) : {}\n5\n6\n// destructor, automatically deletes the pointer when it is invoked\n7\n~IntPtr() {\n8\ndelete ptr;\n9\n} // ~IntPtr()\n10\n11\n// other members to make this object behave as if it were the pointer itself\n12\nint32_t& operator*() {\n13\nreturn *ptr;\n14\n} // operator*()\n15\n16\nint32_t* operator->() {\n17\nreturn ptr;\n18\n} // operator->()\n19\n20\nprivate:\n21\n// stores the pointer itself\n22\nint32_t* ptr;\n23\n};\nIntPtrInstead of allocating the pointer directly, we can encapsulate it within the class. The updated code is shown below:\n1\nvoid foo() {\n2\nIntPtr int_ptr{281};\n3\nstd::cout << *int_ptr << '\\n'; // prints 281\n4\n} // foo()\nUnlike the previous implementation, this function no longer leaks any memory. This is because the dynamic memory used by this function\nIntPtr IntPtris managed by the class, which is initialized as a local variable on the stack. When the is initialized on line 2, it allocates\ndynamic memory within its constructor. Then, when it goes out of scope on line 4, it frees that dynamic memory within its destructor. Since the\ncompiler ensures that constructors and destructors are called automatically for local variables when they are created and go out of scope, the\nprogrammer will not have to directly manage the dynamic memory that is needed by the function themselves!\nptr\n0x5370\nint_ptr\nstack\nheap\n281\n0x5370\nint_ptr int_ptrgoes out of scope, which causes its destructor to be invoked — the destructor frees the dynamic memory stored in\nptr\n0x5370\nint_ptr\nstack\nheap\n???\n0x5370\nptr\n???\nint_ptr\nstack\nheap\n???\n0x5370", "word_count": 367, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "9cacd73a-15f9-5254-afd6-3851d99e2a7a", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1063, "real_page_number": null, "text": "27.3 Introduction to Smart Pointers\n1051\ndeleteRAII can be particularly helpful if exceptions are involved. Recall our previous example, where seemingly responsible code that called\nnewafter still ended up leaking memory due to the presence of an exception.\n1\nvoid foo() {\n2\nint32_t* new int32_t{281};ptr =\n3\nbar(ptr);\n// throws exception\n4\n/* ... other code ... */\n5\ndelete ptr;\n// this line is never reached!\n6\n} // foo()\nIntPtr bar() foo(),If we used an instead, this would no longer be a problem. If the method throws an exception that propagates up to the\nfoo() int_ptrfunction would terminate, and would be automatically deallocated from automatic storage (since it is a local variable). This\nint_ptr,in turn calls the destructor of which successfully cleans up the dynamic memory that was allocated.\n1\nvoid foo() {\n2\nIntPtr int_ptr{281};\n3\nbar(ptr);\n4\n/* ... other code ... */\n5\n} // foo()\nRAII as a concept is not actually new: we actually employed this principle while designing our dynamic array back in chapter 6. In fact, the idea\nstd::vector<>of RAII is utilized by many containers in the STL. Consider the as an example. When you instantiate a vector of values, the\nvalues themselves are stored in dynamic memory by default (only bookkeeping data is located on the stack — see chapter 7 for more information\non how vectors work).\nvec\nbegin\n0x100\nend_size\n0x110\nend_cap\n0x110\nstack\nheap\n1\n2\n3\n4\n0x100\n0x104\n0x108\n0x10c\nstd::vector<>,However, when you instantiate a you do not need to worry about any of this dynamic memory. This is because all dynamic\nnew[].memory is handled internally by the vector. If the vector needs any more memory, it allocates a new array using Then, when the\ndelete[].vector goes out of scope, its destructor cleans up its dynamically allocated array using This is the beauty of RAII: by delegating\nmanagement of a resource to a local object via its constructor and destructor, we remove the need for the programmer to directly manage this\nresource on their own. This in turn reduces the frequency of bugs and makes it easier to write safer and cleaner code.\nIntPtrAs we demonstrated with our initial example, RAII can be used to encapsulate a pointer to dynamic memory within a local object,\nallowing the pointer to free its memory automatically when it goes out of scope via its destructor. This premise forms the foundation of smart\npointers, which are class objects that can be used to manage dynamic memory in a similar manner. We will discuss smart pointers in greater\ndetail in the following sections of this chapter.\n27.3\nIntroduction to Smart Pointers (✽)\n¸ 27.3.1\n(✽)Anatomy of a Smart Pointer\nA smart pointer in C++ is a class object that can be used to manage objects in dynamic memory, automatically deleting the dynamically\nallocated memory for you at the appropriate time. When a smart pointer goes out of scope or has its contents reassigned, any dynamic memory\nthat is no longer needed gets automatically cleaned up (using RAII principles). This prevents many of the issues discussed earlier, such as\nunintended memory leaks, double deletions, and dangling pointers.\noperator* operator->.Smartpointersaredesignedtomimicanormalpointer,withsupportforstandardpointeroperatorssuchas and\nBecause of this, smart pointers can be substituted for raw pointers whenever you are working with dynamic memory, since they be used just like\none (with the added bonus of automatic dynamic memory cleanup). You can think of a smart pointer as a templated class that stores an internal\npointer to dynamic memory, with a destructor that handles the deletion of this dynamic memory (note that this is an oversimplification):\n1\ntemplate <typename T>\n2\nclass SmartPtr {\n3\nprivate:\n4\n// internal pointer that points to dynamic memory\n5\nT* ptr;\n6\npublic:\n7\n// destructor deletes this dynamic memory\n8\n~SmartPtr() {\n9\n// DISCLAIMER: This does not work if this dynamic object is shared by multiple smart pointers!\n10\n// We will address this issue later, but we can only delete if nothing else is using this memory.\n11\ndelete ptr;\n12\n} // ~SmartPtr()\n13\n};", "word_count": 709, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "79399a5e-8882-57a1-83d7-205d1c221864", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1064, "real_page_number": null, "text": "1052\nChapter 27. Smart Pointers and Memory Management\nSmart pointers also have overloaded operators that allow them to behave like a normal pointer, allowing you to use them in place of a normal\nraw pointer when working with dynamic memory:\n1\ntemplate <typename T>\n2\nclass SmartPtr {\n3\nprivate:\n4\n// internal pointer that points to dynamic memory\n5\nT* ptr;\n6\npublic:\n7\n// overloaded dereference operator\n8\noperator*() returnT& { *ptr; }\n9\n10\n// overloaded indirection operator\n11\noperator->() returnT* { ptr; }\n12\n};\nSmart pointers also provide a method for retrieving the internal pointer that it manages. When using smart pointers in C++, you can return the\nget()stored pointer by using the supplied method (although you have to be careful when using this pointer, since you could undermine the\nbehavior of the smart pointer if you do anything bad with it).\n1\ntemplate <typename T>\n2\nclass SmartPtr {\n3\nprivate:\n4\n// internal pointer that points to dynamic memory\n5\nT* ptr;\n6\npublic:\n7\n// returns the internal pointer to dynamic memory\n8\nreturnT* get() { ptr; }\n9\n};\nLastly, it should be emphasized that smart pointers are used to manage allocated memory. You should not be using smart pointers todynamically\nmanage local objects on the stack, since those objects are already managed automatically by the compiler. Furthermore, because smart pointers\ndelete deleteattempt to their internal pointer after they are destructed, storing a pointer to a stack object would result in an error (since\nnew).can only be called on objects allocated with Luckily, there are methods that can be used to check for this condition at compile time.\n¸ 27.3.2\n(✽)Categories of Ownership\nWhen working with smart pointers, one important factor to consider is the of the dynamic memory that you want to manage. In C++,ownership\ndeleteownership of dynamic memory is given to the entity that is responsible for calling on the pointer after it is done using it. Prior to the\nadvent of smart pointers, ownership could at times be difficult to discern, since a raw pointer on its own does not provide any information about\nwhether it should be cleaned up by the entity that is using it (since you may not know if the object it points to is still being used by something\nelse). Smart pointers were designed to fix this problem, as they can act as a clear source of ownership for an object in dynamic memory —\na smart pointer accepts all responsibility for (1) keeping track of the number of entities that are using the object at any point in time and (2)\ndeleting the memory for the object after it is no longer needed.\nTherearetwomajorcategoriesofownershipthatwewilldiscuss. Thefirst(andmostcommon)categoryis ownership. Asitsnameexclusive\nimplies, adynamicallyallocatedobject underexclusive ownership onlyhas oneownerthatis responsible fordeleting it. Anexampleofexclusive\nClassA, ResourceX.ownership is shown in the figure below. Here, a single class instance, is using the dynamically allocated resource\nResourceX ClassA ResourceXis not needed by any other class instances, so when gets destructed, then can be safely deleted as well.\nClassA\nResourceX\nExclusive ownership is the simplest form of ownership to implement for a smart pointer. If you can ensure that only one entity owns a smart\npointer at any point in time, then you can implement the smart pointer like we did previously in this section, since it would be safe to delete the\npointer’s dynamic memory after its owner no longer needs it.\nA second, more complicated form of ownership is ownership. With shared ownership, an object in dynamic memory may be jointlyshared\nowned by multiple entities at the same time. In this case, the object can only be deleted if all of its owners no longer need it. An example is\nResourceXdemonstrated below, where is referenced by multiple class instances.\nClassA\nClassB\nClassC\nClassD\nResourceX\nClassA, ClassB, ClassC, ClassD ResourceX.In this example, and all need to use the shared If one of these class instances goes out of\nscope, the resource should be deleted, since there would still be three additional class instances that require the resource (in fact, deleting annot\nobject under shared ownership using our previous implementation would result in a double delete, since the same resource would be deleted\nmultiple times). Only when all four class instances go out of scope can the resource be safely deallocated.\nFor a smart pointer to support multiple owners, it must apply additional logic to keep track of how many objects still need to use the resource\nit is managing, so that it can correctly identify when to delete the resource. This is done using a technique known as counting.reference", "word_count": 815, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "f8ddac4b-26ec-59cb-8aa8-0ffb81be8a84", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1065, "real_page_number": null, "text": "27.3 Introduction to Smart Pointers\n1053\n¸ 27.3.3\n(✽)Reference Counting\nReference counting is a strategy that can be used to manage resources that may be under shared ownership. With reference counting, a smart\npointer keeps track of the number of references to a shared object in dynamic memory. If a new entity needs to use the shared object, the\nreference count of the object is incremented. Similarly, if an existing entity no longer needs to use the shared object, the reference count of the\nobject is decremented. Once the shared object reaches a reference count of zero, its memory is cleaned up by the smart pointer. To illustrate this\nResourceX,process, consider our previous example. In this case, four different entities need to use so it has a reference count of four.\nClassA\nClassB\nClassC\nClassD\nResourceX\nreference count: 4\nClassE, ResourceX.Suppose we create a new instance of a class, that initializes a smart pointer that also references When this happens, the\nResourceXreference count for is incremented to five.\nClassA\nClassB\nClassC\nClassD\nClassE\nResourceX\nreference count: 5\nClassA ClassD ResourceX.Now, suppose the instances of and go out of scope and no longer need to use When this happens, the reference\nResourceXcount of drops to three.\nClassB\nClassC\nClassE\nResourceX\nreference count: 3\nEventually, the remaining class instances will go out of scope. Once this happens, there would no longer be any instances that still need to use\nResourceX, so its reference count would drop to zero. The final smart pointer would detect this and subsequently clean up its memory.\nResourceX\nreference count: 0\nHow can we implement reference counting when designing a smart pointer that can handle shared objects? There are two primary strategies that\ncan be used: intrusive reference counting and non-intrusive reference counting. With intrusive reference counting, the pointed to object must\nkeep track of the reference count itself. When a new smart pointer references a shared object (either through creation or assignment), the object\nincrements its reference count. When an existing smart pointer no longer needs to reference a shared object, the object decrements its reference\ncount. Then, when the reference count becomes zero, the object is deleted. This is typically done by implementing a new class that derives from\nthe type of the shared object and adding a reference count member variable to it. An illustration of intrusive reference counting is shown below.\nintrusive smart pointer\nintrusive smart pointer\nintrusive smart pointer\nintrusive smart pointer\nshared resource\nreference count", "word_count": 419, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "209ad048-d30e-5823-8985-b2e46bd12569", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1066, "real_page_number": null, "text": "1054\nChapter 27. Smart Pointers and Memory Management\nOn the other hand, with non-intrusive reference counting, the shared resource does not need to contain a reference counter itself. Instead, the\nreference count is identified through another means without altering the shared resource. The most common implementation of non-intrusive\nreference counting is to allocate a separate that manages the reference counter of a shared resource in dynamic memory. In thiscontrol block\nimplementation, the first smart pointer that points to a shared resource dynamically allocates a new control block that contains (1) a pointer\nto the shared object and (2) its reference count. This control block then has its reference count incremented and decremented as the number\nof smart pointer references changes. Once the reference count reaches zero, both the control block and the shared resource are automatically\ndeleted. An illustration of this implementation is shown below.\nnon-intrusive smart pointer\nnon-intrusive smart pointer\nnon-intrusive smart pointer\nnon-intrusive smart pointer\ncontrol block\nreference count\nptr to resource\nshared resource\nAnother non-intrusive reference counting implementation uses a doubly-linked list to identify when a shared object can be deleted. In this\nimplementation, smart pointers that reference the same shared object are placed together in a linked list. When a new smart pointer needs to\naccess the shared object, it gets spliced into the list. When an existing smart point no longer needs to access the shared object, it gets spliced out\nof the list. When the last smart pointer in the list is removed, it deletes the shared resource. Note that this implementation does not actually keep\ntrack of the actual reference count, but this is okay, since we only need to know whether the reference count is zero or non-zero.\nsmart pointer X\nptr to resource\nsmart pointer Y\nptr to resource\nshared resource\nsmart pointer Z\nptr to resource\nOf these three methods, the control block implementation is the one that is typically the most practical. This is because it is best able to handle a\nother.1specific edge case that occurs when a cycle of objects have smart pointers that reference each This edge case is known as the cycle\nproblem, and we will discuss it in more detail when we cover weak pointers in a later section.\n27.4\nUnique Pointers (✽)\n¸ 27.4.1\n(✽)std::unique_ptr\n<memory>The C++ library provides several implementations of smart pointers that you can use to manage dynamic memory. The simplest of\nstd::unique_ptr<>,these smart pointers is the which can be used to manage a dynamically allocated object under ownership.exclusive\nstd::unique_ptr<>An instance of a is very lightweight, as it basically just stores the pointer to dynamic memory that it manages (or\nnullptr std::unique_ptr<>if it does not own any dynamic memory). When a is destructed, its destructor deletes the dynamic memory\nreferenced by its internal pointer — this is safe because a unique pointer can only be owned by a single entity at any point in time. In addition,\n<memory>like all the other smart pointers in the library, a unique pointer can be used syntatically as if it were a normal, built-in pointer (e.g.,\nifdereferencing, implicit conversion to a Boolean when placed in a conditional check such as an statement, etc.).\nstd::unique_ptr<>.There are two ways to initialize a One method to initialize a unique pointer is to pass a pointer to dynamic\nmemory directly into its constructor. This is shown in the example code below:\n// initialize a unique pointer to a dynamically allocated integer\nstd::unique_ptr<int32_t> ptr{new int32_t{281}};\nHowever, there are a few disadvantages with this approach, the most notable of which involves exception safety. Consider the following function,\nwhich takes in a unique pointer and an integer:\nvoid int32_tfoo(std::unique_ptr<Object> object, next_id);\n1Thecontrolblockversionisalsoeasiertoimplementwhenmultithreadingisinvolved;i.e.,whenmultiplepartsofaprogramarerunatthesametimeandmay\neachtry tomodify smartpointers thatreference the sameshared object. Whilethe intrusivepointer andlinked listversionscan alsobe adaptedto addressthe\ncycleproblemandefficientmultithreading,itistoughertodosowiththeseapproaches.", "word_count": 691, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3b7ad821-0886-5fd7-b5b5-3fba9ac649e6", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1067, "real_page_number": null, "text": "27.4 Unique Pointers\n1055\nget_next_id(),Assume that there is another function, that can be used to get the next available ID to be used in the program. Knowing\nthis, the following function call should be valid:\nfoo(std::unique_ptr<Object>(new Object{}), get_next_id());\nfoo()From this function call, it is reasonable to assume that the following steps must happen before is invoked:\nObject new.1. A new is dynamically allocated on the heap via\nObject2. This new is passed into the constructor of the unique pointer.\nget_next_id()3. The function is run to get the next available ID.\nHowever, when converting this function call to machine code, the compiler is not required to construct the function arguments in any specific\norder. It is entirely acceptable for the steps to be executed in this order:\nObject new.1. A new is dynamically allocated on the heap via\nget_next_id()2. The function is run to get the next available ID.\nObject3. This new is passed into the constructor of the unique pointer.\nget_next_id() ObjectNow, consider what happens if the function throws an exception in step 2. Since the has already been dynamically\nallocated in step 1 but has not yet been passed into the constructor, it ends up being leaked! This is certainly not we want.\npointer.2std::make_unique(),C++14 fixed this issue with the introduction of which is a better way to initialize a unique The\nstd::make_unique() method takes in the constructor arguments of the object to be dynamically allocated and forwards it directly to the\nstd::make_unique()object’s constructor. The function signature of is shown below.\ntemplate <typename typename...T, Args>\nstd::unique_ptr<T> std::make_unique(Args&&... args);\nT args std::unique_ptr<>.Constructs an object of type using its constructor arguments and wraps it in a\nstd::make_unique(),Using we can create the same unique pointer to an integer as follows:\n// initialize a unique pointer to a dynamic allocated integer\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);ptr =\nstd::make_unique()Additionally, when is called, the dynamically allocated object is guaranteed to be stored in the smart pointer without\nget_next_id()any interruptions. This matters for the ID example, which previously leaked memory if the threw an exception after the new\nstd::make_unique()object was dynamically allocated, but before it could be stored in the smart pointer. prevents this from happening.\n// better, does not leak memory if an exception is thrown in get_next_id()\nfoo(std::make_unique<Object>(), get_next_id());\nstd::make_unique().Tocreateauniquepointerthatmanagesacustomobject,youshouldpassintheargumentsoftheobject’sconstructorinto\nAn example is shown below.\n1\nstruct Object {\n2\nint32_t member1;\n3\nbool member2;\n4\nObject(int32_t boolmember1_in, member2_in) : member1{member1_in}, member2{member2_in} {}\n5\n};\n6\n7\nvoid foo() {\n8\n// create a smart pointer for an Object where member1 = 281 and member2 = true\n9\nstd::unique_ptr<int32_t> true);ptr = std::make_unique<Object>(281,\n10\n/* ... other code ... */\n11\n} // ptr goes out of scope, dynamic memory for Object automatically cleaned up\nDataManagerAnotherexampleinvolvingauniquepointerisshownbelow. Inthisexample,the classstoresapointertoadynamicallyallocated\nCalculator Calculator DataManagerobject. The is created on the heap when the object is constructed and deleted automatically by\nDataManagerthe smart pointer when its owning object goes out of scope.\n1\n// a calculator object that can be used to calculate something\n2\nclass Calculator {\n3\npublic:\n4\nint32_t calculate();\n5\n};\n6\n7\nclass DataManager {\n8\nprivate:\n9\n// a unique pointer that stores a dynamically allocated Calculator object - the smart pointer\n10\n// will automatically delete the Calculator from the heap after the DataManager goes out of scope\n11\nstd::unique_ptr<Calculator> calculator;\n12\n13\npublic:\n14\n// DataManager constructor, use std::make_unique() to construct the internal calculator\n15\nDataManager(/* ... constructor args ... */) :\n16\ncalculator{std::make_unique<Calculator>(/* ... calculator constructor args ... */)} {}\n17\n18\n// the smart pointer behaves just like a normal pointer, so you can use pointer operators on it\n19\nint32_t returncalculate() { calculator->calculate(); }\n20\n};\n2For newmostcases,thatis. Therearesituationswhereinitializingfrom mayberequired,buttheyarelesscommonedgecasesthatwewon’tbediscussinghere.", "word_count": 695, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "426a0dd2-7b51-56ea-bb3d-96b68cae5dea", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1068, "real_page_number": null, "text": "1056\nChapter 27. Smart Pointers and Memory Management\nSome additional member functions provided by unique pointers are summarized below.\ntemplate <typename T>\nconst noexcept;T* std::unique_ptr<T>::get()\nnullptrReturns a pointer to the unique pointer’s managed object, or if no object is owned.\n1\n{\n2\nauto std::make_unique<int32_t>(281);ptr =\n3\nint32_t* raw_ptr = ptr.get();\n4\nstd::cout << *raw_ptr << '\\n';\n// prints 281\n5\n} // raw_ptr is still managed internally by the smart pointer, so it gets cleaned up here\ntemplate <typename T>\nnoexcept;T* std::unique_ptr<T>::release()\nrelease(),Releases ownership of the unique pointer’s managed object, if any. After calling the unique pointer’s internal pointer becomes\nnullptr, release()and the user who called is now responsible for deleting the memory of the released object.\n1\n{\n2\nauto std::make_unique<int32_t>(281);ptr =\n3\nint32_t* raw_ptr = ptr.release();\n// the smart pointer no longer owns the pointer!\n4\nstd::cout << *raw_ptr << '\\n';\n// prints 281\n5\ndelete raw_ptr;\n// the code that called release() must clean up\n6\n}\ntemplate <typename T>\nvoid noexcept;std::unique_ptr<T>::reset()\nRelinquishes ownership of the unique pointer’s managed object. The previously managed object is deleted (if non-empty), and the unique\nnullptr.pointer’s internal pointer now becomes\ntemplate <typename T>\nvoid noexcept;std::unique_ptr<T>::reset(T* ptr)\nptr.Replaces the currently managed object with another object in dynamic memory pointed to by The previously managed object is\ndeleted (if non-empty).\n1\n{\n2\nauto std::make_unique<int32_t>(281);ptr1 =\n3\nauto std::make_unique<int32_t>(370);ptr2 =\n4\nptr1.reset(new int32_t{183});\n// ptr1 now owns 183, previous memory for 281 cleaned up\n5\nptr1.reset(ptr2.release());\n// ptr1 now owns 370, ptr2 now owns nothing (due to release())\n6\nptr1.reset();\n// ptr1 no longer owns anything\n7\n}\n¸ 27.4.2\n(✽)Transferring Ownership of a Unique Pointer\nOne important restriction of unique pointers is that they must be used under exclusive ownership. This is because a unique pointer automatically\ndeletes its managed object once it goes out of scope, so if multiple entities shared ownership of the same unique pointer, the unique pointer’s\ndata would be deleted multiple times (since the same unique pointer would go out of scope multiple times). Because of this, copy construction\nstd::unique_ptr<>.and copy assignment are both for adisabled\n1\n{\n2\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);ptr1 =\n3\nstd::unique_ptr<int32_t> ptr2{ptr1};\n// ERROR: copy construction not allowed\n4\nstd::unique_ptr<int32_t> ptr3;\n// empty unique pointer with nullptr (okay)\n5\nptr3 = ptr1;\n// ERROR: copy assignment not allowed\n6\n}\nrelease() reset(),If you want a new entity to take control of a unique pointer, you must its ownership by using either or or bytransfer\noperator).3 std::move()moving the smart pointer using move semantics (i.e., with the move constructor or move assignment Calling on a\nrelease() reset()unique pointer is equivalent to calling and then on that pointer. After transferring ownership, the unique pointer that\nnullptr).was moved from would no longer own anything (and would be set to An example is shown below:\n1\n{\n2\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);ptr1 =\n3\n// ptr1 gives up ownership of 281 to ptr2\n4\nstd::unique_ptr<int32_t> ptr2 = std::move(ptr1);\n5\n6\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(370);ptr3 =\n7\n// ownership of ptr3's 370 is assigned to ptr2, ptr3 no longer owns anything\n8\nptr2 = std::move(ptr3);\n9\n10\n// ownership of ptr2's 370 is assigned to ptr4, ptr2 no longer owns anything\n11\nstd::unique_ptr<int32_t> ptr4{std::move(ptr2)};\n12\n}\n3Movesemantics(includinglvaluesandrvalues)werecoveredinsection6.9,soyoucanlookthereforamorein-depthdiscussiononhowtheywork.", "word_count": 582, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2169d9e0-fd92-5f3d-a5a7-90feac251d6d", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1069, "real_page_number": null, "text": "27.4 Unique Pointers\n1057\n¸ 27.4.3\n(✽)Using Unique Pointers in Functions\nSince function return values are rvalues that can be moved using the move constructor or move assignment operator without making a copy, it is\nperfectly appropriate to return a unique pointer from a function. If you return a unique pointer from a function and assign it to another unique\npointer, then the ownership of that object is transferred out of the function and into the assigned unique pointer. An example is shown below:\n1\nstd::unique_ptr<int32_t> create_unique_ptr(int32_t num) {\n2\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(num);func_ptr =\n3\nreturn func_ptr;\n// func_ptr gives up ownership to the object that claims the return value\n4\n} // create_unique_ptr()\n5\n6\nvoid foo() {\n7\nstd::unique_ptr<int32_t> ptr1{create_unique_ptr(281)};\n// ptr1 claims ownership of returned value\n8\n9\nstd::unique_ptr<int32_t> ptr2;\n10\nptr2 = create_unique_ptr(370);\n// ptr2 claims ownership of returned value\n11\n} // foo()\nYou may also encounter situations where you want to pass a unique pointer into a function. If you want a function to take ownership of the\nstd::move()unique pointer being passed in, you should pass the unique pointer and call when passing the pointer in.by value\n1\nvoid foo(std::unique_ptr<int32_t> ptr);\n// the foo() function takes ownership of the passed in ptr\n2\n3\nint main() {\n4\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);ptr =\n5\n// pass the pointer into the function using std::move()\n6\n// (otherwise, you would be trying to make a copy of the unique pointer, which is disallowed)\n7\nfoo(std::move(ptr));\n8\n} // main()\nThis convention is something you may see if you want to construct a class object that stores a unique pointer as a member. If the unique pointer\nassociated with this member variable was constructed before an instance of this class object was created, you will have to transfer it into the\nstd::move(),class object’s constructor using as shown:\n1\nclass DataManager {\n2\nprivate:\n3\nstd::unique_ptr<Calculator> calculator;\n// private member variable\n4\npublic:\n5\n// constructor takes in a calculator constructed elsewhere\n6\n// the DataManager takes ownership of the calculator that is passed in\n7\nDataManager(std::unique_ptr<Calculator> calculator_in) : calculator{std::move(calculator_in)} {}\n8\n};\n9\n10\nint main() {\n11\nstd::unique_ptr<Calculator> calc = std::make_unique<Calculator>();\n12\n// pass the Calculator into the constructor of the DataManager using std::move()\n13\nDataManager dm{std::move(calc)};\n14\n} // main()\nThere are also cases where you may need to pass a unique pointer by reference. The best example of this is if you have a function that might\nmodify or reset the contents of the unique pointer without assuming ownership of the pointer. When passing a unique pointer by reference, you\nstd::move()do not need to explicitly call (since passing by reference does not invoke the copy constructor or copy assignment operator).\n1\n// function that may update or reset the passed in pointer, so pass by reference\n2\nvoid update_ptr(std::unique_ptr<int32_t>& ptr);\n3\n4\nint main() {\n5\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);ptr =\n6\nupdate_ptr(ptr);\n// pass by reference, so no need to explicitly call std::move()\n7\n} // main()\nLastly, it should be mentioned that, while it is possible to pass a unique pointer by const reference, this is considered to be bad style. This is\nbecause a function that accepts a const reference to a unique pointer can neither manipulate it nor take ownership, so it thereby does not need to\ncare about how this data is managed. Since the function only needs to view the data stored by the pointer without making any changes, it is\nget()better to pass in the raw pointer instead — this can be done using the member function of a unique pointer.\n1\n// not ideal, you do not need to pass the entire unique_ptr if it cannot be manipulated\n2\nvoid foo(const std::unique_ptr<int32_t>& ptr);\n3\n4\n// this is a better way to pass in the contents of the pointer\n5\nvoid bar(const int32_t* ptr);\n6\n7\nint main() {\n8\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);ptr =\n9\nbar(ptr.get());\n// pass in raw pointer, which you can retrieve using get()\n10\n} // main()", "word_count": 679, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8e68fef4-c3f3-5eaa-b6c4-db86677db205", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1070, "real_page_number": null, "text": "1058\nChapter 27. Smart Pointers and Memory Management\n¸ 27.4.4\n(✽)Storing Unique Pointers in Containers\nEventhoughthey cannotbecopied, uniquepointerscaninfactbestoredinSTLcontainersthankstomovesemantics. Thiscanbedonebyfilling\nstd::unique_ptr<>thecontainerwith objectssothattheybemovedintothecontainer(thiscanbedonebyeitherusinganunnamedrvalue\nstd::move()such as a function return value, or by calling on an existing unique pointer object). An example is shown below:\n1\nstd::vector<std::unique_ptr<int32_t>> unique_ptr_vec;\n2\n3\n// to insert a unique pointer, you must pass in an rvalue, as shown below\n4\n// \"std::make_unique<int32_t>(281)\" is an rvalue since it is a function return value\n5\nunique_ptr_vec.push_back(std::make_unique<int32_t>(281));\n6\n7\n// if a unique pointer already exists, you must call std::move() to add it to the container\n8\n// recall that std::move() can be used to cast an lvalue to an rvalue\n9\n// after calling std::move() on ptr, ptr will no longer own anything\n10\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);ptr =\n11\nunique_ptr_vec.push_back(std::move(ptr));\nIf we store unique pointers in a container, then the ownership of those pointers also belongs to the container. If you erase a unique pointer from\nthe container, it is destroyed and its memory is cleaned up. Similarly, once the container gets emptied or goes out of scope, all of its unique\npointers will also be destroyed and cleaned up.\n1\nstd::vector<std::unique_ptr<int32_t>> unique_ptr_vec;\n2\nunique_ptr_vec.push_back(std::make_unique<int32_t>(281));\n3\nunique_ptr_vec.push_back(std::make_unique<int32_t>(370));\n4\nunique_ptr_vec.push_back(std::make_unique<int32_t>(376));\n5\n6\n// erases value at index 1, so the unique pointer that manages 370 is destroyed\n7\n// and its memory is cleaned up (index 0 is now 281 and index 1 is now 376 after the erase)\n8\nunique_ptr_vec.erase(unique_ptr_vec.begin() + 1);\n9\n10\nfor (const auto& ptr : unique_ptr_vec) {\n11\ncout << *ptr << ' ';\n// prints \"281 376 \"\n12\n} // for ptr\n13\n14\n// clears the entire vector - all of its unique pointers are destroyed, with their memory deleted\n15\nunique_ptr_vec.clear();\nTo remove ownership of a unique pointer from the container, you will have to move the pointer out of the container. If you transfer ownership of\na unique pointer out of a container without erasing it from the container, then the empty unique pointer would remain in the container. Because\nof this, you may need to check if a unique pointer in the container is empty before using it (luckily, unique pointers can be implicitly converted\nifto a Boolean when used in a conditional statement, so you can check if a unique pointer has a value in an statement, as shown below).\n1\n{ // begin a new scope\n2\nstd::vector<std::unique_ptr<int32_t>> unique_ptr_vec;\n3\nunique_ptr_vec.push_back(std::make_unique<int32_t>(281));\n4\nunique_ptr_vec.push_back(std::make_unique<int32_t>(370));\n5\nunique_ptr_vec.push_back(std::make_unique<int32_t>(376));\n6\n7\n// move ownership of the pointer at index 1 out of the container\n8\n// now, ptr is in charge of managing the dynamic memory for 370 (the value at index 1)\n9\nstd::unique_ptr<int32_t> ptr = std::move(unique_ptr_vec[1]);\n10\n11\n// after being moved out, index 1 of the vector stores an empty unique pointer\n12\n// you will need to check for this when trying to use values in the container\n13\nfor (const std::unique_ptr<int32_t>& ptr_in_vec : unique_ptr_vec) {\n14\nif (ptr_in_vec) {\n// checks if ptr has value\n15\nstd::cout << *ptr_in_vec << '\\n';\n16\n} // if\n17\nelse {\n18\nstd::cout << \"ptr is empty!\\n\";\n19\n} // else\n20\n} // for ptr_in_vec\n21\n22\n// vector of pointers is cleared, so all of the smart pointers it owns are cleaned up; however,\n23\n// since 370 was transferred out of the container earlier, its memory would still be valid here\n24\nunique_ptr_vec.clear();\n25\n26\n/* ... additional code ... */\n27\n28\n} // ptr goes out of scope here, so the memory for 370 gets deleted\nThe output of the above code is:\n281\nptr is empty!\n376", "word_count": 649, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "2fef0461-af7e-5a0e-949c-ed142910e781", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1071, "real_page_number": null, "text": "27.4 Unique Pointers\n1059\n¸ 27.4.5\n(✽)Const Smart Pointers\nconst <memory>Additionally, it is meaningful to discuss how can be used with unique pointers (and other smart pointers in the C++ library).\nconst T:There are two places that can be placed when declaring a unique pointer that manages an object of type\nconst std::unique_ptr<T>•\nstd::unique_ptr<const T>•\nconst, constIn the first declaration of the pointer itself is and not what it points to. In other words, you cannot change what the pointer\npoints to, but you can change the value it holds.\n1\nconst std::unique_ptr<int32_t> std::make_unique<int32_t>(281);ptr =\n2\nptr.reset();\n// ERROR: you cannot change what ptr points to\n3\nstd::make_unique<int32_t>(370);ptr =\n// ERROR: you cannot change what ptr points to\n4\n*ptr = 376;\n// OK: the data itself is not const\nconst, const,In the second declaration of the value that the pointer points to is but the pointer itself is not. That is, you cannot change the\nvalue that the pointer holds, but you can change what the pointer points to.\n1\nstd::unique_ptr<const int32_t> std::make_unique<int32_t>(281);ptr =\n2\nptr.reset();\n// OK: pointer not const and can point to something else\n3\nstd::make_unique<int32_t>(370);ptr =\n// OK: pointer not const and can point to something else\n4\n*ptr = 376;\n// ERROR: cannot change the value itself since it is const\n¸ 27.4.6\n(✽)Smart Pointers and Polymorphism\nstd::unique_ptr<>Much like normal pointers, a to a base class can be used to refer to an object of a derived type that publicly inherits\nfrom it. An example is shown below:\n1\nclass Shape { ... };\n// base class\n2\nclass publicCircle : Shape { ... };\n// derived class\n3\nclass publicTriangle : Shape { ... };\n// derived class\n4\nclass publicRectangle : Shape { ... };\n// derived class\n5\n6\n// OK: can use derived class to construct pointer to base class type (polymorphism)\n7\nstd::unique_ptr<Shape> circle_ptr = std::make_unique<Circle>();\n8\nstd::unique_ptr<Shape> triangle_ptr = std::make_unique<Triangle>();\n9\nstd::unique_ptr<Shape> rectangle_ptr = std::make_unique<Rectangle>();\n10\n11\n// these derived types can also be stored in a vector of pointers to the base type\n12\nstd::vector<std::unique_ptr<Shape>> shape_vec;\n13\nshape_vec.push_back(std::move(circle_ptr));\n14\nshape_vec.push_back(std::move(triangle_ptr));\n15\nshape_vec.push_back(std::move(rectangle_ptr));\nBecause of this, unique pointers are useful if you want to create a factory function that can return multiple different types that inherit from the\nsame base class while also ensuring that the object that is returned is properly cleaned up after it is no longer needed. A simple example of a\nfactory function is shown below:\n1\nget_shape(int32_tstd::unique_ptr<Shape> num_sides) {\n2\nif (num_sides == 0) {\n3\nreturn std::make_unique<Circle>();\n4\n} // if\n5\nif (num_sides == 3) {\n6\nreturn std::make_unique<Triangle>();\n7\n} // if\n8\nif (num_sides == 4) {\n9\nreturn std::make_unique<Rectangle>();\n10\n} // if\n11\nif (num_sides == 5) {\n12\n/* ... additional if checks for different shapes ... */\n13\n} // if\n14\n15\nthrow std::invalid_argument{\"Number of sides currently not supported\"};\n16\n} // get_shape()\n17\n18\nvoid foo() {\n19\nstd::unique_ptr<Shape> circle_ptr = get_shape(0);\n20\nstd::unique_ptr<Shape> triangle_ptr = get_shape(3);\n21\nstd::unique_ptr<Shape> rectangle_ptr = get_shape(4);\n22\n} // foo", "word_count": 530, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "de4225ef-34e8-5195-8d75-5f9d49b074e9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1072, "real_page_number": null, "text": "1060\nChapter 27. Smart Pointers and Memory Management\n27.5\nShared Pointers (✽)\n¸ 27.5.1\n(✽)std::shared_ptr\nWhen working with dynamic memory, there is a good chance that a unique pointer is good enough for your needs. The exception, however, is if\nstd::unique_ptr<>youwanttomanagearesourcethatmaybeownedbymultipleentitiesatthesametime. Insuchacase,a wouldnotwork.\n<memory> std::shared_ptr<>.Instead, the C++ library provides another type of smart pointer that can handle shared ownership: the\nAn object in dynamic memory can be managed by multiple shared pointers, and it will only be destructed if there are no longer any shared\npointers that reference that object. As discussed earlier, a shared pointer can tell if it’s the last one pointing to an object via a technique known\nas counting. When a shared pointer is constructed, the reference count of its pointed to object is incremented; when a shared pointer isreference\ndestructed, the reference count of its pointed to object is decremented. Once the reference count reaches zero, the shared object is deleted.\nSince an object under shared ownership can be referenced by multiple shared pointers, its copy constructor and copy assignment operator\nare disabled. If a shared pointer is copied, the reference count of its managed object is incremented. Similarly, if one shared pointer isnot\nassigned to another shared pointer, the object being assigned has its reference count incremented, and the object being overwritten has its\nreference count decremented. Like unique pointers, a shared pointer can be initialized directly using a raw pointer to dynamic memory, but the\nstd::make_shared().preferred method in almost all situations is to initialize a shared pointer using a separate factory method,\ntemplate <typename typename...T, Args>\nstd::shared_ptr<T> std::make_shared(Args&&... args);\nT args std::shared_ptr<>.Constructs an object of type using its constructor arguments and wraps it in a\nget()Similar to unique pointers, you can use to get the internal raw pointer that is managed by a shared pointer.\ntemplate <typename T>\nconst noexcept;T* std::shared_ptr<T>::get()\nnullptrReturns a pointer to the shared pointer’s managed object, or if no object is owned.\nreset() release()Shared pointers also support a method, but they do support a method.not\ntemplate <typename T>\nvoid noexcept;std::shared_ptr<T>::reset()\nRelinquishes ownership of the shared pointer’s managed object. The previously managed object is deleted if no other shared pointer owns it,\nnullptr.and the shared pointer’s internal pointer now becomes\ntemplate <typename T>\nvoid noexcept;std::shared_ptr<T>::reset(T* ptr)\nptr.Replaces the currently managed object with another object in dynamic memory pointed to by The previously managed object is\nptrdeleted if nothing else owns it. If is already owned by another smart pointer, the function results in undefined behavior.\nconst,Many of the features we discussed for unique pointers (such as dereferencing, storing polymorphic classes, applying passing into\nfunctions, storing in containers, etc.) also apply to shared pointers. The main difference is that shared pointers can be copied, so you do not need\nstd::move()to explicitly call on them if you want to construct another owner of the pointer — a simple assignment or copy would suffice.\n1\nint main() {\n2\n// new shared pointer, ref count for 281 is 1\n3\nstd::shared_ptr<int32_t> std::make_shared<int32_t>(281);ptr1 =\n4\n5\n{ // begin a new scope\n6\n// ptr1 and ptr2 share the same object, ref count for 281 now 2\n7\nstd::shared_ptr<int32_t> ptr2 = ptr1;\n8\nstd::cout << *ptr2 << '\\n';\n// prints 281\n9\n} // ptr2 goes out of scope, so ref count of 281 goes back to 1\n10\n11\n// shared pointers can also be stored in containers, no moving necessary due to shared ownership\n12\nstd::vector<std::shared_ptr<int32_t>> shared_ptr_vec;\n13\n// shared pointer of 281 is copied, ref count goes to 2\n14\nshared_ptr_vec.push_back(ptr1);\n15\nshared_ptr_vec.push_back(std::make_shared<int32_t>(370));\n16\n17\n// this loop prints \"281 370 \"\n18\nfor (const auto& shared_ptr : shared_ptr_vec) {\n19\nif (shared_ptr) {\n20\nstd::cout << *shared_ptr << ' ';\n21\n} // if\n22\n} // for shared_ptr\n23\n24\n// vector is cleared; nothing owns 370 anymore, so it gets deleted\n25\n// ptr1 still owns 281, and ref count drops from 2 to 1\n26\nshared_ptr_vec.clear();\n27\n28\nstd::cout << *ptr1 << '\\n';\n// prints 281\n29\n} // ptr1 goes out of scope; it is the last owner of 281, so 281 gets deleted", "word_count": 731, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "41c05e34-9fc0-5447-bc02-2f3e7773441c", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1073, "real_page_number": null, "text": "27.5 Shared Pointers\n1061\nRemark: Even though two shared pointers can reference the same object, you should initialize two shared pointers with the same rawnot\npointer! The following code would cause a double deletion (which is undefined behavior):\n1\nint main() {\n2\nint32_t* new int32_t{281};val =\n3\nstd::shared_ptr<int32_t> ptr1{val};\n4\nstd::shared_ptr<int32_t> ptr2{val};\n// BAD: causes double delete\n5\n} // main()\nWe will discuss why shortly, but this is related to how shared pointers manage their reference counts behind the scenes.\nuse_count()Shared pointers also provide a member method, which can be used to return the number of shared pointers that are referencing\nuse_count()the current object. If a shared pointer has no managed object, calling on it returns zero.\ntemplate <typename T>\nlong const noexcept;std::shared_ptr<T>::use_count()\nReturns the number of different shared pointers that are referencing the current object, and zero if there is no managed object. This method\nis approximate in multithreaded environments.\n¸ 27.5.2\n(✽)Converting Between Unique and Shared Pointers\nA unique pointer (exclusive ownership) can be easily converted to a shared pointer (shared ownership). When this happens, the shared pointer\ntakes ownership of the unique pointer that it is assigned to. This makes a unique pointer a good return type for a factory function, since this\nallows the caller of the function decide whether they want the returned object to be under exclusive or shared ownership.\n1\nstd::unique_ptr<int32_t> foo() {\n2\nreturn std::make_unique<int32_t>(280);\n3\n} // foo()\n4\n5\nint main() {\n6\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);u_ptr1 =\n7\nstd::shared_ptr<int32_t> s_ptr1 = std::move(u_ptr1);\n// OK\n8\nstd::shared_ptr<int32_t> std::make_unique<int32_t>(370);s_ptr2 =\n// OK\n9\nstd::shared_ptr<int32_t> s_ptr3 = foo();\n// OK\n10\n} // main()\nConverting the other way, however, is not possible. Once you place an object under shared ownership, you can no longer manage it with a\nunique pointer, even if its reference count is one.\n1\nint main() {\n2\nstd::unique_ptr<int32_t> std::make_shared<int32_t>(281);u_ptr1 =\n// ERROR\n3\nstd::shared_ptr<int32_t> std::make_unique<int32_t>(370);s_ptr1 =\n4\nstd::unique_ptr<int32_t> u_ptr2 = std::move(s_ptr1);\n// ERROR\n5\n} // main()\nThis rule makes sense: if you have a resource that you know is only owned by one entity (e.g., a unique pointer), then it is perfectly safe to allow\nother entities to share ownership of that resource without any conflicts. However, once multiple entities are allowed to share the resource, it is\nno longer safe to restrict its access back to a single entity, since you would have to take away ownership from all other users of the resource.\nBecause of this, it is good practice to consider if a unique pointer is sufficient before you reach for a shared pointer: not only are unique pointers\nmore lightweight, but an overreliance on shared pointers makes it harder to go back to a unique pointer when exclusive ownership is expected.\n¸ 27.5.3\n(✽)Shared Pointers and Reference Counting\nUnlike unique pointers, shared pointers are roughly twice the size of a raw pointer, since they need to store additional information for reference\nstd::shared_ptr<>,counting. For a reference counting is done through the non-intrusive control block approach. If you recall from\nearlier, this approach involves a dynamically allocated control block object that keeps track of a shared resource’s reference count. Each shared\npointer that references the shared resource points to this control block.\nshared pointer\nshared pointer\nshared pointer\nshared pointer\ncontrol block\nreference count\nptr to resource\nshared resource", "word_count": 567, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "cffd8dde-a7ee-524d-9b11-19b26af05760", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1074, "real_page_number": null, "text": "1062\nChapter 27. Smart Pointers and Memory Management\nA control block needs to be created when the first shared pointer is created for a shared resource. The following rules are used to determine\nwhen a control block is created:\nstd::make_shared()1. A control block is created if is used to create a shared pointer. Since this method returns a shared pointer to a\nnewly constructed object in dynamic memory, the returned shared pointer is guaranteed to be the first one referencing this object.\n// creates a new control block\nstd::shared_ptr<int32_t> std::make_shared<int32_t>(281);ptr =\nstd::shared_ptr<> std::unique_ptr<>.2. A control block is created if a is constructed from a An object managed by a unique\npointer does not have a control block (since it can only be managed by one owner), so transferring the object to a shared pointer means\nthat a control block must be created.\n// creates a new control block\nstd::unique_ptr<int32_t> std::make_unique<int32_t>(281);u_ptr =\nstd::shared_ptr<int32_t> s_ptr = std::move(u_ptr);\nstd::shared_ptr<>’s3. A control block is created if a constructor is called with a raw pointer. If you construct a shared pointer\ndirectly with a raw pointer, it assumes that it is the first smart pointer that references that raw pointer. This is why you cannot initialize\nOtherwise, you would get two control blocks (and thus two reference counts) for thetwo shared pointers with the same raw pointer!\nsame object; this leads to a double delete since both reference counts would eventually reach zero. Instead, if a shared resource is already\nmanaged by other shared pointers, you should create a new shared pointer via a method that does not create a new control block.\n// creates a new control block\nstd::shared_ptr<int32_t> ptr{new int32_t{281}};\nIf a shared pointer is copied or assigned from another shared pointer, then a control block is created. This is because the old shared pointernot\nbeing copied or assigned from must already have a control block of its own. As a result, a shared pointer created via a copy or assignment\nsimply increments the reference count of this existing control block.\nSimilarly, when a shared pointer is destroyed or reassigned, it decrements the reference count in its control block. When a shared pointer\ndecrements its reference count to zero, it deletes the managed object as well as its corresponding control block.\n¸ 27.5.4\n(✽)std::enable_shared_from_this\nthis.One particularly interesting edge case involves shared pointers that return Using the rules for control blocks, we know that the following\ncode causes undefined behavior, since two reference counts would be created for the same object:\n1\nint32_t* new int32_t{281};ptr =\n2\nstd::shared_ptr<int32_t> s_ptr1{ptr};\n3\nstd::shared_ptr<int32_t> s_ptr2{ptr};\nstd::shared_ptr<>However, there may be cases where you will need to return a that owns the object it is called on. Upon first glance, it\nmay seem appropriate to implement this behavior as follows:\n1\nclass Thing {\n2\npublic:\n3\nstd::shared_ptr<Thing> get_pointer_to_self() {\n4\nreturn std::shared_ptr<Thing>(this);\n// \"this\" is a pointer to the current Thing instance\n5\n} // get_pointer_to_self()\n6\n};\nThingUnfortunately, this code is incorrect and may cause undefined behavior. To see why, consider the following code that uses a object:\n1\nvoid foo() {\n2\ns_ptr1{newstd::shared_ptr<Thing> Thing{}};\n3\nstd::shared_ptr<Thing> s_ptr2 = s_ptr1->get_pointer_to_self();\n4\n} // foo()\nshrd_ptr1 ThingWhen is constructed, it is the first one to own the newly allocated instance, so it creates a control block. However, when\nshrd_ptr2 is constructed, it also creates a control block for the exact same object, as it is unaware that another shared pointer is already\nThing. Thingmanaging the current This leads to undefined behavior via a double delete, since the would have its memory deleted twice!\nstd::enable_shared_from_this<>,To account for this edge case, you can inherit the class template which takes in the type of\nstd::enable_shared_from_this<>the object that needs to return a pointer to itself. The class implements a special member function\nshared_from_this(),called which allows an object to return a shared pointer to itself without duplicating control blocks. The previously\nshared_from_this()code is correctly implemented using below:\n1\nclass publicThing : std::enable_shared_from_this<Thing> {\n2\npublic:\n3\nstd::shared_ptr<Thing> get_pointer_to_self() {\n4\nreturn shared_from_this();\n// returns a shared_ptr to self without duplicating control blocks\n5\n} // get_pointer_to_self()\n6\n};\n7\n8\nvoid foo() {\n9\ns_ptr1{newstd::shared_ptr<Thing> Thing{}};\n10\nstd::shared_ptr<Thing> s_ptr2 = s_ptr1->get_pointer_to_self();\n// this is safe now\n11\n} // foo()", "word_count": 728, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "8ddee896-ac80-57e1-9164-a28537a9e5a9", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1075, "real_page_number": null, "text": "27.6 Weak Pointers\n1063\nshared_from_this() this.It is important to note that is only safe to use if there exists another shared pointer that already points to If\nshared_from_this()no shared pointer is currently managing the current object, invoking would result in undefined behavior prior to\nstd::bad_weak_ptrC++17, and a exception since C++17. The following code would not work:\n1\nclass publicThing : std::enable_shared_from_this<Thing> {\n2\npublic:\n3\nstd::shared_ptr<Thing> get_pointer_to_self() {\n4\nreturn shared_from_this();\n5\n} // get_pointer_to_self()\n6\n};\n7\n8\nvoid foo() {\n9\nnewThing* raw_thing = Thing{};\n10\nstd::shared_ptr<Thing> s_ptr = raw_thing->get_pointer_to_self();\n// undef behavior or exception\n11\n} // foo()\nstd::enable_shared_from_this<> privateTo prevent this from happening, objects that inherit often hide their constructors as\nmembers to prevent a user from creating an instance of the class that is not managed by a shared pointer. Instead, separate factory functions are\nprovided to the user that always return a shared pointer to the object.\n1\nclass publicThing : std::enable_shared_from_this<Thing> {\n2\npublic:\n3\n// separate factory function that returns a shared pointer to a new Thing -\n4\n// a user must use this to construct a Thing to prevent get_pointer_to_self()\n5\n// from being incorrectly called on an instance that is not already owned by a shared_ptr\n6\nstatic std::shared_ptr<Thing> create();\n7\n8\nstd::shared_ptr<Thing> get_pointer_to_self() {\n9\nreturn shared_from_this();\n10\n} // get_pointer_to_self()\n11\n12\nprivate:\n13\n// Thing constructors are declared here\n14\n};\n15\n16\nvoid foo() {\n17\nstd::shared_ptr<Thing> s_ptr1 = Thing::create();\n18\nstd::shared_ptr<Thing> s_ptr2 = s_ptr1->get_pointer_to_self();\n19\n} // foo()\n27.6\nWeak Pointers (✽)\n¸ 27.6.1\n(✽)The Cycle Problem\nThere is still one major issue involving shared pointers that we have yet to address, known as the problem. To demonstrate this issue,cycle\nStudent Student Studentconsider the following example involving objects. Here, each can be paired up with another for a homework\nassignment, where a student’s partner is stored internally using a shared pointer.\n1\nclass Student {\n2\nprivate:\n3\nstd::string name;\n4\nstd::shared_ptr<Student> partner;\n5\npublic:\n6\nStudent(const std::string& name_in) : name{name_in} {}\n7\n8\n// sets the partner of two pairs of students\n9\n// NOTE: this is a friend function, so the function is not a class method but can\n10\n// access private member variables of the Student class\n11\nfriend bool set_partners(const conststd::shared_ptr<Student>& s1, std::shared_ptr<Student>& s2) {\n12\nif (!s1 || !s2 || s1 == s2) {\n13\nreturn false;\n14\n} // if\n15\ns1->partner = s2;\n16\ns2->partner = s1;\n17\nreturn true;\n18\n} // set_partners()\n19\n};\nConsider the following code, which creates two students and partners them together. Since these students are managed by shared pointers, it\nfoo()may seem reasonable to assume that both students will be cleaned up after terminates.\n1\nvoid foo() {\n2\nstd::shared_ptr<Student> s1 = std::make_shared<Student>(\"Alice\");\n3\nstd::shared_ptr<Student> s2 = std::make_shared<Student>(\"Bob\");\n4\nset_partners(s1, s2);\n5\n} // foo()", "word_count": 488, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "71262737-1fe2-5717-a4e3-834127c7cb3f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1076, "real_page_number": null, "text": "1064\nChapter 27. Smart Pointers and Memory Management\nHowever, this is not the case! Instead, both students end up getting leaked. To understand why this happens, consider what actually happens\ns1 \"Alice\".behind the hood with these shared pointers. When is created, a shared pointer and control block are created for the student\nstd::shared_ptr<Student> s1 = std::make_shared<Student>(\"Alice\");\ns1\ncontrol block\nref. count = 1\nptr to resource\n\"Alice\"\nnullptr\nname\npartner\ns2 \"Bob\".Similarly, when is created, a shared pointer and control block are created for the student\nstd::shared_ptr<Student> s2 = std::make_shared<Student>(\"Bob\");\ns1\ncontrol block\nref. count = 1\nptr to resource\n\"Alice\"\nnullptr\nname\npartner\ns2\ncontrol block\nref. count = 1\nptr to resource\n\"Bob\"\nnullptr\nname\npartner\nset_partners(). s1’s s2, s2’sThe interesting behavior happens when we call During this function call, partner is set to and partner is set\ns1.to This increments the reference counts of both students by one, as shown:\ns1->partner = s2;\ns2->partner = s1;\ns1\ncontrol block\n2ref. count =\nptr to resource\n\"Alice\"\nname\npartner\ns2\ncontrol block\n2ref. count =\nptr to resource\n\"Bob\"\nname\npartner", "word_count": 189, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "bde47892-06c4-587f-8b90-815795313ab3", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1077, "real_page_number": null, "text": "27.6 Weak Pointers\n1065\nfoo() s1 s2Next, the function runs to completion, and the and shared pointers go out of scope. Since destructors are run in LIFO order at\ns2 s2’s \"Bob\"the end of a scope, goes out of scope first. destructor checks if anything else is still referencing (i.e., is the reference count\npartner \"Alice\". \"Bob\"zero?). In this case, there is: the member of Because of this, the memory for is not immediately deallocated.\ns2 goes out of scope\ns1\ncontrol block\nref. count = 2\nptr to resource\n\"Alice\"\nname\npartner\ncontrol block\n1ref. count =\nptr to resource\n\"Bob\"\nname\npartner\ns1 s1’s \"Alice\". StudentNext, goes out of scope. destructor checks if anything else is still referencing Since the object associated with\n\"Bob\" \"Alice\"has not yet been deallocated, there is, so the memory for is not immediately deallocated either.\ns1 goes out of scope\ncontrol block\n1ref. count =\nptr to resource\n\"Alice\"\nname\npartner\ncontrol block\nref. count = 1\nptr to resource\n\"Bob\"\nname\npartner\ns1 s2We have a problem! Even though and are both out of scope, the memory allocated for their students have not been deallocated. Since\nthis memory is no longer accessible, we now have a memory leak.\nThis is the essence of the cycle problem: if you have multiple shared pointers that point to each other in the form of a circular reference\n(e.g., 𝐴→𝐵→𝐶→𝐴), then those objects will not get properly cleaned up since they would keep each other alive.\ncontainer of shared pointers\nshared pointer\nshared pointer\nshared pointer\ncontainer goes out of scope, but the shared pointers keep each other alive (since the reference count of each object never drops to zero)\ncontainer of shared pointers\nshared pointer\nshared pointer\nshared pointer", "word_count": 302, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "22c22943-9557-51de-9490-9e49ff628f22", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1078, "real_page_number": null, "text": "1066\nChapter 27. Smart Pointers and Memory Management\nTo fix this issue, we will need a new type of smart pointer that can be used to a shared object affecting its reference count, soobserve without\nthat it does not prevent the object from being properly destructed after it is no longer used. This special type of smart pointer is known as a\nweak pointer. By using weak pointers to point to objects in a cycle instead of normal shared pointers, we can ensure that those objects can be\nproperly deallocated once they are no longer being used.\ncontainer of shared pointers\nweak pointer\nweak pointer\nweak pointer\ncontainer goes out of scope, so its memory gets properly cleaned up (weak pointers do not add to reference count, so all ref counts are zero)\ncontainer of shared pointers\nweak pointer\nweak pointer\nweak pointer\n¸ 27.6.2\n(✽)Implementing a Weak Pointer\nA weak pointer can be used to observe a shared object without affecting its reference count, which thereby prevents it from keeping the shared\nobject alive when it should not be. How do we implement the functionalities of a weak pointer? It turns out that the non-intrusive control block\nimplementation we discussed previously can be easily updated to support this new pointer type. This is done by adding a that isweak count\nmaintained by weak pointers, in addition to the reference count that is maintained by shared pointers.\nweak pointer\nshared pointer\nshared pointer\nweak pointer\ncontrol block\nreference count: 2\nweak count: 2\nptr to resource\nshared resource\nWhen a weak pointer is created, it points to the control block of the object it observes, and the weak count of that control block is incremented\nby one. Likewise, when a weak pointer is destroyed or reassigned, the weak count of its associated control block is decremented by one.\nWhy do we need a separate weak count in our control block? Remember that weak pointers cannot modify the reference count since they\nare mere observers (and not users) of the shared object they manage, so the presence of a weak pointer has no influence on when the shared\n≠somethingobject can be safely deleted (i.e., an object having a weak pointer still needs that object to stay around in memory). However, this\nmeans that it is entirely possible for a weak pointer to still observe an object even after its reference count reaches zero — we do not want our\nweak pointer to dangle, so we cannot erase the existence of the object entirely. This is done by keeping the control block around if the weak\ncount is not zero! That is, a shared resource is deallocated once its reference count is zero, but its corresponding control block is deallocated\nonly after its reference count and weak count are zero.both", "word_count": 473, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "196c328b-ac76-50a3-8aac-38c7c4a82537", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1079, "real_page_number": null, "text": "27.6 Weak Pointers\n1067\nTo illustrate this process, consider the following example, where a shared resource is pointed to by two shared pointers and two weak pointers.\nSince weak pointers do not indicate ownership and thus do not affect the reference count, the reference count is two for the number of shared\npointers. The number of weak pointers is stored separately as a weak count, which also has a value of two.\nweak pointer\nshared pointer\nshared pointer\nweak pointer\ncontrol block\nreference count: 2\nweak count: 2\nptr to resource\nshared resource\nNow, suppose both shared pointers go out of scope, which drops the reference count down to zero. Without any weak pointers, both the shared\nresource and control block would get deallocated. In this situation, however, there are still two weak pointers that point to the shared resource.\nWe still want to delete the shared resource (since a reference count of zero indicates that nothing needs to use it anymore), but we cannot leave\nthe weak pointers dangling. To address this, the shared resource gets deleted but the control block stays around.\nweak pointer\nshared pointer\nshared pointer\nweak pointer\ncontrol block\n0reference count:\nweak count: 2\nø\nshared resource\nOnly after both weak pointers go out of scope does the weak count drop back to zero. When this happens and the reference count is zero,also\nthe last weak pointer to be destroyed also cleans up the memory used by the control block.\nweak pointer\nweak pointer\ncontrol block\nreference count: 0\n0weak count:\nø\nItshouldbenotedthat,unlikeuniqueandsharedpointers,weakpointersintheC++standardlibraryaresignificantlymorelimitedinfunctionality\nand cannot be used syntatically like a normal raw pointer. For example, you cannot dereference a weak pointer to access its underlying value,\nand a weak pointer must be initialized from a existing shared pointer. The only thing you can really do with a weak pointer is query whether the\nobject it is observing still exists (which is done by looking at the reference count of the control block, which is guaranteed to stay alive as long\nas there are still weak pointers referencing it). That being said, a weak pointer is still more useful than a normal raw pointer because of this\nability to check if its underlying object is still alive — and, in the case where the object is still alive, you can easily use the weak pointer to\nconstruct a new shared pointer that can be dereferenced to access the object’s value.", "word_count": 429, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "02a7f21d-7367-54b6-91aa-7277d881820f", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1080, "real_page_number": null, "text": "1068\nChapter 27. Smart Pointers and Memory Management\n¸ 27.6.3\n(✽)std::weak_ptr\n<memory> std::weak_ptr<>,The C++ library provides the which is a class that implements the functionalities of a weak pointer that we\ndiscussed earlier. Because weak pointers are designed to observe an existing shared resource, they can only be constructed from an existing\nshared pointer, as shown.\n1\nstd::shared_ptr<int32_t> std::make_shared<int32_t>(281);s_ptr =\n2\n3\n// creates a weak_ptr that observes the memory of s_ptr without upping its reference count\n4\nstd::weak_ptr<int32_t> w_ptr = s_ptr;\nstd::weak_ptr<>A cannot be dereferenced directly — on its own, a weak pointer can only tell you whether the object it is pointing to is\nexpired()alive, and not what the object’s value is. To check if a weak pointer’s object has been deleted or not, you can use the member.\ntemplate <typename T>\nbool std::weak_ptr<T>::expired();\ntrue falseReturns if the object managed by the weak pointer has already been deleted, and otherwise.\n1\nstd::weak_ptr<int32_t> w_ptr;\n2\n3\n{ // begin a new scope\n4\nstd::shared_ptr<int32_t> std::make_shared<int32_t>(281);s_ptr =\n5\nw_ptr = s_ptr;\n6\nstd::cout << w_ptr.expired() << '\\n';\n// prints 0 since s_ptr's object is still alive\n7\n} // ptr goes out of scope\n8\n9\nstd::cout << w_ptr.expired() << '\\n';\n// prints 1 since s_ptr's object has been deleted\nlock() std::shared_ptr<>If a weak pointer is not expired, you can use the member function to return a new to the managed object.\nThis allows you to dereference and access the shared object’s value.\ntemplate <typename T>\nconst noexcept;std::shared_ptr<T> std::weak_ptr<T>::lock()\nstd::shared_ptr<>Creates a new that references the shared object that the weak pointer is observing. If the weak pointer is expired,\nthen the returned shared pointer is empty.\n1\nstd::shared_ptr<int32_t> std::make_shared<int32_t>(281);s_ptr =\n2\nstd::weak_ptr<int32_t> w_ptr = s_ptr;\n3\nif (auto access_ptr = w_ptr.lock()) {\n4\nstd::cout << *access_ptr << '\\n';\n// prints 281\n5\n} // if\nStudentUsing this information, let us return to our initial example. Instead of storing each student’s partner as a shared pointer (which lead\nto a circular reference that prevented the students from being properly deleted), we will store it as a pointer:weak\n1\nclass Student {\n2\nprivate:\n3\nstd::string name;\n4\nstd::weak_ptr<Student> partner;\n// WEAK pointer and not shared!\n5\npublic:\n6\nStudent(const std::string& name_in) : name{name_in} {}\n7\n8\n// sets the partner of two pairs of students\n9\n// NOTE: this is a friend function, so the function is not a class method but can\n10\n// access private member variables of the Student class\n11\nfriend bool set_partners(const conststd::shared_ptr<Student>& s1, std::shared_ptr<Student>& s2) {\n12\nif (!s1 || !s2 || s1 == s2) {\n13\nreturn false;\n14\n} // if\n15\ns1->partner = s2;\n16\ns2->partner = s1;\n17\nreturn true;\n18\n} // set_partners()\n19\n};\nNow, we will run the same code that previously caused a memory leak:\n1\nvoid foo() {\n2\nstd::shared_ptr<Student> s1 = std::make_shared<Student>(\"Alice\");\n3\nstd::shared_ptr<Student> s2 = std::make_shared<Student>(\"Bob\");\n4\nset_partners(s1, s2);\n5\n} // foo()", "word_count": 503, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "3f93e16b-ce35-5d1e-9b00-e2bccf776058", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1081, "real_page_number": null, "text": "27.6 Weak Pointers\n1069\nset_partners()What happens now? When we assign the partners in the method, the new partners do affect each student’s referencenot\ncount since they are stored as weak pointers. Both students would still have a reference count of one after their partners are set!\ns1->partner = s2;\ns2->partner = s1;\ns1\ncontrol block\nref. count = 1\nweak count = 1\nptr to resource\n\"Alice\"\nname\npartner\ns2\ncontrol block\nref. count = 1\nweak count = 1\nptr to resource\n\"Bob\"\nname\npartner\ns2 \"Bob\" \"Bob\"When goes out of scope, the reference count of drops to zero, so the memory allocated to the student is deleted. However,\n\"Alice\" \"Bob\" \"Bob\"since the weak count is not yet zero (as is still referencing via its weak pointer), the control block of is not deleted.\n\"Bob\" \"Alice\".Note that the weak pointer of also gets destructed here, which decrements the weak count of\ns2 goes out of scope\ns1\ncontrol block\nref. count = 1\n0weak count =\nptr to resource\n\"Alice\"\nname\npartner\ns2\ncontrol block\n0ref. count =\nweak count = 1\nø\n\"Bob\"\nname\npartner\ns1 \"Alice\"Next, goes out of scope, so the reference count of is decremented to zero. Since both the reference count weak count ofand\n\"Alice\" \"Alice\"are now zero, the memory allocated to the student and its control block are both cleaned up.\ns1 goes out of scope\ns1\ncontrol block\n0ref. count =\nweak count = 0\nø\n\"Alice\"\nname\npartner\ncontrol block\nref. count = 0\n0weak count =\nø", "word_count": 269, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "188f76a1-58f9-517f-bce9-d68bfa8b5139", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1082, "real_page_number": null, "text": "1070\nChapter 27. Smart Pointers and Memory Management\n\"Alice\", \"Bob\"’sWhile deleting its weak pointer is also destructed, which decrements the weak count of (what was once) control block to\nzero. Since both the reference count and weak count of this control block are now zero, it is deleted as well.\ns1\ncontrol block\nref. count = 0\nweak count = 0\nø\n\"Alice\"\nname\npartner\ncontrol block\nref. count = 0\nweak count = 0\nø\nBoth students (and their control blocks) have been successfully cleaned up, so no memory is leaked.\n27.7\nWhen Should Smart Pointers Be Used? (✽)\nTo summarize, the introduction of smart pointers in C++11 made it easier to manage the lifetime of dynamic memory, ensuring that an object\ncreated on the heap also gets properly cleaned up once it is out of use. There are three main categories of smart pointers provided by the C++\n<memory> library: unique, shared, and weak pointers.\nA unique pointer is the simplest type of smart pointer, designed to maintain ownership of an object in dynamic memory. Theseexclusive\npointers automatically delete their managed object after it is no longer used, and they also have very little overhead compared to other types of\nsmart pointers. As a result, unique pointers are an ideal choice for managing dynamic memory in most situations.\nUnique pointers should not be used, however, if a dynamically allocated resource requires shared ownership. In that case, you can use\na shared pointer instead, which uses reference counting to ensure that an object is only deleted after of its owners no longer need it. Forall\nscenarios where shared resources may form a cycle, you can use a weak pointer in place of a shared pointer, as weak pointers observe a shared\nobject without increasing its reference count (thereby allowing objects in a cycle to be properly cleaned up).\nWith smart pointers at hand, is there ever a case where a normal raw pointer should be used over a smart pointer? This answer is a bit more\nnuanced, since it depends on how the pointer is intended to be used. The core function of smart pointers revolves around the concept of dynamic\nmemory ownership. An entity in charge of owning or managing the lifetime of a dynamically-allocated object should avoid raw pointers in\nalmost all situations — if you are allocating a brand new object on the heap, you should try your best to store it in a smart pointer, asalways\nthey can prevent many types of memory issues and save you a lot of trouble later on.\nHowever, if an entity needs to use the contents of a dynamically allocated object that is already owned and managed by something else, and\nyou can ensure that the object will always be alive when you use it, then it would be okay to pass a raw pointer to that object (provided that you\nServicenever delete it outside the scope in which it is owned). To illustrate, consider the following class that is responsible for starting up\nServicea service. Each instance of a maintains a logger object that can be used log messages from the service to a log file; the logger is\nallocated dynamically and is stored in a unique pointer:\n1\nclass Service {\n2\nprivate:\n3\nstd::unique_ptr<Logger> logger;\n4\npublic:\n5\n// member functions\n6\n};\nService DataRetrieverNow, suppose each also maintains a separate class that is used to retrieve data that is needed for the service to\nDataRetriever Servicerun. The lifetime of a object is restricted within the scope of the instance it resides in, so it is also stored as a\nServicemember variable of the class, as shown:\n1\nclass Service {\n2\nprivate:\n3\nstd::unique_ptr<Logger> logger;\n4\nstd::unique_ptr<DataRetriever> data_retriever;\n5\npublic:\n6\n// member functions\n7\n};\nDataRetrieverAdditionally, suppose the wants to log messages to the log file as well, and thus also needs a copy of the logger object that\nis stored in its encompassing service. How should this be done?\n1\nclass DataRetriever {\n2\nprivate:\n3\n// needs to store the logger here!\n4\npublic:\n5\n// member functions\n6\n};", "word_count": 698, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
{"id": "276828b1-3e03-5e2e-9a47-f11983d09fac", "section_ids": [], "book_id": "44e34b2f-c500-570e-8367-3beb7ada4db1", "pdf_page_number": 1083, "real_page_number": null, "text": "27.7 When Should Smart Pointers Be Used?\n1071\nService DataRetrieverOne potential strategy is to store the logger as a shared pointer within the class and the class. When theboth\nservice is first created, the shared pointer is created normally. Then, when the data retriever is constructed within the service, the shared pointer\nof the service is passed into the data retriever’s constructor and used to initialize its own logger.\n1\nclass Service {\n2\nprivate:\n3\nstd::shared_ptr<Logger> logger;\n4\nstd::unique_ptr<DataRetriever> data_retriever;\n5\npublic:\n6\nService(const std::shared_ptr<Logger>& logger_in) : logger{logger_in} {\n7\n// the data retriever initializes its own logger from a shared pointer\n8\ndata_retriever = std::make_unique<DataRetriever>(logger);\n9\n} // Service()\n10\n};\n11\n12\nclass DataRetriever {\n13\nprivate:\n14\nstd::shared_ptr<Logger> logger;\n15\npublic:\n16\nDataRetriever(const std::shared_ptr<Logger>& logger_in) : logger{logger_in} {}\n17\n};\n18\n19\nint main() {\n20\n// construct logger and pass into Service constructor\n21\nstd::shared_ptr<Logger> service_logger = std::make_shared<Logger>();\n22\nService new_service{service_logger};\n23\n24\n/* ... additional code ... */\n25\n} // main()\nHowever, this is not necessary. Recall from earlier that we define the of a dynamically allocated object as the entity responsible forowner\nServicecleaning up that object after it is done using it. In this case, the logger really only has one owner: the class that it belongs to. The\nDataRetriever is just a component of the service that uses the logger, but it does not need to manage the lifetime of the logger itself.\nBecause the data retriever does not claim ownership on the logger object, and we know that the logger will stay alive during the entire\nlifespan of the data retriever (since they are both managed under the exact same service), it is better to store the logger as a pointer withinraw\nDataRetrieverthe class. Not only does this convey clearer intent on who owns the logger, it also avoids the additional overhead of a shared\npointer when storing an object that can be managed under exclusive ownership.\n1\nclass Service {\n2\nprivate:\n3\nstd::unique_ptr<Logger> logger;\n4\nstd::unique_ptr<DataRetriever> data_retriever;\n5\npublic:\n6\nService(std::unique_ptr<Logger> logger_in) : logger{std::move(logger_in)} {\n7\n// pass in a raw pointer to construct the data retriever (which can be obtained using .get())\n8\ndata_retriever = std::make_unique<DataRetriever>(logger.get());\n9\n} // Service()\n10\n};\n11\n12\nclass DataRetriever {\n13\nprivate:\n14\nLogger* logger;\n15\npublic:\n16\nDataRetriever(Logger* logger_in) : logger{logger_in} {}\n17\n};\n18\n19\nint main() {\n20\n// construct logger and pass into Service constructor\n21\nstd::unique_ptr<Logger> service_logger = std::make_unique<Logger>();\n22\nService new_service{std::move(service_logger)};\n23\n24\n/* ... additional code ... */\n25\n} // main()\nIn conclusion, while there are cases where using a raw pointer may be permissible (such as the one above), it is generally good advice to use\nsmart pointers whenever you are dealing with the of a dynamically allocated object. When allocating dynamic memory, you shouldownership\nnew delete.avoid creating an object that is managed explicitly by and This is because there is little downside and enormous upside in using\nsmart pointers to automate memory management — the implementations provided by the C++ standard library are not only efficient and simple\nto use, but a continual reliance on smart pointers can also make your programs less susceptible to several kinds of memory-related bugs.", "word_count": 548, "has_chapter": false, "has_section": false, "has_question": false, "has_answer": false, "text_embedding": null}
